CNNModel CHEMBL2635 adam 0.001 30 32 0 0.8 False True
Number of active compounds :	166
Number of inactive compounds :	166
---------------------------------
Run id: CNNModel_CHEMBL2635_adam_0.001_30_32_0_0.8_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL2635_adam_0.001_30_32_0.8_True/
---------------------------------
Training samples: 206
Validation samples: 65
--
Training Step: 1  | time: 0.758s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/206
[A[ATraining Step: 2  | total loss: [1m[32m0.62363[0m[0m | time: 1.360s
[2K
| Adam | epoch: 001 | loss: 0.62363 - acc: 0.5625 -- iter: 064/206
[A[ATraining Step: 3  | total loss: [1m[32m0.68392[0m[0m | time: 1.958s
[2K
| Adam | epoch: 001 | loss: 0.68392 - acc: 0.4602 -- iter: 096/206
[A[ATraining Step: 4  | total loss: [1m[32m0.69094[0m[0m | time: 2.590s
[2K
| Adam | epoch: 001 | loss: 0.69094 - acc: 0.4901 -- iter: 128/206
[A[ATraining Step: 5  | total loss: [1m[32m0.69094[0m[0m | time: 3.235s
[2K
| Adam | epoch: 001 | loss: 0.69094 - acc: 0.5835 -- iter: 160/206
[A[ATraining Step: 6  | total loss: [1m[32m0.69364[0m[0m | time: 3.854s
[2K
| Adam | epoch: 001 | loss: 0.69364 - acc: 0.4896 -- iter: 192/206
[A[ATraining Step: 7  | total loss: [1m[32m0.69562[0m[0m | time: 5.178s
[2K
| Adam | epoch: 001 | loss: 0.69562 - acc: 0.4209 | val_loss: 0.69446 - val_acc: 0.4154 -- iter: 206/206
--
Training Step: 8  | total loss: [1m[32m0.69358[0m[0m | time: 0.298s
[2K
| Adam | epoch: 002 | loss: 0.69358 - acc: 0.5056 -- iter: 032/206
[A[ATraining Step: 9  | total loss: [1m[32m0.69298[0m[0m | time: 0.900s
[2K
| Adam | epoch: 002 | loss: 0.69298 - acc: 0.5404 -- iter: 064/206
[A[ATraining Step: 10  | total loss: [1m[32m0.69222[0m[0m | time: 1.517s
[2K
| Adam | epoch: 002 | loss: 0.69222 - acc: 0.5671 -- iter: 096/206
[A[ATraining Step: 11  | total loss: [1m[32m0.69237[0m[0m | time: 2.126s
[2K
| Adam | epoch: 002 | loss: 0.69237 - acc: 0.5501 -- iter: 128/206
[A[ATraining Step: 12  | total loss: [1m[32m0.69349[0m[0m | time: 2.725s
[2K
| Adam | epoch: 002 | loss: 0.69349 - acc: 0.4994 -- iter: 160/206
[A[ATraining Step: 13  | total loss: [1m[32m0.69339[0m[0m | time: 3.349s
[2K
| Adam | epoch: 002 | loss: 0.69339 - acc: 0.4997 -- iter: 192/206
[A[ATraining Step: 14  | total loss: [1m[32m0.69402[0m[0m | time: 4.969s
[2K
| Adam | epoch: 002 | loss: 0.69402 - acc: 0.4742 | val_loss: 0.69521 - val_acc: 0.4154 -- iter: 206/206
--
Training Step: 15  | total loss: [1m[32m0.69371[0m[0m | time: 0.297s
[2K
| Adam | epoch: 003 | loss: 0.69371 - acc: 0.4843 -- iter: 032/206
[A[ATraining Step: 16  | total loss: [1m[32m0.69288[0m[0m | time: 0.605s
[2K
| Adam | epoch: 003 | loss: 0.69288 - acc: 0.5170 -- iter: 064/206
[A[ATraining Step: 17  | total loss: [1m[32m0.69235[0m[0m | time: 1.218s
[2K
| Adam | epoch: 003 | loss: 0.69235 - acc: 0.5366 -- iter: 096/206
[A[ATraining Step: 18  | total loss: [1m[32m0.69235[0m[0m | time: 1.827s
[2K
| Adam | epoch: 003 | loss: 0.69235 - acc: 0.5347 -- iter: 128/206
[A[ATraining Step: 19  | total loss: [1m[32m0.69201[0m[0m | time: 2.429s
[2K
| Adam | epoch: 003 | loss: 0.69201 - acc: 0.5440 -- iter: 160/206
[A[ATraining Step: 20  | total loss: [1m[32m0.69312[0m[0m | time: 3.034s
[2K
| Adam | epoch: 003 | loss: 0.69312 - acc: 0.5098 -- iter: 192/206
[A[ATraining Step: 21  | total loss: [1m[32m0.69282[0m[0m | time: 4.636s
[2K
| Adam | epoch: 003 | loss: 0.69282 - acc: 0.5164 | val_loss: 0.69672 - val_acc: 0.4154 -- iter: 206/206
--
Training Step: 22  | total loss: [1m[32m0.69258[0m[0m | time: 0.622s
[2K
| Adam | epoch: 004 | loss: 0.69258 - acc: 0.5209 -- iter: 032/206
[A[ATraining Step: 23  | total loss: [1m[32m0.69374[0m[0m | time: 0.911s
[2K
| Adam | epoch: 004 | loss: 0.69374 - acc: 0.4876 -- iter: 064/206
[A[ATraining Step: 24  | total loss: [1m[32m0.69286[0m[0m | time: 1.194s
[2K
| Adam | epoch: 004 | loss: 0.69286 - acc: 0.5112 -- iter: 096/206
[A[ATraining Step: 25  | total loss: [1m[32m0.69296[0m[0m | time: 1.795s
[2K
| Adam | epoch: 004 | loss: 0.69296 - acc: 0.5081 -- iter: 128/206
[A[ATraining Step: 26  | total loss: [1m[32m0.69200[0m[0m | time: 2.418s
[2K
| Adam | epoch: 004 | loss: 0.69200 - acc: 0.5308 -- iter: 160/206
[A[ATraining Step: 27  | total loss: [1m[32m0.69232[0m[0m | time: 3.021s
[2K
| Adam | epoch: 004 | loss: 0.69232 - acc: 0.5229 -- iter: 192/206
[A[ATraining Step: 28  | total loss: [1m[32m0.69300[0m[0m | time: 4.631s
[2K
| Adam | epoch: 004 | loss: 0.69300 - acc: 0.5093 | val_loss: 0.69849 - val_acc: 0.4154 -- iter: 206/206
--
Training Step: 29  | total loss: [1m[32m0.69237[0m[0m | time: 0.605s
[2K
| Adam | epoch: 005 | loss: 0.69237 - acc: 0.5223 -- iter: 032/206
[A[ATraining Step: 30  | total loss: [1m[32m0.69395[0m[0m | time: 1.203s
[2K
| Adam | epoch: 005 | loss: 0.69395 - acc: 0.4948 -- iter: 064/206
[A[ATraining Step: 31  | total loss: [1m[32m0.69377[0m[0m | time: 1.488s
[2K
| Adam | epoch: 005 | loss: 0.69377 - acc: 0.4960 -- iter: 096/206
[A[ATraining Step: 32  | total loss: [1m[32m0.69374[0m[0m | time: 1.766s
[2K
| Adam | epoch: 005 | loss: 0.69374 - acc: 0.4969 -- iter: 128/206
[A[ATraining Step: 33  | total loss: [1m[32m0.69354[0m[0m | time: 2.360s
[2K
| Adam | epoch: 005 | loss: 0.69354 - acc: 0.4976 -- iter: 160/206
[A[ATraining Step: 34  | total loss: [1m[32m0.69552[0m[0m | time: 2.958s
[2K
| Adam | epoch: 005 | loss: 0.69552 - acc: 0.4512 -- iter: 192/206
[A[ATraining Step: 35  | total loss: [1m[32m0.69458[0m[0m | time: 4.554s
[2K
| Adam | epoch: 005 | loss: 0.69458 - acc: 0.4745 | val_loss: 0.69577 - val_acc: 0.4154 -- iter: 206/206
--
Training Step: 36  | total loss: [1m[32m0.69312[0m[0m | time: 0.609s
[2K
| Adam | epoch: 006 | loss: 0.69312 - acc: 0.5181 -- iter: 032/206
[A[ATraining Step: 37  | total loss: [1m[32m0.69369[0m[0m | time: 1.214s
[2K
| Adam | epoch: 006 | loss: 0.69369 - acc: 0.4957 -- iter: 064/206
[A[ATraining Step: 38  | total loss: [1m[32m0.69326[0m[0m | time: 1.807s
[2K
| Adam | epoch: 006 | loss: 0.69326 - acc: 0.5088 -- iter: 096/206
[A[ATraining Step: 39  | total loss: [1m[32m0.69310[0m[0m | time: 2.089s
[2K
| Adam | epoch: 006 | loss: 0.69310 - acc: 0.5131 -- iter: 128/206
[A[ATraining Step: 40  | total loss: [1m[32m0.69352[0m[0m | time: 2.373s
[2K
| Adam | epoch: 006 | loss: 0.69352 - acc: 0.4972 -- iter: 160/206
[A[ATraining Step: 41  | total loss: [1m[32m0.69379[0m[0m | time: 2.996s
[2K
| Adam | epoch: 006 | loss: 0.69379 - acc: 0.4846 -- iter: 192/206
[A[ATraining Step: 42  | total loss: [1m[32m0.69368[0m[0m | time: 4.621s
[2K
| Adam | epoch: 006 | loss: 0.69368 - acc: 0.4874 | val_loss: 0.69484 - val_acc: 0.4154 -- iter: 206/206
--
Training Step: 43  | total loss: [1m[32m0.69356[0m[0m | time: 0.602s
[2K
| Adam | epoch: 007 | loss: 0.69356 - acc: 0.4896 -- iter: 032/206
[A[ATraining Step: 44  | total loss: [1m[32m0.69339[0m[0m | time: 1.201s
[2K
| Adam | epoch: 007 | loss: 0.69339 - acc: 0.4968 -- iter: 064/206
[A[ATraining Step: 45  | total loss: [1m[32m0.69339[0m[0m | time: 1.796s
[2K
| Adam | epoch: 007 | loss: 0.69339 - acc: 0.4974 -- iter: 096/206
[A[ATraining Step: 46  | total loss: [1m[32m0.69344[0m[0m | time: 2.409s
[2K
| Adam | epoch: 007 | loss: 0.69344 - acc: 0.4926 -- iter: 128/206
[A[ATraining Step: 47  | total loss: [1m[32m0.69324[0m[0m | time: 2.693s
[2K
| Adam | epoch: 007 | loss: 0.69324 - acc: 0.5040 -- iter: 160/206
[A[ATraining Step: 48  | total loss: [1m[32m0.69299[0m[0m | time: 2.977s
[2K
| Adam | epoch: 007 | loss: 0.69299 - acc: 0.5149 -- iter: 192/206
[A[ATraining Step: 49  | total loss: [1m[32m0.69286[0m[0m | time: 4.582s
[2K
| Adam | epoch: 007 | loss: 0.69286 - acc: 0.5238 | val_loss: 0.69467 - val_acc: 0.4154 -- iter: 206/206
--
Training Step: 50  | total loss: [1m[32m0.69294[0m[0m | time: 0.609s
[2K
| Adam | epoch: 008 | loss: 0.69294 - acc: 0.5201 -- iter: 032/206
[A[ATraining Step: 51  | total loss: [1m[32m0.69315[0m[0m | time: 1.210s
[2K
| Adam | epoch: 008 | loss: 0.69315 - acc: 0.5075 -- iter: 064/206
[A[ATraining Step: 52  | total loss: [1m[32m0.69325[0m[0m | time: 1.812s
[2K
| Adam | epoch: 008 | loss: 0.69325 - acc: 0.5017 -- iter: 096/206
[A[ATraining Step: 53  | total loss: [1m[32m0.69281[0m[0m | time: 2.437s
[2K
| Adam | epoch: 008 | loss: 0.69281 - acc: 0.5245 -- iter: 128/206
[A[ATraining Step: 54  | total loss: [1m[32m0.69294[0m[0m | time: 3.050s
[2K
| Adam | epoch: 008 | loss: 0.69294 - acc: 0.5164 -- iter: 160/206
[A[ATraining Step: 55  | total loss: [1m[32m0.69303[0m[0m | time: 3.332s
[2K
| Adam | epoch: 008 | loss: 0.69303 - acc: 0.5096 -- iter: 192/206
[A[ATraining Step: 56  | total loss: [1m[32m0.69225[0m[0m | time: 4.616s
[2K
| Adam | epoch: 008 | loss: 0.69225 - acc: 0.5484 | val_loss: 0.69592 - val_acc: 0.4154 -- iter: 206/206
--
Training Step: 57  | total loss: [1m[32m0.69140[0m[0m | time: 0.623s
[2K
| Adam | epoch: 009 | loss: 0.69140 - acc: 0.5813 -- iter: 032/206
[A[ATraining Step: 58  | total loss: [1m[32m0.69193[0m[0m | time: 1.220s
[2K
| Adam | epoch: 009 | loss: 0.69193 - acc: 0.5617 -- iter: 064/206
[A[ATraining Step: 59  | total loss: [1m[32m0.69195[0m[0m | time: 1.868s
[2K
| Adam | epoch: 009 | loss: 0.69195 - acc: 0.5576 -- iter: 096/206
[A[ATraining Step: 60  | total loss: [1m[32m0.69232[0m[0m | time: 2.482s
[2K
| Adam | epoch: 009 | loss: 0.69232 - acc: 0.5458 -- iter: 128/206
[A[ATraining Step: 61  | total loss: [1m[32m0.69181[0m[0m | time: 3.074s
[2K
| Adam | epoch: 009 | loss: 0.69181 - acc: 0.5521 -- iter: 160/206
[A[ATraining Step: 62  | total loss: [1m[32m0.69247[0m[0m | time: 3.672s
[2K
| Adam | epoch: 009 | loss: 0.69247 - acc: 0.5373 -- iter: 192/206
[A[ATraining Step: 63  | total loss: [1m[32m0.69302[0m[0m | time: 4.972s
[2K
| Adam | epoch: 009 | loss: 0.69302 - acc: 0.5247 | val_loss: 0.69851 - val_acc: 0.4154 -- iter: 206/206
--
Training Step: 64  | total loss: [1m[32m0.69210[0m[0m | time: 0.320s
[2K
| Adam | epoch: 010 | loss: 0.69210 - acc: 0.5395 -- iter: 032/206
[A[ATraining Step: 65  | total loss: [1m[32m0.69116[0m[0m | time: 0.931s
[2K
| Adam | epoch: 010 | loss: 0.69116 - acc: 0.5522 -- iter: 064/206
[A[ATraining Step: 66  | total loss: [1m[32m0.69169[0m[0m | time: 1.564s
[2K
| Adam | epoch: 010 | loss: 0.69169 - acc: 0.5421 -- iter: 096/206
[A[ATraining Step: 67  | total loss: [1m[32m0.69235[0m[0m | time: 2.167s
[2K
| Adam | epoch: 010 | loss: 0.69235 - acc: 0.5295 -- iter: 128/206
[A[ATraining Step: 68  | total loss: [1m[32m0.69274[0m[0m | time: 2.784s
[2K
| Adam | epoch: 010 | loss: 0.69274 - acc: 0.5223 -- iter: 160/206
[A[ATraining Step: 69  | total loss: [1m[32m0.69287[0m[0m | time: 3.380s
[2K
| Adam | epoch: 010 | loss: 0.69287 - acc: 0.5197 -- iter: 192/206
[A[ATraining Step: 70  | total loss: [1m[32m0.69192[0m[0m | time: 4.981s
[2K
| Adam | epoch: 010 | loss: 0.69192 - acc: 0.5319 | val_loss: 0.70030 - val_acc: 0.4154 -- iter: 206/206
--
Training Step: 71  | total loss: [1m[32m0.69240[0m[0m | time: 0.291s
[2K
| Adam | epoch: 011 | loss: 0.69240 - acc: 0.5247 -- iter: 032/206
[A[ATraining Step: 72  | total loss: [1m[32m0.69114[0m[0m | time: 0.575s
[2K
| Adam | epoch: 011 | loss: 0.69114 - acc: 0.5380 -- iter: 064/206
[A[ATraining Step: 73  | total loss: [1m[32m0.69004[0m[0m | time: 1.185s
[2K
| Adam | epoch: 011 | loss: 0.69004 - acc: 0.5496 -- iter: 096/206
[A[ATraining Step: 74  | total loss: [1m[32m0.69123[0m[0m | time: 1.792s
[2K
| Adam | epoch: 011 | loss: 0.69123 - acc: 0.5373 -- iter: 128/206
[A[ATraining Step: 75  | total loss: [1m[32m0.69160[0m[0m | time: 2.395s
[2K
| Adam | epoch: 011 | loss: 0.69160 - acc: 0.5333 -- iter: 160/206
[A[ATraining Step: 76  | total loss: [1m[32m0.69261[0m[0m | time: 2.990s
[2K
| Adam | epoch: 011 | loss: 0.69261 - acc: 0.5230 -- iter: 192/206
[A[ATraining Step: 77  | total loss: [1m[32m0.69298[0m[0m | time: 4.585s
[2K
| Adam | epoch: 011 | loss: 0.69298 - acc: 0.5173 | val_loss: 0.70189 - val_acc: 0.4154 -- iter: 206/206
--
Training Step: 78  | total loss: [1m[32m0.69272[0m[0m | time: 0.614s
[2K
| Adam | epoch: 012 | loss: 0.69272 - acc: 0.5187 -- iter: 032/206
[A[ATraining Step: 79  | total loss: [1m[32m0.69182[0m[0m | time: 0.896s
[2K
| Adam | epoch: 012 | loss: 0.69182 - acc: 0.5265 -- iter: 064/206
[A[ATraining Step: 80  | total loss: [1m[32m0.69191[0m[0m | time: 1.174s
[2K
| Adam | epoch: 012 | loss: 0.69191 - acc: 0.5238 -- iter: 096/206
[A[ATraining Step: 81  | total loss: [1m[32m0.69214[0m[0m | time: 1.774s
[2K
| Adam | epoch: 012 | loss: 0.69214 - acc: 0.5214 -- iter: 128/206
[A[ATraining Step: 82  | total loss: [1m[32m0.69266[0m[0m | time: 2.371s
[2K
| Adam | epoch: 012 | loss: 0.69266 - acc: 0.5130 -- iter: 160/206
[A[ATraining Step: 83  | total loss: [1m[32m0.69358[0m[0m | time: 2.975s
[2K
| Adam | epoch: 012 | loss: 0.69358 - acc: 0.5023 -- iter: 192/206
[A[ATraining Step: 84  | total loss: [1m[32m0.69301[0m[0m | time: 4.573s
[2K
| Adam | epoch: 012 | loss: 0.69301 - acc: 0.5083 | val_loss: 0.69951 - val_acc: 0.4154 -- iter: 206/206
--
Training Step: 85  | total loss: [1m[32m0.69196[0m[0m | time: 0.614s
[2K
| Adam | epoch: 013 | loss: 0.69196 - acc: 0.5200 -- iter: 032/206
[A[ATraining Step: 86  | total loss: [1m[32m0.69154[0m[0m | time: 1.216s
[2K
| Adam | epoch: 013 | loss: 0.69154 - acc: 0.5180 -- iter: 064/206
[A[ATraining Step: 87  | total loss: [1m[32m0.69112[0m[0m | time: 1.511s
[2K
| Adam | epoch: 013 | loss: 0.69112 - acc: 0.5162 -- iter: 096/206
[A[ATraining Step: 88  | total loss: [1m[32m0.68984[0m[0m | time: 1.799s
[2K
| Adam | epoch: 013 | loss: 0.68984 - acc: 0.5217 -- iter: 128/206
[A[ATraining Step: 89  | total loss: [1m[32m0.68782[0m[0m | time: 2.400s
[2K
| Adam | epoch: 013 | loss: 0.68782 - acc: 0.5267 -- iter: 160/206
[A[ATraining Step: 90  | total loss: [1m[32m0.68439[0m[0m | time: 2.999s
[2K
| Adam | epoch: 013 | loss: 0.68439 - acc: 0.5334 -- iter: 192/206
[A[ATraining Step: 91  | total loss: [1m[32m0.68595[0m[0m | time: 4.609s
[2K
| Adam | epoch: 013 | loss: 0.68595 - acc: 0.5301 | val_loss: 0.69221 - val_acc: 0.4154 -- iter: 206/206
--
Training Step: 92  | total loss: [1m[32m0.68574[0m[0m | time: 0.621s
[2K
| Adam | epoch: 014 | loss: 0.68574 - acc: 0.5239 -- iter: 032/206
[A[ATraining Step: 93  | total loss: [1m[32m0.68473[0m[0m | time: 1.239s
[2K
| Adam | epoch: 014 | loss: 0.68473 - acc: 0.5153 -- iter: 064/206
[A[ATraining Step: 94  | total loss: [1m[32m0.68431[0m[0m | time: 1.841s
[2K
| Adam | epoch: 014 | loss: 0.68431 - acc: 0.5200 -- iter: 096/206
[A[ATraining Step: 95  | total loss: [1m[32m0.68497[0m[0m | time: 2.119s
[2K
| Adam | epoch: 014 | loss: 0.68497 - acc: 0.5149 -- iter: 128/206
[A[ATraining Step: 96  | total loss: [1m[32m0.68422[0m[0m | time: 2.398s
[2K
| Adam | epoch: 014 | loss: 0.68422 - acc: 0.5277 -- iter: 160/206
[A[ATraining Step: 97  | total loss: [1m[32m0.68006[0m[0m | time: 2.992s
[2K
| Adam | epoch: 014 | loss: 0.68006 - acc: 0.5535 -- iter: 192/206
[A[ATraining Step: 98  | total loss: [1m[32m0.67661[0m[0m | time: 4.667s
[2K
| Adam | epoch: 014 | loss: 0.67661 - acc: 0.5606 | val_loss: 0.77293 - val_acc: 0.4154 -- iter: 206/206
--
Training Step: 99  | total loss: [1m[32m0.67670[0m[0m | time: 0.610s
[2K
| Adam | epoch: 015 | loss: 0.67670 - acc: 0.5577 -- iter: 032/206
[A[ATraining Step: 100  | total loss: [1m[32m0.67868[0m[0m | time: 1.223s
[2K
| Adam | epoch: 015 | loss: 0.67868 - acc: 0.5488 -- iter: 064/206
[A[ATraining Step: 101  | total loss: [1m[32m0.67482[0m[0m | time: 1.834s
[2K
| Adam | epoch: 015 | loss: 0.67482 - acc: 0.5502 -- iter: 096/206
[A[ATraining Step: 102  | total loss: [1m[32m0.67154[0m[0m | time: 2.440s
[2K
| Adam | epoch: 015 | loss: 0.67154 - acc: 0.5702 -- iter: 128/206
[A[ATraining Step: 103  | total loss: [1m[32m0.66675[0m[0m | time: 2.725s
[2K
| Adam | epoch: 015 | loss: 0.66675 - acc: 0.5881 -- iter: 160/206
[A[ATraining Step: 104  | total loss: [1m[32m0.66685[0m[0m | time: 3.088s
[2K
| Adam | epoch: 015 | loss: 0.66685 - acc: 0.5793 -- iter: 192/206
[A[ATraining Step: 105  | total loss: [1m[32m0.66327[0m[0m | time: 5.000s
[2K
| Adam | epoch: 015 | loss: 0.66327 - acc: 0.5928 | val_loss: 0.63574 - val_acc: 0.6769 -- iter: 206/206
--
Training Step: 106  | total loss: [1m[32m0.66349[0m[0m | time: 0.745s
[2K
| Adam | epoch: 016 | loss: 0.66349 - acc: 0.5898 -- iter: 032/206
[A[ATraining Step: 107  | total loss: [1m[32m0.65647[0m[0m | time: 1.524s
[2K
| Adam | epoch: 016 | loss: 0.65647 - acc: 0.5964 -- iter: 064/206
[A[ATraining Step: 108  | total loss: [1m[32m0.65428[0m[0m | time: 2.252s
[2K
| Adam | epoch: 016 | loss: 0.65428 - acc: 0.6118 -- iter: 096/206
[A[ATraining Step: 109  | total loss: [1m[32m0.63186[0m[0m | time: 3.014s
[2K
| Adam | epoch: 016 | loss: 0.63186 - acc: 0.6287 -- iter: 128/206
[A[ATraining Step: 110  | total loss: [1m[32m0.62202[0m[0m | time: 3.756s
[2K
| Adam | epoch: 016 | loss: 0.62202 - acc: 0.6346 -- iter: 160/206
[A[ATraining Step: 111  | total loss: [1m[32m0.62224[0m[0m | time: 4.037s
[2K
| Adam | epoch: 016 | loss: 0.62224 - acc: 0.6462 -- iter: 192/206
[A[ATraining Step: 112  | total loss: [1m[32m0.62087[0m[0m | time: 5.321s
[2K
| Adam | epoch: 016 | loss: 0.62087 - acc: 0.6458 | val_loss: 0.64054 - val_acc: 0.6769 -- iter: 206/206
--
Training Step: 113  | total loss: [1m[32m0.60574[0m[0m | time: 0.750s
[2K
| Adam | epoch: 017 | loss: 0.60574 - acc: 0.6670 -- iter: 032/206
[A[ATraining Step: 114  | total loss: [1m[32m0.59326[0m[0m | time: 1.476s
[2K
| Adam | epoch: 017 | loss: 0.59326 - acc: 0.6784 -- iter: 064/206
[A[ATraining Step: 115  | total loss: [1m[32m0.58268[0m[0m | time: 2.228s
[2K
| Adam | epoch: 017 | loss: 0.58268 - acc: 0.6824 -- iter: 096/206
[A[ATraining Step: 116  | total loss: [1m[32m0.56976[0m[0m | time: 2.955s
[2K
| Adam | epoch: 017 | loss: 0.56976 - acc: 0.6923 -- iter: 128/206
[A[ATraining Step: 117  | total loss: [1m[32m0.56830[0m[0m | time: 3.690s
[2K
| Adam | epoch: 017 | loss: 0.56830 - acc: 0.6949 -- iter: 160/206
[A[ATraining Step: 118  | total loss: [1m[32m0.55519[0m[0m | time: 4.414s
[2K
| Adam | epoch: 017 | loss: 0.55519 - acc: 0.7130 -- iter: 192/206
[A[ATraining Step: 119  | total loss: [1m[32m0.54378[0m[0m | time: 5.719s
[2K
| Adam | epoch: 017 | loss: 0.54378 - acc: 0.7167 | val_loss: 0.69454 - val_acc: 0.6615 -- iter: 206/206
--
Training Step: 120  | total loss: [1m[32m0.54260[0m[0m | time: 0.350s
[2K
| Adam | epoch: 018 | loss: 0.54260 - acc: 0.7236 -- iter: 032/206
[A[ATraining Step: 121  | total loss: [1m[32m0.51132[0m[0m | time: 1.048s
[2K
| Adam | epoch: 018 | loss: 0.51132 - acc: 0.7441 -- iter: 064/206
[A[ATraining Step: 122  | total loss: [1m[32m0.58114[0m[0m | time: 1.808s
[2K
| Adam | epoch: 018 | loss: 0.58114 - acc: 0.7072 -- iter: 096/206
[A[ATraining Step: 123  | total loss: [1m[32m0.58025[0m[0m | time: 2.535s
[2K
| Adam | epoch: 018 | loss: 0.58025 - acc: 0.7052 -- iter: 128/206
[A[ATraining Step: 124  | total loss: [1m[32m0.54682[0m[0m | time: 3.251s
[2K
| Adam | epoch: 018 | loss: 0.54682 - acc: 0.7284 -- iter: 160/206
[A[ATraining Step: 125  | total loss: [1m[32m0.53669[0m[0m | time: 3.973s
[2K
| Adam | epoch: 018 | loss: 0.53669 - acc: 0.7368 -- iter: 192/206
[A[ATraining Step: 126  | total loss: [1m[32m0.52224[0m[0m | time: 5.580s
[2K
| Adam | epoch: 018 | loss: 0.52224 - acc: 0.7381 | val_loss: 0.57411 - val_acc: 0.7077 -- iter: 206/206
--
Training Step: 127  | total loss: [1m[32m0.51667[0m[0m | time: 0.352s
[2K
| Adam | epoch: 019 | loss: 0.51667 - acc: 0.7425 -- iter: 032/206
[A[ATraining Step: 128  | total loss: [1m[32m0.48928[0m[0m | time: 0.655s
[2K
| Adam | epoch: 019 | loss: 0.48928 - acc: 0.7611 -- iter: 064/206
[A[ATraining Step: 129  | total loss: [1m[32m0.45870[0m[0m | time: 1.258s
[2K
| Adam | epoch: 019 | loss: 0.45870 - acc: 0.7850 -- iter: 096/206
[A[ATraining Step: 130  | total loss: [1m[32m0.45411[0m[0m | time: 1.860s
[2K
| Adam | epoch: 019 | loss: 0.45411 - acc: 0.7846 -- iter: 128/206
[A[ATraining Step: 131  | total loss: [1m[32m0.43266[0m[0m | time: 2.471s
[2K
| Adam | epoch: 019 | loss: 0.43266 - acc: 0.7999 -- iter: 160/206
[A[ATraining Step: 132  | total loss: [1m[32m0.42294[0m[0m | time: 3.082s
[2K
| Adam | epoch: 019 | loss: 0.42294 - acc: 0.8011 -- iter: 192/206
[A[ATraining Step: 133  | total loss: [1m[32m0.41418[0m[0m | time: 4.687s
[2K
| Adam | epoch: 019 | loss: 0.41418 - acc: 0.8054 | val_loss: 0.60251 - val_acc: 0.7385 -- iter: 206/206
--
Training Step: 134  | total loss: [1m[32m0.39146[0m[0m | time: 0.615s
[2K
| Adam | epoch: 020 | loss: 0.39146 - acc: 0.8186 -- iter: 032/206
[A[ATraining Step: 135  | total loss: [1m[32m0.38164[0m[0m | time: 0.902s
[2K
| Adam | epoch: 020 | loss: 0.38164 - acc: 0.8180 -- iter: 064/206
[A[ATraining Step: 136  | total loss: [1m[32m0.37309[0m[0m | time: 1.182s
[2K
| Adam | epoch: 020 | loss: 0.37309 - acc: 0.8148 -- iter: 096/206
[A[ATraining Step: 137  | total loss: [1m[32m0.35506[0m[0m | time: 1.781s
[2K
| Adam | epoch: 020 | loss: 0.35506 - acc: 0.8262 -- iter: 128/206
[A[ATraining Step: 138  | total loss: [1m[32m0.33891[0m[0m | time: 2.393s
[2K
| Adam | epoch: 020 | loss: 0.33891 - acc: 0.8310 -- iter: 160/206
[A[ATraining Step: 139  | total loss: [1m[32m0.31707[0m[0m | time: 3.089s
[2K
| Adam | epoch: 020 | loss: 0.31707 - acc: 0.8417 -- iter: 192/206
[A[ATraining Step: 140  | total loss: [1m[32m0.30251[0m[0m | time: 4.850s
[2K
| Adam | epoch: 020 | loss: 0.30251 - acc: 0.8481 | val_loss: 0.61553 - val_acc: 0.8000 -- iter: 206/206
--
Training Step: 141  | total loss: [1m[32m0.31078[0m[0m | time: 0.615s
[2K
| Adam | epoch: 021 | loss: 0.31078 - acc: 0.8415 -- iter: 032/206
[A[ATraining Step: 142  | total loss: [1m[32m0.30503[0m[0m | time: 1.212s
[2K
| Adam | epoch: 021 | loss: 0.30503 - acc: 0.8479 -- iter: 064/206
[A[ATraining Step: 143  | total loss: [1m[32m0.29521[0m[0m | time: 1.496s
[2K
| Adam | epoch: 021 | loss: 0.29521 - acc: 0.8600 -- iter: 096/206
[A[ATraining Step: 144  | total loss: [1m[32m0.30710[0m[0m | time: 1.889s
[2K
| Adam | epoch: 021 | loss: 0.30710 - acc: 0.8597 -- iter: 128/206
[A[ATraining Step: 145  | total loss: [1m[32m0.30011[0m[0m | time: 2.629s
[2K
| Adam | epoch: 021 | loss: 0.30011 - acc: 0.8595 -- iter: 160/206
[A[ATraining Step: 146  | total loss: [1m[32m0.28354[0m[0m | time: 3.396s
[2K
| Adam | epoch: 021 | loss: 0.28354 - acc: 0.8673 -- iter: 192/206
[A[ATraining Step: 147  | total loss: [1m[32m0.26920[0m[0m | time: 5.122s
[2K
| Adam | epoch: 021 | loss: 0.26920 - acc: 0.8774 | val_loss: 0.70323 - val_acc: 0.7077 -- iter: 206/206
--
Training Step: 148  | total loss: [1m[32m0.25634[0m[0m | time: 0.631s
[2K
| Adam | epoch: 022 | loss: 0.25634 - acc: 0.8866 -- iter: 032/206
[A[ATraining Step: 149  | total loss: [1m[32m0.26649[0m[0m | time: 1.234s
[2K
| Adam | epoch: 022 | loss: 0.26649 - acc: 0.8791 -- iter: 064/206
[A[ATraining Step: 150  | total loss: [1m[32m0.24887[0m[0m | time: 1.887s
[2K
| Adam | epoch: 022 | loss: 0.24887 - acc: 0.8912 -- iter: 096/206
[A[ATraining Step: 151  | total loss: [1m[32m0.23853[0m[0m | time: 2.168s
[2K
| Adam | epoch: 022 | loss: 0.23853 - acc: 0.8990 -- iter: 128/206
[A[ATraining Step: 152  | total loss: [1m[32m0.26491[0m[0m | time: 2.451s
[2K
| Adam | epoch: 022 | loss: 0.26491 - acc: 0.8948 -- iter: 160/206
[A[ATraining Step: 153  | total loss: [1m[32m0.24438[0m[0m | time: 3.187s
[2K
| Adam | epoch: 022 | loss: 0.24438 - acc: 0.9053 -- iter: 192/206
[A[ATraining Step: 154  | total loss: [1m[32m0.22513[0m[0m | time: 4.912s
[2K
| Adam | epoch: 022 | loss: 0.22513 - acc: 0.9148 | val_loss: 0.66831 - val_acc: 0.7538 -- iter: 206/206
--
Training Step: 155  | total loss: [1m[32m0.20640[0m[0m | time: 0.605s
[2K
| Adam | epoch: 023 | loss: 0.20640 - acc: 0.9233 -- iter: 032/206
[A[ATraining Step: 156  | total loss: [1m[32m0.19254[0m[0m | time: 1.201s
[2K
| Adam | epoch: 023 | loss: 0.19254 - acc: 0.9310 -- iter: 064/206
[A[ATraining Step: 157  | total loss: [1m[32m0.18177[0m[0m | time: 1.795s
[2K
| Adam | epoch: 023 | loss: 0.18177 - acc: 0.9348 -- iter: 096/206
[A[ATraining Step: 158  | total loss: [1m[32m0.17124[0m[0m | time: 2.401s
[2K
| Adam | epoch: 023 | loss: 0.17124 - acc: 0.9382 -- iter: 128/206
[A[ATraining Step: 159  | total loss: [1m[32m0.18385[0m[0m | time: 2.683s
[2K
| Adam | epoch: 023 | loss: 0.18385 - acc: 0.9350 -- iter: 160/206
[A[ATraining Step: 160  | total loss: [1m[32m0.20149[0m[0m | time: 3.042s
[2K
| Adam | epoch: 023 | loss: 0.20149 - acc: 0.9343 -- iter: 192/206
[A[ATraining Step: 161  | total loss: [1m[32m0.18377[0m[0m | time: 4.825s
[2K
| Adam | epoch: 023 | loss: 0.18377 - acc: 0.9409 | val_loss: 0.86957 - val_acc: 0.7231 -- iter: 206/206
--
Training Step: 162  | total loss: [1m[32m0.17317[0m[0m | time: 0.590s
[2K
| Adam | epoch: 024 | loss: 0.17317 - acc: 0.9437 -- iter: 032/206
[A[ATraining Step: 163  | total loss: [1m[32m0.16903[0m[0m | time: 1.202s
[2K
| Adam | epoch: 024 | loss: 0.16903 - acc: 0.9462 -- iter: 064/206
[A[ATraining Step: 164  | total loss: [1m[32m0.15870[0m[0m | time: 1.810s
[2K
| Adam | epoch: 024 | loss: 0.15870 - acc: 0.9516 -- iter: 096/206
[A[ATraining Step: 165  | total loss: [1m[32m0.14558[0m[0m | time: 2.507s
[2K
| Adam | epoch: 024 | loss: 0.14558 - acc: 0.9564 -- iter: 128/206
[A[ATraining Step: 166  | total loss: [1m[32m0.13746[0m[0m | time: 3.253s
[2K
| Adam | epoch: 024 | loss: 0.13746 - acc: 0.9576 -- iter: 160/206
[A[ATraining Step: 167  | total loss: [1m[32m0.12777[0m[0m | time: 3.634s
[2K
| Adam | epoch: 024 | loss: 0.12777 - acc: 0.9619 -- iter: 192/206
[A[ATraining Step: 168  | total loss: [1m[32m0.12781[0m[0m | time: 4.993s
[2K
| Adam | epoch: 024 | loss: 0.12781 - acc: 0.9585 | val_loss: 0.70713 - val_acc: 0.7385 -- iter: 206/206
--
Training Step: 169  | total loss: [1m[32m0.11966[0m[0m | time: 0.607s
[2K
| Adam | epoch: 025 | loss: 0.11966 - acc: 0.9627 -- iter: 032/206
[A[ATraining Step: 170  | total loss: [1m[32m0.11087[0m[0m | time: 1.214s
[2K
| Adam | epoch: 025 | loss: 0.11087 - acc: 0.9664 -- iter: 064/206
[A[ATraining Step: 171  | total loss: [1m[32m0.10359[0m[0m | time: 1.827s
[2K
| Adam | epoch: 025 | loss: 0.10359 - acc: 0.9698 -- iter: 096/206
[A[ATraining Step: 172  | total loss: [1m[32m0.09633[0m[0m | time: 2.538s
[2K
| Adam | epoch: 025 | loss: 0.09633 - acc: 0.9728 -- iter: 128/206
[A[ATraining Step: 173  | total loss: [1m[32m0.08844[0m[0m | time: 3.307s
[2K
| Adam | epoch: 025 | loss: 0.08844 - acc: 0.9755 -- iter: 160/206
[A[ATraining Step: 174  | total loss: [1m[32m0.08165[0m[0m | time: 4.052s
[2K
| Adam | epoch: 025 | loss: 0.08165 - acc: 0.9780 -- iter: 192/206
[A[ATraining Step: 175  | total loss: [1m[32m0.08584[0m[0m | time: 5.396s
[2K
| Adam | epoch: 025 | loss: 0.08584 - acc: 0.9770 | val_loss: 0.67274 - val_acc: 0.8154 -- iter: 206/206
--
Training Step: 176  | total loss: [1m[32m0.17934[0m[0m | time: 0.348s
[2K
| Adam | epoch: 026 | loss: 0.17934 - acc: 0.9579 -- iter: 032/206
[A[ATraining Step: 177  | total loss: [1m[32m0.18425[0m[0m | time: 1.112s
[2K
| Adam | epoch: 026 | loss: 0.18425 - acc: 0.9550 -- iter: 064/206
[A[ATraining Step: 178  | total loss: [1m[32m0.16902[0m[0m | time: 1.818s
[2K
| Adam | epoch: 026 | loss: 0.16902 - acc: 0.9595 -- iter: 096/206
[A[ATraining Step: 179  | total loss: [1m[32m0.15484[0m[0m | time: 2.564s
[2K
| Adam | epoch: 026 | loss: 0.15484 - acc: 0.9635 -- iter: 128/206
[A[ATraining Step: 180  | total loss: [1m[32m0.14287[0m[0m | time: 3.272s
[2K
| Adam | epoch: 026 | loss: 0.14287 - acc: 0.9672 -- iter: 160/206
[A[ATraining Step: 181  | total loss: [1m[32m0.13819[0m[0m | time: 3.855s
[2K
| Adam | epoch: 026 | loss: 0.13819 - acc: 0.9705 -- iter: 192/206
[A[ATraining Step: 182  | total loss: [1m[32m0.12805[0m[0m | time: 5.462s
[2K
| Adam | epoch: 026 | loss: 0.12805 - acc: 0.9734 | val_loss: 0.65469 - val_acc: 0.7231 -- iter: 206/206
--
Training Step: 183  | total loss: [1m[32m0.12491[0m[0m | time: 0.398s
[2K
| Adam | epoch: 027 | loss: 0.12491 - acc: 0.9730 -- iter: 032/206
[A[ATraining Step: 184  | total loss: [1m[32m0.16852[0m[0m | time: 0.715s
[2K
| Adam | epoch: 027 | loss: 0.16852 - acc: 0.9614 -- iter: 064/206
[A[ATraining Step: 185  | total loss: [1m[32m0.15599[0m[0m | time: 1.517s
[2K
| Adam | epoch: 027 | loss: 0.15599 - acc: 0.9652 -- iter: 096/206
[A[ATraining Step: 186  | total loss: [1m[32m0.14924[0m[0m | time: 2.272s
[2K
| Adam | epoch: 027 | loss: 0.14924 - acc: 0.9656 -- iter: 128/206
[A[ATraining Step: 187  | total loss: [1m[32m0.14305[0m[0m | time: 2.961s
[2K
| Adam | epoch: 027 | loss: 0.14305 - acc: 0.9690 -- iter: 160/206
[A[ATraining Step: 188  | total loss: [1m[32m0.13371[0m[0m | time: 3.551s
[2K
| Adam | epoch: 027 | loss: 0.13371 - acc: 0.9721 -- iter: 192/206
[A[ATraining Step: 189  | total loss: [1m[32m0.13029[0m[0m | time: 5.180s
[2K
| Adam | epoch: 027 | loss: 0.13029 - acc: 0.9718 | val_loss: 0.56728 - val_acc: 0.8154 -- iter: 206/206
--
Training Step: 190  | total loss: [1m[32m0.12350[0m[0m | time: 0.672s
[2K
| Adam | epoch: 028 | loss: 0.12350 - acc: 0.9715 -- iter: 032/206
[A[ATraining Step: 191  | total loss: [1m[32m0.12096[0m[0m | time: 0.974s
[2K
| Adam | epoch: 028 | loss: 0.12096 - acc: 0.9712 -- iter: 064/206
[A[ATraining Step: 192  | total loss: [1m[32m0.14451[0m[0m | time: 1.257s
[2K
| Adam | epoch: 028 | loss: 0.14451 - acc: 0.9669 -- iter: 096/206
[A[ATraining Step: 193  | total loss: [1m[32m0.13152[0m[0m | time: 1.891s
[2K
| Adam | epoch: 028 | loss: 0.13152 - acc: 0.9703 -- iter: 128/206
[A[ATraining Step: 194  | total loss: [1m[32m0.12040[0m[0m | time: 2.515s
[2K
| Adam | epoch: 028 | loss: 0.12040 - acc: 0.9732 -- iter: 160/206
[A[ATraining Step: 195  | total loss: [1m[32m0.11147[0m[0m | time: 3.135s
[2K
| Adam | epoch: 028 | loss: 0.11147 - acc: 0.9759 -- iter: 192/206
[A[ATraining Step: 196  | total loss: [1m[32m0.10473[0m[0m | time: 4.731s
[2K
| Adam | epoch: 028 | loss: 0.10473 - acc: 0.9783 | val_loss: 0.62580 - val_acc: 0.7692 -- iter: 206/206
--
Training Step: 197  | total loss: [1m[32m0.11184[0m[0m | time: 0.611s
[2K
| Adam | epoch: 029 | loss: 0.11184 - acc: 0.9742 -- iter: 032/206
[A[ATraining Step: 198  | total loss: [1m[32m0.10478[0m[0m | time: 1.210s
[2K
| Adam | epoch: 029 | loss: 0.10478 - acc: 0.9737 -- iter: 064/206
[A[ATraining Step: 199  | total loss: [1m[32m0.09639[0m[0m | time: 1.511s
[2K
| Adam | epoch: 029 | loss: 0.09639 - acc: 0.9763 -- iter: 096/206
[A[ATraining Step: 200  | total loss: [1m[32m0.08929[0m[0m | time: 2.802s
[2K
| Adam | epoch: 029 | loss: 0.08929 - acc: 0.9787 | val_loss: 1.23064 - val_acc: 0.6308 -- iter: 128/206
--
Training Step: 201  | total loss: [1m[32m0.08463[0m[0m | time: 3.438s
[2K
| Adam | epoch: 029 | loss: 0.08463 - acc: 0.9808 -- iter: 160/206
[A[ATraining Step: 202  | total loss: [1m[32m0.08137[0m[0m | time: 4.038s
[2K
| Adam | epoch: 029 | loss: 0.08137 - acc: 0.9796 -- iter: 192/206
[A[ATraining Step: 203  | total loss: [1m[32m0.07779[0m[0m | time: 5.668s
[2K
| Adam | epoch: 029 | loss: 0.07779 - acc: 0.9785 | val_loss: 0.69075 - val_acc: 0.7846 -- iter: 206/206
--
Training Step: 204  | total loss: [1m[32m0.08106[0m[0m | time: 0.599s
[2K
| Adam | epoch: 030 | loss: 0.08106 - acc: 0.9775 -- iter: 032/206
[A[ATraining Step: 205  | total loss: [1m[32m0.07415[0m[0m | time: 1.191s
[2K
| Adam | epoch: 030 | loss: 0.07415 - acc: 0.9798 -- iter: 064/206
[A[ATraining Step: 206  | total loss: [1m[32m0.06726[0m[0m | time: 1.796s
[2K
| Adam | epoch: 030 | loss: 0.06726 - acc: 0.9818 -- iter: 096/206
[A[ATraining Step: 207  | total loss: [1m[32m0.06189[0m[0m | time: 2.082s
[2K
| Adam | epoch: 030 | loss: 0.06189 - acc: 0.9836 -- iter: 128/206
[A[ATraining Step: 208  | total loss: [1m[32m0.10608[0m[0m | time: 2.364s
[2K
| Adam | epoch: 030 | loss: 0.10608 - acc: 0.9781 -- iter: 160/206
[A[ATraining Step: 209  | total loss: [1m[32m0.09676[0m[0m | time: 2.976s
[2K
| Adam | epoch: 030 | loss: 0.09676 - acc: 0.9803 -- iter: 192/206
[A[ATraining Step: 210  | total loss: [1m[32m0.08880[0m[0m | time: 4.586s
[2K
| Adam | epoch: 030 | loss: 0.08880 - acc: 0.9823 | val_loss: 0.66533 - val_acc: 0.7538 -- iter: 206/206
--
Validation AUC:0.8118908382066277
Validation AUPRC:0.8068149300160266
Test AUC:0.8017077798861479
Test AUPRC:0.8073526413074763
BestTestF1Score	0.79	0.53	0.75	0.7	0.91	31	13	18	3	0.2
BestTestMCCScore	0.83	0.63	0.82	0.79	0.88	30	8	23	4	0.68
BestTestAccuracyScore	0.83	0.63	0.82	0.79	0.88	30	8	23	4	0.68
BestValidationF1Score	0.84	0.58	0.78	0.74	0.97	37	13	14	1	0.2
BestValidationMCC	0.84	0.58	0.8	0.79	0.89	34	9	18	4	0.68
BestValidationAccuracy	0.84	0.58	0.8	0.79	0.89	34	9	18	4	0.68
TestPredictions (Threshold:0.68)
CHEMBL325309,TN,INACT,0.03999999910593033	CHEMBL1945765,FP,INACT,0.9900000095367432	CHEMBL1719276,FP,INACT,0.9900000095367432	CHEMBL110432,TN,INACT,0.05000000074505806	CHEMBL3191015,TP,ACT,0.8500000238418579	CHEMBL2316894,TP,ACT,1.0	CHEMBL1305050,TP,ACT,0.9900000095367432	CHEMBL1478587,TP,ACT,1.0	CHEMBL565810,TP,ACT,1.0	CHEMBL3219181,TN,INACT,0.019999999552965164	CHEMBL1982888,TP,ACT,0.9800000190734863	CHEMBL62224,TN,INACT,0.009999999776482582	CHEMBL1727475,TN,INACT,0.029999999329447746	CHEMBL2062707,TP,ACT,0.9599999785423279	CHEMBL1505246,TP,ACT,1.0	CHEMBL1421970,TP,ACT,0.9900000095367432	CHEMBL567473,FN,ACT,0.5899999737739563	CHEMBL2062598,TP,ACT,0.9599999785423279	CHEMBL565795,TP,ACT,0.9800000190734863	CHEMBL305349,TN,INACT,0.41999998688697815	CHEMBL55155,FP,INACT,0.8899999856948853	CHEMBL393591,TN,INACT,0.05000000074505806	CHEMBL565364,FN,ACT,0.029999999329447746	CHEMBL419196,TN,INACT,0.019999999552965164	CHEMBL108584,TN,INACT,0.03999999910593033	CHEMBL391344,TN,INACT,0.09000000357627869	CHEMBL1537922,TP,ACT,0.800000011920929	CHEMBL108128,TN,INACT,0.0	CHEMBL1447397,TP,ACT,0.8999999761581421	CHEMBL416674,TN,INACT,0.5699999928474426	CHEMBL1672294,TP,ACT,0.949999988079071	CHEMBL111660,TN,INACT,0.009999999776482582	CHEMBL2206752,TN,INACT,0.03999999910593033	CHEMBL103043,FP,INACT,0.9800000190734863	CHEMBL62509,FP,INACT,0.949999988079071	CHEMBL302913,TN,INACT,0.33000001311302185	CHEMBL1609429,TP,ACT,0.9800000190734863	CHEMBL1471409,TP,ACT,0.9800000190734863	CHEMBL1484942,FN,ACT,0.0	CHEMBL1460470,FN,ACT,0.009999999776482582	CHEMBL1390703,TP,ACT,0.9800000190734863	CHEMBL3212539,TP,ACT,0.9900000095367432	CHEMBL106479,TN,INACT,0.0	CHEMBL110922,TN,INACT,0.0	CHEMBL433501,TN,INACT,0.09000000357627869	CHEMBL1605172,TP,ACT,1.0	CHEMBL319997,TN,INACT,0.14000000059604645	CHEMBL1334970,TP,ACT,0.9800000190734863	CHEMBL61058,TN,INACT,0.33000001311302185	CHEMBL62490,FP,INACT,0.9800000190734863	CHEMBL3360905,TP,ACT,0.9700000286102295	CHEMBL228113,TN,INACT,0.12999999523162842	CHEMBL1384370,TP,ACT,0.9800000190734863	CHEMBL599924,TP,ACT,0.9800000190734863	CHEMBL2062591,TP,ACT,0.9599999785423279	CHEMBL1349451,TP,ACT,0.8299999833106995	CHEMBL3260030,TP,ACT,0.9900000095367432	CHEMBL108202,TN,INACT,0.1899999976158142	CHEMBL1988133,TP,ACT,0.6899999976158142	CHEMBL2349151,TP,ACT,0.7699999809265137	CHEMBL228173,TN,INACT,0.6499999761581421	CHEMBL48361,FP,INACT,0.9900000095367432	CHEMBL567047,TP,ACT,0.9800000190734863	CHEMBL108721,TP,ACT,1.0	CHEMBL1595008,FP,INACT,0.9599999785423279	

