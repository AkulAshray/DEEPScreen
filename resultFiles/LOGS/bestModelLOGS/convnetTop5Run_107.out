ImageNetInceptionV2 CHEMBL3198 adam 0.0005 15 0 0 0.6 False True
Number of active compounds :	202
Number of inactive compounds :	202
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL3198_adam_0.0005_15_0_0_0.6_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL3198_adam_0.0005_15_0.6/
---------------------------------
Training samples: 257
Validation samples: 81
--
Training Step: 1  | time: 57.320s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/257
[A[ATraining Step: 2  | total loss: [1m[32m0.68428[0m[0m | time: 97.555s
[2K
| Adam | epoch: 001 | loss: 0.68428 - acc: 0.5062 -- iter: 064/257
[A[ATraining Step: 3  | total loss: [1m[32m0.72442[0m[0m | time: 128.890s
[2K
| Adam | epoch: 001 | loss: 0.72442 - acc: 0.5778 -- iter: 096/257
[A[ATraining Step: 4  | total loss: [1m[32m0.66682[0m[0m | time: 156.185s
[2K
| Adam | epoch: 001 | loss: 0.66682 - acc: 0.6132 -- iter: 128/257
[A[ATraining Step: 5  | total loss: [1m[32m0.63539[0m[0m | time: 184.303s
[2K
| Adam | epoch: 001 | loss: 0.63539 - acc: 0.6646 -- iter: 160/257
[A[ATraining Step: 6  | total loss: [1m[32m0.95704[0m[0m | time: 205.827s
[2K
| Adam | epoch: 001 | loss: 0.95704 - acc: 0.4182 -- iter: 192/257
[A[ATraining Step: 7  | total loss: [1m[32m0.72916[0m[0m | time: 217.589s
[2K
| Adam | epoch: 001 | loss: 0.72916 - acc: 0.5798 -- iter: 224/257
[A[ATraining Step: 8  | total loss: [1m[32m0.56917[0m[0m | time: 227.988s
[2K
| Adam | epoch: 001 | loss: 0.56917 - acc: 0.7458 -- iter: 256/257
[A[ATraining Step: 9  | total loss: [1m[32m0.48811[0m[0m | time: 238.523s
[2K
| Adam | epoch: 001 | loss: 0.48811 - acc: 0.7646 | val_loss: 0.93862 - val_acc: 0.5185 -- iter: 257/257
--
Training Step: 10  | total loss: [1m[32m0.54219[0m[0m | time: 0.662s
[2K
| Adam | epoch: 002 | loss: 0.54219 - acc: 0.8823 -- iter: 032/257
[A[ATraining Step: 11  | total loss: [1m[32m0.41566[0m[0m | time: 40.703s
[2K
| Adam | epoch: 002 | loss: 0.41566 - acc: 0.9380 -- iter: 064/257
[A[ATraining Step: 12  | total loss: [1m[32m0.70692[0m[0m | time: 63.412s
[2K
| Adam | epoch: 002 | loss: 0.70692 - acc: 0.7972 -- iter: 096/257
[A[ATraining Step: 13  | total loss: [1m[32m0.63854[0m[0m | time: 93.499s
[2K
| Adam | epoch: 002 | loss: 0.63854 - acc: 0.7636 -- iter: 128/257
[A[ATraining Step: 14  | total loss: [1m[32m0.65395[0m[0m | time: 111.190s
[2K
| Adam | epoch: 002 | loss: 0.65395 - acc: 0.7452 -- iter: 160/257
[A[ATraining Step: 15  | total loss: [1m[32m0.66749[0m[0m | time: 145.543s
[2K
| Adam | epoch: 002 | loss: 0.66749 - acc: 0.6982 -- iter: 192/257
[A[ATraining Step: 16  | total loss: [1m[32m0.79750[0m[0m | time: 168.006s
[2K
| Adam | epoch: 002 | loss: 0.79750 - acc: 0.6239 -- iter: 224/257
[A[ATraining Step: 17  | total loss: [1m[32m0.66776[0m[0m | time: 178.283s
[2K
| Adam | epoch: 002 | loss: 0.66776 - acc: 0.6805 -- iter: 256/257
[A[ATraining Step: 18  | total loss: [1m[32m0.55212[0m[0m | time: 194.739s
[2K
| Adam | epoch: 002 | loss: 0.55212 - acc: 0.7587 | val_loss: 0.69374 - val_acc: 0.5185 -- iter: 257/257
--
Training Step: 19  | total loss: [1m[32m0.45474[0m[0m | time: 0.613s
[2K
| Adam | epoch: 003 | loss: 0.45474 - acc: 0.8183 -- iter: 032/257
[A[ATraining Step: 20  | total loss: [1m[32m0.54594[0m[0m | time: 1.228s
[2K
| Adam | epoch: 003 | loss: 0.54594 - acc: 0.5553 -- iter: 064/257
[A[ATraining Step: 21  | total loss: [1m[32m0.54941[0m[0m | time: 19.913s
[2K
| Adam | epoch: 003 | loss: 0.54941 - acc: 0.6933 -- iter: 096/257
[A[ATraining Step: 22  | total loss: [1m[32m0.49966[0m[0m | time: 30.044s
[2K
| Adam | epoch: 003 | loss: 0.49966 - acc: 0.7478 -- iter: 128/257
[A[ATraining Step: 23  | total loss: [1m[32m0.48587[0m[0m | time: 41.646s
[2K
| Adam | epoch: 003 | loss: 0.48587 - acc: 0.7847 -- iter: 160/257
[A[ATraining Step: 24  | total loss: [1m[32m0.51794[0m[0m | time: 51.760s
[2K
| Adam | epoch: 003 | loss: 0.51794 - acc: 0.8013 -- iter: 192/257
[A[ATraining Step: 25  | total loss: [1m[32m0.58857[0m[0m | time: 65.636s
[2K
| Adam | epoch: 003 | loss: 0.58857 - acc: 0.7618 -- iter: 224/257
[A[ATraining Step: 26  | total loss: [1m[32m0.58047[0m[0m | time: 75.671s
[2K
| Adam | epoch: 003 | loss: 0.58047 - acc: 0.7669 -- iter: 256/257
[A[ATraining Step: 27  | total loss: [1m[32m0.65839[0m[0m | time: 90.242s
[2K
| Adam | epoch: 003 | loss: 0.65839 - acc: 0.7224 | val_loss: 0.79211 - val_acc: 0.4815 -- iter: 257/257
--
Training Step: 28  | total loss: [1m[32m0.61170[0m[0m | time: 11.327s
[2K
| Adam | epoch: 004 | loss: 0.61170 - acc: 0.7449 -- iter: 032/257
[A[ATraining Step: 29  | total loss: [1m[32m0.57769[0m[0m | time: 11.929s
[2K
| Adam | epoch: 004 | loss: 0.57769 - acc: 0.7614 -- iter: 064/257
[A[ATraining Step: 30  | total loss: [1m[32m0.47836[0m[0m | time: 12.532s
[2K
| Adam | epoch: 004 | loss: 0.47836 - acc: 0.8179 -- iter: 096/257
[A[ATraining Step: 31  | total loss: [1m[32m0.40309[0m[0m | time: 22.613s
[2K
| Adam | epoch: 004 | loss: 0.40309 - acc: 0.8599 -- iter: 128/257
[A[ATraining Step: 32  | total loss: [1m[32m0.36983[0m[0m | time: 32.711s
[2K
| Adam | epoch: 004 | loss: 0.36983 - acc: 0.8774 -- iter: 160/257
[A[ATraining Step: 33  | total loss: [1m[32m0.38824[0m[0m | time: 42.771s
[2K
| Adam | epoch: 004 | loss: 0.38824 - acc: 0.8631 -- iter: 192/257
[A[ATraining Step: 34  | total loss: [1m[32m0.36475[0m[0m | time: 52.743s
[2K
| Adam | epoch: 004 | loss: 0.36475 - acc: 0.8657 -- iter: 224/257
[A[ATraining Step: 35  | total loss: [1m[32m0.33275[0m[0m | time: 62.765s
[2K
| Adam | epoch: 004 | loss: 0.33275 - acc: 0.8807 -- iter: 256/257
[A[ATraining Step: 36  | total loss: [1m[32m0.30818[0m[0m | time: 83.156s
[2K
| Adam | epoch: 004 | loss: 0.30818 - acc: 0.8859 | val_loss: 0.67541 - val_acc: 0.5432 -- iter: 257/257
--
Training Step: 37  | total loss: [1m[32m0.31754[0m[0m | time: 10.179s
[2K
| Adam | epoch: 005 | loss: 0.31754 - acc: 0.8837 -- iter: 032/257
[A[ATraining Step: 38  | total loss: [1m[32m0.38774[0m[0m | time: 19.937s
[2K
| Adam | epoch: 005 | loss: 0.38774 - acc: 0.8637 -- iter: 064/257
[A[ATraining Step: 39  | total loss: [1m[32m0.33805[0m[0m | time: 20.535s
[2K
| Adam | epoch: 005 | loss: 0.33805 - acc: 0.8778 -- iter: 096/257
[A[ATraining Step: 40  | total loss: [1m[32m0.37734[0m[0m | time: 21.145s
[2K
| Adam | epoch: 005 | loss: 0.37734 - acc: 0.9007 -- iter: 128/257
[A[ATraining Step: 41  | total loss: [1m[32m0.38745[0m[0m | time: 31.186s
[2K
| Adam | epoch: 005 | loss: 0.38745 - acc: 0.9190 -- iter: 160/257
[A[ATraining Step: 42  | total loss: [1m[32m0.32789[0m[0m | time: 40.938s
[2K
| Adam | epoch: 005 | loss: 0.32789 - acc: 0.9336 -- iter: 192/257
[A[ATraining Step: 43  | total loss: [1m[32m0.28939[0m[0m | time: 50.779s
[2K
| Adam | epoch: 005 | loss: 0.28939 - acc: 0.9398 -- iter: 224/257
[A[ATraining Step: 44  | total loss: [1m[32m0.31602[0m[0m | time: 60.769s
[2K
| Adam | epoch: 005 | loss: 0.31602 - acc: 0.9177 -- iter: 256/257
[A[ATraining Step: 45  | total loss: [1m[32m0.29064[0m[0m | time: 75.803s
[2K
| Adam | epoch: 005 | loss: 0.29064 - acc: 0.9158 | val_loss: 1.58248 - val_acc: 0.5185 -- iter: 257/257
--
Training Step: 46  | total loss: [1m[32m0.29374[0m[0m | time: 11.151s
[2K
| Adam | epoch: 006 | loss: 0.29374 - acc: 0.9194 -- iter: 032/257
[A[ATraining Step: 47  | total loss: [1m[32m0.28505[0m[0m | time: 21.121s
[2K
| Adam | epoch: 006 | loss: 0.28505 - acc: 0.9224 -- iter: 064/257
[A[ATraining Step: 48  | total loss: [1m[32m0.24371[0m[0m | time: 33.849s
[2K
| Adam | epoch: 006 | loss: 0.24371 - acc: 0.9348 -- iter: 096/257
[A[ATraining Step: 49  | total loss: [1m[32m0.23734[0m[0m | time: 34.490s
[2K
| Adam | epoch: 006 | loss: 0.23734 - acc: 0.9254 -- iter: 128/257
[A[ATraining Step: 50  | total loss: [1m[32m0.23591[0m[0m | time: 35.071s
[2K
| Adam | epoch: 006 | loss: 0.23591 - acc: 0.9370 -- iter: 160/257
[A[ATraining Step: 51  | total loss: [1m[32m0.22694[0m[0m | time: 46.402s
[2K
| Adam | epoch: 006 | loss: 0.22694 - acc: 0.9466 -- iter: 192/257
[A[ATraining Step: 52  | total loss: [1m[32m0.22850[0m[0m | time: 67.104s
[2K
| Adam | epoch: 006 | loss: 0.22850 - acc: 0.9452 -- iter: 224/257
[A[ATraining Step: 53  | total loss: [1m[32m0.23304[0m[0m | time: 77.259s
[2K
| Adam | epoch: 006 | loss: 0.23304 - acc: 0.9395 -- iter: 256/257
[A[ATraining Step: 54  | total loss: [1m[32m0.22545[0m[0m | time: 91.722s
[2K
| Adam | epoch: 006 | loss: 0.22545 - acc: 0.9301 | val_loss: 1.31141 - val_acc: 0.5309 -- iter: 257/257
--
Training Step: 55  | total loss: [1m[32m0.20013[0m[0m | time: 14.108s
[2K
| Adam | epoch: 007 | loss: 0.20013 - acc: 0.9401 -- iter: 032/257
[A[ATraining Step: 56  | total loss: [1m[32m0.18204[0m[0m | time: 23.436s
[2K
| Adam | epoch: 007 | loss: 0.18204 - acc: 0.9485 -- iter: 064/257
[A[ATraining Step: 57  | total loss: [1m[32m0.19909[0m[0m | time: 32.519s
[2K
| Adam | epoch: 007 | loss: 0.19909 - acc: 0.9427 -- iter: 096/257
[A[ATraining Step: 58  | total loss: [1m[32m0.20714[0m[0m | time: 41.473s
[2K
| Adam | epoch: 007 | loss: 0.20714 - acc: 0.9420 -- iter: 128/257
[A[ATraining Step: 59  | total loss: [1m[32m0.19116[0m[0m | time: 42.059s
[2K
| Adam | epoch: 007 | loss: 0.19116 - acc: 0.9456 -- iter: 160/257
[A[ATraining Step: 60  | total loss: [1m[32m0.18369[0m[0m | time: 42.633s
[2K
| Adam | epoch: 007 | loss: 0.18369 - acc: 0.9528 -- iter: 192/257
[A[ATraining Step: 61  | total loss: [1m[32m0.18485[0m[0m | time: 52.009s
[2K
| Adam | epoch: 007 | loss: 0.18485 - acc: 0.9589 -- iter: 224/257
[A[ATraining Step: 62  | total loss: [1m[32m0.19831[0m[0m | time: 61.473s
[2K
| Adam | epoch: 007 | loss: 0.19831 - acc: 0.9562 -- iter: 256/257
[A[ATraining Step: 63  | total loss: [1m[32m0.18122[0m[0m | time: 74.559s
[2K
| Adam | epoch: 007 | loss: 0.18122 - acc: 0.9617 | val_loss: 0.80865 - val_acc: 0.6173 -- iter: 257/257
--
Training Step: 64  | total loss: [1m[32m0.16443[0m[0m | time: 9.817s
[2K
| Adam | epoch: 008 | loss: 0.16443 - acc: 0.9665 -- iter: 032/257
[A[ATraining Step: 65  | total loss: [1m[32m0.18346[0m[0m | time: 19.002s
[2K
| Adam | epoch: 008 | loss: 0.18346 - acc: 0.9514 -- iter: 064/257
[A[ATraining Step: 66  | total loss: [1m[32m0.17001[0m[0m | time: 28.314s
[2K
| Adam | epoch: 008 | loss: 0.17001 - acc: 0.9573 -- iter: 096/257
[A[ATraining Step: 67  | total loss: [1m[32m0.15802[0m[0m | time: 37.617s
[2K
| Adam | epoch: 008 | loss: 0.15802 - acc: 0.9587 -- iter: 128/257
[A[ATraining Step: 68  | total loss: [1m[32m0.18900[0m[0m | time: 47.171s
[2K
| Adam | epoch: 008 | loss: 0.18900 - acc: 0.9562 -- iter: 160/257
[A[ATraining Step: 69  | total loss: [1m[32m0.17829[0m[0m | time: 47.767s
[2K
| Adam | epoch: 008 | loss: 0.17829 - acc: 0.9576 -- iter: 192/257
[A[ATraining Step: 70  | total loss: [1m[32m0.17307[0m[0m | time: 48.344s
[2K
| Adam | epoch: 008 | loss: 0.17307 - acc: 0.9625 -- iter: 224/257
[A[ATraining Step: 71  | total loss: [1m[32m0.15708[0m[0m | time: 58.216s
[2K
| Adam | epoch: 008 | loss: 0.15708 - acc: 0.9668 -- iter: 256/257
[A[ATraining Step: 72  | total loss: [1m[32m0.15393[0m[0m | time: 72.192s
[2K
| Adam | epoch: 008 | loss: 0.15393 - acc: 0.9670 | val_loss: 0.98378 - val_acc: 0.6790 -- iter: 257/257
--
Training Step: 73  | total loss: [1m[32m0.14925[0m[0m | time: 9.225s
[2K
| Adam | epoch: 009 | loss: 0.14925 - acc: 0.9672 -- iter: 032/257
[A[ATraining Step: 74  | total loss: [1m[32m0.21672[0m[0m | time: 18.534s
[2K
| Adam | epoch: 009 | loss: 0.21672 - acc: 0.9399 -- iter: 064/257
[A[ATraining Step: 75  | total loss: [1m[32m0.20385[0m[0m | time: 27.924s
[2K
| Adam | epoch: 009 | loss: 0.20385 - acc: 0.9464 -- iter: 096/257
[A[ATraining Step: 76  | total loss: [1m[32m0.21006[0m[0m | time: 37.356s
[2K
| Adam | epoch: 009 | loss: 0.21006 - acc: 0.9421 -- iter: 128/257
[A[ATraining Step: 77  | total loss: [1m[32m0.20934[0m[0m | time: 46.561s
[2K
| Adam | epoch: 009 | loss: 0.20934 - acc: 0.9450 -- iter: 160/257
[A[ATraining Step: 78  | total loss: [1m[32m0.23417[0m[0m | time: 55.891s
[2K
| Adam | epoch: 009 | loss: 0.23417 - acc: 0.9442 -- iter: 192/257
[A[ATraining Step: 79  | total loss: [1m[32m0.21167[0m[0m | time: 56.470s
[2K
| Adam | epoch: 009 | loss: 0.21167 - acc: 0.9500 -- iter: 224/257
[A[ATraining Step: 80  | total loss: [1m[32m0.19591[0m[0m | time: 57.037s
[2K
| Adam | epoch: 009 | loss: 0.19591 - acc: 0.9551 -- iter: 256/257
[A[ATraining Step: 81  | total loss: [1m[32m0.18160[0m[0m | time: 70.342s
[2K
| Adam | epoch: 009 | loss: 0.18160 - acc: 0.9596 | val_loss: 1.19035 - val_acc: 0.6173 -- iter: 257/257
--
Training Step: 82  | total loss: [1m[32m0.16663[0m[0m | time: 9.411s
[2K
| Adam | epoch: 010 | loss: 0.16663 - acc: 0.9637 -- iter: 032/257
[A[ATraining Step: 83  | total loss: [1m[32m0.15929[0m[0m | time: 19.069s
[2K
| Adam | epoch: 010 | loss: 0.15929 - acc: 0.9610 -- iter: 064/257
[A[ATraining Step: 84  | total loss: [1m[32m0.15226[0m[0m | time: 28.448s
[2K
| Adam | epoch: 010 | loss: 0.15226 - acc: 0.9587 -- iter: 096/257
[A[ATraining Step: 85  | total loss: [1m[32m0.14689[0m[0m | time: 37.706s
[2K
| Adam | epoch: 010 | loss: 0.14689 - acc: 0.9566 -- iter: 128/257
[A[ATraining Step: 86  | total loss: [1m[32m0.14815[0m[0m | time: 47.308s
[2K
| Adam | epoch: 010 | loss: 0.14815 - acc: 0.9578 -- iter: 160/257
[A[ATraining Step: 87  | total loss: [1m[32m0.13954[0m[0m | time: 56.915s
[2K
| Adam | epoch: 010 | loss: 0.13954 - acc: 0.9589 -- iter: 192/257
[A[ATraining Step: 88  | total loss: [1m[32m0.15160[0m[0m | time: 66.407s
[2K
| Adam | epoch: 010 | loss: 0.15160 - acc: 0.9536 -- iter: 224/257
[A[ATraining Step: 89  | total loss: [1m[32m0.13974[0m[0m | time: 66.994s
[2K
| Adam | epoch: 010 | loss: 0.13974 - acc: 0.9583 -- iter: 256/257
[A[ATraining Step: 90  | total loss: [1m[32m0.44917[0m[0m | time: 71.389s
[2K
| Adam | epoch: 010 | loss: 0.44917 - acc: 0.8624 | val_loss: 3.55401 - val_acc: 0.5432 -- iter: 257/257
--
Training Step: 91  | total loss: [1m[32m0.65674[0m[0m | time: 9.790s
[2K
| Adam | epoch: 011 | loss: 0.65674 - acc: 0.7762 -- iter: 032/257
[A[ATraining Step: 92  | total loss: [1m[32m0.59473[0m[0m | time: 19.396s
[2K
| Adam | epoch: 011 | loss: 0.59473 - acc: 0.7986 -- iter: 064/257
[A[ATraining Step: 93  | total loss: [1m[32m0.56050[0m[0m | time: 28.960s
[2K
| Adam | epoch: 011 | loss: 0.56050 - acc: 0.8062 -- iter: 096/257
[A[ATraining Step: 94  | total loss: [1m[32m0.51076[0m[0m | time: 38.734s
[2K
| Adam | epoch: 011 | loss: 0.51076 - acc: 0.8225 -- iter: 128/257
[A[ATraining Step: 95  | total loss: [1m[32m0.46629[0m[0m | time: 48.472s
[2K
| Adam | epoch: 011 | loss: 0.46629 - acc: 0.8371 -- iter: 160/257
[A[ATraining Step: 96  | total loss: [1m[32m0.42403[0m[0m | time: 57.918s
[2K
| Adam | epoch: 011 | loss: 0.42403 - acc: 0.8534 -- iter: 192/257
[A[ATraining Step: 97  | total loss: [1m[32m0.40221[0m[0m | time: 67.729s
[2K
| Adam | epoch: 011 | loss: 0.40221 - acc: 0.8618 -- iter: 224/257
[A[ATraining Step: 98  | total loss: [1m[32m0.36376[0m[0m | time: 77.612s
[2K
| Adam | epoch: 011 | loss: 0.36376 - acc: 0.8756 -- iter: 256/257
[A[ATraining Step: 99  | total loss: [1m[32m0.32999[0m[0m | time: 82.278s
[2K
| Adam | epoch: 011 | loss: 0.32999 - acc: 0.8881 | val_loss: 1.99099 - val_acc: 0.6543 -- iter: 257/257
--
Training Step: 100  | total loss: [1m[32m0.31384[0m[0m | time: 0.602s
[2K
| Adam | epoch: 012 | loss: 0.31384 - acc: 0.8992 -- iter: 032/257
[A[ATraining Step: 101  | total loss: [1m[32m0.29322[0m[0m | time: 10.333s
[2K
| Adam | epoch: 012 | loss: 0.29322 - acc: 0.9093 -- iter: 064/257
[A[ATraining Step: 102  | total loss: [1m[32m0.27391[0m[0m | time: 20.156s
[2K
| Adam | epoch: 012 | loss: 0.27391 - acc: 0.9153 -- iter: 096/257
[A[ATraining Step: 103  | total loss: [1m[32m0.26225[0m[0m | time: 30.159s
[2K
| Adam | epoch: 012 | loss: 0.26225 - acc: 0.9206 -- iter: 128/257
[A[ATraining Step: 104  | total loss: [1m[32m0.23743[0m[0m | time: 39.982s
[2K
| Adam | epoch: 012 | loss: 0.23743 - acc: 0.9286 -- iter: 160/257
[A[ATraining Step: 105  | total loss: [1m[32m0.21511[0m[0m | time: 49.741s
[2K
| Adam | epoch: 012 | loss: 0.21511 - acc: 0.9357 -- iter: 192/257
[A[ATraining Step: 106  | total loss: [1m[32m0.19682[0m[0m | time: 59.704s
[2K
| Adam | epoch: 012 | loss: 0.19682 - acc: 0.9390 -- iter: 224/257
[A[ATraining Step: 107  | total loss: [1m[32m0.17798[0m[0m | time: 69.856s
[2K
| Adam | epoch: 012 | loss: 0.17798 - acc: 0.9451 -- iter: 256/257
[A[ATraining Step: 108  | total loss: [1m[32m0.16127[0m[0m | time: 84.002s
[2K
| Adam | epoch: 012 | loss: 0.16127 - acc: 0.9506 | val_loss: 1.13360 - val_acc: 0.6543 -- iter: 257/257
--
Training Step: 109  | total loss: [1m[32m0.15048[0m[0m | time: 0.598s
[2K
| Adam | epoch: 013 | loss: 0.15048 - acc: 0.9524 -- iter: 032/257
[A[ATraining Step: 110  | total loss: [1m[32m0.14585[0m[0m | time: 1.174s
[2K
| Adam | epoch: 013 | loss: 0.14585 - acc: 0.9572 -- iter: 064/257
[A[ATraining Step: 111  | total loss: [1m[32m0.13981[0m[0m | time: 10.853s
[2K
| Adam | epoch: 013 | loss: 0.13981 - acc: 0.9615 -- iter: 096/257
[A[ATraining Step: 112  | total loss: [1m[32m0.13413[0m[0m | time: 20.554s
[2K
| Adam | epoch: 013 | loss: 0.13413 - acc: 0.9591 -- iter: 128/257
[A[ATraining Step: 113  | total loss: [1m[32m0.13076[0m[0m | time: 30.339s
[2K
| Adam | epoch: 013 | loss: 0.13076 - acc: 0.9600 -- iter: 160/257
[A[ATraining Step: 114  | total loss: [1m[32m0.11950[0m[0m | time: 40.232s
[2K
| Adam | epoch: 013 | loss: 0.11950 - acc: 0.9640 -- iter: 192/257
[A[ATraining Step: 115  | total loss: [1m[32m0.14093[0m[0m | time: 49.941s
[2K
| Adam | epoch: 013 | loss: 0.14093 - acc: 0.9551 -- iter: 224/257
[A[ATraining Step: 116  | total loss: [1m[32m0.24037[0m[0m | time: 59.602s
[2K
| Adam | epoch: 013 | loss: 0.24037 - acc: 0.9315 -- iter: 256/257
[A[ATraining Step: 117  | total loss: [1m[32m0.21803[0m[0m | time: 73.631s
[2K
| Adam | epoch: 013 | loss: 0.21803 - acc: 0.9383 | val_loss: 3.94003 - val_acc: 0.5679 -- iter: 257/257
--
Training Step: 118  | total loss: [1m[32m0.25396[0m[0m | time: 9.960s
[2K
| Adam | epoch: 014 | loss: 0.25396 - acc: 0.9414 -- iter: 032/257
[A[ATraining Step: 119  | total loss: [1m[32m0.23029[0m[0m | time: 10.560s
[2K
| Adam | epoch: 014 | loss: 0.23029 - acc: 0.9472 -- iter: 064/257
[A[ATraining Step: 120  | total loss: [1m[32m0.21274[0m[0m | time: 11.141s
[2K
| Adam | epoch: 014 | loss: 0.21274 - acc: 0.9525 -- iter: 096/257
[A[ATraining Step: 121  | total loss: [1m[32m0.19856[0m[0m | time: 20.993s
[2K
| Adam | epoch: 014 | loss: 0.19856 - acc: 0.9573 -- iter: 128/257
[A[ATraining Step: 122  | total loss: [1m[32m0.22209[0m[0m | time: 30.812s
[2K
| Adam | epoch: 014 | loss: 0.22209 - acc: 0.9459 -- iter: 160/257
[A[ATraining Step: 123  | total loss: [1m[32m0.20143[0m[0m | time: 40.838s
[2K
| Adam | epoch: 014 | loss: 0.20143 - acc: 0.9513 -- iter: 192/257
[A[ATraining Step: 124  | total loss: [1m[32m0.18304[0m[0m | time: 50.767s
[2K
| Adam | epoch: 014 | loss: 0.18304 - acc: 0.9562 -- iter: 224/257
[A[ATraining Step: 125  | total loss: [1m[32m0.17946[0m[0m | time: 60.796s
[2K
| Adam | epoch: 014 | loss: 0.17946 - acc: 0.9574 -- iter: 256/257
[A[ATraining Step: 126  | total loss: [1m[32m0.17620[0m[0m | time: 74.530s
[2K
| Adam | epoch: 014 | loss: 0.17620 - acc: 0.9586 | val_loss: 3.95551 - val_acc: 0.5802 -- iter: 257/257
--
Training Step: 127  | total loss: [1m[32m0.17148[0m[0m | time: 10.070s
[2K
| Adam | epoch: 015 | loss: 0.17148 - acc: 0.9533 -- iter: 032/257
[A[ATraining Step: 128  | total loss: [1m[32m0.15627[0m[0m | time: 19.731s
[2K
| Adam | epoch: 015 | loss: 0.15627 - acc: 0.9580 -- iter: 064/257
[A[ATraining Step: 129  | total loss: [1m[32m0.14344[0m[0m | time: 20.330s
[2K
| Adam | epoch: 015 | loss: 0.14344 - acc: 0.9622 -- iter: 096/257
[A[ATraining Step: 130  | total loss: [1m[32m0.13213[0m[0m | time: 20.910s
[2K
| Adam | epoch: 015 | loss: 0.13213 - acc: 0.9660 -- iter: 128/257
[A[ATraining Step: 131  | total loss: [1m[32m0.12119[0m[0m | time: 30.712s
[2K
| Adam | epoch: 015 | loss: 0.12119 - acc: 0.9694 -- iter: 160/257
[A[ATraining Step: 132  | total loss: [1m[32m0.11304[0m[0m | time: 40.553s
[2K
| Adam | epoch: 015 | loss: 0.11304 - acc: 0.9693 -- iter: 192/257
[A[ATraining Step: 133  | total loss: [1m[32m0.10539[0m[0m | time: 50.223s
[2K
| Adam | epoch: 015 | loss: 0.10539 - acc: 0.9724 -- iter: 224/257
[A[ATraining Step: 134  | total loss: [1m[32m0.09769[0m[0m | time: 59.958s
[2K
| Adam | epoch: 015 | loss: 0.09769 - acc: 0.9752 -- iter: 256/257
[A[ATraining Step: 135  | total loss: [1m[32m0.09321[0m[0m | time: 73.321s
[2K
| Adam | epoch: 015 | loss: 0.09321 - acc: 0.9745 | val_loss: 1.23519 - val_acc: 0.7284 -- iter: 257/257
--
Validation AUC:0.8025030525030524
Validation AUPRC:0.7724402751022601
Test AUC:0.8660493827160494
Test AUPRC:0.8290079858813447
BestTestF1Score	0.81	0.65	0.83	0.79	0.83	30	8	37	6	0.85
BestTestMCCScore	0.81	0.65	0.83	0.79	0.83	30	8	37	6	0.85
BestTestAccuracyScore	0.81	0.65	0.83	0.79	0.83	30	8	37	6	0.85
BestValidationF1Score	0.77	0.56	0.78	0.76	0.79	31	10	32	8	0.85
BestValidationMCC	0.77	0.56	0.78	0.76	0.79	31	10	32	8	0.85
BestValidationAccuracy	0.77	0.56	0.78	0.76	0.79	31	10	32	8	0.85
TestPredictions (Threshold:0.85)
CHEMBL354848,TP,ACT,1.0	CHEMBL1088155,TN,INACT,0.7900000214576721	CHEMBL55968,TP,ACT,1.0	CHEMBL141167,TP,ACT,1.0	CHEMBL3770071,TN,INACT,0.009999999776482582	CHEMBL260535,TN,INACT,0.2800000011920929	CHEMBL355845,TP,ACT,0.9900000095367432	CHEMBL481123,TN,INACT,0.0	CHEMBL220731,TN,INACT,0.019999999552965164	CHEMBL1080547,TP,ACT,1.0	CHEMBL141352,TP,ACT,1.0	CHEMBL55014,TP,ACT,1.0	CHEMBL517712,FN,ACT,0.550000011920929	CHEMBL1594358,TN,INACT,0.0	CHEMBL553263,TP,ACT,1.0	CHEMBL127513,TP,ACT,0.9900000095367432	CHEMBL555107,TP,ACT,1.0	CHEMBL435071,TP,ACT,0.9900000095367432	CHEMBL378404,FP,INACT,0.9800000190734863	CHEMBL57696,TP,ACT,1.0	CHEMBL460808,TN,INACT,0.12999999523162842	CHEMBL3359486,TP,ACT,1.0	CHEMBL229000,TN,INACT,0.5199999809265137	CHEMBL105874,FN,ACT,0.10000000149011612	CHEMBL426410,TN,INACT,0.0	CHEMBL354850,TP,ACT,1.0	CHEMBL230006,TN,INACT,0.009999999776482582	CHEMBL3359485,TP,ACT,1.0	CHEMBL3087797,TN,INACT,0.0	CHEMBL278020,TN,INACT,0.009999999776482582	CHEMBL3359477,TP,ACT,0.9900000095367432	CHEMBL1081085,FP,INACT,1.0	CHEMBL374758,TN,INACT,0.009999999776482582	CHEMBL2159422,FP,INACT,1.0	CHEMBL50,TN,INACT,0.009999999776482582	CHEMBL538069,TP,ACT,1.0	CHEMBL3763832,TN,INACT,0.6499999761581421	CHEMBL3335064,FP,INACT,1.0	CHEMBL636,FN,ACT,0.09000000357627869	CHEMBL3087801,TN,INACT,0.6299999952316284	CHEMBL3087800,TN,INACT,0.009999999776482582	CHEMBL1376358,TN,INACT,0.4300000071525574	CHEMBL3087807,TN,INACT,0.03999999910593033	CHEMBL3764900,TN,INACT,0.05999999865889549	CHEMBL3260186,TN,INACT,0.5	CHEMBL3359496,TP,ACT,1.0	CHEMBL1271536,TN,INACT,0.019999999552965164	CHEMBL2160217,TN,INACT,0.009999999776482582	CHEMBL3087798,TN,INACT,0.25	CHEMBL452833,FP,INACT,0.8999999761581421	CHEMBL54058,TP,ACT,1.0	CHEMBL1912064,FP,INACT,0.9900000095367432	CHEMBL3605436,TN,INACT,0.17000000178813934	CHEMBL145713,FN,ACT,0.7099999785423279	CHEMBL555358,TP,ACT,0.9900000095367432	CHEMBL339630,TP,ACT,0.9900000095367432	CHEMBL3289929,TN,INACT,0.0	CHEMBL371523,TN,INACT,0.07999999821186066	CHEMBL325480,TN,INACT,0.0	CHEMBL411226,TN,INACT,0.20000000298023224	CHEMBL56818,TP,ACT,1.0	CHEMBL191386,TN,INACT,0.11999999731779099	CHEMBL444427,TN,INACT,0.009999999776482582	CHEMBL23838,FN,ACT,0.009999999776482582	CHEMBL3359475,TP,ACT,0.9700000286102295	CHEMBL141276,TP,ACT,1.0	CHEMBL478424,FP,INACT,1.0	CHEMBL524365,TP,ACT,1.0	CHEMBL3359472,TP,ACT,1.0	CHEMBL258468,TN,INACT,0.6399999856948853	CHEMBL3087804,FN,ACT,0.07000000029802322	CHEMBL3764065,TN,INACT,0.009999999776482582	CHEMBL544159,TP,ACT,0.8500000238418579	CHEMBL271389,TN,INACT,0.25	CHEMBL340621,TP,ACT,1.0	CHEMBL3359494,TP,ACT,0.9700000286102295	CHEMBL1081456,FP,INACT,1.0	CHEMBL3277220,TN,INACT,0.07999999821186066	CHEMBL2160225,TN,INACT,0.009999999776482582	CHEMBL543924,TP,ACT,0.8999999761581421	CHEMBL3338950,TN,INACT,0.029999999329447746	

