ImageNetInceptionV2 CHEMBL1844 adam 0.0005 5 0 0 0.8 False True
Number of active compounds :	875
Number of inactive compounds :	875
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL1844_adam_0.0005_5_0_0_0.8_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL1844_adam_0.0005_5_0.8/
---------------------------------
Training samples: 1112
Validation samples: 348
--
Training Step: 1  | time: 36.102s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 0032/1112
[A[ATraining Step: 2  | total loss: [1m[32m0.62445[0m[0m | time: 43.938s
[2K
| Adam | epoch: 001 | loss: 0.62445 - acc: 0.4219 -- iter: 0064/1112
[A[ATraining Step: 3  | total loss: [1m[32m0.78886[0m[0m | time: 51.754s
[2K
| Adam | epoch: 001 | loss: 0.78886 - acc: 0.4602 -- iter: 0096/1112
[A[ATraining Step: 4  | total loss: [1m[32m0.86008[0m[0m | time: 59.536s
[2K
| Adam | epoch: 001 | loss: 0.86008 - acc: 0.4197 -- iter: 0128/1112
[A[ATraining Step: 5  | total loss: [1m[32m0.81226[0m[0m | time: 67.340s
[2K
| Adam | epoch: 001 | loss: 0.81226 - acc: 0.4969 -- iter: 0160/1112
[A[ATraining Step: 6  | total loss: [1m[32m0.74348[0m[0m | time: 75.127s
[2K
| Adam | epoch: 001 | loss: 0.74348 - acc: 0.5793 -- iter: 0192/1112
[A[ATraining Step: 7  | total loss: [1m[32m0.80518[0m[0m | time: 82.888s
[2K
| Adam | epoch: 001 | loss: 0.80518 - acc: 0.4942 -- iter: 0224/1112
[A[ATraining Step: 8  | total loss: [1m[32m0.69964[0m[0m | time: 90.642s
[2K
| Adam | epoch: 001 | loss: 0.69964 - acc: 0.6029 -- iter: 0256/1112
[A[ATraining Step: 9  | total loss: [1m[32m0.67105[0m[0m | time: 98.451s
[2K
| Adam | epoch: 001 | loss: 0.67105 - acc: 0.6312 -- iter: 0288/1112
[A[ATraining Step: 10  | total loss: [1m[32m0.72402[0m[0m | time: 106.096s
[2K
| Adam | epoch: 001 | loss: 0.72402 - acc: 0.5812 -- iter: 0320/1112
[A[ATraining Step: 11  | total loss: [1m[32m0.68190[0m[0m | time: 113.856s
[2K
| Adam | epoch: 001 | loss: 0.68190 - acc: 0.6316 -- iter: 0352/1112
[A[ATraining Step: 12  | total loss: [1m[32m0.70478[0m[0m | time: 121.768s
[2K
| Adam | epoch: 001 | loss: 0.70478 - acc: 0.5864 -- iter: 0384/1112
[A[ATraining Step: 13  | total loss: [1m[32m0.65801[0m[0m | time: 129.532s
[2K
| Adam | epoch: 001 | loss: 0.65801 - acc: 0.6699 -- iter: 0416/1112
[A[ATraining Step: 14  | total loss: [1m[32m0.63234[0m[0m | time: 137.342s
[2K
| Adam | epoch: 001 | loss: 0.63234 - acc: 0.6771 -- iter: 0448/1112
[A[ATraining Step: 15  | total loss: [1m[32m0.61600[0m[0m | time: 145.047s
[2K
| Adam | epoch: 001 | loss: 0.61600 - acc: 0.6689 -- iter: 0480/1112
[A[ATraining Step: 16  | total loss: [1m[32m0.59424[0m[0m | time: 152.665s
[2K
| Adam | epoch: 001 | loss: 0.59424 - acc: 0.6993 -- iter: 0512/1112
[A[ATraining Step: 17  | total loss: [1m[32m0.57050[0m[0m | time: 160.379s
[2K
| Adam | epoch: 001 | loss: 0.57050 - acc: 0.7176 -- iter: 0544/1112
[A[ATraining Step: 18  | total loss: [1m[32m0.57834[0m[0m | time: 168.070s
[2K
| Adam | epoch: 001 | loss: 0.57834 - acc: 0.6855 -- iter: 0576/1112
[A[ATraining Step: 19  | total loss: [1m[32m0.57564[0m[0m | time: 175.775s
[2K
| Adam | epoch: 001 | loss: 0.57564 - acc: 0.7174 -- iter: 0608/1112
[A[ATraining Step: 20  | total loss: [1m[32m0.59211[0m[0m | time: 183.542s
[2K
| Adam | epoch: 001 | loss: 0.59211 - acc: 0.7078 -- iter: 0640/1112
[A[ATraining Step: 21  | total loss: [1m[32m0.61538[0m[0m | time: 191.252s
[2K
| Adam | epoch: 001 | loss: 0.61538 - acc: 0.7015 -- iter: 0672/1112
[A[ATraining Step: 22  | total loss: [1m[32m0.60367[0m[0m | time: 199.020s
[2K
| Adam | epoch: 001 | loss: 0.60367 - acc: 0.7067 -- iter: 0704/1112
[A[ATraining Step: 23  | total loss: [1m[32m0.57247[0m[0m | time: 206.698s
[2K
| Adam | epoch: 001 | loss: 0.57247 - acc: 0.7102 -- iter: 0736/1112
[A[ATraining Step: 24  | total loss: [1m[32m0.60963[0m[0m | time: 214.451s
[2K
| Adam | epoch: 001 | loss: 0.60963 - acc: 0.6862 -- iter: 0768/1112
[A[ATraining Step: 25  | total loss: [1m[32m0.58619[0m[0m | time: 222.256s
[2K
| Adam | epoch: 001 | loss: 0.58619 - acc: 0.7121 -- iter: 0800/1112
[A[ATraining Step: 26  | total loss: [1m[32m0.58003[0m[0m | time: 229.973s
[2K
| Adam | epoch: 001 | loss: 0.58003 - acc: 0.7139 -- iter: 0832/1112
[A[ATraining Step: 27  | total loss: [1m[32m0.57695[0m[0m | time: 237.734s
[2K
| Adam | epoch: 001 | loss: 0.57695 - acc: 0.7151 -- iter: 0864/1112
[A[ATraining Step: 28  | total loss: [1m[32m0.51875[0m[0m | time: 245.426s
[2K
| Adam | epoch: 001 | loss: 0.51875 - acc: 0.7395 -- iter: 0896/1112
[A[ATraining Step: 29  | total loss: [1m[32m0.48806[0m[0m | time: 253.343s
[2K
| Adam | epoch: 001 | loss: 0.48806 - acc: 0.7572 -- iter: 0928/1112
[A[ATraining Step: 30  | total loss: [1m[32m0.49984[0m[0m | time: 261.110s
[2K
| Adam | epoch: 001 | loss: 0.49984 - acc: 0.7407 -- iter: 0960/1112
[A[ATraining Step: 31  | total loss: [1m[32m0.49357[0m[0m | time: 269.025s
[2K
| Adam | epoch: 001 | loss: 0.49357 - acc: 0.7501 -- iter: 0992/1112
[A[ATraining Step: 32  | total loss: [1m[32m0.50328[0m[0m | time: 276.755s
[2K
| Adam | epoch: 001 | loss: 0.50328 - acc: 0.7430 -- iter: 1024/1112
[A[ATraining Step: 33  | total loss: [1m[32m0.51064[0m[0m | time: 284.551s
[2K
| Adam | epoch: 001 | loss: 0.51064 - acc: 0.7377 -- iter: 1056/1112
[A[ATraining Step: 34  | total loss: [1m[32m0.46675[0m[0m | time: 292.286s
[2K
| Adam | epoch: 001 | loss: 0.46675 - acc: 0.7805 -- iter: 1088/1112
[A[ATraining Step: 35  | total loss: [1m[32m0.49983[0m[0m | time: 319.423s
[2K
| Adam | epoch: 001 | loss: 0.49983 - acc: 0.7610 | val_loss: 0.70977 - val_acc: 0.5230 -- iter: 1112/1112
--
Training Step: 36  | total loss: [1m[32m0.49445[0m[0m | time: 6.184s
[2K
| Adam | epoch: 002 | loss: 0.49445 - acc: 0.7588 -- iter: 0032/1112
[A[ATraining Step: 37  | total loss: [1m[32m0.46227[0m[0m | time: 13.770s
[2K
| Adam | epoch: 002 | loss: 0.46227 - acc: 0.7820 -- iter: 0064/1112
[A[ATraining Step: 38  | total loss: [1m[32m0.44285[0m[0m | time: 21.513s
[2K
| Adam | epoch: 002 | loss: 0.44285 - acc: 0.8002 -- iter: 0096/1112
[A[ATraining Step: 39  | total loss: [1m[32m0.48586[0m[0m | time: 29.355s
[2K
| Adam | epoch: 002 | loss: 0.48586 - acc: 0.7786 -- iter: 0128/1112
[A[ATraining Step: 40  | total loss: [1m[32m0.45027[0m[0m | time: 36.995s
[2K
| Adam | epoch: 002 | loss: 0.45027 - acc: 0.7967 -- iter: 0160/1112
[A[ATraining Step: 41  | total loss: [1m[32m0.44223[0m[0m | time: 44.836s
[2K
| Adam | epoch: 002 | loss: 0.44223 - acc: 0.8111 -- iter: 0192/1112
[A[ATraining Step: 42  | total loss: [1m[32m0.43125[0m[0m | time: 52.524s
[2K
| Adam | epoch: 002 | loss: 0.43125 - acc: 0.8282 -- iter: 0224/1112
[A[ATraining Step: 43  | total loss: [1m[32m0.41496[0m[0m | time: 60.061s
[2K
| Adam | epoch: 002 | loss: 0.41496 - acc: 0.8254 -- iter: 0256/1112
[A[ATraining Step: 44  | total loss: [1m[32m0.42521[0m[0m | time: 67.894s
[2K
| Adam | epoch: 002 | loss: 0.42521 - acc: 0.8178 -- iter: 0288/1112
[A[ATraining Step: 45  | total loss: [1m[32m0.45097[0m[0m | time: 75.691s
[2K
| Adam | epoch: 002 | loss: 0.45097 - acc: 0.8116 -- iter: 0320/1112
[A[ATraining Step: 46  | total loss: [1m[32m0.41666[0m[0m | time: 83.410s
[2K
| Adam | epoch: 002 | loss: 0.41666 - acc: 0.8274 -- iter: 0352/1112
[A[ATraining Step: 47  | total loss: [1m[32m0.38651[0m[0m | time: 91.251s
[2K
| Adam | epoch: 002 | loss: 0.38651 - acc: 0.8454 -- iter: 0384/1112
[A[ATraining Step: 48  | total loss: [1m[32m0.39402[0m[0m | time: 98.986s
[2K
| Adam | epoch: 002 | loss: 0.39402 - acc: 0.8250 -- iter: 0416/1112
[A[ATraining Step: 49  | total loss: [1m[32m0.37579[0m[0m | time: 106.549s
[2K
| Adam | epoch: 002 | loss: 0.37579 - acc: 0.8428 -- iter: 0448/1112
[A[ATraining Step: 50  | total loss: [1m[32m0.36545[0m[0m | time: 114.327s
[2K
| Adam | epoch: 002 | loss: 0.36545 - acc: 0.8478 -- iter: 0480/1112
[A[ATraining Step: 51  | total loss: [1m[32m0.36352[0m[0m | time: 121.949s
[2K
| Adam | epoch: 002 | loss: 0.36352 - acc: 0.8376 -- iter: 0512/1112
[A[ATraining Step: 52  | total loss: [1m[32m0.38595[0m[0m | time: 129.634s
[2K
| Adam | epoch: 002 | loss: 0.38595 - acc: 0.8245 -- iter: 0544/1112
[A[ATraining Step: 53  | total loss: [1m[32m0.38488[0m[0m | time: 137.385s
[2K
| Adam | epoch: 002 | loss: 0.38488 - acc: 0.8227 -- iter: 0576/1112
[A[ATraining Step: 54  | total loss: [1m[32m0.37122[0m[0m | time: 145.067s
[2K
| Adam | epoch: 002 | loss: 0.37122 - acc: 0.8348 -- iter: 0608/1112
[A[ATraining Step: 55  | total loss: [1m[32m0.36658[0m[0m | time: 152.771s
[2K
| Adam | epoch: 002 | loss: 0.36658 - acc: 0.8406 -- iter: 0640/1112
[A[ATraining Step: 56  | total loss: [1m[32m0.35939[0m[0m | time: 160.441s
[2K
| Adam | epoch: 002 | loss: 0.35939 - acc: 0.8410 -- iter: 0672/1112
[A[ATraining Step: 57  | total loss: [1m[32m0.36483[0m[0m | time: 168.373s
[2K
| Adam | epoch: 002 | loss: 0.36483 - acc: 0.8371 -- iter: 0704/1112
[A[ATraining Step: 58  | total loss: [1m[32m0.37598[0m[0m | time: 176.068s
[2K
| Adam | epoch: 002 | loss: 0.37598 - acc: 0.8295 -- iter: 0736/1112
[A[ATraining Step: 59  | total loss: [1m[32m0.36290[0m[0m | time: 183.761s
[2K
| Adam | epoch: 002 | loss: 0.36290 - acc: 0.8356 -- iter: 0768/1112
[A[ATraining Step: 60  | total loss: [1m[32m0.35094[0m[0m | time: 191.373s
[2K
| Adam | epoch: 002 | loss: 0.35094 - acc: 0.8491 -- iter: 0800/1112
[A[ATraining Step: 61  | total loss: [1m[32m0.34924[0m[0m | time: 199.065s
[2K
| Adam | epoch: 002 | loss: 0.34924 - acc: 0.8484 -- iter: 0832/1112
[A[ATraining Step: 62  | total loss: [1m[32m0.35212[0m[0m | time: 206.856s
[2K
| Adam | epoch: 002 | loss: 0.35212 - acc: 0.8478 -- iter: 0864/1112
[A[ATraining Step: 63  | total loss: [1m[32m0.36838[0m[0m | time: 214.553s
[2K
| Adam | epoch: 002 | loss: 0.36838 - acc: 0.8393 -- iter: 0896/1112
[A[ATraining Step: 64  | total loss: [1m[32m0.35639[0m[0m | time: 222.302s
[2K
| Adam | epoch: 002 | loss: 0.35639 - acc: 0.8477 -- iter: 0928/1112
[A[ATraining Step: 65  | total loss: [1m[32m0.34981[0m[0m | time: 229.993s
[2K
| Adam | epoch: 002 | loss: 0.34981 - acc: 0.8549 -- iter: 0960/1112
[A[ATraining Step: 66  | total loss: [1m[32m0.35439[0m[0m | time: 237.700s
[2K
| Adam | epoch: 002 | loss: 0.35439 - acc: 0.8460 -- iter: 0992/1112
[A[ATraining Step: 67  | total loss: [1m[32m0.35478[0m[0m | time: 245.344s
[2K
| Adam | epoch: 002 | loss: 0.35478 - acc: 0.8495 -- iter: 1024/1112
[A[ATraining Step: 68  | total loss: [1m[32m0.34734[0m[0m | time: 252.938s
[2K
| Adam | epoch: 002 | loss: 0.34734 - acc: 0.8525 -- iter: 1056/1112
[A[ATraining Step: 69  | total loss: [1m[32m0.36269[0m[0m | time: 260.656s
[2K
| Adam | epoch: 002 | loss: 0.36269 - acc: 0.8442 -- iter: 1088/1112
[A[ATraining Step: 70  | total loss: [1m[32m0.36296[0m[0m | time: 283.512s
[2K
| Adam | epoch: 002 | loss: 0.36296 - acc: 0.8441 | val_loss: 1.21672 - val_acc: 0.5575 -- iter: 1112/1112
--
Training Step: 71  | total loss: [1m[32m0.38176[0m[0m | time: 6.217s
[2K
| Adam | epoch: 003 | loss: 0.38176 - acc: 0.8334 -- iter: 0032/1112
[A[ATraining Step: 72  | total loss: [1m[32m0.36114[0m[0m | time: 12.417s
[2K
| Adam | epoch: 003 | loss: 0.36114 - acc: 0.8474 -- iter: 0064/1112
[A[ATraining Step: 73  | total loss: [1m[32m0.33413[0m[0m | time: 20.004s
[2K
| Adam | epoch: 003 | loss: 0.33413 - acc: 0.8644 -- iter: 0096/1112
[A[ATraining Step: 74  | total loss: [1m[32m0.33516[0m[0m | time: 27.617s
[2K
| Adam | epoch: 003 | loss: 0.33516 - acc: 0.8656 -- iter: 0128/1112
[A[ATraining Step: 75  | total loss: [1m[32m0.34799[0m[0m | time: 35.121s
[2K
| Adam | epoch: 003 | loss: 0.34799 - acc: 0.8530 -- iter: 0160/1112
[A[ATraining Step: 76  | total loss: [1m[32m0.34397[0m[0m | time: 42.853s
[2K
| Adam | epoch: 003 | loss: 0.34397 - acc: 0.8554 -- iter: 0192/1112
[A[ATraining Step: 77  | total loss: [1m[32m0.33002[0m[0m | time: 50.582s
[2K
| Adam | epoch: 003 | loss: 0.33002 - acc: 0.8608 -- iter: 0224/1112
[A[ATraining Step: 78  | total loss: [1m[32m0.31069[0m[0m | time: 58.438s
[2K
| Adam | epoch: 003 | loss: 0.31069 - acc: 0.8721 -- iter: 0256/1112
[A[ATraining Step: 79  | total loss: [1m[32m0.30904[0m[0m | time: 66.284s
[2K
| Adam | epoch: 003 | loss: 0.30904 - acc: 0.8691 -- iter: 0288/1112
[A[ATraining Step: 80  | total loss: [1m[32m0.29003[0m[0m | time: 74.007s
[2K
| Adam | epoch: 003 | loss: 0.29003 - acc: 0.8793 -- iter: 0320/1112
[A[ATraining Step: 81  | total loss: [1m[32m0.27297[0m[0m | time: 81.847s
[2K
| Adam | epoch: 003 | loss: 0.27297 - acc: 0.8884 -- iter: 0352/1112
[A[ATraining Step: 82  | total loss: [1m[32m0.25691[0m[0m | time: 89.473s
[2K
| Adam | epoch: 003 | loss: 0.25691 - acc: 0.8964 -- iter: 0384/1112
[A[ATraining Step: 83  | total loss: [1m[32m0.26138[0m[0m | time: 97.052s
[2K
| Adam | epoch: 003 | loss: 0.26138 - acc: 0.8974 -- iter: 0416/1112
[A[ATraining Step: 84  | total loss: [1m[32m0.24314[0m[0m | time: 104.893s
[2K
| Adam | epoch: 003 | loss: 0.24314 - acc: 0.9077 -- iter: 0448/1112
[A[ATraining Step: 85  | total loss: [1m[32m0.23031[0m[0m | time: 112.460s
[2K
| Adam | epoch: 003 | loss: 0.23031 - acc: 0.9106 -- iter: 0480/1112
[A[ATraining Step: 86  | total loss: [1m[32m0.22276[0m[0m | time: 120.194s
[2K
| Adam | epoch: 003 | loss: 0.22276 - acc: 0.9133 -- iter: 0512/1112
[A[ATraining Step: 87  | total loss: [1m[32m0.26472[0m[0m | time: 127.949s
[2K
| Adam | epoch: 003 | loss: 0.26472 - acc: 0.9032 -- iter: 0544/1112
[A[ATraining Step: 88  | total loss: [1m[32m0.26363[0m[0m | time: 135.617s
[2K
| Adam | epoch: 003 | loss: 0.26363 - acc: 0.9004 -- iter: 0576/1112
[A[ATraining Step: 89  | total loss: [1m[32m0.27632[0m[0m | time: 143.247s
[2K
| Adam | epoch: 003 | loss: 0.27632 - acc: 0.8947 -- iter: 0608/1112
[A[ATraining Step: 90  | total loss: [1m[32m0.26574[0m[0m | time: 150.743s
[2K
| Adam | epoch: 003 | loss: 0.26574 - acc: 0.8990 -- iter: 0640/1112
[A[ATraining Step: 91  | total loss: [1m[32m0.26867[0m[0m | time: 158.482s
[2K
| Adam | epoch: 003 | loss: 0.26867 - acc: 0.8872 -- iter: 0672/1112
[A[ATraining Step: 92  | total loss: [1m[32m0.27447[0m[0m | time: 166.261s
[2K
| Adam | epoch: 003 | loss: 0.27447 - acc: 0.8860 -- iter: 0704/1112
[A[ATraining Step: 93  | total loss: [1m[32m0.28383[0m[0m | time: 173.813s
[2K
| Adam | epoch: 003 | loss: 0.28383 - acc: 0.8880 -- iter: 0736/1112
[A[ATraining Step: 94  | total loss: [1m[32m0.28929[0m[0m | time: 181.599s
[2K
| Adam | epoch: 003 | loss: 0.28929 - acc: 0.8805 -- iter: 0768/1112
[A[ATraining Step: 95  | total loss: [1m[32m0.28861[0m[0m | time: 189.277s
[2K
| Adam | epoch: 003 | loss: 0.28861 - acc: 0.8768 -- iter: 0800/1112
[A[ATraining Step: 96  | total loss: [1m[32m0.28407[0m[0m | time: 197.028s
[2K
| Adam | epoch: 003 | loss: 0.28407 - acc: 0.8798 -- iter: 0832/1112
[A[ATraining Step: 97  | total loss: [1m[32m0.28283[0m[0m | time: 204.748s
[2K
| Adam | epoch: 003 | loss: 0.28283 - acc: 0.8793 -- iter: 0864/1112
[A[ATraining Step: 98  | total loss: [1m[32m0.27145[0m[0m | time: 212.411s
[2K
| Adam | epoch: 003 | loss: 0.27145 - acc: 0.8851 -- iter: 0896/1112
[A[ATraining Step: 99  | total loss: [1m[32m0.27946[0m[0m | time: 219.973s
[2K
| Adam | epoch: 003 | loss: 0.27946 - acc: 0.8747 -- iter: 0928/1112
[A[ATraining Step: 100  | total loss: [1m[32m0.26603[0m[0m | time: 227.676s
[2K
| Adam | epoch: 003 | loss: 0.26603 - acc: 0.8810 -- iter: 0960/1112
[A[ATraining Step: 101  | total loss: [1m[32m0.26370[0m[0m | time: 235.390s
[2K
| Adam | epoch: 003 | loss: 0.26370 - acc: 0.8835 -- iter: 0992/1112
[A[ATraining Step: 102  | total loss: [1m[32m0.26195[0m[0m | time: 242.974s
[2K
| Adam | epoch: 003 | loss: 0.26195 - acc: 0.8858 -- iter: 1024/1112
[A[ATraining Step: 103  | total loss: [1m[32m0.25115[0m[0m | time: 250.695s
[2K
| Adam | epoch: 003 | loss: 0.25115 - acc: 0.8910 -- iter: 1056/1112
[A[ATraining Step: 104  | total loss: [1m[32m0.24156[0m[0m | time: 258.445s
[2K
| Adam | epoch: 003 | loss: 0.24156 - acc: 0.8987 -- iter: 1088/1112
[A[ATraining Step: 105  | total loss: [1m[32m0.23964[0m[0m | time: 281.163s
[2K
| Adam | epoch: 003 | loss: 0.23964 - acc: 0.8964 | val_loss: 3.74347 - val_acc: 0.4684 -- iter: 1112/1112
--
Training Step: 106  | total loss: [1m[32m0.22717[0m[0m | time: 7.658s
[2K
| Adam | epoch: 004 | loss: 0.22717 - acc: 0.9036 -- iter: 0032/1112
[A[ATraining Step: 107  | total loss: [1m[32m0.24101[0m[0m | time: 13.733s
[2K
| Adam | epoch: 004 | loss: 0.24101 - acc: 0.8945 -- iter: 0064/1112
[A[ATraining Step: 108  | total loss: [1m[32m0.23067[0m[0m | time: 19.877s
[2K
| Adam | epoch: 004 | loss: 0.23067 - acc: 0.9009 -- iter: 0096/1112
[A[ATraining Step: 109  | total loss: [1m[32m0.21632[0m[0m | time: 27.405s
[2K
| Adam | epoch: 004 | loss: 0.21632 - acc: 0.9108 -- iter: 0128/1112
[A[ATraining Step: 110  | total loss: [1m[32m0.23725[0m[0m | time: 35.227s
[2K
| Adam | epoch: 004 | loss: 0.23725 - acc: 0.8978 -- iter: 0160/1112
[A[ATraining Step: 111  | total loss: [1m[32m0.25479[0m[0m | time: 42.880s
[2K
| Adam | epoch: 004 | loss: 0.25479 - acc: 0.8893 -- iter: 0192/1112
[A[ATraining Step: 112  | total loss: [1m[32m0.25727[0m[0m | time: 50.652s
[2K
| Adam | epoch: 004 | loss: 0.25727 - acc: 0.8910 -- iter: 0224/1112
[A[ATraining Step: 113  | total loss: [1m[32m0.25422[0m[0m | time: 58.334s
[2K
| Adam | epoch: 004 | loss: 0.25422 - acc: 0.8894 -- iter: 0256/1112
[A[ATraining Step: 114  | total loss: [1m[32m0.24056[0m[0m | time: 66.015s
[2K
| Adam | epoch: 004 | loss: 0.24056 - acc: 0.9005 -- iter: 0288/1112
[A[ATraining Step: 115  | total loss: [1m[32m0.24045[0m[0m | time: 73.660s
[2K
| Adam | epoch: 004 | loss: 0.24045 - acc: 0.9010 -- iter: 0320/1112
[A[ATraining Step: 116  | total loss: [1m[32m0.25406[0m[0m | time: 81.492s
[2K
| Adam | epoch: 004 | loss: 0.25406 - acc: 0.9016 -- iter: 0352/1112
[A[ATraining Step: 117  | total loss: [1m[32m0.24662[0m[0m | time: 89.054s
[2K
| Adam | epoch: 004 | loss: 0.24662 - acc: 0.9052 -- iter: 0384/1112
[A[ATraining Step: 118  | total loss: [1m[32m0.23524[0m[0m | time: 96.775s
[2K
| Adam | epoch: 004 | loss: 0.23524 - acc: 0.9115 -- iter: 0416/1112
[A[ATraining Step: 119  | total loss: [1m[32m0.25416[0m[0m | time: 104.358s
[2K
| Adam | epoch: 004 | loss: 0.25416 - acc: 0.9079 -- iter: 0448/1112
[A[ATraining Step: 120  | total loss: [1m[32m0.25334[0m[0m | time: 112.216s
[2K
| Adam | epoch: 004 | loss: 0.25334 - acc: 0.9077 -- iter: 0480/1112
[A[ATraining Step: 121  | total loss: [1m[32m0.24737[0m[0m | time: 119.754s
[2K
| Adam | epoch: 004 | loss: 0.24737 - acc: 0.9107 -- iter: 0512/1112
[A[ATraining Step: 122  | total loss: [1m[32m0.25572[0m[0m | time: 127.393s
[2K
| Adam | epoch: 004 | loss: 0.25572 - acc: 0.9009 -- iter: 0544/1112
[A[ATraining Step: 123  | total loss: [1m[32m0.25342[0m[0m | time: 134.946s
[2K
| Adam | epoch: 004 | loss: 0.25342 - acc: 0.8983 -- iter: 0576/1112
[A[ATraining Step: 124  | total loss: [1m[32m0.24200[0m[0m | time: 142.606s
[2K
| Adam | epoch: 004 | loss: 0.24200 - acc: 0.9022 -- iter: 0608/1112
[A[ATraining Step: 125  | total loss: [1m[32m0.25374[0m[0m | time: 150.188s
[2K
| Adam | epoch: 004 | loss: 0.25374 - acc: 0.8964 -- iter: 0640/1112
[A[ATraining Step: 126  | total loss: [1m[32m0.24671[0m[0m | time: 157.967s
[2K
| Adam | epoch: 004 | loss: 0.24671 - acc: 0.9005 -- iter: 0672/1112
[A[ATraining Step: 127  | total loss: [1m[32m0.23968[0m[0m | time: 165.585s
[2K
| Adam | epoch: 004 | loss: 0.23968 - acc: 0.9042 -- iter: 0704/1112
[A[ATraining Step: 128  | total loss: [1m[32m0.23081[0m[0m | time: 173.292s
[2K
| Adam | epoch: 004 | loss: 0.23081 - acc: 0.9106 -- iter: 0736/1112
[A[ATraining Step: 129  | total loss: [1m[32m0.23107[0m[0m | time: 180.918s
[2K
| Adam | epoch: 004 | loss: 0.23107 - acc: 0.9102 -- iter: 0768/1112
[A[ATraining Step: 130  | total loss: [1m[32m0.22093[0m[0m | time: 188.439s
[2K
| Adam | epoch: 004 | loss: 0.22093 - acc: 0.9160 -- iter: 0800/1112
[A[ATraining Step: 131  | total loss: [1m[32m0.22992[0m[0m | time: 196.192s
[2K
| Adam | epoch: 004 | loss: 0.22992 - acc: 0.9119 -- iter: 0832/1112
[A[ATraining Step: 132  | total loss: [1m[32m0.26517[0m[0m | time: 203.769s
[2K
| Adam | epoch: 004 | loss: 0.26517 - acc: 0.8957 -- iter: 0864/1112
[A[ATraining Step: 133  | total loss: [1m[32m0.26076[0m[0m | time: 211.242s
[2K
| Adam | epoch: 004 | loss: 0.26076 - acc: 0.8968 -- iter: 0896/1112
[A[ATraining Step: 134  | total loss: [1m[32m0.25487[0m[0m | time: 218.858s
[2K
| Adam | epoch: 004 | loss: 0.25487 - acc: 0.8946 -- iter: 0928/1112
[A[ATraining Step: 135  | total loss: [1m[32m0.26070[0m[0m | time: 226.477s
[2K
| Adam | epoch: 004 | loss: 0.26070 - acc: 0.8958 -- iter: 0960/1112
[A[ATraining Step: 136  | total loss: [1m[32m0.24603[0m[0m | time: 234.192s
[2K
| Adam | epoch: 004 | loss: 0.24603 - acc: 0.9062 -- iter: 0992/1112
[A[ATraining Step: 137  | total loss: [1m[32m0.25220[0m[0m | time: 241.776s
[2K
| Adam | epoch: 004 | loss: 0.25220 - acc: 0.9031 -- iter: 1024/1112
[A[ATraining Step: 138  | total loss: [1m[32m0.24640[0m[0m | time: 249.485s
[2K
| Adam | epoch: 004 | loss: 0.24640 - acc: 0.9034 -- iter: 1056/1112
[A[ATraining Step: 139  | total loss: [1m[32m0.23829[0m[0m | time: 257.027s
[2K
| Adam | epoch: 004 | loss: 0.23829 - acc: 0.9037 -- iter: 1088/1112
[A[ATraining Step: 140  | total loss: [1m[32m0.23031[0m[0m | time: 279.539s
[2K
| Adam | epoch: 004 | loss: 0.23031 - acc: 0.9071 | val_loss: 4.51420 - val_acc: 0.4713 -- iter: 1112/1112
--
Training Step: 141  | total loss: [1m[32m0.22083[0m[0m | time: 7.543s
[2K
| Adam | epoch: 005 | loss: 0.22083 - acc: 0.9132 -- iter: 0032/1112
[A[ATraining Step: 142  | total loss: [1m[32m0.21463[0m[0m | time: 15.111s
[2K
| Adam | epoch: 005 | loss: 0.21463 - acc: 0.9157 -- iter: 0064/1112
[A[ATraining Step: 143  | total loss: [1m[32m0.20366[0m[0m | time: 21.203s
[2K
| Adam | epoch: 005 | loss: 0.20366 - acc: 0.9210 -- iter: 0096/1112
[A[ATraining Step: 144  | total loss: [1m[32m0.22878[0m[0m | time: 27.340s
[2K
| Adam | epoch: 005 | loss: 0.22878 - acc: 0.9122 -- iter: 0128/1112
[A[ATraining Step: 145  | total loss: [1m[32m0.21945[0m[0m | time: 35.047s
[2K
| Adam | epoch: 005 | loss: 0.21945 - acc: 0.9168 -- iter: 0160/1112
[A[ATraining Step: 146  | total loss: [1m[32m0.21509[0m[0m | time: 42.718s
[2K
| Adam | epoch: 005 | loss: 0.21509 - acc: 0.9158 -- iter: 0192/1112
[A[ATraining Step: 147  | total loss: [1m[32m0.21057[0m[0m | time: 50.316s
[2K
| Adam | epoch: 005 | loss: 0.21057 - acc: 0.9179 -- iter: 0224/1112
[A[ATraining Step: 148  | total loss: [1m[32m0.20004[0m[0m | time: 57.973s
[2K
| Adam | epoch: 005 | loss: 0.20004 - acc: 0.9199 -- iter: 0256/1112
[A[ATraining Step: 149  | total loss: [1m[32m0.21538[0m[0m | time: 65.636s
[2K
| Adam | epoch: 005 | loss: 0.21538 - acc: 0.9123 -- iter: 0288/1112
[A[ATraining Step: 150  | total loss: [1m[32m0.19727[0m[0m | time: 73.297s
[2K
| Adam | epoch: 005 | loss: 0.19727 - acc: 0.9211 -- iter: 0320/1112
[A[ATraining Step: 151  | total loss: [1m[32m0.19359[0m[0m | time: 80.929s
[2K
| Adam | epoch: 005 | loss: 0.19359 - acc: 0.9227 -- iter: 0352/1112
[A[ATraining Step: 152  | total loss: [1m[32m0.18487[0m[0m | time: 88.513s
[2K
| Adam | epoch: 005 | loss: 0.18487 - acc: 0.9273 -- iter: 0384/1112
[A[ATraining Step: 153  | total loss: [1m[32m0.18662[0m[0m | time: 96.123s
[2K
| Adam | epoch: 005 | loss: 0.18662 - acc: 0.9252 -- iter: 0416/1112
[A[ATraining Step: 154  | total loss: [1m[32m0.19841[0m[0m | time: 103.795s
[2K
| Adam | epoch: 005 | loss: 0.19841 - acc: 0.9202 -- iter: 0448/1112
[A[ATraining Step: 155  | total loss: [1m[32m0.19080[0m[0m | time: 111.463s
[2K
| Adam | epoch: 005 | loss: 0.19080 - acc: 0.9219 -- iter: 0480/1112
[A[ATraining Step: 156  | total loss: [1m[32m0.19650[0m[0m | time: 119.123s
[2K
| Adam | epoch: 005 | loss: 0.19650 - acc: 0.9203 -- iter: 0512/1112
[A[ATraining Step: 157  | total loss: [1m[32m0.18069[0m[0m | time: 126.697s
[2K
| Adam | epoch: 005 | loss: 0.18069 - acc: 0.9283 -- iter: 0544/1112
[A[ATraining Step: 158  | total loss: [1m[32m0.17123[0m[0m | time: 134.328s
[2K
| Adam | epoch: 005 | loss: 0.17123 - acc: 0.9292 -- iter: 0576/1112
[A[ATraining Step: 159  | total loss: [1m[32m0.16013[0m[0m | time: 142.028s
[2K
| Adam | epoch: 005 | loss: 0.16013 - acc: 0.9363 -- iter: 0608/1112
[A[ATraining Step: 160  | total loss: [1m[32m0.15661[0m[0m | time: 149.711s
[2K
| Adam | epoch: 005 | loss: 0.15661 - acc: 0.9364 -- iter: 0640/1112
[A[ATraining Step: 161  | total loss: [1m[32m0.14353[0m[0m | time: 157.300s
[2K
| Adam | epoch: 005 | loss: 0.14353 - acc: 0.9428 -- iter: 0672/1112
[A[ATraining Step: 162  | total loss: [1m[32m0.13274[0m[0m | time: 164.907s
[2K
| Adam | epoch: 005 | loss: 0.13274 - acc: 0.9485 -- iter: 0704/1112
[A[ATraining Step: 163  | total loss: [1m[32m0.16429[0m[0m | time: 172.726s
[2K
| Adam | epoch: 005 | loss: 0.16429 - acc: 0.9349 -- iter: 0736/1112
[A[ATraining Step: 164  | total loss: [1m[32m0.15342[0m[0m | time: 180.336s
[2K
| Adam | epoch: 005 | loss: 0.15342 - acc: 0.9383 -- iter: 0768/1112
[A[ATraining Step: 165  | total loss: [1m[32m0.20783[0m[0m | time: 188.075s
[2K
| Adam | epoch: 005 | loss: 0.20783 - acc: 0.9257 -- iter: 0800/1112
[A[ATraining Step: 166  | total loss: [1m[32m0.21401[0m[0m | time: 195.670s
[2K
| Adam | epoch: 005 | loss: 0.21401 - acc: 0.9206 -- iter: 0832/1112
[A[ATraining Step: 167  | total loss: [1m[32m0.20036[0m[0m | time: 203.284s
[2K
| Adam | epoch: 005 | loss: 0.20036 - acc: 0.9254 -- iter: 0864/1112
[A[ATraining Step: 168  | total loss: [1m[32m0.18793[0m[0m | time: 210.881s
[2K
| Adam | epoch: 005 | loss: 0.18793 - acc: 0.9267 -- iter: 0896/1112
[A[ATraining Step: 169  | total loss: [1m[32m0.19890[0m[0m | time: 218.419s
[2K
| Adam | epoch: 005 | loss: 0.19890 - acc: 0.9215 -- iter: 0928/1112
[A[ATraining Step: 170  | total loss: [1m[32m0.18766[0m[0m | time: 226.069s
[2K
| Adam | epoch: 005 | loss: 0.18766 - acc: 0.9231 -- iter: 0960/1112
[A[ATraining Step: 171  | total loss: [1m[32m0.17370[0m[0m | time: 233.722s
[2K
| Adam | epoch: 005 | loss: 0.17370 - acc: 0.9277 -- iter: 0992/1112
[A[ATraining Step: 172  | total loss: [1m[32m0.16100[0m[0m | time: 241.395s
[2K
| Adam | epoch: 005 | loss: 0.16100 - acc: 0.9349 -- iter: 1024/1112
[A[ATraining Step: 173  | total loss: [1m[32m0.16033[0m[0m | time: 249.102s
[2K
| Adam | epoch: 005 | loss: 0.16033 - acc: 0.9320 -- iter: 1056/1112
[A[ATraining Step: 174  | total loss: [1m[32m0.16488[0m[0m | time: 256.772s
[2K
| Adam | epoch: 005 | loss: 0.16488 - acc: 0.9294 -- iter: 1088/1112
[A[ATraining Step: 175  | total loss: [1m[32m0.18497[0m[0m | time: 279.407s
[2K
| Adam | epoch: 005 | loss: 0.18497 - acc: 0.9271 | val_loss: 1.54951 - val_acc: 0.5891 -- iter: 1112/1112
--
Validation AUC:0.9418573753612117
Validation AUPRC:0.935960756285928
Test AUC:0.9251862891207154
Test AUPRC:0.9306326236053417
BestTestF1Score	0.85	0.71	0.85	0.82	0.89	147	33	150	18	1.0
BestTestMCCScore	0.85	0.71	0.85	0.82	0.89	147	33	150	18	1.0
BestTestAccuracyScore	0.85	0.71	0.85	0.82	0.89	147	33	150	18	1.0
BestValidationF1Score	0.85	0.72	0.86	0.82	0.89	144	32	155	17	1.0
BestValidationMCC	0.85	0.72	0.86	0.82	0.89	144	32	155	17	1.0
BestValidationAccuracy	0.85	0.72	0.86	0.82	0.89	144	32	155	17	1.0
TestPredictions (Threshold:1.0)
CHEMBL3644208,TP,ACT,1.0	CHEMBL3644201,TP,ACT,1.0	CHEMBL552634,FP,INACT,1.0	CHEMBL453593,FP,INACT,1.0	CHEMBL345730,TN,INACT,0.3400000035762787	CHEMBL3644171,TP,ACT,1.0	CHEMBL507619,TP,ACT,1.0	CHEMBL258474,TP,ACT,1.0	CHEMBL269871,FP,INACT,1.0	CHEMBL1808239,TN,INACT,0.9599999785423279	CHEMBL3085980,TP,ACT,1.0	CHEMBL2337367,FP,INACT,1.0	CHEMBL3781939,FP,INACT,1.0	CHEMBL3678295,TP,ACT,1.0	CHEMBL1088599,TP,ACT,1.0	CHEMBL1170785,TP,ACT,1.0	CHEMBL356679,TN,INACT,0.9700000286102295	CHEMBL195437,TN,INACT,0.46000000834465027	CHEMBL1098561,TN,INACT,0.7200000286102295	CHEMBL559628,TN,INACT,0.9900000095367432	CHEMBL3128234,TN,INACT,0.9900000095367432	CHEMBL249574,TP,ACT,1.0	CHEMBL3586404,TP,ACT,1.0	CHEMBL246356,FP,INACT,1.0	CHEMBL2036728,FP,INACT,1.0	CHEMBL558752,TP,ACT,1.0	CHEMBL113985,FP,INACT,1.0	CHEMBL334891,TN,INACT,0.4099999964237213	CHEMBL45149,TN,INACT,0.8100000023841858	CHEMBL1095465,TN,INACT,0.9800000190734863	CHEMBL257153,TP,ACT,1.0	CHEMBL3644170,TP,ACT,1.0	CHEMBL461573,TP,ACT,1.0	CHEMBL3673478,TP,ACT,1.0	CHEMBL509044,TP,ACT,1.0	CHEMBL3629013,TN,INACT,0.699999988079071	CHEMBL18276,TN,INACT,0.9900000095367432	CHEMBL251603,TP,ACT,1.0	CHEMBL1908391,TP,ACT,1.0	CHEMBL279003,TN,INACT,0.9200000166893005	CHEMBL3358991,TN,INACT,0.8700000047683716	CHEMBL466496,TN,INACT,0.10000000149011612	CHEMBL2372113,TN,INACT,0.9800000190734863	CHEMBL481222,FN,ACT,0.9700000286102295	CHEMBL1241391,TN,INACT,0.3700000047683716	CHEMBL430845,TN,INACT,0.800000011920929	CHEMBL3642094,TP,ACT,1.0	CHEMBL1092013,TN,INACT,0.949999988079071	CHEMBL402337,TP,ACT,1.0	CHEMBL405681,TN,INACT,0.9599999785423279	CHEMBL3673472,TP,ACT,1.0	CHEMBL3644199,TP,ACT,1.0	CHEMBL3086171,TP,ACT,1.0	CHEMBL3642081,FN,ACT,0.9200000166893005	CHEMBL558475,FP,INACT,1.0	CHEMBL293251,TN,INACT,0.9200000166893005	CHEMBL429855,TP,ACT,1.0	CHEMBL211327,TN,INACT,0.9700000286102295	CHEMBL267213,FP,INACT,1.0	CHEMBL217090,FP,INACT,1.0	CHEMBL3098318,TN,INACT,0.019999999552965164	CHEMBL64459,TN,INACT,0.8799999952316284	CHEMBL500152,TN,INACT,0.9900000095367432	CHEMBL448117,TP,ACT,1.0	CHEMBL3673467,TP,ACT,1.0	CHEMBL572878,TP,ACT,1.0	CHEMBL479446,TP,ACT,1.0	CHEMBL3644156,TP,ACT,1.0	CHEMBL3085976,TP,ACT,1.0	CHEMBL1242977,TP,ACT,1.0	CHEMBL2088110,TN,INACT,0.9800000190734863	CHEMBL1253838,TN,INACT,0.9800000190734863	CHEMBL132089,TN,INACT,0.11999999731779099	CHEMBL3642062,TP,ACT,1.0	CHEMBL3673473,TP,ACT,1.0	CHEMBL377048,TN,INACT,0.8500000238418579	CHEMBL253027,TP,ACT,1.0	CHEMBL3644174,TP,ACT,1.0	CHEMBL458248,TN,INACT,0.9900000095367432	CHEMBL3642105,TP,ACT,1.0	CHEMBL1829271,TN,INACT,0.009999999776482582	CHEMBL3115494,TN,INACT,0.9399999976158142	CHEMBL591504,TN,INACT,0.9700000286102295	CHEMBL348834,TN,INACT,0.9900000095367432	CHEMBL2023478,TP,ACT,1.0	CHEMBL1242379,TN,INACT,0.9800000190734863	CHEMBL481223,TP,ACT,1.0	CHEMBL87080,TN,INACT,0.8700000047683716	CHEMBL3086165,TP,ACT,1.0	CHEMBL3780571,FP,INACT,1.0	CHEMBL226813,TN,INACT,0.4300000071525574	CHEMBL402429,FN,ACT,0.9700000286102295	CHEMBL3642069,TP,ACT,1.0	CHEMBL3644195,TP,ACT,1.0	CHEMBL2335379,TN,INACT,0.1599999964237213	CHEMBL255443,TP,ACT,1.0	CHEMBL479267,TP,ACT,1.0	CHEMBL1254465,TN,INACT,0.8999999761581421	CHEMBL343050,TN,INACT,0.7300000190734863	CHEMBL522834,TP,ACT,1.0	CHEMBL300335,FP,INACT,1.0	CHEMBL596808,TN,INACT,0.8299999833106995	CHEMBL346200,TN,INACT,0.5199999809265137	CHEMBL132006,TN,INACT,0.6700000166893005	CHEMBL457077,TN,INACT,0.8500000238418579	CHEMBL539942,TN,INACT,0.6299999952316284	CHEMBL510215,TP,ACT,1.0	CHEMBL444064,TP,ACT,1.0	CHEMBL3779960,FP,INACT,1.0	CHEMBL3644158,TP,ACT,1.0	CHEMBL517530,TP,ACT,1.0	CHEMBL477772,FN,ACT,0.9900000095367432	CHEMBL271460,TP,ACT,1.0	CHEMBL199865,FP,INACT,1.0	CHEMBL3085971,TP,ACT,1.0	CHEMBL2036727,TN,INACT,0.9900000095367432	CHEMBL3673487,TP,ACT,1.0	CHEMBL3673471,TP,ACT,1.0	CHEMBL79704,TN,INACT,0.3100000023841858	CHEMBL56697,TN,INACT,0.9900000095367432	CHEMBL1970317,TP,ACT,1.0	CHEMBL329642,TN,INACT,0.6100000143051147	CHEMBL2023802,TP,ACT,1.0	CHEMBL401038,TN,INACT,0.5099999904632568	CHEMBL3673462,TP,ACT,1.0	CHEMBL480455,TP,ACT,1.0	CHEMBL249987,FN,ACT,0.9300000071525574	CHEMBL2023492,TP,ACT,1.0	CHEMBL1242029,TN,INACT,0.8100000023841858	CHEMBL249751,TP,ACT,1.0	CHEMBL3644178,TP,ACT,1.0	CHEMBL86480,TN,INACT,0.800000011920929	CHEMBL332497,TN,INACT,0.9900000095367432	CHEMBL1940114,TP,ACT,1.0	CHEMBL1908397,FN,ACT,0.9300000071525574	CHEMBL3639723,TP,ACT,1.0	CHEMBL602645,TN,INACT,0.9300000071525574	CHEMBL3318030,TN,INACT,0.9800000190734863	CHEMBL480646,TP,ACT,1.0	CHEMBL3642088,TP,ACT,1.0	CHEMBL607707,TP,ACT,1.0	CHEMBL80810,TN,INACT,0.5799999833106995	CHEMBL3236668,FP,INACT,1.0	CHEMBL399914,TN,INACT,0.4300000071525574	CHEMBL215152,FN,ACT,0.9200000166893005	CHEMBL295528,TN,INACT,0.9800000190734863	CHEMBL1829273,TN,INACT,0.3799999952316284	CHEMBL3642077,TP,ACT,1.0	CHEMBL2335015,TN,INACT,0.9200000166893005	CHEMBL401691,TP,ACT,1.0	CHEMBL3644184,TP,ACT,1.0	CHEMBL489881,TP,ACT,1.0	CHEMBL485320,TN,INACT,0.23999999463558197	CHEMBL459470,TP,ACT,1.0	CHEMBL439233,TP,ACT,1.0	CHEMBL1094475,TN,INACT,0.18000000715255737	CHEMBL1940112,FN,ACT,0.9200000166893005	CHEMBL3673444,TP,ACT,1.0	CHEMBL2023118,TP,ACT,1.0	CHEMBL3086315,TP,ACT,1.0	CHEMBL336330,TN,INACT,0.6100000143051147	CHEMBL1807520,TN,INACT,0.949999988079071	CHEMBL249749,TP,ACT,1.0	CHEMBL357347,TN,INACT,0.9800000190734863	CHEMBL591457,TN,INACT,0.6200000047683716	CHEMBL381207,FN,ACT,0.6899999976158142	CHEMBL480417,TP,ACT,1.0	CHEMBL301845,TN,INACT,0.8999999761581421	CHEMBL2158302,TP,ACT,1.0	CHEMBL3109343,TP,ACT,1.0	CHEMBL89483,TN,INACT,0.18000000715255737	CHEMBL1829274,TN,INACT,0.38999998569488525	CHEMBL3086170,TP,ACT,1.0	CHEMBL271461,TP,ACT,1.0	CHEMBL128568,TN,INACT,0.10000000149011612	CHEMBL374206,TN,INACT,0.9800000190734863	CHEMBL1791360,TN,INACT,0.949999988079071	CHEMBL199299,TN,INACT,0.8999999761581421	CHEMBL347537,FN,ACT,0.9900000095367432	CHEMBL47202,TN,INACT,0.9800000190734863	CHEMBL3642103,TP,ACT,1.0	CHEMBL541265,TN,INACT,0.9900000095367432	CHEMBL398243,TN,INACT,0.3799999952316284	CHEMBL461365,TP,ACT,1.0	CHEMBL2206277,TP,ACT,1.0	CHEMBL1172877,TN,INACT,0.0	CHEMBL217092,FP,INACT,1.0	CHEMBL2023488,TP,ACT,1.0	CHEMBL2158228,TP,ACT,1.0	CHEMBL149458,TN,INACT,0.8999999761581421	CHEMBL511451,TN,INACT,0.4699999988079071	CHEMBL1956898,TN,INACT,0.1599999964237213	CHEMBL248930,TP,ACT,1.0	CHEMBL481596,FN,ACT,0.9900000095367432	CHEMBL254909,TP,ACT,1.0	CHEMBL103464,TN,INACT,0.27000001072883606	CHEMBL3644190,TP,ACT,1.0	CHEMBL3085828,TP,ACT,1.0	CHEMBL3644164,TP,ACT,1.0	CHEMBL80171,TN,INACT,0.6299999952316284	CHEMBL3644220,TP,ACT,1.0	CHEMBL3642083,TP,ACT,1.0	CHEMBL1908395,TP,ACT,1.0	CHEMBL304284,FP,INACT,1.0	CHEMBL230232,TN,INACT,0.9900000095367432	CHEMBL1172529,TP,ACT,1.0	CHEMBL132399,TN,INACT,0.6800000071525574	CHEMBL3358976,TN,INACT,0.9399999976158142	CHEMBL499067,TN,INACT,0.7200000286102295	CHEMBL117488,TN,INACT,0.5	CHEMBL395665,TN,INACT,0.7300000190734863	CHEMBL3358993,TN,INACT,0.9900000095367432	CHEMBL1242845,TN,INACT,0.9900000095367432	CHEMBL1087661,TP,ACT,1.0	CHEMBL2203435,TP,ACT,1.0	CHEMBL47660,TN,INACT,0.949999988079071	CHEMBL24828,FN,ACT,0.9800000190734863	CHEMBL273011,TP,ACT,1.0	CHEMBL436455,TN,INACT,0.699999988079071	CHEMBL295316,TN,INACT,0.8999999761581421	CHEMBL1956885,TN,INACT,0.5099999904632568	CHEMBL314021,TN,INACT,0.9800000190734863	CHEMBL245966,TN,INACT,0.9200000166893005	CHEMBL277697,TN,INACT,0.7799999713897705	CHEMBL2036868,TN,INACT,0.8600000143051147	CHEMBL3673454,TP,ACT,1.0	CHEMBL3237855,TN,INACT,0.20999999344348907	CHEMBL3644237,TP,ACT,1.0	CHEMBL1095776,TN,INACT,0.949999988079071	CHEMBL58142,TN,INACT,0.9399999976158142	CHEMBL472342,TP,ACT,1.0	CHEMBL521580,TP,ACT,1.0	CHEMBL3642060,TP,ACT,1.0	CHEMBL1829272,TN,INACT,0.41999998688697815	CHEMBL3673446,TP,ACT,1.0	CHEMBL70360,TN,INACT,0.8899999856948853	CHEMBL3086163,TP,ACT,1.0	CHEMBL598523,TN,INACT,0.699999988079071	CHEMBL3085986,TP,ACT,1.0	CHEMBL252436,TP,ACT,1.0	CHEMBL3128232,TN,INACT,0.5199999809265137	CHEMBL373882,FP,INACT,1.0	CHEMBL220444,TN,INACT,0.8999999761581421	CHEMBL85403,TN,INACT,0.05999999865889549	CHEMBL3642075,TP,ACT,1.0	CHEMBL1242660,TN,INACT,0.7900000214576721	CHEMBL3085833,TP,ACT,1.0	CHEMBL201865,TN,INACT,0.9900000095367432	CHEMBL1956893,TN,INACT,0.20999999344348907	CHEMBL3644203,TP,ACT,1.0	CHEMBL2047244,TN,INACT,0.9900000095367432	CHEMBL497350,TP,ACT,1.0	CHEMBL2216905,FP,INACT,1.0	CHEMBL246165,FP,INACT,1.0	CHEMBL398508,TP,ACT,1.0	CHEMBL247228,FN,ACT,0.25999999046325684	CHEMBL3644213,TP,ACT,1.0	CHEMBL556669,FP,INACT,1.0	CHEMBL399117,TP,ACT,1.0	CHEMBL1172530,TP,ACT,1.0	CHEMBL3678289,TP,ACT,1.0	CHEMBL414001,TP,ACT,1.0	CHEMBL520742,TP,ACT,1.0	CHEMBL406375,FN,ACT,0.3400000035762787	CHEMBL2206278,TP,ACT,1.0	CHEMBL124660,TP,ACT,1.0	CHEMBL521407,TP,ACT,1.0	CHEMBL480813,TP,ACT,1.0	CHEMBL3086161,TP,ACT,1.0	CHEMBL3355065,TP,ACT,1.0	CHEMBL556746,FP,INACT,1.0	CHEMBL3644217,TP,ACT,1.0	CHEMBL119385,FN,ACT,0.5099999904632568	CHEMBL3644239,TP,ACT,1.0	CHEMBL589847,TN,INACT,0.8799999952316284	CHEMBL1242209,TN,INACT,0.41999998688697815	CHEMBL444454,TP,ACT,1.0	CHEMBL425615,TN,INACT,0.9900000095367432	CHEMBL3673485,TP,ACT,1.0	CHEMBL79152,TN,INACT,0.23999999463558197	CHEMBL591440,TN,INACT,0.05000000074505806	CHEMBL396788,TN,INACT,0.6200000047683716	CHEMBL3689079,FP,INACT,1.0	CHEMBL589064,FP,INACT,1.0	CHEMBL516200,TP,ACT,1.0	CHEMBL162157,TN,INACT,0.8799999952316284	CHEMBL3353355,FP,INACT,1.0	CHEMBL3633278,TN,INACT,0.9800000190734863	CHEMBL3673455,TP,ACT,1.0	CHEMBL55993,TN,INACT,0.800000011920929	CHEMBL3086177,TP,ACT,1.0	CHEMBL336949,FP,INACT,1.0	CHEMBL555321,TN,INACT,0.20000000298023224	CHEMBL395080,TN,INACT,0.9100000262260437	CHEMBL3644206,TP,ACT,1.0	CHEMBL606027,TN,INACT,0.36000001430511475	CHEMBL3673452,TP,ACT,1.0	CHEMBL245949,FP,INACT,1.0	CHEMBL3642107,TP,ACT,1.0	CHEMBL1096415,TN,INACT,0.6600000262260437	CHEMBL80030,TN,INACT,0.9700000286102295	CHEMBL438610,TN,INACT,0.9599999785423279	CHEMBL430902,TN,INACT,0.9900000095367432	CHEMBL150825,TN,INACT,0.9100000262260437	CHEMBL56731,TN,INACT,0.6600000262260437	CHEMBL1761510,TP,ACT,1.0	CHEMBL89723,TN,INACT,0.949999988079071	CHEMBL328452,TN,INACT,0.8399999737739563	CHEMBL508375,TN,INACT,0.9900000095367432	CHEMBL450914,FN,ACT,0.9599999785423279	CHEMBL1096416,TN,INACT,0.6200000047683716	CHEMBL3678301,TP,ACT,1.0	CHEMBL76202,TN,INACT,0.6000000238418579	CHEMBL334333,TN,INACT,0.03999999910593033	CHEMBL304340,FP,INACT,1.0	CHEMBL480617,FN,ACT,0.9900000095367432	CHEMBL78802,TN,INACT,0.8399999737739563	CHEMBL2086746,TN,INACT,0.9800000190734863	CHEMBL3673468,TP,ACT,1.0	CHEMBL3353407,TN,INACT,0.9900000095367432	CHEMBL3644236,TP,ACT,1.0	CHEMBL2158296,TP,ACT,1.0	CHEMBL475051,FP,INACT,1.0	CHEMBL2381116,FP,INACT,1.0	CHEMBL249544,TP,ACT,1.0	CHEMBL443800,TN,INACT,0.46000000834465027	CHEMBL153624,TN,INACT,0.9599999785423279	CHEMBL310193,FP,INACT,1.0	CHEMBL2158223,TP,ACT,1.0	CHEMBL450929,TN,INACT,0.75	CHEMBL3086162,TP,ACT,1.0	CHEMBL451544,TN,INACT,0.28999999165534973	CHEMBL256712,TP,ACT,1.0	CHEMBL433805,TN,INACT,0.3799999952316284	CHEMBL250775,TP,ACT,1.0	CHEMBL253026,TP,ACT,1.0	CHEMBL249377,TP,ACT,1.0	CHEMBL80510,TN,INACT,0.8299999833106995	CHEMBL1828876,TN,INACT,0.36000001430511475	CHEMBL529663,TN,INACT,0.9399999976158142	CHEMBL17637,TN,INACT,0.9800000190734863	CHEMBL311318,TN,INACT,0.8600000143051147	CHEMBL40583,FP,INACT,1.0	CHEMBL165751,TN,INACT,0.33000001311302185	CHEMBL3086175,TP,ACT,1.0	CHEMBL1684370,TN,INACT,0.8299999833106995	CHEMBL3673459,TP,ACT,1.0	CHEMBL3425865,FN,ACT,0.6399999856948853	

