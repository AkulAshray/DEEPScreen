CNNModel CHEMBL1075323 adam 0.0005 15 32 0 0.8 False True
Number of active compounds :	112
Number of inactive compounds :	112
---------------------------------
Run id: CNNModel_CHEMBL1075323_adam_0.0005_15_32_0_0.8_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL1075323_adam_0.0005_15_32_0.8_True/
---------------------------------
Training samples: 137
Validation samples: 44
--
Training Step: 1  | time: 0.795s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/137
[A[ATraining Step: 2  | total loss: [1m[32m0.62379[0m[0m | time: 1.398s
[2K
| Adam | epoch: 001 | loss: 0.62379 - acc: 0.5344 -- iter: 064/137
[A[ATraining Step: 3  | total loss: [1m[32m0.68088[0m[0m | time: 2.029s
[2K
| Adam | epoch: 001 | loss: 0.68088 - acc: 0.4551 -- iter: 096/137
[A[ATraining Step: 4  | total loss: [1m[32m0.69018[0m[0m | time: 2.630s
[2K
| Adam | epoch: 001 | loss: 0.69018 - acc: 0.4888 -- iter: 128/137
[A[ATraining Step: 5  | total loss: [1m[32m0.69200[0m[0m | time: 3.868s
[2K
| Adam | epoch: 001 | loss: 0.69200 - acc: 0.5831 | val_loss: 0.69353 - val_acc: 0.4318 -- iter: 137/137
--
Training Step: 6  | total loss: [1m[32m0.69399[0m[0m | time: 0.228s
[2K
| Adam | epoch: 002 | loss: 0.69399 - acc: 0.4225 -- iter: 032/137
[A[ATraining Step: 7  | total loss: [1m[32m0.69424[0m[0m | time: 0.847s
[2K
| Adam | epoch: 002 | loss: 0.69424 - acc: 0.3690 -- iter: 064/137
[A[ATraining Step: 8  | total loss: [1m[32m0.69335[0m[0m | time: 1.448s
[2K
| Adam | epoch: 002 | loss: 0.69335 - acc: 0.5306 -- iter: 096/137
[A[ATraining Step: 9  | total loss: [1m[32m0.69279[0m[0m | time: 2.065s
[2K
| Adam | epoch: 002 | loss: 0.69279 - acc: 0.5475 -- iter: 128/137
[A[ATraining Step: 10  | total loss: [1m[32m0.69207[0m[0m | time: 3.706s
[2K
| Adam | epoch: 002 | loss: 0.69207 - acc: 0.5550 | val_loss: 0.68957 - val_acc: 0.5682 -- iter: 137/137
--
Training Step: 11  | total loss: [1m[32m0.69082[0m[0m | time: 0.222s
[2K
| Adam | epoch: 003 | loss: 0.69082 - acc: 0.5882 -- iter: 032/137
[A[ATraining Step: 12  | total loss: [1m[32m0.69950[0m[0m | time: 0.420s
[2K
| Adam | epoch: 003 | loss: 0.69950 - acc: 0.4235 -- iter: 064/137
[A[ATraining Step: 13  | total loss: [1m[32m0.70246[0m[0m | time: 1.026s
[2K
| Adam | epoch: 003 | loss: 0.70246 - acc: 0.3372 -- iter: 096/137
[A[ATraining Step: 14  | total loss: [1m[32m0.69753[0m[0m | time: 1.636s
[2K
| Adam | epoch: 003 | loss: 0.69753 - acc: 0.4422 -- iter: 128/137
[A[ATraining Step: 15  | total loss: [1m[32m0.69704[0m[0m | time: 3.257s
[2K
| Adam | epoch: 003 | loss: 0.69704 - acc: 0.4159 | val_loss: 0.69223 - val_acc: 0.5682 -- iter: 137/137
--
Training Step: 16  | total loss: [1m[32m0.69468[0m[0m | time: 0.621s
[2K
| Adam | epoch: 004 | loss: 0.69468 - acc: 0.5060 -- iter: 032/137
[A[ATraining Step: 17  | total loss: [1m[32m0.69423[0m[0m | time: 0.839s
[2K
| Adam | epoch: 004 | loss: 0.69423 - acc: 0.4814 -- iter: 064/137
[A[ATraining Step: 18  | total loss: [1m[32m0.69364[0m[0m | time: 1.048s
[2K
| Adam | epoch: 004 | loss: 0.69364 - acc: 0.5070 -- iter: 096/137
[A[ATraining Step: 19  | total loss: [1m[32m0.69319[0m[0m | time: 1.683s
[2K
| Adam | epoch: 004 | loss: 0.69319 - acc: 0.5232 -- iter: 128/137
[A[ATraining Step: 20  | total loss: [1m[32m0.69318[0m[0m | time: 3.309s
[2K
| Adam | epoch: 004 | loss: 0.69318 - acc: 0.5057 | val_loss: 0.69248 - val_acc: 0.5682 -- iter: 137/137
--
Training Step: 21  | total loss: [1m[32m0.69310[0m[0m | time: 0.618s
[2K
| Adam | epoch: 005 | loss: 0.69310 - acc: 0.5039 -- iter: 032/137
[A[ATraining Step: 22  | total loss: [1m[32m0.69304[0m[0m | time: 1.232s
[2K
| Adam | epoch: 005 | loss: 0.69304 - acc: 0.5028 -- iter: 064/137
[A[ATraining Step: 23  | total loss: [1m[32m0.69275[0m[0m | time: 1.443s
[2K
| Adam | epoch: 005 | loss: 0.69275 - acc: 0.5382 -- iter: 096/137
[A[ATraining Step: 24  | total loss: [1m[32m0.69227[0m[0m | time: 1.666s
[2K
| Adam | epoch: 005 | loss: 0.69227 - acc: 0.5744 -- iter: 128/137
[A[ATraining Step: 25  | total loss: [1m[32m0.69184[0m[0m | time: 3.280s
[2K
| Adam | epoch: 005 | loss: 0.69184 - acc: 0.5995 | val_loss: 0.69176 - val_acc: 0.5682 -- iter: 137/137
--
Training Step: 26  | total loss: [1m[32m0.69220[0m[0m | time: 0.611s
[2K
| Adam | epoch: 006 | loss: 0.69220 - acc: 0.5649 -- iter: 032/137
[A[ATraining Step: 27  | total loss: [1m[32m0.69235[0m[0m | time: 1.249s
[2K
| Adam | epoch: 006 | loss: 0.69235 - acc: 0.5322 -- iter: 064/137
[A[ATraining Step: 28  | total loss: [1m[32m0.69241[0m[0m | time: 1.861s
[2K
| Adam | epoch: 006 | loss: 0.69241 - acc: 0.5163 -- iter: 096/137
[A[ATraining Step: 29  | total loss: [1m[32m0.69231[0m[0m | time: 2.066s
[2K
| Adam | epoch: 006 | loss: 0.69231 - acc: 0.5123 -- iter: 128/137
[A[ATraining Step: 30  | total loss: [1m[32m0.69195[0m[0m | time: 3.289s
[2K
| Adam | epoch: 006 | loss: 0.69195 - acc: 0.5489 | val_loss: 0.69035 - val_acc: 0.5682 -- iter: 137/137
--
Training Step: 31  | total loss: [1m[32m0.69127[0m[0m | time: 0.616s
[2K
| Adam | epoch: 007 | loss: 0.69127 - acc: 0.5761 -- iter: 032/137
[A[ATraining Step: 32  | total loss: [1m[32m0.69156[0m[0m | time: 1.223s
[2K
| Adam | epoch: 007 | loss: 0.69156 - acc: 0.5449 -- iter: 064/137
[A[ATraining Step: 33  | total loss: [1m[32m0.69085[0m[0m | time: 1.833s
[2K
| Adam | epoch: 007 | loss: 0.69085 - acc: 0.5625 -- iter: 096/137
[A[ATraining Step: 34  | total loss: [1m[32m0.69084[0m[0m | time: 2.444s
[2K
| Adam | epoch: 007 | loss: 0.69084 - acc: 0.5424 -- iter: 128/137
[A[ATraining Step: 35  | total loss: [1m[32m0.69006[0m[0m | time: 3.652s
[2K
| Adam | epoch: 007 | loss: 0.69006 - acc: 0.5662 | val_loss: 0.68674 - val_acc: 0.6136 -- iter: 137/137
--
Training Step: 36  | total loss: [1m[32m0.68828[0m[0m | time: 0.221s
[2K
| Adam | epoch: 008 | loss: 0.68828 - acc: 0.6322 -- iter: 032/137
[A[ATraining Step: 37  | total loss: [1m[32m0.68612[0m[0m | time: 0.833s
[2K
| Adam | epoch: 008 | loss: 0.68612 - acc: 0.6613 -- iter: 064/137
[A[ATraining Step: 38  | total loss: [1m[32m0.68889[0m[0m | time: 1.446s
[2K
| Adam | epoch: 008 | loss: 0.68889 - acc: 0.5992 -- iter: 096/137
[A[ATraining Step: 39  | total loss: [1m[32m0.68715[0m[0m | time: 2.061s
[2K
| Adam | epoch: 008 | loss: 0.68715 - acc: 0.6101 -- iter: 128/137
[A[ATraining Step: 40  | total loss: [1m[32m0.68749[0m[0m | time: 3.672s
[2K
| Adam | epoch: 008 | loss: 0.68749 - acc: 0.5895 | val_loss: 0.67781 - val_acc: 0.7045 -- iter: 137/137
--
Training Step: 41  | total loss: [1m[32m0.68579[0m[0m | time: 0.219s
[2K
| Adam | epoch: 009 | loss: 0.68579 - acc: 0.6132 -- iter: 032/137
[A[ATraining Step: 42  | total loss: [1m[32m0.68444[0m[0m | time: 0.408s
[2K
| Adam | epoch: 009 | loss: 0.68444 - acc: 0.6228 -- iter: 064/137
[A[ATraining Step: 43  | total loss: [1m[32m0.68241[0m[0m | time: 1.012s
[2K
| Adam | epoch: 009 | loss: 0.68241 - acc: 0.6502 -- iter: 096/137
[A[ATraining Step: 44  | total loss: [1m[32m0.67958[0m[0m | time: 1.623s
[2K
| Adam | epoch: 009 | loss: 0.67958 - acc: 0.6729 -- iter: 128/137
[A[ATraining Step: 45  | total loss: [1m[32m0.67701[0m[0m | time: 3.232s
[2K
| Adam | epoch: 009 | loss: 0.67701 - acc: 0.6700 | val_loss: 0.64394 - val_acc: 0.7045 -- iter: 137/137
--
Training Step: 46  | total loss: [1m[32m0.67255[0m[0m | time: 0.603s
[2K
| Adam | epoch: 010 | loss: 0.67255 - acc: 0.6886 -- iter: 032/137
[A[ATraining Step: 47  | total loss: [1m[32m0.66704[0m[0m | time: 0.802s
[2K
| Adam | epoch: 010 | loss: 0.66704 - acc: 0.6986 -- iter: 064/137
[A[ATraining Step: 48  | total loss: [1m[32m0.66057[0m[0m | time: 1.024s
[2K
| Adam | epoch: 010 | loss: 0.66057 - acc: 0.7292 -- iter: 096/137
[A[ATraining Step: 49  | total loss: [1m[32m0.64845[0m[0m | time: 1.634s
[2K
| Adam | epoch: 010 | loss: 0.64845 - acc: 0.7193 -- iter: 128/137
[A[ATraining Step: 50  | total loss: [1m[32m0.65071[0m[0m | time: 3.258s
[2K
| Adam | epoch: 010 | loss: 0.65071 - acc: 0.6901 | val_loss: 0.59238 - val_acc: 0.6591 -- iter: 137/137
--
Training Step: 51  | total loss: [1m[32m0.64447[0m[0m | time: 0.616s
[2K
| Adam | epoch: 011 | loss: 0.64447 - acc: 0.6897 -- iter: 032/137
[A[ATraining Step: 52  | total loss: [1m[32m0.64154[0m[0m | time: 1.215s
[2K
| Adam | epoch: 011 | loss: 0.64154 - acc: 0.6800 -- iter: 064/137
[A[ATraining Step: 53  | total loss: [1m[32m0.61649[0m[0m | time: 1.420s
[2K
| Adam | epoch: 011 | loss: 0.61649 - acc: 0.7180 -- iter: 096/137
[A[ATraining Step: 54  | total loss: [1m[32m0.61816[0m[0m | time: 1.625s
[2K
| Adam | epoch: 011 | loss: 0.61816 - acc: 0.6783 -- iter: 128/137
[A[ATraining Step: 55  | total loss: [1m[32m0.61250[0m[0m | time: 3.232s
[2K
| Adam | epoch: 011 | loss: 0.61250 - acc: 0.6925 | val_loss: 0.50919 - val_acc: 0.7955 -- iter: 137/137
--
Training Step: 56  | total loss: [1m[32m0.59989[0m[0m | time: 0.612s
[2K
| Adam | epoch: 012 | loss: 0.59989 - acc: 0.7050 -- iter: 032/137
[A[ATraining Step: 57  | total loss: [1m[32m0.58354[0m[0m | time: 1.207s
[2K
| Adam | epoch: 012 | loss: 0.58354 - acc: 0.7156 -- iter: 064/137
[A[ATraining Step: 58  | total loss: [1m[32m0.56556[0m[0m | time: 1.805s
[2K
| Adam | epoch: 012 | loss: 0.56556 - acc: 0.7330 -- iter: 096/137
[A[ATraining Step: 59  | total loss: [1m[32m0.55085[0m[0m | time: 2.027s
[2K
| Adam | epoch: 012 | loss: 0.55085 - acc: 0.7353 -- iter: 128/137
[A[ATraining Step: 60  | total loss: [1m[32m0.53158[0m[0m | time: 3.235s
[2K
| Adam | epoch: 012 | loss: 0.53158 - acc: 0.7409 | val_loss: 0.61366 - val_acc: 0.7727 -- iter: 137/137
--
Training Step: 61  | total loss: [1m[32m0.50190[0m[0m | time: 0.596s
[2K
| Adam | epoch: 013 | loss: 0.50190 - acc: 0.7602 -- iter: 032/137
[A[ATraining Step: 62  | total loss: [1m[32m0.51070[0m[0m | time: 1.201s
[2K
| Adam | epoch: 013 | loss: 0.51070 - acc: 0.7549 -- iter: 064/137
[A[ATraining Step: 63  | total loss: [1m[32m0.51296[0m[0m | time: 1.822s
[2K
| Adam | epoch: 013 | loss: 0.51296 - acc: 0.7464 -- iter: 096/137
[A[ATraining Step: 64  | total loss: [1m[32m0.50895[0m[0m | time: 2.474s
[2K
| Adam | epoch: 013 | loss: 0.50895 - acc: 0.7546 -- iter: 128/137
[A[ATraining Step: 65  | total loss: [1m[32m0.50976[0m[0m | time: 3.695s
[2K
| Adam | epoch: 013 | loss: 0.50976 - acc: 0.7656 | val_loss: 0.59078 - val_acc: 0.7273 -- iter: 137/137
--
Training Step: 66  | total loss: [1m[32m0.47177[0m[0m | time: 0.223s
[2K
| Adam | epoch: 014 | loss: 0.47177 - acc: 0.7806 -- iter: 032/137
[A[ATraining Step: 67  | total loss: [1m[32m0.43124[0m[0m | time: 0.821s
[2K
| Adam | epoch: 014 | loss: 0.43124 - acc: 0.8069 -- iter: 064/137
[A[ATraining Step: 68  | total loss: [1m[32m0.44732[0m[0m | time: 1.452s
[2K
| Adam | epoch: 014 | loss: 0.44732 - acc: 0.8002 -- iter: 096/137
[A[ATraining Step: 69  | total loss: [1m[32m0.42047[0m[0m | time: 2.057s
[2K
| Adam | epoch: 014 | loss: 0.42047 - acc: 0.8126 -- iter: 128/137
[A[ATraining Step: 70  | total loss: [1m[32m0.40302[0m[0m | time: 3.675s
[2K
| Adam | epoch: 014 | loss: 0.40302 - acc: 0.8234 | val_loss: 0.57432 - val_acc: 0.7273 -- iter: 137/137
--
Training Step: 71  | total loss: [1m[32m0.39434[0m[0m | time: 0.210s
[2K
| Adam | epoch: 015 | loss: 0.39434 - acc: 0.8293 -- iter: 032/137
[A[ATraining Step: 72  | total loss: [1m[32m0.44569[0m[0m | time: 0.419s
[2K
| Adam | epoch: 015 | loss: 0.44569 - acc: 0.7860 -- iter: 064/137
[A[ATraining Step: 73  | total loss: [1m[32m0.44769[0m[0m | time: 1.016s
[2K
| Adam | epoch: 015 | loss: 0.44769 - acc: 0.7604 -- iter: 096/137
[A[ATraining Step: 74  | total loss: [1m[32m0.42890[0m[0m | time: 1.641s
[2K
| Adam | epoch: 015 | loss: 0.42890 - acc: 0.7798 -- iter: 128/137
[A[ATraining Step: 75  | total loss: [1m[32m0.42227[0m[0m | time: 3.247s
[2K
| Adam | epoch: 015 | loss: 0.42227 - acc: 0.7901 | val_loss: 0.68801 - val_acc: 0.6591 -- iter: 137/137
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.8926315789473684
Validation AUPRC:0.8647487854350042
Test AUC:0.9831578947368421
Test AUPRC:0.9856275344792508
BestTestF1Score	0.92	0.82	0.91	0.96	0.88	22	1	18	3	0.07
BestTestMCCScore	0.87	0.74	0.86	0.95	0.8	20	1	18	5	0.17
BestTestAccuracyScore	0.87	0.74	0.86	0.95	0.8	20	1	18	5	0.17
BestValidationF1Score	0.78	0.6	0.8	0.73	0.84	16	6	19	3	0.07
BestValidationMCC	0.78	0.63	0.82	0.82	0.74	14	3	22	5	0.17
BestValidationAccuracy	0.78	0.63	0.82	0.82	0.74	14	3	22	5	0.17
TestPredictions (Threshold:0.17)
CHEMBL2042889,TN,INACT,0.029999999329447746	CHEMBL1403324,TN,INACT,0.019999999552965164	CHEMBL3804944,TP,ACT,0.47999998927116394	CHEMBL3264526,TN,INACT,0.019999999552965164	CHEMBL3264541,TN,INACT,0.019999999552965164	CHEMBL192145,TN,INACT,0.019999999552965164	CHEMBL3126055,TN,INACT,0.029999999329447746	CHEMBL3235846,TN,INACT,0.029999999329447746	CHEMBL3805146,FN,ACT,0.05999999865889549	CHEMBL504181,TN,INACT,0.029999999329447746	CHEMBL364450,TP,ACT,0.3700000047683716	CHEMBL3805286,TP,ACT,0.46000000834465027	CHEMBL3426787,TP,ACT,0.6399999856948853	CHEMBL3235338,TP,ACT,0.23000000417232513	CHEMBL3804981,TP,ACT,0.5799999833106995	CHEMBL3104859,FN,ACT,0.05999999865889549	CHEMBL3360774,TN,INACT,0.019999999552965164	CHEMBL3426739,TP,ACT,0.8700000047683716	CHEMBL3126068,TN,INACT,0.019999999552965164	CHEMBL3686094,TN,INACT,0.029999999329447746	CHEMBL3426780,TP,ACT,0.5899999737739563	CHEMBL192013,TP,ACT,0.8700000047683716	CHEMBL3805874,TP,ACT,0.6399999856948853	CHEMBL3805428,TP,ACT,0.6200000047683716	CHEMBL1706253,TN,INACT,0.029999999329447746	CHEMBL3805249,FN,ACT,0.05999999865889549	CHEMBL1213327,TN,INACT,0.029999999329447746	CHEMBL1923070,FP,INACT,0.33000001311302185	CHEMBL3426734,TP,ACT,0.8700000047683716	CHEMBL3126070,TN,INACT,0.019999999552965164	CHEMBL3360775,TN,INACT,0.03999999910593033	CHEMBL3426744,TP,ACT,0.5199999809265137	CHEMBL560895,FN,ACT,0.07999999821186066	CHEMBL3805570,TP,ACT,0.47999998927116394	CHEMBL3426785,TP,ACT,0.3700000047683716	CHEMBL3805350,TP,ACT,0.38999998569488525	CHEMBL3235847,TN,INACT,0.019999999552965164	CHEMBL3805853,TP,ACT,0.6299999952316284	CHEMBL3426758,FN,ACT,0.10999999940395355	CHEMBL173055,TN,INACT,0.009999999776482582	CHEMBL3235333,TP,ACT,0.2199999988079071	CHEMBL1567778,TN,INACT,0.019999999552965164	CHEMBL3426742,TP,ACT,0.25	CHEMBL3426757,TP,ACT,0.4099999964237213	

