CNNModel CHEMBL4482 adam 0.0005 15 128 0 0.6 False True
Number of active compounds :	173
Number of inactive compounds :	173
---------------------------------
Run id: CNNModel_CHEMBL4482_adam_0.0005_15_128_0_0.6_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL4482_adam_0.0005_15_128_0.6_True/
---------------------------------
Training samples: 220
Validation samples: 69
--
Training Step: 1  | time: 0.776s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/220
[A[ATraining Step: 2  | total loss: [1m[32m0.62437[0m[0m | time: 1.390s
[2K
| Adam | epoch: 001 | loss: 0.62437 - acc: 0.3375 -- iter: 064/220
[A[ATraining Step: 3  | total loss: [1m[32m0.67928[0m[0m | time: 2.003s
[2K
| Adam | epoch: 001 | loss: 0.67928 - acc: 0.5983 -- iter: 096/220
[A[ATraining Step: 4  | total loss: [1m[32m0.68902[0m[0m | time: 2.607s
[2K
| Adam | epoch: 001 | loss: 0.68902 - acc: 0.5480 -- iter: 128/220
[A[ATraining Step: 5  | total loss: [1m[32m0.70006[0m[0m | time: 3.219s
[2K
| Adam | epoch: 001 | loss: 0.70006 - acc: 0.3850 -- iter: 160/220
[A[ATraining Step: 6  | total loss: [1m[32m0.69476[0m[0m | time: 3.841s
[2K
| Adam | epoch: 001 | loss: 0.69476 - acc: 0.4790 -- iter: 192/220
[A[ATraining Step: 7  | total loss: [1m[32m0.69392[0m[0m | time: 5.432s
[2K
| Adam | epoch: 001 | loss: 0.69392 - acc: 0.4916 | val_loss: 0.69381 - val_acc: 0.4638 -- iter: 220/220
--
Training Step: 8  | total loss: [1m[32m0.69335[0m[0m | time: 0.542s
[2K
| Adam | epoch: 002 | loss: 0.69335 - acc: 0.4963 -- iter: 032/220
[A[ATraining Step: 9  | total loss: [1m[32m0.69329[0m[0m | time: 1.159s
[2K
| Adam | epoch: 002 | loss: 0.69329 - acc: 0.4983 -- iter: 064/220
[A[ATraining Step: 10  | total loss: [1m[32m0.69355[0m[0m | time: 1.775s
[2K
| Adam | epoch: 002 | loss: 0.69355 - acc: 0.4835 -- iter: 096/220
[A[ATraining Step: 11  | total loss: [1m[32m0.69376[0m[0m | time: 2.379s
[2K
| Adam | epoch: 002 | loss: 0.69376 - acc: 0.4617 -- iter: 128/220
[A[ATraining Step: 12  | total loss: [1m[32m0.69383[0m[0m | time: 2.996s
[2K
| Adam | epoch: 002 | loss: 0.69383 - acc: 0.4368 -- iter: 160/220
[A[ATraining Step: 13  | total loss: [1m[32m0.69340[0m[0m | time: 3.618s
[2K
| Adam | epoch: 002 | loss: 0.69340 - acc: 0.4906 -- iter: 192/220
[A[ATraining Step: 14  | total loss: [1m[32m0.69318[0m[0m | time: 5.212s
[2K
| Adam | epoch: 002 | loss: 0.69318 - acc: 0.5456 | val_loss: 0.69330 - val_acc: 0.4638 -- iter: 220/220
--
Training Step: 15  | total loss: [1m[32m0.69305[0m[0m | time: 0.557s
[2K
| Adam | epoch: 003 | loss: 0.69305 - acc: 0.5522 -- iter: 032/220
[A[ATraining Step: 16  | total loss: [1m[32m0.69303[0m[0m | time: 1.092s
[2K
| Adam | epoch: 003 | loss: 0.69303 - acc: 0.5460 -- iter: 064/220
[A[ATraining Step: 17  | total loss: [1m[32m0.69292[0m[0m | time: 1.693s
[2K
| Adam | epoch: 003 | loss: 0.69292 - acc: 0.5423 -- iter: 096/220
[A[ATraining Step: 18  | total loss: [1m[32m0.69293[0m[0m | time: 2.314s
[2K
| Adam | epoch: 003 | loss: 0.69293 - acc: 0.5277 -- iter: 128/220
[A[ATraining Step: 19  | total loss: [1m[32m0.69306[0m[0m | time: 2.931s
[2K
| Adam | epoch: 003 | loss: 0.69306 - acc: 0.5080 -- iter: 160/220
[A[ATraining Step: 20  | total loss: [1m[32m0.69311[0m[0m | time: 3.553s
[2K
| Adam | epoch: 003 | loss: 0.69311 - acc: 0.5054 -- iter: 192/220
[A[ATraining Step: 21  | total loss: [1m[32m0.69324[0m[0m | time: 5.166s
[2K
| Adam | epoch: 003 | loss: 0.69324 - acc: 0.4941 | val_loss: 0.69356 - val_acc: 0.4638 -- iter: 220/220
--
Training Step: 22  | total loss: [1m[32m0.69289[0m[0m | time: 0.632s
[2K
| Adam | epoch: 004 | loss: 0.69289 - acc: 0.5240 -- iter: 032/220
[A[ATraining Step: 23  | total loss: [1m[32m0.69292[0m[0m | time: 1.161s
[2K
| Adam | epoch: 004 | loss: 0.69292 - acc: 0.5170 -- iter: 064/220
[A[ATraining Step: 24  | total loss: [1m[32m0.69278[0m[0m | time: 1.706s
[2K
| Adam | epoch: 004 | loss: 0.69278 - acc: 0.5223 -- iter: 096/220
[A[ATraining Step: 25  | total loss: [1m[32m0.69304[0m[0m | time: 2.325s
[2K
| Adam | epoch: 004 | loss: 0.69304 - acc: 0.5065 -- iter: 128/220
[A[ATraining Step: 26  | total loss: [1m[32m0.69181[0m[0m | time: 2.965s
[2K
| Adam | epoch: 004 | loss: 0.69181 - acc: 0.5709 -- iter: 160/220
[A[ATraining Step: 27  | total loss: [1m[32m0.69248[0m[0m | time: 3.571s
[2K
| Adam | epoch: 004 | loss: 0.69248 - acc: 0.5366 -- iter: 192/220
[A[ATraining Step: 28  | total loss: [1m[32m0.69307[0m[0m | time: 5.196s
[2K
| Adam | epoch: 004 | loss: 0.69307 - acc: 0.5118 | val_loss: 0.69414 - val_acc: 0.4638 -- iter: 220/220
--
Training Step: 29  | total loss: [1m[32m0.69305[0m[0m | time: 0.655s
[2K
| Adam | epoch: 005 | loss: 0.69305 - acc: 0.5090 -- iter: 032/220
[A[ATraining Step: 30  | total loss: [1m[32m0.69336[0m[0m | time: 1.288s
[2K
| Adam | epoch: 005 | loss: 0.69336 - acc: 0.4994 -- iter: 064/220
[A[ATraining Step: 31  | total loss: [1m[32m0.69330[0m[0m | time: 1.821s
[2K
| Adam | epoch: 005 | loss: 0.69330 - acc: 0.4996 -- iter: 096/220
[A[ATraining Step: 32  | total loss: [1m[32m0.69296[0m[0m | time: 2.342s
[2K
| Adam | epoch: 005 | loss: 0.69296 - acc: 0.5077 -- iter: 128/220
[A[ATraining Step: 33  | total loss: [1m[32m0.69271[0m[0m | time: 2.948s
[2K
| Adam | epoch: 005 | loss: 0.69271 - acc: 0.5138 -- iter: 160/220
[A[ATraining Step: 34  | total loss: [1m[32m0.69276[0m[0m | time: 3.541s
[2K
| Adam | epoch: 005 | loss: 0.69276 - acc: 0.5109 -- iter: 192/220
[A[ATraining Step: 35  | total loss: [1m[32m0.69300[0m[0m | time: 5.158s
[2K
| Adam | epoch: 005 | loss: 0.69300 - acc: 0.5021 | val_loss: 0.69362 - val_acc: 0.4638 -- iter: 220/220
--
Training Step: 36  | total loss: [1m[32m0.69330[0m[0m | time: 0.602s
[2K
| Adam | epoch: 006 | loss: 0.69330 - acc: 0.4889 -- iter: 032/220
[A[ATraining Step: 37  | total loss: [1m[32m0.69235[0m[0m | time: 1.233s
[2K
| Adam | epoch: 006 | loss: 0.69235 - acc: 0.5161 -- iter: 064/220
[A[ATraining Step: 38  | total loss: [1m[32m0.69322[0m[0m | time: 1.833s
[2K
| Adam | epoch: 006 | loss: 0.69322 - acc: 0.4885 -- iter: 096/220
[A[ATraining Step: 39  | total loss: [1m[32m0.69232[0m[0m | time: 2.368s
[2K
| Adam | epoch: 006 | loss: 0.69232 - acc: 0.5146 -- iter: 128/220
[A[ATraining Step: 40  | total loss: [1m[32m0.69212[0m[0m | time: 2.903s
[2K
| Adam | epoch: 006 | loss: 0.69212 - acc: 0.5186 -- iter: 160/220
[A[ATraining Step: 41  | total loss: [1m[32m0.69191[0m[0m | time: 3.509s
[2K
| Adam | epoch: 006 | loss: 0.69191 - acc: 0.5217 -- iter: 192/220
[A[ATraining Step: 42  | total loss: [1m[32m0.69158[0m[0m | time: 5.114s
[2K
| Adam | epoch: 006 | loss: 0.69158 - acc: 0.5234 | val_loss: 0.69265 - val_acc: 0.4638 -- iter: 220/220
--
Training Step: 43  | total loss: [1m[32m0.69355[0m[0m | time: 0.622s
[2K
| Adam | epoch: 007 | loss: 0.69355 - acc: 0.4862 -- iter: 032/220
[A[ATraining Step: 44  | total loss: [1m[32m0.69229[0m[0m | time: 1.259s
[2K
| Adam | epoch: 007 | loss: 0.69229 - acc: 0.5048 -- iter: 064/220
[A[ATraining Step: 45  | total loss: [1m[32m0.69184[0m[0m | time: 1.860s
[2K
| Adam | epoch: 007 | loss: 0.69184 - acc: 0.5093 -- iter: 096/220
[A[ATraining Step: 46  | total loss: [1m[32m0.69043[0m[0m | time: 2.493s
[2K
| Adam | epoch: 007 | loss: 0.69043 - acc: 0.5338 -- iter: 128/220
[A[ATraining Step: 47  | total loss: [1m[32m0.69118[0m[0m | time: 3.031s
[2K
| Adam | epoch: 007 | loss: 0.69118 - acc: 0.5129 -- iter: 160/220
[A[ATraining Step: 48  | total loss: [1m[32m0.68980[0m[0m | time: 3.571s
[2K
| Adam | epoch: 007 | loss: 0.68980 - acc: 0.5338 -- iter: 192/220
[A[ATraining Step: 49  | total loss: [1m[32m0.68803[0m[0m | time: 5.174s
[2K
| Adam | epoch: 007 | loss: 0.68803 - acc: 0.5510 | val_loss: 0.69271 - val_acc: 0.4638 -- iter: 220/220
--
Training Step: 50  | total loss: [1m[32m0.68861[0m[0m | time: 0.593s
[2K
| Adam | epoch: 008 | loss: 0.68861 - acc: 0.5383 -- iter: 032/220
[A[ATraining Step: 51  | total loss: [1m[32m0.68891[0m[0m | time: 1.195s
[2K
| Adam | epoch: 008 | loss: 0.68891 - acc: 0.5277 -- iter: 064/220
[A[ATraining Step: 52  | total loss: [1m[32m0.68789[0m[0m | time: 1.817s
[2K
| Adam | epoch: 008 | loss: 0.68789 - acc: 0.5235 -- iter: 096/220
[A[ATraining Step: 53  | total loss: [1m[32m0.68734[0m[0m | time: 2.414s
[2K
| Adam | epoch: 008 | loss: 0.68734 - acc: 0.5154 -- iter: 128/220
[A[ATraining Step: 54  | total loss: [1m[32m0.68675[0m[0m | time: 3.010s
[2K
| Adam | epoch: 008 | loss: 0.68675 - acc: 0.5132 -- iter: 160/220
[A[ATraining Step: 55  | total loss: [1m[32m0.68551[0m[0m | time: 3.568s
[2K
| Adam | epoch: 008 | loss: 0.68551 - acc: 0.5292 -- iter: 192/220
[A[ATraining Step: 56  | total loss: [1m[32m0.68395[0m[0m | time: 5.085s
[2K
| Adam | epoch: 008 | loss: 0.68395 - acc: 0.5351 | val_loss: 0.67820 - val_acc: 0.4638 -- iter: 220/220
--
Training Step: 57  | total loss: [1m[32m0.68141[0m[0m | time: 0.609s
[2K
| Adam | epoch: 009 | loss: 0.68141 - acc: 0.5352 -- iter: 032/220
[A[ATraining Step: 58  | total loss: [1m[32m0.67649[0m[0m | time: 1.220s
[2K
| Adam | epoch: 009 | loss: 0.67649 - acc: 0.5304 -- iter: 064/220
[A[ATraining Step: 59  | total loss: [1m[32m0.67031[0m[0m | time: 1.862s
[2K
| Adam | epoch: 009 | loss: 0.67031 - acc: 0.5431 -- iter: 096/220
[A[ATraining Step: 60  | total loss: [1m[32m0.66836[0m[0m | time: 2.467s
[2K
| Adam | epoch: 009 | loss: 0.66836 - acc: 0.5333 -- iter: 128/220
[A[ATraining Step: 61  | total loss: [1m[32m0.67027[0m[0m | time: 3.079s
[2K
| Adam | epoch: 009 | loss: 0.67027 - acc: 0.5330 -- iter: 160/220
[A[ATraining Step: 62  | total loss: [1m[32m0.67032[0m[0m | time: 3.674s
[2K
| Adam | epoch: 009 | loss: 0.67032 - acc: 0.5368 -- iter: 192/220
[A[ATraining Step: 63  | total loss: [1m[32m0.67405[0m[0m | time: 5.202s
[2K
| Adam | epoch: 009 | loss: 0.67405 - acc: 0.5282 | val_loss: 0.64415 - val_acc: 0.7101 -- iter: 220/220
--
Training Step: 64  | total loss: [1m[32m0.67093[0m[0m | time: 0.532s
[2K
| Adam | epoch: 010 | loss: 0.67093 - acc: 0.5470 -- iter: 032/220
[A[ATraining Step: 65  | total loss: [1m[32m0.66514[0m[0m | time: 1.129s
[2K
| Adam | epoch: 010 | loss: 0.66514 - acc: 0.5676 -- iter: 064/220
[A[ATraining Step: 66  | total loss: [1m[32m0.65822[0m[0m | time: 1.727s
[2K
| Adam | epoch: 010 | loss: 0.65822 - acc: 0.5898 -- iter: 096/220
[A[ATraining Step: 67  | total loss: [1m[32m0.65353[0m[0m | time: 2.351s
[2K
| Adam | epoch: 010 | loss: 0.65353 - acc: 0.6053 -- iter: 128/220
[A[ATraining Step: 68  | total loss: [1m[32m0.64712[0m[0m | time: 2.970s
[2K
| Adam | epoch: 010 | loss: 0.64712 - acc: 0.5928 -- iter: 160/220
[A[ATraining Step: 69  | total loss: [1m[32m0.63705[0m[0m | time: 3.576s
[2K
| Adam | epoch: 010 | loss: 0.63705 - acc: 0.5929 -- iter: 192/220
[A[ATraining Step: 70  | total loss: [1m[32m0.63787[0m[0m | time: 5.180s
[2K
| Adam | epoch: 010 | loss: 0.63787 - acc: 0.5966 | val_loss: 0.58001 - val_acc: 0.7246 -- iter: 220/220
--
Training Step: 71  | total loss: [1m[32m0.63217[0m[0m | time: 0.532s
[2K
| Adam | epoch: 011 | loss: 0.63217 - acc: 0.6105 -- iter: 032/220
[A[ATraining Step: 72  | total loss: [1m[32m0.62343[0m[0m | time: 1.066s
[2K
| Adam | epoch: 011 | loss: 0.62343 - acc: 0.6222 -- iter: 064/220
[A[ATraining Step: 73  | total loss: [1m[32m0.61572[0m[0m | time: 1.675s
[2K
| Adam | epoch: 011 | loss: 0.61572 - acc: 0.6364 -- iter: 096/220
[A[ATraining Step: 74  | total loss: [1m[32m0.61499[0m[0m | time: 2.290s
[2K
| Adam | epoch: 011 | loss: 0.61499 - acc: 0.6386 -- iter: 128/220
[A[ATraining Step: 75  | total loss: [1m[32m0.60254[0m[0m | time: 2.885s
[2K
| Adam | epoch: 011 | loss: 0.60254 - acc: 0.6507 -- iter: 160/220
[A[ATraining Step: 76  | total loss: [1m[32m0.58348[0m[0m | time: 3.482s
[2K
| Adam | epoch: 011 | loss: 0.58348 - acc: 0.6780 -- iter: 192/220
[A[ATraining Step: 77  | total loss: [1m[32m0.58040[0m[0m | time: 5.091s
[2K
| Adam | epoch: 011 | loss: 0.58040 - acc: 0.6824 | val_loss: 0.47940 - val_acc: 0.7826 -- iter: 220/220
--
Training Step: 78  | total loss: [1m[32m0.57092[0m[0m | time: 0.612s
[2K
| Adam | epoch: 012 | loss: 0.57092 - acc: 0.6960 -- iter: 032/220
[A[ATraining Step: 79  | total loss: [1m[32m0.56106[0m[0m | time: 1.154s
[2K
| Adam | epoch: 012 | loss: 0.56106 - acc: 0.7048 -- iter: 064/220
[A[ATraining Step: 80  | total loss: [1m[32m0.56721[0m[0m | time: 1.686s
[2K
| Adam | epoch: 012 | loss: 0.56721 - acc: 0.6985 -- iter: 096/220
[A[ATraining Step: 81  | total loss: [1m[32m0.56952[0m[0m | time: 2.297s
[2K
| Adam | epoch: 012 | loss: 0.56952 - acc: 0.6964 -- iter: 128/220
[A[ATraining Step: 82  | total loss: [1m[32m0.55060[0m[0m | time: 2.893s
[2K
| Adam | epoch: 012 | loss: 0.55060 - acc: 0.7143 -- iter: 160/220
[A[ATraining Step: 83  | total loss: [1m[32m0.53662[0m[0m | time: 3.502s
[2K
| Adam | epoch: 012 | loss: 0.53662 - acc: 0.7241 -- iter: 192/220
[A[ATraining Step: 84  | total loss: [1m[32m0.52630[0m[0m | time: 5.113s
[2K
| Adam | epoch: 012 | loss: 0.52630 - acc: 0.7298 | val_loss: 0.48152 - val_acc: 0.7681 -- iter: 220/220
--
Training Step: 85  | total loss: [1m[32m0.50095[0m[0m | time: 0.609s
[2K
| Adam | epoch: 013 | loss: 0.50095 - acc: 0.7537 -- iter: 032/220
[A[ATraining Step: 86  | total loss: [1m[32m0.48499[0m[0m | time: 1.212s
[2K
| Adam | epoch: 013 | loss: 0.48499 - acc: 0.7721 -- iter: 064/220
[A[ATraining Step: 87  | total loss: [1m[32m0.47462[0m[0m | time: 1.772s
[2K
| Adam | epoch: 013 | loss: 0.47462 - acc: 0.7761 -- iter: 096/220
[A[ATraining Step: 88  | total loss: [1m[32m0.46490[0m[0m | time: 2.310s
[2K
| Adam | epoch: 013 | loss: 0.46490 - acc: 0.7842 -- iter: 128/220
[A[ATraining Step: 89  | total loss: [1m[32m0.45358[0m[0m | time: 2.918s
[2K
| Adam | epoch: 013 | loss: 0.45358 - acc: 0.7915 -- iter: 160/220
[A[ATraining Step: 90  | total loss: [1m[32m0.44073[0m[0m | time: 3.522s
[2K
| Adam | epoch: 013 | loss: 0.44073 - acc: 0.8030 -- iter: 192/220
[A[ATraining Step: 91  | total loss: [1m[32m0.43205[0m[0m | time: 5.122s
[2K
| Adam | epoch: 013 | loss: 0.43205 - acc: 0.8102 | val_loss: 0.42287 - val_acc: 0.8116 -- iter: 220/220
--
Training Step: 92  | total loss: [1m[32m0.41141[0m[0m | time: 0.634s
[2K
| Adam | epoch: 014 | loss: 0.41141 - acc: 0.8198 -- iter: 032/220
[A[ATraining Step: 93  | total loss: [1m[32m0.40257[0m[0m | time: 1.242s
[2K
| Adam | epoch: 014 | loss: 0.40257 - acc: 0.8222 -- iter: 064/220
[A[ATraining Step: 94  | total loss: [1m[32m0.42521[0m[0m | time: 1.853s
[2K
| Adam | epoch: 014 | loss: 0.42521 - acc: 0.8056 -- iter: 096/220
[A[ATraining Step: 95  | total loss: [1m[32m0.39398[0m[0m | time: 2.385s
[2K
| Adam | epoch: 014 | loss: 0.39398 - acc: 0.8219 -- iter: 128/220
[A[ATraining Step: 96  | total loss: [1m[32m0.38176[0m[0m | time: 2.924s
[2K
| Adam | epoch: 014 | loss: 0.38176 - acc: 0.8290 -- iter: 160/220
[A[ATraining Step: 97  | total loss: [1m[32m0.36729[0m[0m | time: 3.522s
[2K
| Adam | epoch: 014 | loss: 0.36729 - acc: 0.8390 -- iter: 192/220
[A[ATraining Step: 98  | total loss: [1m[32m0.35067[0m[0m | time: 5.142s
[2K
| Adam | epoch: 014 | loss: 0.35067 - acc: 0.8488 | val_loss: 0.44212 - val_acc: 0.8551 -- iter: 220/220
--
Training Step: 99  | total loss: [1m[32m0.34598[0m[0m | time: 0.610s
[2K
| Adam | epoch: 015 | loss: 0.34598 - acc: 0.8514 -- iter: 032/220
[A[ATraining Step: 100  | total loss: [1m[32m0.33088[0m[0m | time: 1.216s
[2K
| Adam | epoch: 015 | loss: 0.33088 - acc: 0.8632 -- iter: 064/220
[A[ATraining Step: 101  | total loss: [1m[32m0.31172[0m[0m | time: 1.822s
[2K
| Adam | epoch: 015 | loss: 0.31172 - acc: 0.8769 -- iter: 096/220
[A[ATraining Step: 102  | total loss: [1m[32m0.31689[0m[0m | time: 2.450s
[2K
| Adam | epoch: 015 | loss: 0.31689 - acc: 0.8673 -- iter: 128/220
[A[ATraining Step: 103  | total loss: [1m[32m0.30129[0m[0m | time: 3.006s
[2K
| Adam | epoch: 015 | loss: 0.30129 - acc: 0.8774 -- iter: 160/220
[A[ATraining Step: 104  | total loss: [1m[32m0.29650[0m[0m | time: 3.535s
[2K
| Adam | epoch: 015 | loss: 0.29650 - acc: 0.8790 -- iter: 192/220
[A[ATraining Step: 105  | total loss: [1m[32m0.28700[0m[0m | time: 5.150s
[2K
| Adam | epoch: 015 | loss: 0.28700 - acc: 0.8839 | val_loss: 0.45311 - val_acc: 0.8551 -- iter: 220/220
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.8657094594594595
Validation AUPRC:0.8942370094235675
Test AUC:0.946218487394958
Test AUPRC:0.9522316965695259
BestTestF1Score	0.89	0.77	0.88	0.86	0.91	31	5	30	3	0.62
BestTestMCCScore	0.9	0.8	0.9	0.91	0.88	30	3	32	4	0.67
BestTestAccuracyScore	0.9	0.8	0.9	0.91	0.88	30	3	32	4	0.67
BestValidationF1Score	0.85	0.74	0.87	0.93	0.78	25	2	35	7	0.62
BestValidationMCC	0.84	0.75	0.87	0.96	0.75	24	1	36	8	0.67
BestValidationAccuracy	0.84	0.75	0.87	0.96	0.75	24	1	36	8	0.67
TestPredictions (Threshold:0.67)
CHEMBL3580948,FN,ACT,0.1899999976158142	CHEMBL3674612,TP,ACT,0.9900000095367432	CHEMBL1910754,TN,INACT,0.09000000357627869	CHEMBL3580971,TP,ACT,1.0	CHEMBL3669619,FN,ACT,0.6299999952316284	CHEMBL3596899,TP,ACT,0.9599999785423279	CHEMBL3669618,TP,ACT,1.0	CHEMBL563948,TN,INACT,0.029999999329447746	CHEMBL3580964,TP,ACT,0.9800000190734863	CHEMBL1436125,TN,INACT,0.3499999940395355	CHEMBL3669596,TP,ACT,1.0	CHEMBL3580970,TP,ACT,0.7200000286102295	CHEMBL1910373,TN,INACT,0.4300000071525574	CHEMBL3669624,TP,ACT,0.9800000190734863	CHEMBL1908397,TP,ACT,0.8600000143051147	CHEMBL3596900,TP,ACT,0.9599999785423279	CHEMBL456964,TN,INACT,0.1599999964237213	CHEMBL498248,TN,INACT,0.05000000074505806	CHEMBL456965,TN,INACT,0.10000000149011612	CHEMBL557050,TN,INACT,0.05999999865889549	CHEMBL3128050,TP,ACT,0.9399999976158142	CHEMBL3669591,TP,ACT,1.0	CHEMBL1910753,FP,INACT,0.8999999761581421	CHEMBL1682354,TN,INACT,0.09000000357627869	CHEMBL2392378,TN,INACT,0.05999999865889549	CHEMBL101868,FP,INACT,0.9800000190734863	CHEMBL3669587,TP,ACT,1.0	CHEMBL3669632,TP,ACT,0.949999988079071	CHEMBL48614,TN,INACT,0.38999998569488525	CHEMBL3669602,TP,ACT,1.0	CHEMBL2392390,TN,INACT,0.10000000149011612	CHEMBL527039,FP,INACT,0.6899999976158142	CHEMBL3593824,TP,ACT,0.8799999952316284	CHEMBL560393,TN,INACT,0.10999999940395355	CHEMBL334248,TN,INACT,0.23999999463558197	CHEMBL456143,TN,INACT,0.23999999463558197	CHEMBL3580959,TP,ACT,0.7599999904632568	CHEMBL558601,TN,INACT,0.029999999329447746	CHEMBL2070409,TN,INACT,0.09000000357627869	CHEMBL1258913,TP,ACT,0.7300000190734863	CHEMBL2064563,TP,ACT,1.0	CHEMBL3580667,TP,ACT,0.8299999833106995	CHEMBL1767126,TN,INACT,0.05999999865889549	CHEMBL1922210,TN,INACT,0.07999999821186066	CHEMBL2064629,TP,ACT,1.0	CHEMBL3669614,TP,ACT,1.0	CHEMBL2392355,TN,INACT,0.15000000596046448	CHEMBL3674606,TP,ACT,0.949999988079071	CHEMBL457191,TN,INACT,0.23000000417232513	CHEMBL486487,TN,INACT,0.6600000262260437	CHEMBL2064564,TP,ACT,1.0	CHEMBL1922122,TN,INACT,0.07999999821186066	CHEMBL3669609,TP,ACT,1.0	CHEMBL1922121,TN,INACT,0.07999999821186066	CHEMBL3128042,TP,ACT,0.9700000286102295	CHEMBL3669593,TP,ACT,0.9900000095367432	CHEMBL572878,FN,ACT,0.05999999865889549	CHEMBL3669623,TP,ACT,1.0	CHEMBL14762,FN,ACT,0.5699999928474426	CHEMBL3128043,TP,ACT,1.0	CHEMBL1762116,TN,INACT,0.03999999910593033	CHEMBL559683,TN,INACT,0.03999999910593033	CHEMBL1910602,TN,INACT,0.10000000149011612	CHEMBL1331525,TN,INACT,0.6399999856948853	CHEMBL3674600,TP,ACT,1.0	CHEMBL1287945,TN,INACT,0.05999999865889549	CHEMBL457180,TN,INACT,0.07999999821186066	CHEMBL2392238,TN,INACT,0.10000000149011612	CHEMBL488811,TN,INACT,0.03999999910593033	

