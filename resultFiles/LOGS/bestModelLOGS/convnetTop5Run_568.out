CNNModel CHEMBL3060 adam 0.0005 15 128 0 0.8 False True
Number of active compounds :	119
Number of inactive compounds :	119
---------------------------------
Run id: CNNModel_CHEMBL3060_adam_0.0005_15_128_0_0.8_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL3060_adam_0.0005_15_128_0.8_True/
---------------------------------
Training samples: 150
Validation samples: 47
--
Training Step: 1  | time: 126.252s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/150
[A[ATraining Step: 2  | total loss: [1m[32m0.62395[0m[0m | time: 176.219s
[2K
| Adam | epoch: 001 | loss: 0.62395 - acc: 0.3656 -- iter: 064/150
[A[ATraining Step: 3  | total loss: [1m[32m0.68070[0m[0m | time: 177.430s
[2K
| Adam | epoch: 001 | loss: 0.68070 - acc: 0.4500 -- iter: 096/150
[A[ATraining Step: 4  | total loss: [1m[32m0.68846[0m[0m | time: 210.276s
[2K
| Adam | epoch: 001 | loss: 0.68846 - acc: 0.6516 -- iter: 128/150
[A[ATraining Step: 5  | total loss: [1m[32m0.69204[0m[0m | time: 224.600s
[2K
| Adam | epoch: 001 | loss: 0.69204 - acc: 0.5466 | val_loss: 0.69736 - val_acc: 0.4255 -- iter: 150/150
--
Training Step: 6  | total loss: [1m[32m0.69432[0m[0m | time: 0.857s
[2K
| Adam | epoch: 002 | loss: 0.69432 - acc: 0.4874 -- iter: 032/150
[A[ATraining Step: 7  | total loss: [1m[32m0.69547[0m[0m | time: 7.848s
[2K
| Adam | epoch: 002 | loss: 0.69547 - acc: 0.4677 -- iter: 064/150
[A[ATraining Step: 8  | total loss: [1m[32m0.69711[0m[0m | time: 21.766s
[2K
| Adam | epoch: 002 | loss: 0.69711 - acc: 0.4156 -- iter: 096/150
[A[ATraining Step: 9  | total loss: [1m[32m0.69348[0m[0m | time: 23.118s
[2K
| Adam | epoch: 002 | loss: 0.69348 - acc: 0.5264 -- iter: 128/150
[A[ATraining Step: 10  | total loss: [1m[32m0.69196[0m[0m | time: 25.423s
[2K
| Adam | epoch: 002 | loss: 0.69196 - acc: 0.5757 | val_loss: 0.69550 - val_acc: 0.4255 -- iter: 150/150
--
Training Step: 11  | total loss: [1m[32m0.69113[0m[0m | time: 30.105s
[2K
| Adam | epoch: 003 | loss: 0.69113 - acc: 0.5991 -- iter: 032/150
[A[ATraining Step: 12  | total loss: [1m[32m0.69129[0m[0m | time: 55.956s
[2K
| Adam | epoch: 003 | loss: 0.69129 - acc: 0.5749 -- iter: 064/150
[A[ATraining Step: 13  | total loss: [1m[32m0.69103[0m[0m | time: 67.535s
[2K
| Adam | epoch: 003 | loss: 0.69103 - acc: 0.5623 -- iter: 096/150
[A[ATraining Step: 14  | total loss: [1m[32m0.69526[0m[0m | time: 84.620s
[2K
| Adam | epoch: 003 | loss: 0.69526 - acc: 0.4729 -- iter: 128/150
[A[ATraining Step: 15  | total loss: [1m[32m0.69329[0m[0m | time: 154.915s
[2K
| Adam | epoch: 003 | loss: 0.69329 - acc: 0.5080 | val_loss: 0.69693 - val_acc: 0.4255 -- iter: 150/150
--
Training Step: 16  | total loss: [1m[32m0.69343[0m[0m | time: 1.301s
[2K
| Adam | epoch: 004 | loss: 0.69343 - acc: 0.5050 -- iter: 032/150
[A[ATraining Step: 17  | total loss: [1m[32m0.69492[0m[0m | time: 2.134s
[2K
| Adam | epoch: 004 | loss: 0.69492 - acc: 0.4694 -- iter: 064/150
[A[ATraining Step: 18  | total loss: [1m[32m0.69247[0m[0m | time: 2.887s
[2K
| Adam | epoch: 004 | loss: 0.69247 - acc: 0.5272 -- iter: 096/150
[A[ATraining Step: 19  | total loss: [1m[32m0.69055[0m[0m | time: 61.833s
[2K
| Adam | epoch: 004 | loss: 0.69055 - acc: 0.5636 -- iter: 128/150
[A[ATraining Step: 20  | total loss: [1m[32m0.68885[0m[0m | time: 79.768s
[2K
| Adam | epoch: 004 | loss: 0.68885 - acc: 0.5934 | val_loss: 0.69980 - val_acc: 0.4255 -- iter: 150/150
--
Training Step: 21  | total loss: [1m[32m0.68957[0m[0m | time: 15.182s
[2K
| Adam | epoch: 005 | loss: 0.68957 - acc: 0.5741 -- iter: 032/150
[A[ATraining Step: 22  | total loss: [1m[32m0.68789[0m[0m | time: 60.844s
[2K
| Adam | epoch: 005 | loss: 0.68789 - acc: 0.5894 -- iter: 064/150
[A[ATraining Step: 23  | total loss: [1m[32m0.69282[0m[0m | time: 75.005s
[2K
| Adam | epoch: 005 | loss: 0.69282 - acc: 0.5362 -- iter: 096/150
[A[ATraining Step: 24  | total loss: [1m[32m0.69301[0m[0m | time: 89.768s
[2K
| Adam | epoch: 005 | loss: 0.69301 - acc: 0.5260 -- iter: 128/150
[A[ATraining Step: 25  | total loss: [1m[32m0.69355[0m[0m | time: 91.761s
[2K
| Adam | epoch: 005 | loss: 0.69355 - acc: 0.5189 | val_loss: 0.70199 - val_acc: 0.4255 -- iter: 150/150
--
Training Step: 26  | total loss: [1m[32m0.69370[0m[0m | time: 0.897s
[2K
| Adam | epoch: 006 | loss: 0.69370 - acc: 0.5139 -- iter: 032/150
[A[ATraining Step: 27  | total loss: [1m[32m0.68917[0m[0m | time: 26.790s
[2K
| Adam | epoch: 006 | loss: 0.68917 - acc: 0.5505 -- iter: 064/150
[A[ATraining Step: 28  | total loss: [1m[32m0.68902[0m[0m | time: 39.454s
[2K
| Adam | epoch: 006 | loss: 0.68902 - acc: 0.5457 -- iter: 096/150
[A[ATraining Step: 29  | total loss: [1m[32m0.68999[0m[0m | time: 39.957s
[2K
| Adam | epoch: 006 | loss: 0.68999 - acc: 0.5346 -- iter: 128/150
[A[ATraining Step: 30  | total loss: [1m[32m0.68762[0m[0m | time: 41.639s
[2K
| Adam | epoch: 006 | loss: 0.68762 - acc: 0.5479 | val_loss: 0.71393 - val_acc: 0.4255 -- iter: 150/150
--
Training Step: 31  | total loss: [1m[32m0.68523[0m[0m | time: 6.878s
[2K
| Adam | epoch: 007 | loss: 0.68523 - acc: 0.5578 -- iter: 032/150
[A[ATraining Step: 32  | total loss: [1m[32m0.68763[0m[0m | time: 7.767s
[2K
| Adam | epoch: 007 | loss: 0.68763 - acc: 0.5448 -- iter: 064/150
[A[ATraining Step: 33  | total loss: [1m[32m0.68458[0m[0m | time: 26.017s
[2K
| Adam | epoch: 007 | loss: 0.68458 - acc: 0.5556 -- iter: 096/150
[A[ATraining Step: 34  | total loss: [1m[32m0.69226[0m[0m | time: 75.850s
[2K
| Adam | epoch: 007 | loss: 0.69226 - acc: 0.5303 -- iter: 128/150
[A[ATraining Step: 35  | total loss: [1m[32m0.69258[0m[0m | time: 123.938s
[2K
| Adam | epoch: 007 | loss: 0.69258 - acc: 0.5239 | val_loss: 0.70499 - val_acc: 0.4255 -- iter: 150/150
--
Training Step: 36  | total loss: [1m[32m0.68833[0m[0m | time: 29.451s
[2K
| Adam | epoch: 008 | loss: 0.68833 - acc: 0.5376 -- iter: 032/150
[A[ATraining Step: 37  | total loss: [1m[32m0.68474[0m[0m | time: 73.452s
[2K
| Adam | epoch: 008 | loss: 0.68474 - acc: 0.5483 -- iter: 064/150
[A[ATraining Step: 38  | total loss: [1m[32m0.68461[0m[0m | time: 93.487s
[2K
| Adam | epoch: 008 | loss: 0.68461 - acc: 0.5450 -- iter: 096/150
[A[ATraining Step: 39  | total loss: [1m[32m0.67974[0m[0m | time: 94.675s
[2K
| Adam | epoch: 008 | loss: 0.67974 - acc: 0.5663 -- iter: 128/150
[A[ATraining Step: 40  | total loss: [1m[32m0.68267[0m[0m | time: 153.888s
[2K
| Adam | epoch: 008 | loss: 0.68267 - acc: 0.5480 | val_loss: 0.70818 - val_acc: 0.4255 -- iter: 150/150
--
Training Step: 41  | total loss: [1m[32m0.68160[0m[0m | time: 19.911s
[2K
| Adam | epoch: 009 | loss: 0.68160 - acc: 0.5449 -- iter: 032/150
[A[ATraining Step: 42  | total loss: [1m[32m0.67447[0m[0m | time: 45.457s
[2K
| Adam | epoch: 009 | loss: 0.67447 - acc: 0.5614 -- iter: 064/150
[A[ATraining Step: 43  | total loss: [1m[32m0.66756[0m[0m | time: 99.274s
[2K
| Adam | epoch: 009 | loss: 0.66756 - acc: 0.5746 -- iter: 096/150
[A[ATraining Step: 44  | total loss: [1m[32m0.66959[0m[0m | time: 162.510s
[2K
| Adam | epoch: 009 | loss: 0.66959 - acc: 0.5671 -- iter: 128/150
[A[ATraining Step: 45  | total loss: [1m[32m0.66939[0m[0m | time: 210.184s
[2K
| Adam | epoch: 009 | loss: 0.66939 - acc: 0.5663 | val_loss: 0.70065 - val_acc: 0.4255 -- iter: 150/150
--
Training Step: 46  | total loss: [1m[32m0.67666[0m[0m | time: 33.112s
[2K
| Adam | epoch: 010 | loss: 0.67666 - acc: 0.5448 -- iter: 032/150
[A[ATraining Step: 47  | total loss: [1m[32m0.67068[0m[0m | time: 34.016s
[2K
| Adam | epoch: 010 | loss: 0.67068 - acc: 0.5529 -- iter: 064/150
[A[ATraining Step: 48  | total loss: [1m[32m0.66617[0m[0m | time: 81.663s
[2K
| Adam | epoch: 010 | loss: 0.66617 - acc: 0.5517 -- iter: 096/150
[A[ATraining Step: 49  | total loss: [1m[32m0.66151[0m[0m | time: 86.619s
[2K
| Adam | epoch: 010 | loss: 0.66151 - acc: 0.5507 -- iter: 128/150
[A[ATraining Step: 50  | total loss: [1m[32m0.65431[0m[0m | time: 110.435s
[2K
| Adam | epoch: 010 | loss: 0.65431 - acc: 0.5574 | val_loss: 0.69218 - val_acc: 0.4255 -- iter: 150/150
--
Training Step: 51  | total loss: [1m[32m0.65095[0m[0m | time: 30.445s
[2K
| Adam | epoch: 011 | loss: 0.65095 - acc: 0.5534 -- iter: 032/150
[A[ATraining Step: 52  | total loss: [1m[32m0.64853[0m[0m | time: 41.871s
[2K
| Adam | epoch: 011 | loss: 0.64853 - acc: 0.5547 -- iter: 064/150
[A[ATraining Step: 53  | total loss: [1m[32m0.64859[0m[0m | time: 42.568s
[2K
| Adam | epoch: 011 | loss: 0.64859 - acc: 0.5421 -- iter: 096/150
[A[ATraining Step: 54  | total loss: [1m[32m0.64070[0m[0m | time: 43.439s
[2K
| Adam | epoch: 011 | loss: 0.64070 - acc: 0.5426 -- iter: 128/150
[A[ATraining Step: 55  | total loss: [1m[32m0.63353[0m[0m | time: 45.689s
[2K
| Adam | epoch: 011 | loss: 0.63353 - acc: 0.5430 | val_loss: 0.66450 - val_acc: 0.4255 -- iter: 150/150
--
Training Step: 56  | total loss: [1m[32m0.61646[0m[0m | time: 1.085s
[2K
| Adam | epoch: 012 | loss: 0.61646 - acc: 0.5589 -- iter: 032/150
[A[ATraining Step: 57  | total loss: [1m[32m0.61887[0m[0m | time: 2.182s
[2K
| Adam | epoch: 012 | loss: 0.61887 - acc: 0.5464 -- iter: 064/150
[A[ATraining Step: 58  | total loss: [1m[32m0.63586[0m[0m | time: 3.567s
[2K
| Adam | epoch: 012 | loss: 0.63586 - acc: 0.5443 -- iter: 096/150
[A[ATraining Step: 59  | total loss: [1m[32m0.63422[0m[0m | time: 19.948s
[2K
| Adam | epoch: 012 | loss: 0.63422 - acc: 0.5636 -- iter: 128/150
[A[ATraining Step: 60  | total loss: [1m[32m0.62750[0m[0m | time: 21.674s
[2K
| Adam | epoch: 012 | loss: 0.62750 - acc: 0.5913 | val_loss: 0.58564 - val_acc: 0.7660 -- iter: 150/150
--
Training Step: 61  | total loss: [1m[32m0.61985[0m[0m | time: 21.242s
[2K
| Adam | epoch: 013 | loss: 0.61985 - acc: 0.6149 -- iter: 032/150
[A[ATraining Step: 62  | total loss: [1m[32m0.61539[0m[0m | time: 28.123s
[2K
| Adam | epoch: 013 | loss: 0.61539 - acc: 0.6443 -- iter: 064/150
[A[ATraining Step: 63  | total loss: [1m[32m0.58774[0m[0m | time: 72.541s
[2K
| Adam | epoch: 013 | loss: 0.58774 - acc: 0.6657 -- iter: 096/150
[A[ATraining Step: 64  | total loss: [1m[32m0.58429[0m[0m | time: 133.713s
[2K
| Adam | epoch: 013 | loss: 0.58429 - acc: 0.6567 -- iter: 128/150
[A[ATraining Step: 65  | total loss: [1m[32m0.58528[0m[0m | time: 135.620s
[2K
| Adam | epoch: 013 | loss: 0.58528 - acc: 0.6489 | val_loss: 0.55500 - val_acc: 0.7872 -- iter: 150/150
--
Training Step: 66  | total loss: [1m[32m0.59763[0m[0m | time: 8.830s
[2K
| Adam | epoch: 014 | loss: 0.59763 - acc: 0.6474 -- iter: 032/150
[A[ATraining Step: 67  | total loss: [1m[32m0.60202[0m[0m | time: 30.736s
[2K
| Adam | epoch: 014 | loss: 0.60202 - acc: 0.6570 -- iter: 064/150
[A[ATraining Step: 68  | total loss: [1m[32m0.59355[0m[0m | time: 42.375s
[2K
| Adam | epoch: 014 | loss: 0.59355 - acc: 0.6717 -- iter: 096/150
[A[ATraining Step: 69  | total loss: [1m[32m0.59011[0m[0m | time: 50.065s
[2K
| Adam | epoch: 014 | loss: 0.59011 - acc: 0.6808 -- iter: 128/150
[A[ATraining Step: 70  | total loss: [1m[32m0.58759[0m[0m | time: 129.917s
[2K
| Adam | epoch: 014 | loss: 0.58759 - acc: 0.6888 | val_loss: 0.53821 - val_acc: 0.7021 -- iter: 150/150
--
Training Step: 71  | total loss: [1m[32m0.58365[0m[0m | time: 0.785s
[2K
| Adam | epoch: 015 | loss: 0.58365 - acc: 0.6994 -- iter: 032/150
[A[ATraining Step: 72  | total loss: [1m[32m0.56881[0m[0m | time: 32.525s
[2K
| Adam | epoch: 015 | loss: 0.56881 - acc: 0.7178 -- iter: 064/150
[A[ATraining Step: 73  | total loss: [1m[32m0.55437[0m[0m | time: 33.471s
[2K
| Adam | epoch: 015 | loss: 0.55437 - acc: 0.7290 -- iter: 096/150
[A[ATraining Step: 74  | total loss: [1m[32m0.53609[0m[0m | time: 34.539s
[2K
| Adam | epoch: 015 | loss: 0.53609 - acc: 0.7450 -- iter: 128/150
[A[ATraining Step: 75  | total loss: [1m[32m0.55959[0m[0m | time: 36.779s
[2K
| Adam | epoch: 015 | loss: 0.55959 - acc: 0.7320 | val_loss: 0.51341 - val_acc: 0.7660 -- iter: 150/150
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.8851851851851852
Validation AUPRC:0.8826174675819987
Test AUC:0.825925925925926
Test AUPRC:0.8567020432765988
BestTestF1Score	0.71	0.49	0.74	0.68	0.75	15	7	20	5	0.46
BestTestMCCScore	0.71	0.57	0.79	0.86	0.6	12	2	25	8	0.76
BestTestAccuracyScore	0.71	0.57	0.79	0.86	0.6	12	2	25	8	0.76
BestValidationF1Score	0.78	0.59	0.77	0.66	0.95	19	10	17	1	0.46
BestValidationMCC	0.75	0.68	0.83	1.0	0.6	12	0	27	8	0.76
BestValidationAccuracy	0.75	0.68	0.83	1.0	0.6	12	0	27	8	0.76
TestPredictions (Threshold:0.76)
CHEMBL599060,FN,ACT,0.44999998807907104	CHEMBL476568,FP,INACT,0.7900000214576721	CHEMBL381419,TN,INACT,0.3700000047683716	CHEMBL3325501,TP,ACT,0.9200000166893005	CHEMBL392350,FN,ACT,0.3799999952316284	CHEMBL200621,TN,INACT,0.3799999952316284	CHEMBL214039,TN,INACT,0.3799999952316284	CHEMBL796,TN,INACT,0.38999998569488525	CHEMBL3325504,TP,ACT,0.8999999761581421	CHEMBL3115211,TN,INACT,0.44999998807907104	CHEMBL89324,TN,INACT,0.3799999952316284	CHEMBL479084,FN,ACT,0.6600000262260437	CHEMBL209678,TN,INACT,0.6399999856948853	CHEMBL484680,FN,ACT,0.6399999856948853	CHEMBL200883,FN,ACT,0.3700000047683716	CHEMBL457967,TN,INACT,0.49000000953674316	CHEMBL1762481,TN,INACT,0.3799999952316284	CHEMBL2096874,TN,INACT,0.3700000047683716	CHEMBL421793,FN,ACT,0.3700000047683716	CHEMBL3325507,TP,ACT,0.9100000262260437	CHEMBL103138,TN,INACT,0.3799999952316284	CHEMBL153680,TP,ACT,0.9900000095367432	CHEMBL3325449,TP,ACT,0.9399999976158142	CHEMBL373198,TN,INACT,0.3799999952316284	CHEMBL3325505,TP,ACT,0.949999988079071	CHEMBL3325510,TP,ACT,0.8999999761581421	CHEMBL3325497,TP,ACT,0.9399999976158142	CHEMBL476763,FN,ACT,0.44999998807907104	CHEMBL152937,TP,ACT,0.9200000166893005	CHEMBL1914695,TN,INACT,0.6299999952316284	CHEMBL327868,TN,INACT,0.3799999952316284	CHEMBL438682,TN,INACT,0.3700000047683716	CHEMBL248825,TN,INACT,0.3799999952316284	CHEMBL3325509,TP,ACT,0.9100000262260437	CHEMBL209679,TN,INACT,0.38999998569488525	CHEMBL264262,FP,INACT,0.8399999737739563	CHEMBL382955,TN,INACT,0.3799999952316284	CHEMBL3325500,TP,ACT,0.949999988079071	CHEMBL1171650,TN,INACT,0.4699999988079071	CHEMBL371966,TN,INACT,0.3700000047683716	CHEMBL89208,TN,INACT,0.49000000953674316	CHEMBL232870,TN,INACT,0.4099999964237213	CHEMBL496121,FN,ACT,0.49000000953674316	CHEMBL1631007,TN,INACT,0.3799999952316284	CHEMBL64102,TN,INACT,0.38999998569488525	CHEMBL3325511,TP,ACT,0.949999988079071	CHEMBL1078818,TN,INACT,0.3700000047683716	

