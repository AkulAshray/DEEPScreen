CNNModel CHEMBL4908 RMSprop 0.0005 15 256 0 0.8 False True
Number of active compounds :	944
Number of inactive compounds :	944
---------------------------------
Run id: CNNModel_CHEMBL4908_RMSprop_0.0005_15_256_0_0.8_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL4908_RMSprop_0.0005_15_256_0.8_True/
---------------------------------
Training samples: 1192
Validation samples: 373
--
Training Step: 1  | time: 1.600s
[2K
| RMSProp | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 0032/1192
[A[ATraining Step: 2  | total loss: [1m[32m0.62417[0m[0m | time: 2.714s
[2K
| RMSProp | epoch: 001 | loss: 0.62417 - acc: 0.3937 -- iter: 0064/1192
[A[ATraining Step: 3  | total loss: [1m[32m0.68086[0m[0m | time: 3.899s
[2K
| RMSProp | epoch: 001 | loss: 0.68086 - acc: 0.4295 -- iter: 0096/1192
[A[ATraining Step: 4  | total loss: [1m[32m0.69021[0m[0m | time: 5.105s
[2K
| RMSProp | epoch: 001 | loss: 0.69021 - acc: 0.4589 -- iter: 0128/1192
[A[ATraining Step: 5  | total loss: [1m[32m0.69216[0m[0m | time: 6.228s
[2K
| RMSProp | epoch: 001 | loss: 0.69216 - acc: 0.5090 -- iter: 0160/1192
[A[ATraining Step: 6  | total loss: [1m[32m0.69301[0m[0m | time: 7.340s
[2K
| RMSProp | epoch: 001 | loss: 0.69301 - acc: 0.4630 -- iter: 0192/1192
[A[ATraining Step: 7  | total loss: [1m[32m0.69287[0m[0m | time: 8.582s
[2K
| RMSProp | epoch: 001 | loss: 0.69287 - acc: 0.5227 -- iter: 0224/1192
[A[ATraining Step: 8  | total loss: [1m[32m0.69314[0m[0m | time: 9.594s
[2K
| RMSProp | epoch: 001 | loss: 0.69314 - acc: 0.4924 -- iter: 0256/1192
[A[ATraining Step: 9  | total loss: [1m[32m0.69289[0m[0m | time: 10.704s
[2K
| RMSProp | epoch: 001 | loss: 0.69289 - acc: 0.5460 -- iter: 0288/1192
[A[ATraining Step: 10  | total loss: [1m[32m0.69272[0m[0m | time: 11.970s
[2K
| RMSProp | epoch: 001 | loss: 0.69272 - acc: 0.5699 -- iter: 0320/1192
[A[ATraining Step: 11  | total loss: [1m[32m0.69274[0m[0m | time: 13.262s
[2K
| RMSProp | epoch: 001 | loss: 0.69274 - acc: 0.5516 -- iter: 0352/1192
[A[ATraining Step: 12  | total loss: [1m[32m0.69305[0m[0m | time: 14.580s
[2K
| RMSProp | epoch: 001 | loss: 0.69305 - acc: 0.5002 -- iter: 0384/1192
[A[ATraining Step: 13  | total loss: [1m[32m0.69270[0m[0m | time: 15.615s
[2K
| RMSProp | epoch: 001 | loss: 0.69270 - acc: 0.5805 -- iter: 0416/1192
[A[ATraining Step: 14  | total loss: [1m[32m0.69295[0m[0m | time: 16.774s
[2K
| RMSProp | epoch: 001 | loss: 0.69295 - acc: 0.5476 -- iter: 0448/1192
[A[ATraining Step: 15  | total loss: [1m[32m0.69296[0m[0m | time: 17.885s
[2K
| RMSProp | epoch: 001 | loss: 0.69296 - acc: 0.5412 -- iter: 0480/1192
[A[ATraining Step: 16  | total loss: [1m[32m0.69275[0m[0m | time: 19.062s
[2K
| RMSProp | epoch: 001 | loss: 0.69275 - acc: 0.5843 -- iter: 0512/1192
[A[ATraining Step: 17  | total loss: [1m[32m0.69288[0m[0m | time: 20.233s
[2K
| RMSProp | epoch: 001 | loss: 0.69288 - acc: 0.5652 -- iter: 0544/1192
[A[ATraining Step: 18  | total loss: [1m[32m0.69322[0m[0m | time: 21.410s
[2K
| RMSProp | epoch: 001 | loss: 0.69322 - acc: 0.4994 -- iter: 0576/1192
[A[ATraining Step: 19  | total loss: [1m[32m0.69341[0m[0m | time: 22.569s
[2K
| RMSProp | epoch: 001 | loss: 0.69341 - acc: 0.4579 -- iter: 0608/1192
[A[ATraining Step: 20  | total loss: [1m[32m0.69330[0m[0m | time: 23.734s
[2K
| RMSProp | epoch: 001 | loss: 0.69330 - acc: 0.4815 -- iter: 0640/1192
[A[ATraining Step: 21  | total loss: [1m[32m0.69327[0m[0m | time: 24.998s
[2K
| RMSProp | epoch: 001 | loss: 0.69327 - acc: 0.4872 -- iter: 0672/1192
[A[ATraining Step: 22  | total loss: [1m[32m0.69324[0m[0m | time: 26.159s
[2K
| RMSProp | epoch: 001 | loss: 0.69324 - acc: 0.5004 -- iter: 0704/1192
[A[ATraining Step: 23  | total loss: [1m[32m0.69337[0m[0m | time: 27.308s
[2K
| RMSProp | epoch: 001 | loss: 0.69337 - acc: 0.4731 -- iter: 0736/1192
[A[ATraining Step: 24  | total loss: [1m[32m0.69327[0m[0m | time: 28.529s
[2K
| RMSProp | epoch: 001 | loss: 0.69327 - acc: 0.4807 -- iter: 0768/1192
[A[ATraining Step: 25  | total loss: [1m[32m0.69319[0m[0m | time: 29.850s
[2K
| RMSProp | epoch: 001 | loss: 0.69319 - acc: 0.5030 -- iter: 0800/1192
[A[ATraining Step: 26  | total loss: [1m[32m0.69342[0m[0m | time: 31.110s
[2K
| RMSProp | epoch: 001 | loss: 0.69342 - acc: 0.4608 -- iter: 0832/1192
[A[ATraining Step: 27  | total loss: [1m[32m0.69342[0m[0m | time: 32.109s
[2K
| RMSProp | epoch: 001 | loss: 0.69342 - acc: 0.4548 -- iter: 0864/1192
[A[ATraining Step: 28  | total loss: [1m[32m0.69322[0m[0m | time: 33.187s
[2K
| RMSProp | epoch: 001 | loss: 0.69322 - acc: 0.5052 -- iter: 0896/1192
[A[ATraining Step: 29  | total loss: [1m[32m0.69290[0m[0m | time: 34.368s
[2K
| RMSProp | epoch: 001 | loss: 0.69290 - acc: 0.5647 -- iter: 0928/1192
[A[ATraining Step: 30  | total loss: [1m[32m0.69288[0m[0m | time: 35.550s
[2K
| RMSProp | epoch: 001 | loss: 0.69288 - acc: 0.5642 -- iter: 0960/1192
[A[ATraining Step: 31  | total loss: [1m[32m0.69298[0m[0m | time: 36.730s
[2K
| RMSProp | epoch: 001 | loss: 0.69298 - acc: 0.5422 -- iter: 0992/1192
[A[ATraining Step: 32  | total loss: [1m[32m0.69306[0m[0m | time: 37.943s
[2K
| RMSProp | epoch: 001 | loss: 0.69306 - acc: 0.5327 -- iter: 1024/1192
[A[ATraining Step: 33  | total loss: [1m[32m0.69316[0m[0m | time: 39.230s
[2K
| RMSProp | epoch: 001 | loss: 0.69316 - acc: 0.5118 -- iter: 1056/1192
[A[ATraining Step: 34  | total loss: [1m[32m0.69303[0m[0m | time: 40.355s
[2K
| RMSProp | epoch: 001 | loss: 0.69303 - acc: 0.5361 -- iter: 1088/1192
[A[ATraining Step: 35  | total loss: [1m[32m0.69294[0m[0m | time: 41.603s
[2K
| RMSProp | epoch: 001 | loss: 0.69294 - acc: 0.5416 -- iter: 1120/1192
[A[ATraining Step: 36  | total loss: [1m[32m0.69289[0m[0m | time: 42.767s
[2K
| RMSProp | epoch: 001 | loss: 0.69289 - acc: 0.5459 -- iter: 1152/1192
[A[ATraining Step: 37  | total loss: [1m[32m0.69288[0m[0m | time: 43.941s
[2K
| RMSProp | epoch: 001 | loss: 0.69288 - acc: 0.5492 -- iter: 1184/1192
[A[ATraining Step: 38  | total loss: [1m[32m0.69299[0m[0m | time: 47.290s
[2K
| RMSProp | epoch: 001 | loss: 0.69299 - acc: 0.5335 | val_loss: 0.69340 - val_acc: 0.4611 -- iter: 1192/1192
--
Training Step: 39  | total loss: [1m[32m0.69339[0m[0m | time: 0.287s
[2K
| RMSProp | epoch: 002 | loss: 0.69339 - acc: 0.4792 -- iter: 0032/1192
[A[ATraining Step: 40  | total loss: [1m[32m0.69366[0m[0m | time: 1.306s
[2K
| RMSProp | epoch: 002 | loss: 0.69366 - acc: 0.4362 -- iter: 0064/1192
[A[ATraining Step: 41  | total loss: [1m[32m0.69347[0m[0m | time: 2.364s
[2K
| RMSProp | epoch: 002 | loss: 0.69347 - acc: 0.4766 -- iter: 0096/1192
[A[ATraining Step: 42  | total loss: [1m[32m0.69342[0m[0m | time: 3.538s
[2K
| RMSProp | epoch: 002 | loss: 0.69342 - acc: 0.4752 -- iter: 0128/1192
[A[ATraining Step: 43  | total loss: [1m[32m0.69323[0m[0m | time: 4.745s
[2K
| RMSProp | epoch: 002 | loss: 0.69323 - acc: 0.5072 -- iter: 0160/1192
[A[ATraining Step: 44  | total loss: [1m[32m0.69326[0m[0m | time: 5.941s
[2K
| RMSProp | epoch: 002 | loss: 0.69326 - acc: 0.5005 -- iter: 0192/1192
[A[ATraining Step: 45  | total loss: [1m[32m0.69335[0m[0m | time: 7.062s
[2K
| RMSProp | epoch: 002 | loss: 0.69335 - acc: 0.4792 -- iter: 0224/1192
[A[ATraining Step: 46  | total loss: [1m[32m0.69335[0m[0m | time: 8.326s
[2K
| RMSProp | epoch: 002 | loss: 0.69335 - acc: 0.4827 -- iter: 0256/1192
[A[ATraining Step: 47  | total loss: [1m[32m0.69321[0m[0m | time: 9.605s
[2K
| RMSProp | epoch: 002 | loss: 0.69321 - acc: 0.5060 -- iter: 0288/1192
[A[ATraining Step: 48  | total loss: [1m[32m0.69305[0m[0m | time: 10.783s
[2K
| RMSProp | epoch: 002 | loss: 0.69305 - acc: 0.5301 -- iter: 0320/1192
[A[ATraining Step: 49  | total loss: [1m[32m0.69277[0m[0m | time: 11.872s
[2K
| RMSProp | epoch: 002 | loss: 0.69277 - acc: 0.5698 -- iter: 0352/1192
[A[ATraining Step: 50  | total loss: [1m[32m0.69281[0m[0m | time: 13.005s
[2K
| RMSProp | epoch: 002 | loss: 0.69281 - acc: 0.5638 -- iter: 0384/1192
[A[ATraining Step: 51  | total loss: [1m[32m0.69286[0m[0m | time: 14.268s
[2K
| RMSProp | epoch: 002 | loss: 0.69286 - acc: 0.5541 -- iter: 0416/1192
[A[ATraining Step: 52  | total loss: [1m[32m0.69276[0m[0m | time: 15.704s
[2K
| RMSProp | epoch: 002 | loss: 0.69276 - acc: 0.5600 -- iter: 0448/1192
[A[ATraining Step: 53  | total loss: [1m[32m0.69298[0m[0m | time: 16.998s
[2K
| RMSProp | epoch: 002 | loss: 0.69298 - acc: 0.5373 -- iter: 0480/1192
[A[ATraining Step: 54  | total loss: [1m[32m0.69305[0m[0m | time: 18.007s
[2K
| RMSProp | epoch: 002 | loss: 0.69305 - acc: 0.5274 -- iter: 0512/1192
[A[ATraining Step: 55  | total loss: [1m[32m0.69290[0m[0m | time: 18.909s
[2K
| RMSProp | epoch: 002 | loss: 0.69290 - acc: 0.5413 -- iter: 0544/1192
[A[ATraining Step: 56  | total loss: [1m[32m0.69289[0m[0m | time: 19.748s
[2K
| RMSProp | epoch: 002 | loss: 0.69289 - acc: 0.5443 -- iter: 0576/1192
[A[ATraining Step: 57  | total loss: [1m[32m0.69292[0m[0m | time: 20.632s
[2K
| RMSProp | epoch: 002 | loss: 0.69292 - acc: 0.5425 -- iter: 0608/1192
[A[ATraining Step: 58  | total loss: [1m[32m0.69285[0m[0m | time: 21.450s
[2K
| RMSProp | epoch: 002 | loss: 0.69285 - acc: 0.5452 -- iter: 0640/1192
[A[ATraining Step: 59  | total loss: [1m[32m0.69293[0m[0m | time: 22.286s
[2K
| RMSProp | epoch: 002 | loss: 0.69293 - acc: 0.5349 -- iter: 0672/1192
[A[ATraining Step: 60  | total loss: [1m[32m0.69270[0m[0m | time: 23.199s
[2K
| RMSProp | epoch: 002 | loss: 0.69270 - acc: 0.5510 -- iter: 0704/1192
[A[ATraining Step: 61  | total loss: [1m[32m0.69260[0m[0m | time: 24.075s
[2K
| RMSProp | epoch: 002 | loss: 0.69260 - acc: 0.5566 -- iter: 0736/1192
[A[ATraining Step: 62  | total loss: [1m[32m0.69262[0m[0m | time: 24.929s
[2K
| RMSProp | epoch: 002 | loss: 0.69262 - acc: 0.5533 -- iter: 0768/1192
[A[ATraining Step: 63  | total loss: [1m[32m0.69251[0m[0m | time: 25.837s
[2K
| RMSProp | epoch: 002 | loss: 0.69251 - acc: 0.5584 -- iter: 0800/1192
[A[ATraining Step: 64  | total loss: [1m[32m0.69251[0m[0m | time: 26.919s
[2K
| RMSProp | epoch: 002 | loss: 0.69251 - acc: 0.5550 -- iter: 0832/1192
[A[ATraining Step: 65  | total loss: [1m[32m0.69252[0m[0m | time: 28.011s
[2K
| RMSProp | epoch: 002 | loss: 0.69252 - acc: 0.5521 -- iter: 0864/1192
[A[ATraining Step: 66  | total loss: [1m[32m0.69271[0m[0m | time: 28.875s
[2K
| RMSProp | epoch: 002 | loss: 0.69271 - acc: 0.5382 -- iter: 0896/1192
[A[ATraining Step: 67  | total loss: [1m[32m0.69282[0m[0m | time: 29.730s
[2K
| RMSProp | epoch: 002 | loss: 0.69282 - acc: 0.5298 -- iter: 0928/1192
[A[ATraining Step: 68  | total loss: [1m[32m0.69282[0m[0m | time: 30.595s
[2K
| RMSProp | epoch: 002 | loss: 0.69282 - acc: 0.5300 -- iter: 0960/1192
[A[ATraining Step: 69  | total loss: [1m[32m0.69299[0m[0m | time: 31.455s
[2K
| RMSProp | epoch: 002 | loss: 0.69299 - acc: 0.5192 -- iter: 0992/1192
[A[ATraining Step: 70  | total loss: [1m[32m0.69289[0m[0m | time: 32.338s
[2K
| RMSProp | epoch: 002 | loss: 0.69289 - acc: 0.5242 -- iter: 1024/1192
[A[ATraining Step: 71  | total loss: [1m[32m0.69305[0m[0m | time: 33.190s
[2K
| RMSProp | epoch: 002 | loss: 0.69305 - acc: 0.5143 -- iter: 1056/1192
[A[ATraining Step: 72  | total loss: [1m[32m0.69307[0m[0m | time: 34.116s
[2K
| RMSProp | epoch: 002 | loss: 0.69307 - acc: 0.5127 -- iter: 1088/1192
[A[ATraining Step: 73  | total loss: [1m[32m0.69323[0m[0m | time: 34.993s
[2K
| RMSProp | epoch: 002 | loss: 0.69323 - acc: 0.5009 -- iter: 1120/1192
[A[ATraining Step: 74  | total loss: [1m[32m0.69314[0m[0m | time: 35.894s
[2K
| RMSProp | epoch: 002 | loss: 0.69314 - acc: 0.5076 -- iter: 1152/1192
[A[ATraining Step: 75  | total loss: [1m[32m0.69301[0m[0m | time: 36.738s
[2K
| RMSProp | epoch: 002 | loss: 0.69301 - acc: 0.5170 -- iter: 1184/1192
[A[ATraining Step: 76  | total loss: [1m[32m0.69305[0m[0m | time: 39.919s
[2K
| RMSProp | epoch: 002 | loss: 0.69305 - acc: 0.5118 | val_loss: 0.69372 - val_acc: 0.4611 -- iter: 1192/1192
--
Training Step: 77  | total loss: [1m[32m0.69317[0m[0m | time: 0.214s
[2K
| RMSProp | epoch: 003 | loss: 0.69317 - acc: 0.5039 -- iter: 0032/1192
[A[ATraining Step: 78  | total loss: [1m[32m0.69317[0m[0m | time: 0.440s
[2K
| RMSProp | epoch: 003 | loss: 0.69317 - acc: 0.5035 -- iter: 0064/1192
[A[ATraining Step: 79  | total loss: [1m[32m0.69319[0m[0m | time: 1.328s
[2K
| RMSProp | epoch: 003 | loss: 0.69319 - acc: 0.5032 -- iter: 0096/1192
[A[ATraining Step: 80  | total loss: [1m[32m0.69322[0m[0m | time: 2.180s
[2K
| RMSProp | epoch: 003 | loss: 0.69322 - acc: 0.4996 -- iter: 0128/1192
[A[ATraining Step: 81  | total loss: [1m[32m0.69328[0m[0m | time: 3.068s
[2K
| RMSProp | epoch: 003 | loss: 0.69328 - acc: 0.4934 -- iter: 0160/1192
[A[ATraining Step: 82  | total loss: [1m[32m0.69342[0m[0m | time: 3.920s
[2K
| RMSProp | epoch: 003 | loss: 0.69342 - acc: 0.4784 -- iter: 0192/1192
[A[ATraining Step: 83  | total loss: [1m[32m0.69330[0m[0m | time: 5.254s
[2K
| RMSProp | epoch: 003 | loss: 0.69330 - acc: 0.4931 -- iter: 0224/1192
[A[ATraining Step: 84  | total loss: [1m[32m0.69337[0m[0m | time: 6.531s
[2K
| RMSProp | epoch: 003 | loss: 0.69337 - acc: 0.4844 -- iter: 0256/1192
[A[ATraining Step: 85  | total loss: [1m[32m0.69335[0m[0m | time: 7.841s
[2K
| RMSProp | epoch: 003 | loss: 0.69335 - acc: 0.4828 -- iter: 0288/1192
[A[ATraining Step: 86  | total loss: [1m[32m0.69327[0m[0m | time: 8.871s
[2K
| RMSProp | epoch: 003 | loss: 0.69327 - acc: 0.4939 -- iter: 0320/1192
[A[ATraining Step: 87  | total loss: [1m[32m0.69334[0m[0m | time: 9.860s
[2K
| RMSProp | epoch: 003 | loss: 0.69334 - acc: 0.4851 -- iter: 0352/1192
[A[ATraining Step: 88  | total loss: [1m[32m0.69322[0m[0m | time: 11.193s
[2K
| RMSProp | epoch: 003 | loss: 0.69322 - acc: 0.5023 -- iter: 0384/1192
[A[ATraining Step: 89  | total loss: [1m[32m0.69339[0m[0m | time: 12.568s
[2K
| RMSProp | epoch: 003 | loss: 0.69339 - acc: 0.4864 -- iter: 0416/1192
[A[ATraining Step: 90  | total loss: [1m[32m0.69334[0m[0m | time: 13.849s
[2K
| RMSProp | epoch: 003 | loss: 0.69334 - acc: 0.4940 -- iter: 0448/1192
[A[ATraining Step: 91  | total loss: [1m[32m0.69328[0m[0m | time: 14.962s
[2K
| RMSProp | epoch: 003 | loss: 0.69328 - acc: 0.5009 -- iter: 0480/1192
[A[ATraining Step: 92  | total loss: [1m[32m0.69313[0m[0m | time: 16.110s
[2K
| RMSProp | epoch: 003 | loss: 0.69313 - acc: 0.5164 -- iter: 0512/1192
[A[ATraining Step: 93  | total loss: [1m[32m0.69311[0m[0m | time: 17.202s
[2K
| RMSProp | epoch: 003 | loss: 0.69311 - acc: 0.5179 -- iter: 0544/1192
[A[ATraining Step: 94  | total loss: [1m[32m0.69301[0m[0m | time: 18.432s
[2K
| RMSProp | epoch: 003 | loss: 0.69301 - acc: 0.5223 -- iter: 0576/1192
[A[ATraining Step: 95  | total loss: [1m[32m0.69303[0m[0m | time: 19.612s
[2K
| RMSProp | epoch: 003 | loss: 0.69303 - acc: 0.5201 -- iter: 0608/1192
[A[ATraining Step: 96  | total loss: [1m[32m0.69317[0m[0m | time: 20.700s
[2K
| RMSProp | epoch: 003 | loss: 0.69317 - acc: 0.5119 -- iter: 0640/1192
[A[ATraining Step: 97  | total loss: [1m[32m0.69305[0m[0m | time: 21.810s
[2K
| RMSProp | epoch: 003 | loss: 0.69305 - acc: 0.5200 -- iter: 0672/1192
[A[ATraining Step: 98  | total loss: [1m[32m0.69297[0m[0m | time: 23.067s
[2K
| RMSProp | epoch: 003 | loss: 0.69297 - acc: 0.5243 -- iter: 0704/1192
[A[ATraining Step: 99  | total loss: [1m[32m0.69280[0m[0m | time: 24.367s
[2K
| RMSProp | epoch: 003 | loss: 0.69280 - acc: 0.5312 -- iter: 0736/1192
[A[ATraining Step: 100  | total loss: [1m[32m0.69270[0m[0m | time: 25.422s
[2K
| RMSProp | epoch: 003 | loss: 0.69270 - acc: 0.5344 -- iter: 0768/1192
[A[ATraining Step: 101  | total loss: [1m[32m0.69285[0m[0m | time: 26.575s
[2K
| RMSProp | epoch: 003 | loss: 0.69285 - acc: 0.5278 -- iter: 0800/1192
[A[ATraining Step: 102  | total loss: [1m[32m0.69288[0m[0m | time: 27.755s
[2K
| RMSProp | epoch: 003 | loss: 0.69288 - acc: 0.5250 -- iter: 0832/1192
[A[ATraining Step: 103  | total loss: [1m[32m0.69315[0m[0m | time: 29.085s
[2K
| RMSProp | epoch: 003 | loss: 0.69315 - acc: 0.5131 -- iter: 0864/1192
[A[ATraining Step: 104  | total loss: [1m[32m0.69321[0m[0m | time: 30.203s
[2K
| RMSProp | epoch: 003 | loss: 0.69321 - acc: 0.5087 -- iter: 0896/1192
[A[ATraining Step: 105  | total loss: [1m[32m0.69321[0m[0m | time: 31.200s
[2K
| RMSProp | epoch: 003 | loss: 0.69321 - acc: 0.5078 -- iter: 0928/1192
[A[ATraining Step: 106  | total loss: [1m[32m0.69321[0m[0m | time: 32.327s
[2K
| RMSProp | epoch: 003 | loss: 0.69321 - acc: 0.5070 -- iter: 0960/1192
[A[ATraining Step: 107  | total loss: [1m[32m0.69327[0m[0m | time: 33.450s
[2K
| RMSProp | epoch: 003 | loss: 0.69327 - acc: 0.5032 -- iter: 0992/1192
[A[ATraining Step: 108  | total loss: [1m[32m0.69313[0m[0m | time: 34.522s
[2K
| RMSProp | epoch: 003 | loss: 0.69313 - acc: 0.5123 -- iter: 1024/1192
[A[ATraining Step: 109  | total loss: [1m[32m0.69284[0m[0m | time: 35.671s
[2K
| RMSProp | epoch: 003 | loss: 0.69284 - acc: 0.5267 -- iter: 1056/1192
[A[ATraining Step: 110  | total loss: [1m[32m0.69269[0m[0m | time: 36.858s
[2K
| RMSProp | epoch: 003 | loss: 0.69269 - acc: 0.5303 -- iter: 1088/1192
[A[ATraining Step: 111  | total loss: [1m[32m0.69341[0m[0m | time: 37.960s
[2K
| RMSProp | epoch: 003 | loss: 0.69341 - acc: 0.5054 -- iter: 1120/1192
[A[ATraining Step: 112  | total loss: [1m[32m0.69347[0m[0m | time: 39.174s
[2K
| RMSProp | epoch: 003 | loss: 0.69347 - acc: 0.5017 -- iter: 1152/1192
[A[ATraining Step: 113  | total loss: [1m[32m0.69340[0m[0m | time: 40.531s
[2K
| RMSProp | epoch: 003 | loss: 0.69340 - acc: 0.5046 -- iter: 1184/1192
[A[ATraining Step: 114  | total loss: [1m[32m0.69359[0m[0m | time: 44.167s
[2K
| RMSProp | epoch: 003 | loss: 0.69359 - acc: 0.4917 | val_loss: 0.69371 - val_acc: 0.4611 -- iter: 1192/1192
--
Training Step: 115  | total loss: [1m[32m0.69344[0m[0m | time: 1.538s
[2K
| RMSProp | epoch: 004 | loss: 0.69344 - acc: 0.4988 -- iter: 0032/1192
[A[ATraining Step: 116  | total loss: [1m[32m0.69318[0m[0m | time: 1.780s
[2K
| RMSProp | epoch: 004 | loss: 0.69318 - acc: 0.5145 -- iter: 0064/1192
[A[ATraining Step: 117  | total loss: [1m[32m0.69351[0m[0m | time: 2.036s
[2K
| RMSProp | epoch: 004 | loss: 0.69351 - acc: 0.5006 -- iter: 0096/1192
[A[ATraining Step: 118  | total loss: [1m[32m0.69370[0m[0m | time: 3.046s
[2K
| RMSProp | epoch: 004 | loss: 0.69370 - acc: 0.4880 -- iter: 0128/1192
[A[ATraining Step: 119  | total loss: [1m[32m0.69355[0m[0m | time: 4.101s
[2K
| RMSProp | epoch: 004 | loss: 0.69355 - acc: 0.5017 -- iter: 0160/1192
[A[ATraining Step: 120  | total loss: [1m[32m0.69350[0m[0m | time: 5.231s
[2K
| RMSProp | epoch: 004 | loss: 0.69350 - acc: 0.5015 -- iter: 0192/1192
[A[ATraining Step: 121  | total loss: [1m[32m0.69335[0m[0m | time: 6.305s
[2K
| RMSProp | epoch: 004 | loss: 0.69335 - acc: 0.5108 -- iter: 0224/1192
[A[ATraining Step: 122  | total loss: [1m[32m0.69322[0m[0m | time: 7.417s
[2K
| RMSProp | epoch: 004 | loss: 0.69322 - acc: 0.5159 -- iter: 0256/1192
[A[ATraining Step: 123  | total loss: [1m[32m0.69313[0m[0m | time: 8.715s
[2K
| RMSProp | epoch: 004 | loss: 0.69313 - acc: 0.5175 -- iter: 0288/1192
[A[ATraining Step: 124  | total loss: [1m[32m0.69320[0m[0m | time: 9.833s
[2K
| RMSProp | epoch: 004 | loss: 0.69320 - acc: 0.5126 -- iter: 0320/1192
[A[ATraining Step: 125  | total loss: [1m[32m0.69295[0m[0m | time: 10.990s
[2K
| RMSProp | epoch: 004 | loss: 0.69295 - acc: 0.5238 -- iter: 0352/1192
[A[ATraining Step: 126  | total loss: [1m[32m0.69325[0m[0m | time: 12.193s
[2K
| RMSProp | epoch: 004 | loss: 0.69325 - acc: 0.5121 -- iter: 0384/1192
[A[ATraining Step: 127  | total loss: [1m[32m0.69297[0m[0m | time: 13.401s
[2K
| RMSProp | epoch: 004 | loss: 0.69297 - acc: 0.5234 -- iter: 0416/1192
[A[ATraining Step: 128  | total loss: [1m[32m0.69271[0m[0m | time: 14.684s
[2K
| RMSProp | epoch: 004 | loss: 0.69271 - acc: 0.5304 -- iter: 0448/1192
[A[ATraining Step: 129  | total loss: [1m[32m0.69245[0m[0m | time: 15.808s
[2K
| RMSProp | epoch: 004 | loss: 0.69245 - acc: 0.5367 -- iter: 0480/1192
[A[ATraining Step: 130  | total loss: [1m[32m0.69284[0m[0m | time: 17.064s
[2K
| RMSProp | epoch: 004 | loss: 0.69284 - acc: 0.5268 -- iter: 0512/1192
[A[ATraining Step: 131  | total loss: [1m[32m0.69298[0m[0m | time: 18.109s
[2K
| RMSProp | epoch: 004 | loss: 0.69298 - acc: 0.5210 -- iter: 0544/1192
[A[ATraining Step: 132  | total loss: [1m[32m0.69300[0m[0m | time: 19.181s
[2K
| RMSProp | epoch: 004 | loss: 0.69300 - acc: 0.5189 -- iter: 0576/1192
[A[ATraining Step: 133  | total loss: [1m[32m0.69330[0m[0m | time: 20.245s
[2K
| RMSProp | epoch: 004 | loss: 0.69330 - acc: 0.5076 -- iter: 0608/1192
[A[ATraining Step: 134  | total loss: [1m[32m0.69318[0m[0m | time: 21.375s
[2K
| RMSProp | epoch: 004 | loss: 0.69318 - acc: 0.5131 -- iter: 0640/1192
[A[ATraining Step: 135  | total loss: [1m[32m0.69332[0m[0m | time: 22.512s
[2K
| RMSProp | epoch: 004 | loss: 0.69332 - acc: 0.5056 -- iter: 0672/1192
[A[ATraining Step: 136  | total loss: [1m[32m0.69318[0m[0m | time: 23.548s
[2K
| RMSProp | epoch: 004 | loss: 0.69318 - acc: 0.5144 -- iter: 0704/1192
[A[ATraining Step: 137  | total loss: [1m[32m0.69308[0m[0m | time: 24.733s
[2K
| RMSProp | epoch: 004 | loss: 0.69308 - acc: 0.5161 -- iter: 0736/1192
[A[ATraining Step: 138  | total loss: [1m[32m0.69317[0m[0m | time: 25.771s
[2K
| RMSProp | epoch: 004 | loss: 0.69317 - acc: 0.5113 -- iter: 0768/1192
[A[ATraining Step: 139  | total loss: [1m[32m0.69314[0m[0m | time: 26.893s
[2K
| RMSProp | epoch: 004 | loss: 0.69314 - acc: 0.5102 -- iter: 0800/1192
[A[ATraining Step: 140  | total loss: [1m[32m0.69298[0m[0m | time: 28.135s
[2K
| RMSProp | epoch: 004 | loss: 0.69298 - acc: 0.5154 -- iter: 0832/1192
[A[ATraining Step: 141  | total loss: [1m[32m0.69286[0m[0m | time: 29.341s
[2K
| RMSProp | epoch: 004 | loss: 0.69286 - acc: 0.5170 -- iter: 0864/1192
[A[ATraining Step: 142  | total loss: [1m[32m0.69312[0m[0m | time: 30.656s
[2K
| RMSProp | epoch: 004 | loss: 0.69312 - acc: 0.5091 -- iter: 0896/1192
[A[ATraining Step: 143  | total loss: [1m[32m0.69297[0m[0m | time: 31.833s
[2K
| RMSProp | epoch: 004 | loss: 0.69297 - acc: 0.5144 -- iter: 0928/1192
[A[ATraining Step: 144  | total loss: [1m[32m0.69299[0m[0m | time: 33.005s
[2K
| RMSProp | epoch: 004 | loss: 0.69299 - acc: 0.5130 -- iter: 0960/1192
[A[ATraining Step: 145  | total loss: [1m[32m0.69283[0m[0m | time: 34.002s
[2K
| RMSProp | epoch: 004 | loss: 0.69283 - acc: 0.5179 -- iter: 0992/1192
[A[ATraining Step: 146  | total loss: [1m[32m0.69228[0m[0m | time: 35.027s
[2K
| RMSProp | epoch: 004 | loss: 0.69228 - acc: 0.5318 -- iter: 1024/1192
[A[ATraining Step: 147  | total loss: [1m[32m0.69160[0m[0m | time: 36.153s
[2K
| RMSProp | epoch: 004 | loss: 0.69160 - acc: 0.5411 -- iter: 1056/1192
[A[ATraining Step: 148  | total loss: [1m[32m0.69037[0m[0m | time: 37.260s
[2K
| RMSProp | epoch: 004 | loss: 0.69037 - acc: 0.5526 -- iter: 1088/1192
[A[ATraining Step: 149  | total loss: [1m[32m0.68938[0m[0m | time: 38.442s
[2K
| RMSProp | epoch: 004 | loss: 0.68938 - acc: 0.5567 -- iter: 1120/1192
[A[ATraining Step: 150  | total loss: [1m[32m0.69075[0m[0m | time: 39.538s
[2K
| RMSProp | epoch: 004 | loss: 0.69075 - acc: 0.5510 -- iter: 1152/1192
[A[ATraining Step: 151  | total loss: [1m[32m0.69151[0m[0m | time: 40.643s
[2K
| RMSProp | epoch: 004 | loss: 0.69151 - acc: 0.5428 -- iter: 1184/1192
[A[ATraining Step: 152  | total loss: [1m[32m0.69125[0m[0m | time: 44.193s
[2K
| RMSProp | epoch: 004 | loss: 0.69125 - acc: 0.5448 | val_loss: 0.69527 - val_acc: 0.4611 -- iter: 1192/1192
--
Training Step: 153  | total loss: [1m[32m0.69248[0m[0m | time: 1.475s
[2K
| RMSProp | epoch: 005 | loss: 0.69248 - acc: 0.5309 -- iter: 0032/1192
[A[ATraining Step: 154  | total loss: [1m[32m0.69258[0m[0m | time: 2.806s
[2K
| RMSProp | epoch: 005 | loss: 0.69258 - acc: 0.5278 -- iter: 0064/1192
[A[ATraining Step: 155  | total loss: [1m[32m0.69306[0m[0m | time: 3.087s
[2K
| RMSProp | epoch: 005 | loss: 0.69306 - acc: 0.5157 -- iter: 0096/1192
[A[ATraining Step: 156  | total loss: [1m[32m0.69273[0m[0m | time: 3.462s
[2K
| RMSProp | epoch: 005 | loss: 0.69273 - acc: 0.5266 -- iter: 0128/1192
[A[ATraining Step: 157  | total loss: [1m[32m0.69214[0m[0m | time: 4.502s
[2K
| RMSProp | epoch: 005 | loss: 0.69214 - acc: 0.5364 -- iter: 0160/1192
[A[ATraining Step: 158  | total loss: [1m[32m0.69211[0m[0m | time: 5.509s
[2K
| RMSProp | epoch: 005 | loss: 0.69211 - acc: 0.5359 -- iter: 0192/1192
[A[ATraining Step: 159  | total loss: [1m[32m0.69201[0m[0m | time: 6.599s
[2K
| RMSProp | epoch: 005 | loss: 0.69201 - acc: 0.5355 -- iter: 0224/1192
[A[ATraining Step: 160  | total loss: [1m[32m0.69165[0m[0m | time: 7.679s
[2K
| RMSProp | epoch: 005 | loss: 0.69165 - acc: 0.5382 -- iter: 0256/1192
[A[ATraining Step: 161  | total loss: [1m[32m0.69223[0m[0m | time: 8.690s
[2K
| RMSProp | epoch: 005 | loss: 0.69223 - acc: 0.5312 -- iter: 0288/1192
[A[ATraining Step: 162  | total loss: [1m[32m0.69281[0m[0m | time: 9.843s
[2K
| RMSProp | epoch: 005 | loss: 0.69281 - acc: 0.5219 -- iter: 0320/1192
[A[ATraining Step: 163  | total loss: [1m[32m0.69235[0m[0m | time: 10.937s
[2K
| RMSProp | epoch: 005 | loss: 0.69235 - acc: 0.5322 -- iter: 0352/1192
[A[ATraining Step: 164  | total loss: [1m[32m0.69205[0m[0m | time: 12.048s
[2K
| RMSProp | epoch: 005 | loss: 0.69205 - acc: 0.5352 -- iter: 0384/1192
[A[ATraining Step: 165  | total loss: [1m[32m0.69252[0m[0m | time: 13.259s
[2K
| RMSProp | epoch: 005 | loss: 0.69252 - acc: 0.5286 -- iter: 0416/1192
[A[ATraining Step: 166  | total loss: [1m[32m0.69262[0m[0m | time: 14.429s
[2K
| RMSProp | epoch: 005 | loss: 0.69262 - acc: 0.5257 -- iter: 0448/1192
[A[ATraining Step: 167  | total loss: [1m[32m0.69343[0m[0m | time: 15.538s
[2K
| RMSProp | epoch: 005 | loss: 0.69343 - acc: 0.5106 -- iter: 0480/1192
[A[ATraining Step: 168  | total loss: [1m[32m0.69310[0m[0m | time: 16.693s
[2K
| RMSProp | epoch: 005 | loss: 0.69310 - acc: 0.5189 -- iter: 0512/1192
[A[ATraining Step: 169  | total loss: [1m[32m0.69301[0m[0m | time: 17.961s
[2K
| RMSProp | epoch: 005 | loss: 0.69301 - acc: 0.5202 -- iter: 0544/1192
[A[ATraining Step: 170  | total loss: [1m[32m0.69225[0m[0m | time: 19.314s
[2K
| RMSProp | epoch: 005 | loss: 0.69225 - acc: 0.5338 -- iter: 0576/1192
[A[ATraining Step: 171  | total loss: [1m[32m0.69163[0m[0m | time: 20.528s
[2K
| RMSProp | epoch: 005 | loss: 0.69163 - acc: 0.5398 -- iter: 0608/1192
[A[ATraining Step: 172  | total loss: [1m[32m0.69122[0m[0m | time: 21.428s
[2K
| RMSProp | epoch: 005 | loss: 0.69122 - acc: 0.5420 -- iter: 0640/1192
[A[ATraining Step: 173  | total loss: [1m[32m0.69050[0m[0m | time: 22.458s
[2K
| RMSProp | epoch: 005 | loss: 0.69050 - acc: 0.5441 -- iter: 0672/1192
[A[ATraining Step: 174  | total loss: [1m[32m0.69285[0m[0m | time: 23.583s
[2K
| RMSProp | epoch: 005 | loss: 0.69285 - acc: 0.5366 -- iter: 0704/1192
[A[ATraining Step: 175  | total loss: [1m[32m0.69212[0m[0m | time: 24.628s
[2K
| RMSProp | epoch: 005 | loss: 0.69212 - acc: 0.5454 -- iter: 0736/1192
[A[ATraining Step: 176  | total loss: [1m[32m0.69140[0m[0m | time: 25.786s
[2K
| RMSProp | epoch: 005 | loss: 0.69140 - acc: 0.5502 -- iter: 0768/1192
[A[ATraining Step: 177  | total loss: [1m[32m0.69135[0m[0m | time: 26.883s
[2K
| RMSProp | epoch: 005 | loss: 0.69135 - acc: 0.5483 -- iter: 0800/1192
[A[ATraining Step: 178  | total loss: [1m[32m0.69166[0m[0m | time: 28.088s
[2K
| RMSProp | epoch: 005 | loss: 0.69166 - acc: 0.5435 -- iter: 0832/1192
[A[ATraining Step: 179  | total loss: [1m[32m0.69192[0m[0m | time: 29.256s
[2K
| RMSProp | epoch: 005 | loss: 0.69192 - acc: 0.5392 -- iter: 0864/1192
[A[ATraining Step: 180  | total loss: [1m[32m0.69182[0m[0m | time: 30.407s
[2K
| RMSProp | epoch: 005 | loss: 0.69182 - acc: 0.5384 -- iter: 0896/1192
[A[ATraining Step: 181  | total loss: [1m[32m0.69301[0m[0m | time: 31.513s
[2K
| RMSProp | epoch: 005 | loss: 0.69301 - acc: 0.5252 -- iter: 0928/1192
[A[ATraining Step: 182  | total loss: [1m[32m0.69335[0m[0m | time: 32.706s
[2K
| RMSProp | epoch: 005 | loss: 0.69335 - acc: 0.5164 -- iter: 0960/1192
[A[ATraining Step: 183  | total loss: [1m[32m0.69309[0m[0m | time: 34.070s
[2K
| RMSProp | epoch: 005 | loss: 0.69309 - acc: 0.5210 -- iter: 0992/1192
[A[ATraining Step: 184  | total loss: [1m[32m0.69336[0m[0m | time: 35.361s
[2K
| RMSProp | epoch: 005 | loss: 0.69336 - acc: 0.5158 -- iter: 1024/1192
[A[ATraining Step: 185  | total loss: [1m[32m0.69315[0m[0m | time: 36.538s
[2K
| RMSProp | epoch: 005 | loss: 0.69315 - acc: 0.5173 -- iter: 1056/1192
[A[ATraining Step: 186  | total loss: [1m[32m0.69299[0m[0m | time: 37.477s
[2K
| RMSProp | epoch: 005 | loss: 0.69299 - acc: 0.5187 -- iter: 1088/1192
[A[ATraining Step: 187  | total loss: [1m[32m0.69348[0m[0m | time: 38.517s
[2K
| RMSProp | epoch: 005 | loss: 0.69348 - acc: 0.5106 -- iter: 1120/1192
[A[ATraining Step: 188  | total loss: [1m[32m0.69318[0m[0m | time: 39.689s
[2K
| RMSProp | epoch: 005 | loss: 0.69318 - acc: 0.5158 -- iter: 1152/1192
[A[ATraining Step: 189  | total loss: [1m[32m0.69267[0m[0m | time: 40.749s
[2K
| RMSProp | epoch: 005 | loss: 0.69267 - acc: 0.5236 -- iter: 1184/1192
[A[ATraining Step: 190  | total loss: [1m[32m0.69231[0m[0m | time: 44.094s
[2K
| RMSProp | epoch: 005 | loss: 0.69231 - acc: 0.5275 | val_loss: 0.69668 - val_acc: 0.4611 -- iter: 1192/1192
--
Training Step: 191  | total loss: [1m[32m0.69243[0m[0m | time: 1.173s
[2K
| RMSProp | epoch: 006 | loss: 0.69243 - acc: 0.5247 -- iter: 0032/1192
[A[ATraining Step: 192  | total loss: [1m[32m0.69178[0m[0m | time: 2.292s
[2K
| RMSProp | epoch: 006 | loss: 0.69178 - acc: 0.5316 -- iter: 0064/1192
[A[ATraining Step: 193  | total loss: [1m[32m0.69247[0m[0m | time: 3.333s
[2K
| RMSProp | epoch: 006 | loss: 0.69247 - acc: 0.5253 -- iter: 0096/1192
[A[ATraining Step: 194  | total loss: [1m[32m0.69229[0m[0m | time: 3.664s
[2K
| RMSProp | epoch: 006 | loss: 0.69229 - acc: 0.5259 -- iter: 0128/1192
[A[ATraining Step: 195  | total loss: [1m[32m0.69142[0m[0m | time: 3.973s
[2K
| RMSProp | epoch: 006 | loss: 0.69142 - acc: 0.5358 -- iter: 0160/1192
[A[ATraining Step: 196  | total loss: [1m[32m0.68990[0m[0m | time: 5.326s
[2K
| RMSProp | epoch: 006 | loss: 0.68990 - acc: 0.5448 -- iter: 0192/1192
[A[ATraining Step: 197  | total loss: [1m[32m0.68721[0m[0m | time: 6.732s
[2K
| RMSProp | epoch: 006 | loss: 0.68721 - acc: 0.5528 -- iter: 0224/1192
[A[ATraining Step: 198  | total loss: [1m[32m0.69084[0m[0m | time: 7.962s
[2K
| RMSProp | epoch: 006 | loss: 0.69084 - acc: 0.5538 -- iter: 0256/1192
[A[ATraining Step: 199  | total loss: [1m[32m0.69214[0m[0m | time: 8.888s
[2K
| RMSProp | epoch: 006 | loss: 0.69214 - acc: 0.5421 -- iter: 0288/1192
[A[ATraining Step: 200  | total loss: [1m[32m0.69264[0m[0m | time: 12.198s
[2K
| RMSProp | epoch: 006 | loss: 0.69264 - acc: 0.5348 | val_loss: 0.69472 - val_acc: 0.4611 -- iter: 0320/1192
--
Training Step: 201  | total loss: [1m[32m0.69447[0m[0m | time: 13.298s
[2K
| RMSProp | epoch: 006 | loss: 0.69447 - acc: 0.5094 -- iter: 0352/1192
[A[ATraining Step: 202  | total loss: [1m[32m0.69435[0m[0m | time: 14.391s
[2K
| RMSProp | epoch: 006 | loss: 0.69435 - acc: 0.5085 -- iter: 0384/1192
[A[ATraining Step: 203  | total loss: [1m[32m0.69403[0m[0m | time: 15.644s
[2K
| RMSProp | epoch: 006 | loss: 0.69403 - acc: 0.5139 -- iter: 0416/1192
[A[ATraining Step: 204  | total loss: [1m[32m0.69426[0m[0m | time: 16.920s
[2K
| RMSProp | epoch: 006 | loss: 0.69426 - acc: 0.5063 -- iter: 0448/1192
[A[ATraining Step: 205  | total loss: [1m[32m0.69384[0m[0m | time: 18.062s
[2K
| RMSProp | epoch: 006 | loss: 0.69384 - acc: 0.5150 -- iter: 0480/1192
[A[ATraining Step: 206  | total loss: [1m[32m0.69366[0m[0m | time: 19.092s
[2K
| RMSProp | epoch: 006 | loss: 0.69366 - acc: 0.5166 -- iter: 0512/1192
[A[ATraining Step: 207  | total loss: [1m[32m0.69411[0m[0m | time: 20.345s
[2K
| RMSProp | epoch: 006 | loss: 0.69411 - acc: 0.5056 -- iter: 0544/1192
[A[ATraining Step: 208  | total loss: [1m[32m0.69317[0m[0m | time: 21.799s
[2K
| RMSProp | epoch: 006 | loss: 0.69317 - acc: 0.5300 -- iter: 0576/1192
[A[ATraining Step: 209  | total loss: [1m[32m0.69284[0m[0m | time: 23.100s
[2K
| RMSProp | epoch: 006 | loss: 0.69284 - acc: 0.5333 -- iter: 0608/1192
[A[ATraining Step: 210  | total loss: [1m[32m0.69294[0m[0m | time: 28.404s
[2K
| RMSProp | epoch: 006 | loss: 0.69294 - acc: 0.5299 -- iter: 0640/1192
[A[ATraining Step: 211  | total loss: [1m[32m0.69224[0m[0m | time: 35.985s
[2K
| RMSProp | epoch: 006 | loss: 0.69224 - acc: 0.5395 -- iter: 0672/1192
[A[ATraining Step: 212  | total loss: [1m[32m0.69131[0m[0m | time: 40.374s
[2K
| RMSProp | epoch: 006 | loss: 0.69131 - acc: 0.5480 -- iter: 0704/1192
[A[ATraining Step: 213  | total loss: [1m[32m0.69418[0m[0m | time: 41.416s
[2K
| RMSProp | epoch: 006 | loss: 0.69418 - acc: 0.5276 -- iter: 0736/1192
[A[ATraining Step: 214  | total loss: [1m[32m0.69448[0m[0m | time: 42.493s
[2K
| RMSProp | epoch: 006 | loss: 0.69448 - acc: 0.5186 -- iter: 0768/1192
[A[ATraining Step: 215  | total loss: [1m[32m0.69468[0m[0m | time: 43.602s
[2K
| RMSProp | epoch: 006 | loss: 0.69468 - acc: 0.5105 -- iter: 0800/1192
[A[ATraining Step: 216  | total loss: [1m[32m0.69514[0m[0m | time: 44.766s
[2K
| RMSProp | epoch: 006 | loss: 0.69514 - acc: 0.4938 -- iter: 0832/1192
[A[ATraining Step: 217  | total loss: [1m[32m0.69502[0m[0m | time: 45.994s
[2K
| RMSProp | epoch: 006 | loss: 0.69502 - acc: 0.4882 -- iter: 0864/1192
[A[ATraining Step: 218  | total loss: [1m[32m0.69482[0m[0m | time: 47.093s
[2K
| RMSProp | epoch: 006 | loss: 0.69482 - acc: 0.4893 -- iter: 0896/1192
[A[ATraining Step: 219  | total loss: [1m[32m0.69458[0m[0m | time: 48.189s
[2K
| RMSProp | epoch: 006 | loss: 0.69458 - acc: 0.5123 -- iter: 0928/1192
[A[ATraining Step: 220  | total loss: [1m[32m0.69446[0m[0m | time: 49.359s
[2K
| RMSProp | epoch: 006 | loss: 0.69446 - acc: 0.5111 -- iter: 0960/1192
[A[ATraining Step: 221  | total loss: [1m[32m0.69424[0m[0m | time: 50.399s
[2K
| RMSProp | epoch: 006 | loss: 0.69424 - acc: 0.5131 -- iter: 0992/1192
[A[ATraining Step: 222  | total loss: [1m[32m0.69412[0m[0m | time: 51.559s
[2K
| RMSProp | epoch: 006 | loss: 0.69412 - acc: 0.5118 -- iter: 1024/1192
[A[ATraining Step: 223  | total loss: [1m[32m0.69451[0m[0m | time: 52.890s
[2K
| RMSProp | epoch: 006 | loss: 0.69451 - acc: 0.4918 -- iter: 1056/1192
[A[ATraining Step: 224  | total loss: [1m[32m0.69436[0m[0m | time: 54.282s
[2K
| RMSProp | epoch: 006 | loss: 0.69436 - acc: 0.4927 -- iter: 1088/1192
[A[ATraining Step: 225  | total loss: [1m[32m0.69435[0m[0m | time: 55.503s
[2K
| RMSProp | epoch: 006 | loss: 0.69435 - acc: 0.4809 -- iter: 1120/1192
[A[ATraining Step: 226  | total loss: [1m[32m0.69399[0m[0m | time: 56.619s
[2K
| RMSProp | epoch: 006 | loss: 0.69399 - acc: 0.4922 -- iter: 1152/1192
[A[ATraining Step: 227  | total loss: [1m[32m0.69341[0m[0m | time: 57.586s
[2K
| RMSProp | epoch: 006 | loss: 0.69341 - acc: 0.5023 -- iter: 1184/1192
[A[ATraining Step: 228  | total loss: [1m[32m0.69177[0m[0m | time: 61.034s
[2K
| RMSProp | epoch: 006 | loss: 0.69177 - acc: 0.5240 | val_loss: 0.73649 - val_acc: 0.4611 -- iter: 1192/1192
--
Training Step: 229  | total loss: [1m[32m0.68952[0m[0m | time: 1.196s
[2K
| RMSProp | epoch: 007 | loss: 0.68952 - acc: 0.5372 -- iter: 0032/1192
[A[ATraining Step: 230  | total loss: [1m[32m0.68728[0m[0m | time: 2.353s
[2K
| RMSProp | epoch: 007 | loss: 0.68728 - acc: 0.5460 -- iter: 0064/1192
[A[ATraining Step: 231  | total loss: [1m[32m0.68761[0m[0m | time: 3.507s
[2K
| RMSProp | epoch: 007 | loss: 0.68761 - acc: 0.5476 -- iter: 0096/1192
[A[ATraining Step: 232  | total loss: [1m[32m0.68689[0m[0m | time: 4.530s
[2K
| RMSProp | epoch: 007 | loss: 0.68689 - acc: 0.5522 -- iter: 0128/1192
[A[ATraining Step: 233  | total loss: [1m[32m0.69012[0m[0m | time: 4.854s
[2K
| RMSProp | epoch: 007 | loss: 0.69012 - acc: 0.5408 -- iter: 0160/1192
[A[ATraining Step: 234  | total loss: [1m[32m0.69185[0m[0m | time: 5.249s
[2K
| RMSProp | epoch: 007 | loss: 0.69185 - acc: 0.5242 -- iter: 0192/1192
[A[ATraining Step: 235  | total loss: [1m[32m0.69278[0m[0m | time: 6.519s
[2K
| RMSProp | epoch: 007 | loss: 0.69278 - acc: 0.5093 -- iter: 0224/1192
[A[ATraining Step: 236  | total loss: [1m[32m0.69272[0m[0m | time: 7.747s
[2K
| RMSProp | epoch: 007 | loss: 0.69272 - acc: 0.5115 -- iter: 0256/1192
[A[ATraining Step: 237  | total loss: [1m[32m0.69327[0m[0m | time: 9.200s
[2K
| RMSProp | epoch: 007 | loss: 0.69327 - acc: 0.4978 -- iter: 0288/1192
[A[ATraining Step: 238  | total loss: [1m[32m0.69338[0m[0m | time: 10.232s
[2K
| RMSProp | epoch: 007 | loss: 0.69338 - acc: 0.4887 -- iter: 0320/1192
[A[ATraining Step: 239  | total loss: [1m[32m0.69335[0m[0m | time: 11.298s
[2K
| RMSProp | epoch: 007 | loss: 0.69335 - acc: 0.4804 -- iter: 0352/1192
[A[ATraining Step: 240  | total loss: [1m[32m0.69321[0m[0m | time: 12.266s
[2K
| RMSProp | epoch: 007 | loss: 0.69321 - acc: 0.4886 -- iter: 0384/1192
[A[ATraining Step: 241  | total loss: [1m[32m0.69297[0m[0m | time: 13.365s
[2K
| RMSProp | epoch: 007 | loss: 0.69297 - acc: 0.4960 -- iter: 0416/1192
[A[ATraining Step: 242  | total loss: [1m[32m0.69332[0m[0m | time: 14.476s
[2K
| RMSProp | epoch: 007 | loss: 0.69332 - acc: 0.4902 -- iter: 0448/1192
[A[ATraining Step: 243  | total loss: [1m[32m0.69300[0m[0m | time: 15.666s
[2K
| RMSProp | epoch: 007 | loss: 0.69300 - acc: 0.5005 -- iter: 0480/1192
[A[ATraining Step: 244  | total loss: [1m[32m0.69303[0m[0m | time: 16.814s
[2K
| RMSProp | epoch: 007 | loss: 0.69303 - acc: 0.5005 -- iter: 0512/1192
[A[ATraining Step: 245  | total loss: [1m[32m0.69308[0m[0m | time: 17.919s
[2K
| RMSProp | epoch: 007 | loss: 0.69308 - acc: 0.5004 -- iter: 0544/1192
[A[ATraining Step: 246  | total loss: [1m[32m0.69274[0m[0m | time: 19.207s
[2K
| RMSProp | epoch: 007 | loss: 0.69274 - acc: 0.5098 -- iter: 0576/1192
[A[ATraining Step: 247  | total loss: [1m[32m0.69233[0m[0m | time: 20.433s
[2K
| RMSProp | epoch: 007 | loss: 0.69233 - acc: 0.5150 -- iter: 0608/1192
[A[ATraining Step: 248  | total loss: [1m[32m0.69318[0m[0m | time: 21.539s
[2K
| RMSProp | epoch: 007 | loss: 0.69318 - acc: 0.5042 -- iter: 0640/1192
[A[ATraining Step: 249  | total loss: [1m[32m0.69314[0m[0m | time: 22.839s
[2K
| RMSProp | epoch: 007 | loss: 0.69314 - acc: 0.5037 -- iter: 0672/1192
[A[ATraining Step: 250  | total loss: [1m[32m0.69243[0m[0m | time: 24.189s
[2K
| RMSProp | epoch: 007 | loss: 0.69243 - acc: 0.5221 -- iter: 0704/1192
[A[ATraining Step: 251  | total loss: [1m[32m0.69276[0m[0m | time: 25.564s
[2K
| RMSProp | epoch: 007 | loss: 0.69276 - acc: 0.5168 -- iter: 0736/1192
[A[ATraining Step: 252  | total loss: [1m[32m0.69206[0m[0m | time: 26.765s
[2K
| RMSProp | epoch: 007 | loss: 0.69206 - acc: 0.5245 -- iter: 0768/1192
[A[ATraining Step: 253  | total loss: [1m[32m0.69211[0m[0m | time: 27.827s
[2K
| RMSProp | epoch: 007 | loss: 0.69211 - acc: 0.5220 -- iter: 0800/1192
[A[ATraining Step: 254  | total loss: [1m[32m0.69184[0m[0m | time: 28.826s
[2K
| RMSProp | epoch: 007 | loss: 0.69184 - acc: 0.5230 -- iter: 0832/1192
[A[ATraining Step: 255  | total loss: [1m[32m0.69182[0m[0m | time: 30.003s
[2K
| RMSProp | epoch: 007 | loss: 0.69182 - acc: 0.5207 -- iter: 0864/1192
[A[ATraining Step: 256  | total loss: [1m[32m0.69115[0m[0m | time: 31.077s
[2K
| RMSProp | epoch: 007 | loss: 0.69115 - acc: 0.5248 -- iter: 0896/1192
[A[ATraining Step: 257  | total loss: [1m[32m0.69298[0m[0m | time: 32.211s
[2K
| RMSProp | epoch: 007 | loss: 0.69298 - acc: 0.5130 -- iter: 0928/1192
[A[ATraining Step: 258  | total loss: [1m[32m0.69236[0m[0m | time: 33.297s
[2K
| RMSProp | epoch: 007 | loss: 0.69236 - acc: 0.5179 -- iter: 0960/1192
[A[ATraining Step: 259  | total loss: [1m[32m0.69169[0m[0m | time: 34.474s
[2K
| RMSProp | epoch: 007 | loss: 0.69169 - acc: 0.5193 -- iter: 0992/1192
[A[ATraining Step: 260  | total loss: [1m[32m0.69078[0m[0m | time: 35.626s
[2K
| RMSProp | epoch: 007 | loss: 0.69078 - acc: 0.5267 -- iter: 1024/1192
[A[ATraining Step: 261  | total loss: [1m[32m0.69173[0m[0m | time: 36.850s
[2K
| RMSProp | epoch: 007 | loss: 0.69173 - acc: 0.5178 -- iter: 1056/1192
[A[ATraining Step: 262  | total loss: [1m[32m0.69178[0m[0m | time: 38.005s
[2K
| RMSProp | epoch: 007 | loss: 0.69178 - acc: 0.5098 -- iter: 1088/1192
[A[ATraining Step: 263  | total loss: [1m[32m0.69092[0m[0m | time: 39.210s
[2K
| RMSProp | epoch: 007 | loss: 0.69092 - acc: 0.5338 -- iter: 1120/1192
[A[ATraining Step: 264  | total loss: [1m[32m0.69116[0m[0m | time: 40.626s
[2K
| RMSProp | epoch: 007 | loss: 0.69116 - acc: 0.5210 -- iter: 1152/1192
[A[ATraining Step: 265  | total loss: [1m[32m0.69077[0m[0m | time: 41.872s
[2K
| RMSProp | epoch: 007 | loss: 0.69077 - acc: 0.5314 -- iter: 1184/1192
[A[ATraining Step: 266  | total loss: [1m[32m0.68858[0m[0m | time: 45.026s
[2K
| RMSProp | epoch: 007 | loss: 0.68858 - acc: 0.5564 | val_loss: 0.68386 - val_acc: 0.5121 -- iter: 1192/1192
--
Training Step: 267  | total loss: [1m[32m0.68946[0m[0m | time: 1.110s
[2K
| RMSProp | epoch: 008 | loss: 0.68946 - acc: 0.5508 -- iter: 0032/1192
[A[ATraining Step: 268  | total loss: [1m[32m0.68946[0m[0m | time: 2.332s
[2K
| RMSProp | epoch: 008 | loss: 0.68946 - acc: 0.5394 -- iter: 0064/1192
[A[ATraining Step: 269  | total loss: [1m[32m0.68785[0m[0m | time: 3.465s
[2K
| RMSProp | epoch: 008 | loss: 0.68785 - acc: 0.5636 -- iter: 0096/1192
[A[ATraining Step: 270  | total loss: [1m[32m0.68401[0m[0m | time: 4.637s
[2K
| RMSProp | epoch: 008 | loss: 0.68401 - acc: 0.5635 -- iter: 0128/1192
[A[ATraining Step: 271  | total loss: [1m[32m0.67823[0m[0m | time: 5.754s
[2K
| RMSProp | epoch: 008 | loss: 0.67823 - acc: 0.5884 -- iter: 0160/1192
[A[ATraining Step: 272  | total loss: [1m[32m0.67484[0m[0m | time: 6.058s
[2K
| RMSProp | epoch: 008 | loss: 0.67484 - acc: 0.5889 -- iter: 0192/1192
[A[ATraining Step: 273  | total loss: [1m[32m0.67901[0m[0m | time: 6.387s
[2K
| RMSProp | epoch: 008 | loss: 0.67901 - acc: 0.5800 -- iter: 0224/1192
[A[ATraining Step: 274  | total loss: [1m[32m0.66068[0m[0m | time: 7.482s
[2K
| RMSProp | epoch: 008 | loss: 0.66068 - acc: 0.6095 -- iter: 0256/1192
[A[ATraining Step: 275  | total loss: [1m[32m0.66004[0m[0m | time: 8.738s
[2K
| RMSProp | epoch: 008 | loss: 0.66004 - acc: 0.6111 -- iter: 0288/1192
[A[ATraining Step: 276  | total loss: [1m[32m0.66486[0m[0m | time: 10.109s
[2K
| RMSProp | epoch: 008 | loss: 0.66486 - acc: 0.6062 -- iter: 0320/1192
[A[ATraining Step: 277  | total loss: [1m[32m0.66350[0m[0m | time: 11.433s
[2K
| RMSProp | epoch: 008 | loss: 0.66350 - acc: 0.6175 -- iter: 0352/1192
[A[ATraining Step: 278  | total loss: [1m[32m0.66256[0m[0m | time: 12.540s
[2K
| RMSProp | epoch: 008 | loss: 0.66256 - acc: 0.6214 -- iter: 0384/1192
[A[ATraining Step: 279  | total loss: [1m[32m0.65644[0m[0m | time: 13.707s
[2K
| RMSProp | epoch: 008 | loss: 0.65644 - acc: 0.6342 -- iter: 0416/1192
[A[ATraining Step: 280  | total loss: [1m[32m0.64676[0m[0m | time: 14.662s
[2K
| RMSProp | epoch: 008 | loss: 0.64676 - acc: 0.6427 -- iter: 0448/1192
[A[ATraining Step: 281  | total loss: [1m[32m0.63499[0m[0m | time: 15.752s
[2K
| RMSProp | epoch: 008 | loss: 0.63499 - acc: 0.6534 -- iter: 0480/1192
[A[ATraining Step: 282  | total loss: [1m[32m0.63653[0m[0m | time: 16.987s
[2K
| RMSProp | epoch: 008 | loss: 0.63653 - acc: 0.6537 -- iter: 0512/1192
[A[ATraining Step: 283  | total loss: [1m[32m0.65045[0m[0m | time: 18.260s
[2K
| RMSProp | epoch: 008 | loss: 0.65045 - acc: 0.6289 -- iter: 0544/1192
[A[ATraining Step: 284  | total loss: [1m[32m0.65658[0m[0m | time: 19.388s
[2K
| RMSProp | epoch: 008 | loss: 0.65658 - acc: 0.6192 -- iter: 0576/1192
[A[ATraining Step: 285  | total loss: [1m[32m0.65287[0m[0m | time: 20.545s
[2K
| RMSProp | epoch: 008 | loss: 0.65287 - acc: 0.6354 -- iter: 0608/1192
[A[ATraining Step: 286  | total loss: [1m[32m0.64542[0m[0m | time: 21.717s
[2K
| RMSProp | epoch: 008 | loss: 0.64542 - acc: 0.6468 -- iter: 0640/1192
[A[ATraining Step: 287  | total loss: [1m[32m0.63421[0m[0m | time: 22.809s
[2K
| RMSProp | epoch: 008 | loss: 0.63421 - acc: 0.6634 -- iter: 0672/1192
[A[ATraining Step: 288  | total loss: [1m[32m0.63026[0m[0m | time: 24.040s
[2K
| RMSProp | epoch: 008 | loss: 0.63026 - acc: 0.6721 -- iter: 0704/1192
[A[ATraining Step: 289  | total loss: [1m[32m0.62301[0m[0m | time: 25.371s
[2K
| RMSProp | epoch: 008 | loss: 0.62301 - acc: 0.6767 -- iter: 0736/1192
[A[ATraining Step: 290  | total loss: [1m[32m0.61742[0m[0m | time: 26.721s
[2K
| RMSProp | epoch: 008 | loss: 0.61742 - acc: 0.6809 -- iter: 0768/1192
[A[ATraining Step: 291  | total loss: [1m[32m0.61670[0m[0m | time: 27.876s
[2K
| RMSProp | epoch: 008 | loss: 0.61670 - acc: 0.6722 -- iter: 0800/1192
[A[ATraining Step: 292  | total loss: [1m[32m0.61285[0m[0m | time: 29.184s
[2K
| RMSProp | epoch: 008 | loss: 0.61285 - acc: 0.6769 -- iter: 0832/1192
[A[ATraining Step: 293  | total loss: [1m[32m0.60369[0m[0m | time: 30.314s
[2K
| RMSProp | epoch: 008 | loss: 0.60369 - acc: 0.6873 -- iter: 0864/1192
[A[ATraining Step: 294  | total loss: [1m[32m0.60465[0m[0m | time: 31.354s
[2K
| RMSProp | epoch: 008 | loss: 0.60465 - acc: 0.6842 -- iter: 0896/1192
[A[ATraining Step: 295  | total loss: [1m[32m0.62563[0m[0m | time: 32.514s
[2K
| RMSProp | epoch: 008 | loss: 0.62563 - acc: 0.6564 -- iter: 0928/1192
[A[ATraining Step: 296  | total loss: [1m[32m0.61856[0m[0m | time: 33.652s
[2K
| RMSProp | epoch: 008 | loss: 0.61856 - acc: 0.6689 -- iter: 0960/1192
[A[ATraining Step: 297  | total loss: [1m[32m0.60967[0m[0m | time: 34.854s
[2K
| RMSProp | epoch: 008 | loss: 0.60967 - acc: 0.6833 -- iter: 0992/1192
[A[ATraining Step: 298  | total loss: [1m[32m0.59728[0m[0m | time: 35.911s
[2K
| RMSProp | epoch: 008 | loss: 0.59728 - acc: 0.6868 -- iter: 1024/1192
[A[ATraining Step: 299  | total loss: [1m[32m0.60402[0m[0m | time: 37.100s
[2K
| RMSProp | epoch: 008 | loss: 0.60402 - acc: 0.6775 -- iter: 1056/1192
[A[ATraining Step: 300  | total loss: [1m[32m0.60112[0m[0m | time: 38.134s
[2K
| RMSProp | epoch: 008 | loss: 0.60112 - acc: 0.6816 -- iter: 1088/1192
[A[ATraining Step: 301  | total loss: [1m[32m0.59251[0m[0m | time: 39.371s
[2K
| RMSProp | epoch: 008 | loss: 0.59251 - acc: 0.6947 -- iter: 1120/1192
[A[ATraining Step: 302  | total loss: [1m[32m0.59485[0m[0m | time: 40.785s
[2K
| RMSProp | epoch: 008 | loss: 0.59485 - acc: 0.6909 -- iter: 1152/1192
[A[ATraining Step: 303  | total loss: [1m[32m0.60364[0m[0m | time: 42.066s
[2K
| RMSProp | epoch: 008 | loss: 0.60364 - acc: 0.6874 -- iter: 1184/1192
[A[ATraining Step: 304  | total loss: [1m[32m0.59952[0m[0m | time: 45.836s
[2K
| RMSProp | epoch: 008 | loss: 0.59952 - acc: 0.6937 | val_loss: 0.53163 - val_acc: 0.7534 -- iter: 1192/1192
--
Training Step: 305  | total loss: [1m[32m0.59871[0m[0m | time: 0.931s
[2K
| RMSProp | epoch: 009 | loss: 0.59871 - acc: 0.6930 -- iter: 0032/1192
[A[ATraining Step: 306  | total loss: [1m[32m0.58225[0m[0m | time: 2.021s
[2K
| RMSProp | epoch: 009 | loss: 0.58225 - acc: 0.7112 -- iter: 0064/1192
[A[ATraining Step: 307  | total loss: [1m[32m0.58575[0m[0m | time: 3.217s
[2K
| RMSProp | epoch: 009 | loss: 0.58575 - acc: 0.7026 -- iter: 0096/1192
[A[ATraining Step: 308  | total loss: [1m[32m0.58535[0m[0m | time: 4.321s
[2K
| RMSProp | epoch: 009 | loss: 0.58535 - acc: 0.6980 -- iter: 0128/1192
[A[ATraining Step: 309  | total loss: [1m[32m0.58021[0m[0m | time: 5.445s
[2K
| RMSProp | epoch: 009 | loss: 0.58021 - acc: 0.7032 -- iter: 0160/1192
[A[ATraining Step: 310  | total loss: [1m[32m0.57785[0m[0m | time: 6.563s
[2K
| RMSProp | epoch: 009 | loss: 0.57785 - acc: 0.7016 -- iter: 0192/1192
[A[ATraining Step: 311  | total loss: [1m[32m0.57673[0m[0m | time: 6.906s
[2K
| RMSProp | epoch: 009 | loss: 0.57673 - acc: 0.7033 -- iter: 0224/1192
[A[ATraining Step: 312  | total loss: [1m[32m0.56296[0m[0m | time: 7.192s
[2K
| RMSProp | epoch: 009 | loss: 0.56296 - acc: 0.7205 -- iter: 0256/1192
[A[ATraining Step: 313  | total loss: [1m[32m0.54334[0m[0m | time: 8.347s
[2K
| RMSProp | epoch: 009 | loss: 0.54334 - acc: 0.7359 -- iter: 0288/1192
[A[ATraining Step: 314  | total loss: [1m[32m0.54851[0m[0m | time: 9.595s
[2K
| RMSProp | epoch: 009 | loss: 0.54851 - acc: 0.7280 -- iter: 0320/1192
[A[ATraining Step: 315  | total loss: [1m[32m0.53950[0m[0m | time: 10.910s
[2K
| RMSProp | epoch: 009 | loss: 0.53950 - acc: 0.7427 -- iter: 0352/1192
[A[ATraining Step: 316  | total loss: [1m[32m0.51797[0m[0m | time: 12.153s
[2K
| RMSProp | epoch: 009 | loss: 0.51797 - acc: 0.7590 -- iter: 0384/1192
[A[ATraining Step: 317  | total loss: [1m[32m0.51624[0m[0m | time: 13.177s
[2K
| RMSProp | epoch: 009 | loss: 0.51624 - acc: 0.7550 -- iter: 0416/1192
[A[ATraining Step: 318  | total loss: [1m[32m0.50068[0m[0m | time: 14.421s
[2K
| RMSProp | epoch: 009 | loss: 0.50068 - acc: 0.7608 -- iter: 0448/1192
[A[ATraining Step: 319  | total loss: [1m[32m0.49531[0m[0m | time: 15.720s
[2K
| RMSProp | epoch: 009 | loss: 0.49531 - acc: 0.7628 -- iter: 0480/1192
[A[ATraining Step: 320  | total loss: [1m[32m0.52298[0m[0m | time: 16.828s
[2K
| RMSProp | epoch: 009 | loss: 0.52298 - acc: 0.7522 -- iter: 0512/1192
[A[ATraining Step: 321  | total loss: [1m[32m0.51783[0m[0m | time: 17.878s
[2K
| RMSProp | epoch: 009 | loss: 0.51783 - acc: 0.7551 -- iter: 0544/1192
[A[ATraining Step: 322  | total loss: [1m[32m0.51694[0m[0m | time: 19.025s
[2K
| RMSProp | epoch: 009 | loss: 0.51694 - acc: 0.7546 -- iter: 0576/1192
[A[ATraining Step: 323  | total loss: [1m[32m0.51751[0m[0m | time: 20.176s
[2K
| RMSProp | epoch: 009 | loss: 0.51751 - acc: 0.7447 -- iter: 0608/1192
[A[ATraining Step: 324  | total loss: [1m[32m0.51935[0m[0m | time: 21.375s
[2K
| RMSProp | epoch: 009 | loss: 0.51935 - acc: 0.7359 -- iter: 0640/1192
[A[ATraining Step: 325  | total loss: [1m[32m0.51715[0m[0m | time: 22.544s
[2K
| RMSProp | epoch: 009 | loss: 0.51715 - acc: 0.7373 -- iter: 0672/1192
[A[ATraining Step: 326  | total loss: [1m[32m0.50246[0m[0m | time: 23.617s
[2K
| RMSProp | epoch: 009 | loss: 0.50246 - acc: 0.7511 -- iter: 0704/1192
[A[ATraining Step: 327  | total loss: [1m[32m0.51848[0m[0m | time: 24.778s
[2K
| RMSProp | epoch: 009 | loss: 0.51848 - acc: 0.7385 -- iter: 0736/1192
[A[ATraining Step: 328  | total loss: [1m[32m0.51028[0m[0m | time: 26.033s
[2K
| RMSProp | epoch: 009 | loss: 0.51028 - acc: 0.7490 -- iter: 0768/1192
[A[ATraining Step: 329  | total loss: [1m[32m0.49398[0m[0m | time: 27.463s
[2K
| RMSProp | epoch: 009 | loss: 0.49398 - acc: 0.7616 -- iter: 0800/1192
[A[ATraining Step: 330  | total loss: [1m[32m0.49408[0m[0m | time: 28.548s
[2K
| RMSProp | epoch: 009 | loss: 0.49408 - acc: 0.7573 -- iter: 0832/1192
[A[ATraining Step: 331  | total loss: [1m[32m0.49383[0m[0m | time: 29.664s
[2K
| RMSProp | epoch: 009 | loss: 0.49383 - acc: 0.7503 -- iter: 0864/1192
[A[ATraining Step: 332  | total loss: [1m[32m0.47199[0m[0m | time: 30.911s
[2K
| RMSProp | epoch: 009 | loss: 0.47199 - acc: 0.7659 -- iter: 0896/1192
[A[ATraining Step: 333  | total loss: [1m[32m0.46586[0m[0m | time: 32.161s
[2K
| RMSProp | epoch: 009 | loss: 0.46586 - acc: 0.7706 -- iter: 0928/1192
[A[ATraining Step: 334  | total loss: [1m[32m0.45361[0m[0m | time: 33.284s
[2K
| RMSProp | epoch: 009 | loss: 0.45361 - acc: 0.7748 -- iter: 0960/1192
[A[ATraining Step: 335  | total loss: [1m[32m0.45842[0m[0m | time: 34.352s
[2K
| RMSProp | epoch: 009 | loss: 0.45842 - acc: 0.7660 -- iter: 0992/1192
[A[ATraining Step: 336  | total loss: [1m[32m0.45681[0m[0m | time: 35.480s
[2K
| RMSProp | epoch: 009 | loss: 0.45681 - acc: 0.7738 -- iter: 1024/1192
[A[ATraining Step: 337  | total loss: [1m[32m0.44324[0m[0m | time: 36.704s
[2K
| RMSProp | epoch: 009 | loss: 0.44324 - acc: 0.7808 -- iter: 1056/1192
[A[ATraining Step: 338  | total loss: [1m[32m0.43944[0m[0m | time: 37.849s
[2K
| RMSProp | epoch: 009 | loss: 0.43944 - acc: 0.7871 -- iter: 1088/1192
[A[ATraining Step: 339  | total loss: [1m[32m0.42885[0m[0m | time: 38.945s
[2K
| RMSProp | epoch: 009 | loss: 0.42885 - acc: 0.7865 -- iter: 1120/1192
[A[ATraining Step: 340  | total loss: [1m[32m0.42255[0m[0m | time: 40.084s
[2K
| RMSProp | epoch: 009 | loss: 0.42255 - acc: 0.7922 -- iter: 1152/1192
[A[ATraining Step: 341  | total loss: [1m[32m0.43334[0m[0m | time: 41.286s
[2K
| RMSProp | epoch: 009 | loss: 0.43334 - acc: 0.7911 -- iter: 1184/1192
[A[ATraining Step: 342  | total loss: [1m[32m0.45896[0m[0m | time: 45.104s
[2K
| RMSProp | epoch: 009 | loss: 0.45896 - acc: 0.7808 | val_loss: 0.43121 - val_acc: 0.8633 -- iter: 1192/1192
--
Training Step: 343  | total loss: [1m[32m0.46355[0m[0m | time: 1.291s
[2K
| RMSProp | epoch: 010 | loss: 0.46355 - acc: 0.7746 -- iter: 0032/1192
[A[ATraining Step: 344  | total loss: [1m[32m0.45045[0m[0m | time: 2.602s
[2K
| RMSProp | epoch: 010 | loss: 0.45045 - acc: 0.7940 -- iter: 0064/1192
[A[ATraining Step: 345  | total loss: [1m[32m0.44378[0m[0m | time: 3.499s
[2K
| RMSProp | epoch: 010 | loss: 0.44378 - acc: 0.7958 -- iter: 0096/1192
[A[ATraining Step: 346  | total loss: [1m[32m0.47225[0m[0m | time: 4.528s
[2K
| RMSProp | epoch: 010 | loss: 0.47225 - acc: 0.7725 -- iter: 0128/1192
[A[ATraining Step: 347  | total loss: [1m[32m0.48654[0m[0m | time: 5.728s
[2K
| RMSProp | epoch: 010 | loss: 0.48654 - acc: 0.7578 -- iter: 0160/1192
[A[ATraining Step: 348  | total loss: [1m[32m0.48917[0m[0m | time: 6.870s
[2K
| RMSProp | epoch: 010 | loss: 0.48917 - acc: 0.7539 -- iter: 0192/1192
[A[ATraining Step: 349  | total loss: [1m[32m0.48428[0m[0m | time: 7.985s
[2K
| RMSProp | epoch: 010 | loss: 0.48428 - acc: 0.7597 -- iter: 0224/1192
[A[ATraining Step: 350  | total loss: [1m[32m0.48113[0m[0m | time: 8.324s
[2K
| RMSProp | epoch: 010 | loss: 0.48113 - acc: 0.7681 -- iter: 0256/1192
[A[ATraining Step: 351  | total loss: [1m[32m0.45002[0m[0m | time: 8.622s
[2K
| RMSProp | epoch: 010 | loss: 0.45002 - acc: 0.7913 -- iter: 0288/1192
[A[ATraining Step: 352  | total loss: [1m[32m0.41160[0m[0m | time: 9.744s
[2K
| RMSProp | epoch: 010 | loss: 0.41160 - acc: 0.8122 -- iter: 0320/1192
[A[ATraining Step: 353  | total loss: [1m[32m0.43787[0m[0m | time: 10.983s
[2K
| RMSProp | epoch: 010 | loss: 0.43787 - acc: 0.7997 -- iter: 0352/1192
[A[ATraining Step: 354  | total loss: [1m[32m0.44252[0m[0m | time: 12.293s
[2K
| RMSProp | epoch: 010 | loss: 0.44252 - acc: 0.8072 -- iter: 0384/1192
[A[ATraining Step: 355  | total loss: [1m[32m0.43770[0m[0m | time: 13.660s
[2K
| RMSProp | epoch: 010 | loss: 0.43770 - acc: 0.8109 -- iter: 0416/1192
[A[ATraining Step: 356  | total loss: [1m[32m0.43873[0m[0m | time: 14.598s
[2K
| RMSProp | epoch: 010 | loss: 0.43873 - acc: 0.7986 -- iter: 0448/1192
[A[ATraining Step: 357  | total loss: [1m[32m0.43272[0m[0m | time: 15.827s
[2K
| RMSProp | epoch: 010 | loss: 0.43272 - acc: 0.8062 -- iter: 0480/1192
[A[ATraining Step: 358  | total loss: [1m[32m0.45814[0m[0m | time: 17.029s
[2K
| RMSProp | epoch: 010 | loss: 0.45814 - acc: 0.7975 -- iter: 0512/1192
[A[ATraining Step: 359  | total loss: [1m[32m0.44702[0m[0m | time: 18.265s
[2K
| RMSProp | epoch: 010 | loss: 0.44702 - acc: 0.8115 -- iter: 0544/1192
[A[ATraining Step: 360  | total loss: [1m[32m0.43754[0m[0m | time: 19.502s
[2K
| RMSProp | epoch: 010 | loss: 0.43754 - acc: 0.8272 -- iter: 0576/1192
[A[ATraining Step: 361  | total loss: [1m[32m0.41401[0m[0m | time: 20.457s
[2K
| RMSProp | epoch: 010 | loss: 0.41401 - acc: 0.8382 -- iter: 0608/1192
[A[ATraining Step: 362  | total loss: [1m[32m0.39916[0m[0m | time: 21.637s
[2K
| RMSProp | epoch: 010 | loss: 0.39916 - acc: 0.8388 -- iter: 0640/1192
[A[ATraining Step: 363  | total loss: [1m[32m0.39069[0m[0m | time: 22.777s
[2K
| RMSProp | epoch: 010 | loss: 0.39069 - acc: 0.8455 -- iter: 0672/1192
[A[ATraining Step: 364  | total loss: [1m[32m0.41243[0m[0m | time: 23.945s
[2K
| RMSProp | epoch: 010 | loss: 0.41243 - acc: 0.8297 -- iter: 0704/1192
[A[ATraining Step: 365  | total loss: [1m[32m0.42085[0m[0m | time: 25.130s
[2K
| RMSProp | epoch: 010 | loss: 0.42085 - acc: 0.8280 -- iter: 0736/1192
[A[ATraining Step: 366  | total loss: [1m[32m0.41878[0m[0m | time: 26.410s
[2K
| RMSProp | epoch: 010 | loss: 0.41878 - acc: 0.8296 -- iter: 0768/1192
[A[ATraining Step: 367  | total loss: [1m[32m0.41257[0m[0m | time: 27.797s
[2K
| RMSProp | epoch: 010 | loss: 0.41257 - acc: 0.8310 -- iter: 0800/1192
[A[ATraining Step: 368  | total loss: [1m[32m0.39874[0m[0m | time: 29.173s
[2K
| RMSProp | epoch: 010 | loss: 0.39874 - acc: 0.8385 -- iter: 0832/1192
[A[ATraining Step: 369  | total loss: [1m[32m0.40514[0m[0m | time: 30.165s
[2K
| RMSProp | epoch: 010 | loss: 0.40514 - acc: 0.8390 -- iter: 0864/1192
[A[ATraining Step: 370  | total loss: [1m[32m0.39141[0m[0m | time: 31.296s
[2K
| RMSProp | epoch: 010 | loss: 0.39141 - acc: 0.8395 -- iter: 0896/1192
[A[ATraining Step: 371  | total loss: [1m[32m0.38329[0m[0m | time: 32.328s
[2K
| RMSProp | epoch: 010 | loss: 0.38329 - acc: 0.8493 -- iter: 0928/1192
[A[ATraining Step: 372  | total loss: [1m[32m0.37107[0m[0m | time: 33.560s
[2K
| RMSProp | epoch: 010 | loss: 0.37107 - acc: 0.8613 -- iter: 0960/1192
[A[ATraining Step: 373  | total loss: [1m[32m0.35539[0m[0m | time: 34.877s
[2K
| RMSProp | epoch: 010 | loss: 0.35539 - acc: 0.8751 -- iter: 0992/1192
[A[ATraining Step: 374  | total loss: [1m[32m0.34938[0m[0m | time: 36.178s
[2K
| RMSProp | epoch: 010 | loss: 0.34938 - acc: 0.8720 -- iter: 1024/1192
[A[ATraining Step: 375  | total loss: [1m[32m0.37736[0m[0m | time: 37.136s
[2K
| RMSProp | epoch: 010 | loss: 0.37736 - acc: 0.8598 -- iter: 1056/1192
[A[ATraining Step: 376  | total loss: [1m[32m0.36337[0m[0m | time: 38.237s
[2K
| RMSProp | epoch: 010 | loss: 0.36337 - acc: 0.8644 -- iter: 1088/1192
[A[ATraining Step: 377  | total loss: [1m[32m0.38429[0m[0m | time: 39.379s
[2K
| RMSProp | epoch: 010 | loss: 0.38429 - acc: 0.8499 -- iter: 1120/1192
[A[ATraining Step: 378  | total loss: [1m[32m0.38688[0m[0m | time: 40.385s
[2K
| RMSProp | epoch: 010 | loss: 0.38688 - acc: 0.8368 -- iter: 1152/1192
[A[ATraining Step: 379  | total loss: [1m[32m0.37939[0m[0m | time: 41.515s
[2K
| RMSProp | epoch: 010 | loss: 0.37939 - acc: 0.8406 -- iter: 1184/1192
[A[ATraining Step: 380  | total loss: [1m[32m0.36991[0m[0m | time: 45.505s
[2K
| RMSProp | epoch: 010 | loss: 0.36991 - acc: 0.8409 | val_loss: 0.50751 - val_acc: 0.7453 -- iter: 1192/1192
--
Training Step: 381  | total loss: [1m[32m0.36525[0m[0m | time: 1.211s
[2K
| RMSProp | epoch: 011 | loss: 0.36525 - acc: 0.8381 -- iter: 0032/1192
[A[ATraining Step: 382  | total loss: [1m[32m0.36340[0m[0m | time: 2.360s
[2K
| RMSProp | epoch: 011 | loss: 0.36340 - acc: 0.8418 -- iter: 0064/1192
[A[ATraining Step: 383  | total loss: [1m[32m0.34558[0m[0m | time: 3.611s
[2K
| RMSProp | epoch: 011 | loss: 0.34558 - acc: 0.8513 -- iter: 0096/1192
[A[ATraining Step: 384  | total loss: [1m[32m0.32710[0m[0m | time: 4.859s
[2K
| RMSProp | epoch: 011 | loss: 0.32710 - acc: 0.8631 -- iter: 0128/1192
[A[ATraining Step: 385  | total loss: [1m[32m0.38769[0m[0m | time: 6.072s
[2K
| RMSProp | epoch: 011 | loss: 0.38769 - acc: 0.8393 -- iter: 0160/1192
[A[ATraining Step: 386  | total loss: [1m[32m0.38895[0m[0m | time: 7.044s
[2K
| RMSProp | epoch: 011 | loss: 0.38895 - acc: 0.8397 -- iter: 0192/1192
[A[ATraining Step: 387  | total loss: [1m[32m0.37385[0m[0m | time: 8.129s
[2K
| RMSProp | epoch: 011 | loss: 0.37385 - acc: 0.8495 -- iter: 0224/1192
[A[ATraining Step: 388  | total loss: [1m[32m0.35694[0m[0m | time: 9.303s
[2K
| RMSProp | epoch: 011 | loss: 0.35694 - acc: 0.8552 -- iter: 0256/1192
[A[ATraining Step: 389  | total loss: [1m[32m0.34942[0m[0m | time: 9.639s
[2K
| RMSProp | epoch: 011 | loss: 0.34942 - acc: 0.8509 -- iter: 0288/1192
[A[ATraining Step: 390  | total loss: [1m[32m0.34512[0m[0m | time: 9.921s
[2K
| RMSProp | epoch: 011 | loss: 0.34512 - acc: 0.8533 -- iter: 0320/1192
[A[ATraining Step: 391  | total loss: [1m[32m0.32780[0m[0m | time: 11.044s
[2K
| RMSProp | epoch: 011 | loss: 0.32780 - acc: 0.8555 -- iter: 0352/1192
[A[ATraining Step: 392  | total loss: [1m[32m0.31541[0m[0m | time: 12.377s
[2K
| RMSProp | epoch: 011 | loss: 0.31541 - acc: 0.8606 -- iter: 0384/1192
[A[ATraining Step: 393  | total loss: [1m[32m0.30923[0m[0m | time: 13.677s
[2K
| RMSProp | epoch: 011 | loss: 0.30923 - acc: 0.8620 -- iter: 0416/1192
[A[ATraining Step: 394  | total loss: [1m[32m0.30652[0m[0m | time: 14.740s
[2K
| RMSProp | epoch: 011 | loss: 0.30652 - acc: 0.8602 -- iter: 0448/1192
[A[ATraining Step: 395  | total loss: [1m[32m0.33928[0m[0m | time: 15.726s
[2K
| RMSProp | epoch: 011 | loss: 0.33928 - acc: 0.8367 -- iter: 0480/1192
[A[ATraining Step: 396  | total loss: [1m[32m0.33509[0m[0m | time: 16.756s
[2K
| RMSProp | epoch: 011 | loss: 0.33509 - acc: 0.8436 -- iter: 0512/1192
[A[ATraining Step: 397  | total loss: [1m[32m0.33563[0m[0m | time: 17.863s
[2K
| RMSProp | epoch: 011 | loss: 0.33563 - acc: 0.8436 -- iter: 0544/1192
[A[ATraining Step: 398  | total loss: [1m[32m0.34253[0m[0m | time: 19.161s
[2K
| RMSProp | epoch: 011 | loss: 0.34253 - acc: 0.8468 -- iter: 0576/1192
[A[ATraining Step: 399  | total loss: [1m[32m0.35253[0m[0m | time: 20.474s
[2K
| RMSProp | epoch: 011 | loss: 0.35253 - acc: 0.8371 -- iter: 0608/1192
[A[ATraining Step: 400  | total loss: [1m[32m0.34272[0m[0m | time: 23.989s
[2K
| RMSProp | epoch: 011 | loss: 0.34272 - acc: 0.8471 | val_loss: 0.34746 - val_acc: 0.8445 -- iter: 0640/1192
--
Training Step: 401  | total loss: [1m[32m0.33975[0m[0m | time: 25.157s
[2K
| RMSProp | epoch: 011 | loss: 0.33975 - acc: 0.8530 -- iter: 0672/1192
[A[ATraining Step: 402  | total loss: [1m[32m0.33936[0m[0m | time: 26.249s
[2K
| RMSProp | epoch: 011 | loss: 0.33936 - acc: 0.8521 -- iter: 0704/1192
[A[ATraining Step: 403  | total loss: [1m[32m0.32480[0m[0m | time: 27.416s
[2K
| RMSProp | epoch: 011 | loss: 0.32480 - acc: 0.8638 -- iter: 0736/1192
[A[ATraining Step: 404  | total loss: [1m[32m0.32794[0m[0m | time: 28.746s
[2K
| RMSProp | epoch: 011 | loss: 0.32794 - acc: 0.8586 -- iter: 0768/1192
[A[ATraining Step: 405  | total loss: [1m[32m0.32647[0m[0m | time: 30.112s
[2K
| RMSProp | epoch: 011 | loss: 0.32647 - acc: 0.8603 -- iter: 0800/1192
[A[ATraining Step: 406  | total loss: [1m[32m0.32667[0m[0m | time: 31.053s
[2K
| RMSProp | epoch: 011 | loss: 0.32667 - acc: 0.8586 -- iter: 0832/1192
[A[ATraining Step: 407  | total loss: [1m[32m0.33137[0m[0m | time: 32.159s
[2K
| RMSProp | epoch: 011 | loss: 0.33137 - acc: 0.8571 -- iter: 0864/1192
[A[ATraining Step: 408  | total loss: [1m[32m0.34612[0m[0m | time: 33.245s
[2K
| RMSProp | epoch: 011 | loss: 0.34612 - acc: 0.8464 -- iter: 0896/1192
[A[ATraining Step: 409  | total loss: [1m[32m0.35423[0m[0m | time: 34.331s
[2K
| RMSProp | epoch: 011 | loss: 0.35423 - acc: 0.8430 -- iter: 0928/1192
[A[ATraining Step: 410  | total loss: [1m[32m0.33810[0m[0m | time: 35.446s
[2K
| RMSProp | epoch: 011 | loss: 0.33810 - acc: 0.8556 -- iter: 0960/1192
[A[ATraining Step: 411  | total loss: [1m[32m0.32527[0m[0m | time: 36.723s
[2K
| RMSProp | epoch: 011 | loss: 0.32527 - acc: 0.8607 -- iter: 0992/1192
[A[ATraining Step: 412  | total loss: [1m[32m0.31030[0m[0m | time: 37.940s
[2K
| RMSProp | epoch: 011 | loss: 0.31030 - acc: 0.8684 -- iter: 1024/1192
[A[ATraining Step: 413  | total loss: [1m[32m0.31740[0m[0m | time: 39.002s
[2K
| RMSProp | epoch: 011 | loss: 0.31740 - acc: 0.8659 -- iter: 1056/1192
[A[ATraining Step: 414  | total loss: [1m[32m0.31510[0m[0m | time: 40.199s
[2K
| RMSProp | epoch: 011 | loss: 0.31510 - acc: 0.8699 -- iter: 1088/1192
[A[ATraining Step: 415  | total loss: [1m[32m0.31228[0m[0m | time: 41.288s
[2K
| RMSProp | epoch: 011 | loss: 0.31228 - acc: 0.8736 -- iter: 1120/1192
[A[ATraining Step: 416  | total loss: [1m[32m0.29802[0m[0m | time: 42.513s
[2K
| RMSProp | epoch: 011 | loss: 0.29802 - acc: 0.8831 -- iter: 1152/1192
[A[ATraining Step: 417  | total loss: [1m[32m0.28953[0m[0m | time: 43.984s
[2K
| RMSProp | epoch: 011 | loss: 0.28953 - acc: 0.8885 -- iter: 1184/1192
[A[ATraining Step: 418  | total loss: [1m[32m0.29969[0m[0m | time: 47.512s
[2K
| RMSProp | epoch: 011 | loss: 0.29969 - acc: 0.8778 | val_loss: 0.32076 - val_acc: 0.8579 -- iter: 1192/1192
--
Training Step: 419  | total loss: [1m[32m0.29848[0m[0m | time: 1.067s
[2K
| RMSProp | epoch: 012 | loss: 0.29848 - acc: 0.8775 -- iter: 0032/1192
[A[ATraining Step: 420  | total loss: [1m[32m0.28467[0m[0m | time: 2.070s
[2K
| RMSProp | epoch: 012 | loss: 0.28467 - acc: 0.8804 -- iter: 0064/1192
[A[ATraining Step: 421  | total loss: [1m[32m0.26626[0m[0m | time: 3.364s
[2K
| RMSProp | epoch: 012 | loss: 0.26626 - acc: 0.8924 -- iter: 0096/1192
[A[ATraining Step: 422  | total loss: [1m[32m0.24844[0m[0m | time: 4.606s
[2K
| RMSProp | epoch: 012 | loss: 0.24844 - acc: 0.9000 -- iter: 0128/1192
[A[ATraining Step: 423  | total loss: [1m[32m0.25106[0m[0m | time: 5.960s
[2K
| RMSProp | epoch: 012 | loss: 0.25106 - acc: 0.8975 -- iter: 0160/1192
[A[ATraining Step: 424  | total loss: [1m[32m0.27275[0m[0m | time: 7.032s
[2K
| RMSProp | epoch: 012 | loss: 0.27275 - acc: 0.8859 -- iter: 0192/1192
[A[ATraining Step: 425  | total loss: [1m[32m0.26408[0m[0m | time: 8.072s
[2K
| RMSProp | epoch: 012 | loss: 0.26408 - acc: 0.8910 -- iter: 0224/1192
[A[ATraining Step: 426  | total loss: [1m[32m0.25883[0m[0m | time: 9.219s
[2K
| RMSProp | epoch: 012 | loss: 0.25883 - acc: 0.8926 -- iter: 0256/1192
[A[ATraining Step: 427  | total loss: [1m[32m0.25459[0m[0m | time: 10.256s
[2K
| RMSProp | epoch: 012 | loss: 0.25459 - acc: 0.8939 -- iter: 0288/1192
[A[ATraining Step: 428  | total loss: [1m[32m0.24789[0m[0m | time: 10.615s
[2K
| RMSProp | epoch: 012 | loss: 0.24789 - acc: 0.8952 -- iter: 0320/1192
[A[ATraining Step: 429  | total loss: [1m[32m0.23072[0m[0m | time: 10.995s
[2K
| RMSProp | epoch: 012 | loss: 0.23072 - acc: 0.9056 -- iter: 0352/1192
[A[ATraining Step: 430  | total loss: [1m[32m0.21373[0m[0m | time: 12.333s
[2K
| RMSProp | epoch: 012 | loss: 0.21373 - acc: 0.9151 -- iter: 0384/1192
[A[ATraining Step: 431  | total loss: [1m[32m0.20790[0m[0m | time: 13.601s
[2K
| RMSProp | epoch: 012 | loss: 0.20790 - acc: 0.9204 -- iter: 0416/1192
[A[ATraining Step: 432  | total loss: [1m[32m0.23903[0m[0m | time: 14.724s
[2K
| RMSProp | epoch: 012 | loss: 0.23903 - acc: 0.9096 -- iter: 0448/1192
[A[ATraining Step: 433  | total loss: [1m[32m0.24748[0m[0m | time: 15.815s
[2K
| RMSProp | epoch: 012 | loss: 0.24748 - acc: 0.8999 -- iter: 0480/1192
[A[ATraining Step: 434  | total loss: [1m[32m0.25307[0m[0m | time: 16.965s
[2K
| RMSProp | epoch: 012 | loss: 0.25307 - acc: 0.8943 -- iter: 0512/1192
[A[ATraining Step: 435  | total loss: [1m[32m0.24641[0m[0m | time: 18.077s
[2K
| RMSProp | epoch: 012 | loss: 0.24641 - acc: 0.8955 -- iter: 0544/1192
[A[ATraining Step: 436  | total loss: [1m[32m0.25448[0m[0m | time: 19.237s
[2K
| RMSProp | epoch: 012 | loss: 0.25448 - acc: 0.8903 -- iter: 0576/1192
[A[ATraining Step: 437  | total loss: [1m[32m0.25005[0m[0m | time: 20.406s
[2K
| RMSProp | epoch: 012 | loss: 0.25005 - acc: 0.8919 -- iter: 0608/1192
[A[ATraining Step: 438  | total loss: [1m[32m0.25267[0m[0m | time: 21.704s
[2K
| RMSProp | epoch: 012 | loss: 0.25267 - acc: 0.8902 -- iter: 0640/1192
[A[ATraining Step: 439  | total loss: [1m[32m0.24755[0m[0m | time: 23.059s
[2K
| RMSProp | epoch: 012 | loss: 0.24755 - acc: 0.8950 -- iter: 0672/1192
[A[ATraining Step: 440  | total loss: [1m[32m0.24113[0m[0m | time: 24.093s
[2K
| RMSProp | epoch: 012 | loss: 0.24113 - acc: 0.9023 -- iter: 0704/1192
[A[ATraining Step: 441  | total loss: [1m[32m0.23390[0m[0m | time: 25.173s
[2K
| RMSProp | epoch: 012 | loss: 0.23390 - acc: 0.9027 -- iter: 0736/1192
[A[ATraining Step: 442  | total loss: [1m[32m0.22762[0m[0m | time: 26.273s
[2K
| RMSProp | epoch: 012 | loss: 0.22762 - acc: 0.9062 -- iter: 0768/1192
[A[ATraining Step: 443  | total loss: [1m[32m0.22009[0m[0m | time: 27.507s
[2K
| RMSProp | epoch: 012 | loss: 0.22009 - acc: 0.9125 -- iter: 0800/1192
[A[ATraining Step: 444  | total loss: [1m[32m0.22571[0m[0m | time: 28.771s
[2K
| RMSProp | epoch: 012 | loss: 0.22571 - acc: 0.9087 -- iter: 0832/1192
[A[ATraining Step: 445  | total loss: [1m[32m0.22522[0m[0m | time: 29.865s
[2K
| RMSProp | epoch: 012 | loss: 0.22522 - acc: 0.9116 -- iter: 0864/1192
[A[ATraining Step: 446  | total loss: [1m[32m0.22336[0m[0m | time: 30.917s
[2K
| RMSProp | epoch: 012 | loss: 0.22336 - acc: 0.9111 -- iter: 0896/1192
[A[ATraining Step: 447  | total loss: [1m[32m0.24496[0m[0m | time: 32.078s
[2K
| RMSProp | epoch: 012 | loss: 0.24496 - acc: 0.9012 -- iter: 0928/1192
[A[ATraining Step: 448  | total loss: [1m[32m0.24040[0m[0m | time: 33.192s
[2K
| RMSProp | epoch: 012 | loss: 0.24040 - acc: 0.9048 -- iter: 0960/1192
[A[ATraining Step: 449  | total loss: [1m[32m0.22666[0m[0m | time: 34.299s
[2K
| RMSProp | epoch: 012 | loss: 0.22666 - acc: 0.9143 -- iter: 0992/1192
[A[ATraining Step: 450  | total loss: [1m[32m0.23215[0m[0m | time: 35.409s
[2K
| RMSProp | epoch: 012 | loss: 0.23215 - acc: 0.9135 -- iter: 1024/1192
[A[ATraining Step: 451  | total loss: [1m[32m0.22515[0m[0m | time: 36.597s
[2K
| RMSProp | epoch: 012 | loss: 0.22515 - acc: 0.9159 -- iter: 1056/1192
[A[ATraining Step: 452  | total loss: [1m[32m0.23309[0m[0m | time: 37.891s
[2K
| RMSProp | epoch: 012 | loss: 0.23309 - acc: 0.9150 -- iter: 1088/1192
[A[ATraining Step: 453  | total loss: [1m[32m0.23756[0m[0m | time: 39.151s
[2K
| RMSProp | epoch: 012 | loss: 0.23756 - acc: 0.9110 -- iter: 1120/1192
[A[ATraining Step: 454  | total loss: [1m[32m0.22653[0m[0m | time: 40.111s
[2K
| RMSProp | epoch: 012 | loss: 0.22653 - acc: 0.9199 -- iter: 1152/1192
[A[ATraining Step: 455  | total loss: [1m[32m0.22250[0m[0m | time: 41.188s
[2K
| RMSProp | epoch: 012 | loss: 0.22250 - acc: 0.9216 -- iter: 1184/1192
[A[ATraining Step: 456  | total loss: [1m[32m0.21612[0m[0m | time: 45.321s
[2K
| RMSProp | epoch: 012 | loss: 0.21612 - acc: 0.9201 | val_loss: 0.24015 - val_acc: 0.9088 -- iter: 1192/1192
--
Training Step: 457  | total loss: [1m[32m0.20526[0m[0m | time: 1.139s
[2K
| RMSProp | epoch: 013 | loss: 0.20526 - acc: 0.9250 -- iter: 0032/1192
[A[ATraining Step: 458  | total loss: [1m[32m0.20466[0m[0m | time: 2.306s
[2K
| RMSProp | epoch: 013 | loss: 0.20466 - acc: 0.9200 -- iter: 0064/1192
[A[ATraining Step: 459  | total loss: [1m[32m0.23321[0m[0m | time: 3.399s
[2K
| RMSProp | epoch: 013 | loss: 0.23321 - acc: 0.9061 -- iter: 0096/1192
[A[ATraining Step: 460  | total loss: [1m[32m0.24350[0m[0m | time: 4.457s
[2K
| RMSProp | epoch: 013 | loss: 0.24350 - acc: 0.8999 -- iter: 0128/1192
[A[ATraining Step: 461  | total loss: [1m[32m0.25673[0m[0m | time: 5.598s
[2K
| RMSProp | epoch: 013 | loss: 0.25673 - acc: 0.8880 -- iter: 0160/1192
[A[ATraining Step: 462  | total loss: [1m[32m0.25578[0m[0m | time: 6.733s
[2K
| RMSProp | epoch: 013 | loss: 0.25578 - acc: 0.8929 -- iter: 0192/1192
[A[ATraining Step: 463  | total loss: [1m[32m0.24851[0m[0m | time: 7.978s
[2K
| RMSProp | epoch: 013 | loss: 0.24851 - acc: 0.9005 -- iter: 0224/1192
[A[ATraining Step: 464  | total loss: [1m[32m0.23239[0m[0m | time: 9.173s
[2K
| RMSProp | epoch: 013 | loss: 0.23239 - acc: 0.9074 -- iter: 0256/1192
[A[ATraining Step: 465  | total loss: [1m[32m0.22652[0m[0m | time: 10.296s
[2K
| RMSProp | epoch: 013 | loss: 0.22652 - acc: 0.9104 -- iter: 0288/1192
[A[ATraining Step: 466  | total loss: [1m[32m0.22265[0m[0m | time: 11.387s
[2K
| RMSProp | epoch: 013 | loss: 0.22265 - acc: 0.9131 -- iter: 0320/1192
[A[ATraining Step: 467  | total loss: [1m[32m0.22063[0m[0m | time: 11.729s
[2K
| RMSProp | epoch: 013 | loss: 0.22063 - acc: 0.9124 -- iter: 0352/1192
[A[ATraining Step: 468  | total loss: [1m[32m0.25504[0m[0m | time: 12.055s
[2K
| RMSProp | epoch: 013 | loss: 0.25504 - acc: 0.8837 -- iter: 0384/1192
[A[ATraining Step: 469  | total loss: [1m[32m0.24818[0m[0m | time: 13.372s
[2K
| RMSProp | epoch: 013 | loss: 0.24818 - acc: 0.8828 -- iter: 0416/1192
[A[ATraining Step: 470  | total loss: [1m[32m0.25144[0m[0m | time: 14.652s
[2K
| RMSProp | epoch: 013 | loss: 0.25144 - acc: 0.8789 -- iter: 0448/1192
[A[ATraining Step: 471  | total loss: [1m[32m0.26988[0m[0m | time: 15.849s
[2K
| RMSProp | epoch: 013 | loss: 0.26988 - acc: 0.8722 -- iter: 0480/1192
[A[ATraining Step: 472  | total loss: [1m[32m0.26129[0m[0m | time: 16.904s
[2K
| RMSProp | epoch: 013 | loss: 0.26129 - acc: 0.8788 -- iter: 0512/1192
[A[ATraining Step: 473  | total loss: [1m[32m0.24834[0m[0m | time: 18.037s
[2K
| RMSProp | epoch: 013 | loss: 0.24834 - acc: 0.8909 -- iter: 0544/1192
[A[ATraining Step: 474  | total loss: [1m[32m0.23615[0m[0m | time: 19.253s
[2K
| RMSProp | epoch: 013 | loss: 0.23615 - acc: 0.8987 -- iter: 0576/1192
[A[ATraining Step: 475  | total loss: [1m[32m0.38806[0m[0m | time: 20.360s
[2K
| RMSProp | epoch: 013 | loss: 0.38806 - acc: 0.8619 -- iter: 0608/1192
[A[ATraining Step: 476  | total loss: [1m[32m0.37187[0m[0m | time: 21.416s
[2K
| RMSProp | epoch: 013 | loss: 0.37187 - acc: 0.8726 -- iter: 0640/1192
[A[ATraining Step: 477  | total loss: [1m[32m0.35162[0m[0m | time: 22.636s
[2K
| RMSProp | epoch: 013 | loss: 0.35162 - acc: 0.8791 -- iter: 0672/1192
[A[ATraining Step: 478  | total loss: [1m[32m0.33216[0m[0m | time: 23.939s
[2K
| RMSProp | epoch: 013 | loss: 0.33216 - acc: 0.8881 -- iter: 0704/1192
[A[ATraining Step: 479  | total loss: [1m[32m0.31129[0m[0m | time: 25.209s
[2K
| RMSProp | epoch: 013 | loss: 0.31129 - acc: 0.8961 -- iter: 0736/1192
[A[ATraining Step: 480  | total loss: [1m[32m0.29843[0m[0m | time: 26.208s
[2K
| RMSProp | epoch: 013 | loss: 0.29843 - acc: 0.9034 -- iter: 0768/1192
[A[ATraining Step: 481  | total loss: [1m[32m0.31116[0m[0m | time: 27.252s
[2K
| RMSProp | epoch: 013 | loss: 0.31116 - acc: 0.8943 -- iter: 0800/1192
[A[ATraining Step: 482  | total loss: [1m[32m0.29623[0m[0m | time: 28.470s
[2K
| RMSProp | epoch: 013 | loss: 0.29623 - acc: 0.8986 -- iter: 0832/1192
[A[ATraining Step: 483  | total loss: [1m[32m0.27759[0m[0m | time: 29.745s
[2K
| RMSProp | epoch: 013 | loss: 0.27759 - acc: 0.9056 -- iter: 0864/1192
[A[ATraining Step: 484  | total loss: [1m[32m0.25966[0m[0m | time: 30.887s
[2K
| RMSProp | epoch: 013 | loss: 0.25966 - acc: 0.9151 -- iter: 0896/1192
[A[ATraining Step: 485  | total loss: [1m[32m0.24478[0m[0m | time: 31.810s
[2K
| RMSProp | epoch: 013 | loss: 0.24478 - acc: 0.9204 -- iter: 0928/1192
[A[ATraining Step: 486  | total loss: [1m[32m0.22310[0m[0m | time: 32.929s
[2K
| RMSProp | epoch: 013 | loss: 0.22310 - acc: 0.9284 -- iter: 0960/1192
[A[ATraining Step: 487  | total loss: [1m[32m0.20936[0m[0m | time: 34.089s
[2K
| RMSProp | epoch: 013 | loss: 0.20936 - acc: 0.9324 -- iter: 0992/1192
[A[ATraining Step: 488  | total loss: [1m[32m0.23709[0m[0m | time: 35.217s
[2K
| RMSProp | epoch: 013 | loss: 0.23709 - acc: 0.9236 -- iter: 1024/1192
[A[ATraining Step: 489  | total loss: [1m[32m0.23571[0m[0m | time: 36.367s
[2K
| RMSProp | epoch: 013 | loss: 0.23571 - acc: 0.9187 -- iter: 1056/1192
[A[ATraining Step: 490  | total loss: [1m[32m0.23874[0m[0m | time: 37.450s
[2K
| RMSProp | epoch: 013 | loss: 0.23874 - acc: 0.9175 -- iter: 1088/1192
[A[ATraining Step: 491  | total loss: [1m[32m0.23351[0m[0m | time: 38.606s
[2K
| RMSProp | epoch: 013 | loss: 0.23351 - acc: 0.9163 -- iter: 1120/1192
[A[ATraining Step: 492  | total loss: [1m[32m0.23011[0m[0m | time: 39.807s
[2K
| RMSProp | epoch: 013 | loss: 0.23011 - acc: 0.9185 -- iter: 1152/1192
[A[ATraining Step: 493  | total loss: [1m[32m0.23372[0m[0m | time: 41.026s
[2K
| RMSProp | epoch: 013 | loss: 0.23372 - acc: 0.9204 -- iter: 1184/1192
[A[ATraining Step: 494  | total loss: [1m[32m0.22726[0m[0m | time: 44.183s
[2K
| RMSProp | epoch: 013 | loss: 0.22726 - acc: 0.9190 | val_loss: 0.47524 - val_acc: 0.7775 -- iter: 1192/1192
--
Training Step: 495  | total loss: [1m[32m0.23480[0m[0m | time: 1.233s
[2K
| RMSProp | epoch: 014 | loss: 0.23480 - acc: 0.9177 -- iter: 0032/1192
[A[ATraining Step: 496  | total loss: [1m[32m0.25722[0m[0m | time: 2.437s
[2K
| RMSProp | epoch: 014 | loss: 0.25722 - acc: 0.9009 -- iter: 0064/1192
[A[ATraining Step: 497  | total loss: [1m[32m0.24048[0m[0m | time: 3.639s
[2K
| RMSProp | epoch: 014 | loss: 0.24048 - acc: 0.9108 -- iter: 0096/1192
[A[ATraining Step: 498  | total loss: [1m[32m0.22490[0m[0m | time: 4.836s
[2K
| RMSProp | epoch: 014 | loss: 0.22490 - acc: 0.9166 -- iter: 0128/1192
[A[ATraining Step: 499  | total loss: [1m[32m0.21847[0m[0m | time: 6.037s
[2K
| RMSProp | epoch: 014 | loss: 0.21847 - acc: 0.9187 -- iter: 0160/1192
[A[ATraining Step: 500  | total loss: [1m[32m0.21006[0m[0m | time: 7.315s
[2K
| RMSProp | epoch: 014 | loss: 0.21006 - acc: 0.9237 -- iter: 0192/1192
[A[ATraining Step: 501  | total loss: [1m[32m0.19428[0m[0m | time: 8.515s
[2K
| RMSProp | epoch: 014 | loss: 0.19428 - acc: 0.9313 -- iter: 0224/1192
[A[ATraining Step: 502  | total loss: [1m[32m0.17954[0m[0m | time: 9.746s
[2K
| RMSProp | epoch: 014 | loss: 0.17954 - acc: 0.9382 -- iter: 0256/1192
[A[ATraining Step: 503  | total loss: [1m[32m0.16478[0m[0m | time: 10.926s
[2K
| RMSProp | epoch: 014 | loss: 0.16478 - acc: 0.9444 -- iter: 0288/1192
[A[ATraining Step: 504  | total loss: [1m[32m0.15103[0m[0m | time: 12.157s
[2K
| RMSProp | epoch: 014 | loss: 0.15103 - acc: 0.9499 -- iter: 0320/1192
[A[ATraining Step: 505  | total loss: [1m[32m0.13914[0m[0m | time: 13.420s
[2K
| RMSProp | epoch: 014 | loss: 0.13914 - acc: 0.9550 -- iter: 0352/1192
[A[ATraining Step: 506  | total loss: [1m[32m0.13236[0m[0m | time: 13.838s
[2K
| RMSProp | epoch: 014 | loss: 0.13236 - acc: 0.9563 -- iter: 0384/1192
[A[ATraining Step: 507  | total loss: [1m[32m0.17259[0m[0m | time: 14.204s
[2K
| RMSProp | epoch: 014 | loss: 0.17259 - acc: 0.9357 -- iter: 0416/1192
[A[ATraining Step: 508  | total loss: [1m[32m0.18967[0m[0m | time: 15.329s
[2K
| RMSProp | epoch: 014 | loss: 0.18967 - acc: 0.9296 -- iter: 0448/1192
[A[ATraining Step: 509  | total loss: [1m[32m0.20360[0m[0m | time: 16.457s
[2K
| RMSProp | epoch: 014 | loss: 0.20360 - acc: 0.9273 -- iter: 0480/1192
[A[ATraining Step: 510  | total loss: [1m[32m0.19444[0m[0m | time: 17.666s
[2K
| RMSProp | epoch: 014 | loss: 0.19444 - acc: 0.9346 -- iter: 0512/1192
[A[ATraining Step: 511  | total loss: [1m[32m0.19391[0m[0m | time: 18.856s
[2K
| RMSProp | epoch: 014 | loss: 0.19391 - acc: 0.9349 -- iter: 0544/1192
[A[ATraining Step: 512  | total loss: [1m[32m0.18256[0m[0m | time: 20.010s
[2K
| RMSProp | epoch: 014 | loss: 0.18256 - acc: 0.9414 -- iter: 0576/1192
[A[ATraining Step: 513  | total loss: [1m[32m0.19663[0m[0m | time: 21.154s
[2K
| RMSProp | epoch: 014 | loss: 0.19663 - acc: 0.9379 -- iter: 0608/1192
[A[ATraining Step: 514  | total loss: [1m[32m0.24825[0m[0m | time: 22.398s
[2K
| RMSProp | epoch: 014 | loss: 0.24825 - acc: 0.9222 -- iter: 0640/1192
[A[ATraining Step: 515  | total loss: [1m[32m0.25901[0m[0m | time: 23.625s
[2K
| RMSProp | epoch: 014 | loss: 0.25901 - acc: 0.9144 -- iter: 0672/1192
[A[ATraining Step: 516  | total loss: [1m[32m0.25106[0m[0m | time: 24.814s
[2K
| RMSProp | epoch: 014 | loss: 0.25106 - acc: 0.9167 -- iter: 0704/1192
[A[ATraining Step: 517  | total loss: [1m[32m0.24623[0m[0m | time: 26.123s
[2K
| RMSProp | epoch: 014 | loss: 0.24623 - acc: 0.9156 -- iter: 0736/1192
[A[ATraining Step: 518  | total loss: [1m[32m0.23313[0m[0m | time: 27.366s
[2K
| RMSProp | epoch: 014 | loss: 0.23313 - acc: 0.9241 -- iter: 0768/1192
[A[ATraining Step: 519  | total loss: [1m[32m0.22744[0m[0m | time: 28.610s
[2K
| RMSProp | epoch: 014 | loss: 0.22744 - acc: 0.9223 -- iter: 0800/1192
[A[ATraining Step: 520  | total loss: [1m[32m0.21307[0m[0m | time: 29.894s
[2K
| RMSProp | epoch: 014 | loss: 0.21307 - acc: 0.9269 -- iter: 0832/1192
[A[ATraining Step: 521  | total loss: [1m[32m0.20209[0m[0m | time: 31.144s
[2K
| RMSProp | epoch: 014 | loss: 0.20209 - acc: 0.9280 -- iter: 0864/1192
[A[ATraining Step: 522  | total loss: [1m[32m0.20419[0m[0m | time: 32.382s
[2K
| RMSProp | epoch: 014 | loss: 0.20419 - acc: 0.9289 -- iter: 0896/1192
[A[ATraining Step: 523  | total loss: [1m[32m0.19349[0m[0m | time: 33.667s
[2K
| RMSProp | epoch: 014 | loss: 0.19349 - acc: 0.9298 -- iter: 0928/1192
[A[ATraining Step: 524  | total loss: [1m[32m0.17881[0m[0m | time: 34.922s
[2K
| RMSProp | epoch: 014 | loss: 0.17881 - acc: 0.9337 -- iter: 0960/1192
[A[ATraining Step: 525  | total loss: [1m[32m0.17478[0m[0m | time: 36.323s
[2K
| RMSProp | epoch: 014 | loss: 0.17478 - acc: 0.9341 -- iter: 0992/1192
[A[ATraining Step: 526  | total loss: [1m[32m0.17596[0m[0m | time: 37.619s
[2K
| RMSProp | epoch: 014 | loss: 0.17596 - acc: 0.9375 -- iter: 1024/1192
[A[ATraining Step: 527  | total loss: [1m[32m0.18110[0m[0m | time: 38.895s
[2K
| RMSProp | epoch: 014 | loss: 0.18110 - acc: 0.9313 -- iter: 1056/1192
[A[ATraining Step: 528  | total loss: [1m[32m0.18238[0m[0m | time: 40.155s
[2K
| RMSProp | epoch: 014 | loss: 0.18238 - acc: 0.9350 -- iter: 1088/1192
[A[ATraining Step: 529  | total loss: [1m[32m0.17262[0m[0m | time: 41.356s
[2K
| RMSProp | epoch: 014 | loss: 0.17262 - acc: 0.9415 -- iter: 1120/1192
[A[ATraining Step: 530  | total loss: [1m[32m0.16644[0m[0m | time: 42.584s
[2K
| RMSProp | epoch: 014 | loss: 0.16644 - acc: 0.9442 -- iter: 1152/1192
[A[ATraining Step: 531  | total loss: [1m[32m0.16105[0m[0m | time: 43.850s
[2K
| RMSProp | epoch: 014 | loss: 0.16105 - acc: 0.9467 -- iter: 1184/1192
[A[ATraining Step: 532  | total loss: [1m[32m0.16497[0m[0m | time: 47.515s
[2K
| RMSProp | epoch: 014 | loss: 0.16497 - acc: 0.9458 | val_loss: 0.17941 - val_acc: 0.9303 -- iter: 1192/1192
--
Training Step: 533  | total loss: [1m[32m0.15770[0m[0m | time: 1.286s
[2K
| RMSProp | epoch: 015 | loss: 0.15770 - acc: 0.9481 -- iter: 0032/1192
[A[ATraining Step: 534  | total loss: [1m[32m0.15018[0m[0m | time: 2.580s
[2K
| RMSProp | epoch: 015 | loss: 0.15018 - acc: 0.9501 -- iter: 0064/1192
[A[ATraining Step: 535  | total loss: [1m[32m0.14830[0m[0m | time: 3.808s
[2K
| RMSProp | epoch: 015 | loss: 0.14830 - acc: 0.9520 -- iter: 0096/1192
[A[ATraining Step: 536  | total loss: [1m[32m0.13825[0m[0m | time: 5.051s
[2K
| RMSProp | epoch: 015 | loss: 0.13825 - acc: 0.9537 -- iter: 0128/1192
[A[ATraining Step: 537  | total loss: [1m[32m0.12947[0m[0m | time: 6.278s
[2K
| RMSProp | epoch: 015 | loss: 0.12947 - acc: 0.9552 -- iter: 0160/1192
[A[ATraining Step: 538  | total loss: [1m[32m0.12421[0m[0m | time: 7.470s
[2K
| RMSProp | epoch: 015 | loss: 0.12421 - acc: 0.9534 -- iter: 0192/1192
[A[ATraining Step: 539  | total loss: [1m[32m0.15698[0m[0m | time: 8.750s
[2K
| RMSProp | epoch: 015 | loss: 0.15698 - acc: 0.9456 -- iter: 0224/1192
[A[ATraining Step: 540  | total loss: [1m[32m0.14801[0m[0m | time: 10.121s
[2K
| RMSProp | epoch: 015 | loss: 0.14801 - acc: 0.9510 -- iter: 0256/1192
[A[ATraining Step: 541  | total loss: [1m[32m0.13757[0m[0m | time: 11.328s
[2K
| RMSProp | epoch: 015 | loss: 0.13757 - acc: 0.9559 -- iter: 0288/1192
[A[ATraining Step: 542  | total loss: [1m[32m0.13744[0m[0m | time: 12.637s
[2K
| RMSProp | epoch: 015 | loss: 0.13744 - acc: 0.9509 -- iter: 0320/1192
[A[ATraining Step: 543  | total loss: [1m[32m0.13197[0m[0m | time: 13.780s
[2K
| RMSProp | epoch: 015 | loss: 0.13197 - acc: 0.9527 -- iter: 0352/1192
[A[ATraining Step: 544  | total loss: [1m[32m0.12793[0m[0m | time: 15.033s
[2K
| RMSProp | epoch: 015 | loss: 0.12793 - acc: 0.9543 -- iter: 0384/1192
[A[ATraining Step: 545  | total loss: [1m[32m0.12927[0m[0m | time: 15.430s
[2K
| RMSProp | epoch: 015 | loss: 0.12927 - acc: 0.9526 -- iter: 0416/1192
[A[ATraining Step: 546  | total loss: [1m[32m0.11973[0m[0m | time: 15.797s
[2K
| RMSProp | epoch: 015 | loss: 0.11973 - acc: 0.9574 -- iter: 0448/1192
[A[ATraining Step: 547  | total loss: [1m[32m0.10899[0m[0m | time: 17.131s
[2K
| RMSProp | epoch: 015 | loss: 0.10899 - acc: 0.9616 -- iter: 0480/1192
[A[ATraining Step: 548  | total loss: [1m[32m0.10602[0m[0m | time: 18.411s
[2K
| RMSProp | epoch: 015 | loss: 0.10602 - acc: 0.9624 -- iter: 0512/1192
[A[ATraining Step: 549  | total loss: [1m[32m0.12667[0m[0m | time: 19.687s
[2K
| RMSProp | epoch: 015 | loss: 0.12667 - acc: 0.9599 -- iter: 0544/1192
[A[ATraining Step: 550  | total loss: [1m[32m0.14915[0m[0m | time: 20.909s
[2K
| RMSProp | epoch: 015 | loss: 0.14915 - acc: 0.9514 -- iter: 0576/1192
[A[ATraining Step: 551  | total loss: [1m[32m0.16033[0m[0m | time: 22.100s
[2K
| RMSProp | epoch: 015 | loss: 0.16033 - acc: 0.9469 -- iter: 0608/1192
[A[ATraining Step: 552  | total loss: [1m[32m0.16771[0m[0m | time: 23.298s
[2K
| RMSProp | epoch: 015 | loss: 0.16771 - acc: 0.9428 -- iter: 0640/1192
[A[ATraining Step: 553  | total loss: [1m[32m0.16439[0m[0m | time: 24.543s
[2K
| RMSProp | epoch: 015 | loss: 0.16439 - acc: 0.9423 -- iter: 0672/1192
[A[ATraining Step: 554  | total loss: [1m[32m0.16360[0m[0m | time: 25.777s
[2K
| RMSProp | epoch: 015 | loss: 0.16360 - acc: 0.9418 -- iter: 0704/1192
[A[ATraining Step: 555  | total loss: [1m[32m0.15072[0m[0m | time: 27.050s
[2K
| RMSProp | epoch: 015 | loss: 0.15072 - acc: 0.9476 -- iter: 0736/1192
[A[ATraining Step: 556  | total loss: [1m[32m0.14057[0m[0m | time: 28.186s
[2K
| RMSProp | epoch: 015 | loss: 0.14057 - acc: 0.9529 -- iter: 0768/1192
[A[ATraining Step: 557  | total loss: [1m[32m0.12872[0m[0m | time: 29.310s
[2K
| RMSProp | epoch: 015 | loss: 0.12872 - acc: 0.9576 -- iter: 0800/1192
[A[ATraining Step: 558  | total loss: [1m[32m0.11805[0m[0m | time: 30.621s
[2K
| RMSProp | epoch: 015 | loss: 0.11805 - acc: 0.9618 -- iter: 0832/1192
[A[ATraining Step: 559  | total loss: [1m[32m0.12135[0m[0m | time: 31.860s
[2K
| RMSProp | epoch: 015 | loss: 0.12135 - acc: 0.9625 -- iter: 0864/1192
[A[ATraining Step: 560  | total loss: [1m[32m0.11092[0m[0m | time: 33.109s
[2K
| RMSProp | epoch: 015 | loss: 0.11092 - acc: 0.9663 -- iter: 0896/1192
[A[ATraining Step: 561  | total loss: [1m[32m0.10769[0m[0m | time: 34.351s
[2K
| RMSProp | epoch: 015 | loss: 0.10769 - acc: 0.9634 -- iter: 0928/1192
[A[ATraining Step: 562  | total loss: [1m[32m0.12442[0m[0m | time: 35.601s
[2K
| RMSProp | epoch: 015 | loss: 0.12442 - acc: 0.9608 -- iter: 0960/1192
[A[ATraining Step: 563  | total loss: [1m[32m0.11535[0m[0m | time: 36.844s
[2K
| RMSProp | epoch: 015 | loss: 0.11535 - acc: 0.9647 -- iter: 0992/1192
[A[ATraining Step: 564  | total loss: [1m[32m0.10742[0m[0m | time: 37.888s
[2K
| RMSProp | epoch: 015 | loss: 0.10742 - acc: 0.9682 -- iter: 1024/1192
[A[ATraining Step: 565  | total loss: [1m[32m0.09924[0m[0m | time: 38.853s
[2K
| RMSProp | epoch: 015 | loss: 0.09924 - acc: 0.9714 -- iter: 1056/1192
[A[ATraining Step: 566  | total loss: [1m[32m0.09397[0m[0m | time: 39.823s
[2K
| RMSProp | epoch: 015 | loss: 0.09397 - acc: 0.9743 -- iter: 1088/1192
[A[ATraining Step: 567  | total loss: [1m[32m0.08925[0m[0m | time: 40.812s
[2K
| RMSProp | epoch: 015 | loss: 0.08925 - acc: 0.9737 -- iter: 1120/1192
[A[ATraining Step: 568  | total loss: [1m[32m0.08278[0m[0m | time: 41.751s
[2K
| RMSProp | epoch: 015 | loss: 0.08278 - acc: 0.9764 -- iter: 1152/1192
[A[ATraining Step: 569  | total loss: [1m[32m0.07628[0m[0m | time: 42.784s
[2K
| RMSProp | epoch: 015 | loss: 0.07628 - acc: 0.9787 -- iter: 1184/1192
[A[ATraining Step: 570  | total loss: [1m[32m0.07352[0m[0m | time: 45.580s
[2K
| RMSProp | epoch: 015 | loss: 0.07352 - acc: 0.9777 | val_loss: 0.21659 - val_acc: 0.9303 -- iter: 1192/1192
--
Validation AUC:0.9833969686451463
Validation AUPRC:0.9829263849061192
Test AUC:0.9882403680276021
Test AUPRC:0.9896944006385557
BestTestF1Score	0.94	0.89	0.94	0.95	0.94	177	10	175	11	0.1
BestTestMCCScore	0.94	0.89	0.94	0.95	0.94	177	10	175	11	0.1
BestTestAccuracyScore	0.94	0.89	0.94	0.95	0.94	177	10	175	11	0.1
BestValidationF1Score	0.95	0.9	0.95	0.95	0.94	162	8	193	10	0.1
BestValidationMCC	0.95	0.9	0.95	0.95	0.94	162	8	193	10	0.1
BestValidationAccuracy	0.95	0.9	0.95	0.95	0.94	162	8	193	10	0.1
TestPredictions (Threshold:0.1)
CHEMBL3701982,TP,ACT,1.0	CHEMBL1180343,TN,INACT,0.0	CHEMBL44262,TN,INACT,0.0	CHEMBL438915,TN,INACT,0.0	CHEMBL62421,TN,INACT,0.0	CHEMBL3656525,TP,ACT,1.0	CHEMBL307326,TN,INACT,0.0	CHEMBL282426,TN,INACT,0.0	CHEMBL2093084,TN,INACT,0.009999999776482582	CHEMBL233957,TN,INACT,0.0	CHEMBL3680135,FN,ACT,0.029999999329447746	CHEMBL88506,TN,INACT,0.0	CHEMBL3590085,TN,INACT,0.0	CHEMBL3663718,TP,ACT,0.9900000095367432	CHEMBL2042403,TN,INACT,0.03999999910593033	CHEMBL3684796,TP,ACT,1.0	CHEMBL611496,TN,INACT,0.0	CHEMBL3660702,TP,ACT,0.9800000190734863	CHEMBL3673009,TP,ACT,0.9900000095367432	CHEMBL3684807,TP,ACT,1.0	CHEMBL294502,TN,INACT,0.009999999776482582	CHEMBL3656531,TP,ACT,0.8500000238418579	CHEMBL3645459,TP,ACT,0.3199999928474426	CHEMBL304714,TN,INACT,0.07000000029802322	CHEMBL3680128,TP,ACT,1.0	CHEMBL3701949,TP,ACT,1.0	CHEMBL3645420,TP,ACT,0.7699999809265137	CHEMBL3656509,TP,ACT,0.8299999833106995	CHEMBL3680160,TP,ACT,1.0	CHEMBL2206384,TP,ACT,0.9900000095367432	CHEMBL241100,TN,INACT,0.03999999910593033	CHEMBL104180,TN,INACT,0.009999999776482582	CHEMBL3645381,TP,ACT,1.0	CHEMBL485,TN,INACT,0.0	CHEMBL1230845,FN,ACT,0.029999999329447746	CHEMBL62804,TN,INACT,0.0	CHEMBL3684800,TP,ACT,1.0	CHEMBL3660707,TP,ACT,1.0	CHEMBL160932,TN,INACT,0.009999999776482582	CHEMBL3701962,TP,ACT,0.9900000095367432	CHEMBL3702027,TP,ACT,1.0	CHEMBL3645397,TP,ACT,0.9800000190734863	CHEMBL3701971,TP,ACT,1.0	CHEMBL524026,TN,INACT,0.0	CHEMBL574597,TN,INACT,0.0	CHEMBL6568,TN,INACT,0.0	CHEMBL3656514,TP,ACT,1.0	CHEMBL2311547,TN,INACT,0.0	CHEMBL3639516,TP,ACT,0.9800000190734863	CHEMBL2163917,TN,INACT,0.0	CHEMBL3734955,TN,INACT,0.0	CHEMBL106602,TN,INACT,0.0	CHEMBL3656526,TP,ACT,0.9399999976158142	CHEMBL1916635,TN,INACT,0.0	CHEMBL407818,TN,INACT,0.009999999776482582	CHEMBL322547,TN,INACT,0.0	CHEMBL3680142,TP,ACT,0.949999988079071	CHEMBL3701995,TP,ACT,0.9900000095367432	CHEMBL262787,FP,INACT,0.699999988079071	CHEMBL61120,TN,INACT,0.009999999776482582	CHEMBL1923416,FP,INACT,0.8100000023841858	CHEMBL63289,TN,INACT,0.009999999776482582	CHEMBL52867,TN,INACT,0.019999999552965164	CHEMBL3684775,TP,ACT,1.0	CHEMBL162095,TN,INACT,0.0	CHEMBL3652756,TP,ACT,0.9900000095367432	CHEMBL513277,TN,INACT,0.009999999776482582	CHEMBL1381098,TN,INACT,0.0	CHEMBL297335,TN,INACT,0.0	CHEMBL3663699,TP,ACT,1.0	CHEMBL3645382,TP,ACT,1.0	CHEMBL164968,TN,INACT,0.0	CHEMBL1091790,TN,INACT,0.0	CHEMBL54885,TN,INACT,0.029999999329447746	CHEMBL433814,TN,INACT,0.009999999776482582	CHEMBL88629,TN,INACT,0.0	CHEMBL308414,TN,INACT,0.0	CHEMBL3701914,TP,ACT,1.0	CHEMBL3701975,TP,ACT,0.9900000095367432	CHEMBL3684871,TP,ACT,1.0	CHEMBL3663707,TP,ACT,0.9900000095367432	CHEMBL3684908,TP,ACT,1.0	CHEMBL331394,TN,INACT,0.0	CHEMBL3701954,TP,ACT,1.0	CHEMBL140495,TN,INACT,0.0	CHEMBL1223696,TN,INACT,0.0	CHEMBL2436721,TN,INACT,0.0	CHEMBL3660703,TP,ACT,0.9900000095367432	CHEMBL297173,TN,INACT,0.0	CHEMBL3656480,TP,ACT,1.0	CHEMBL76860,TN,INACT,0.0	CHEMBL3684903,TP,ACT,0.9900000095367432	CHEMBL3701924,TP,ACT,0.9900000095367432	CHEMBL123654,TN,INACT,0.0	CHEMBL3701938,TP,ACT,1.0	CHEMBL3680165,TP,ACT,0.9900000095367432	CHEMBL3680169,TP,ACT,1.0	CHEMBL3580906,TP,ACT,0.9300000071525574	CHEMBL297599,TN,INACT,0.009999999776482582	CHEMBL3684921,TP,ACT,1.0	CHEMBL3652778,TP,ACT,1.0	CHEMBL3652741,TP,ACT,1.0	CHEMBL3393993,TN,INACT,0.0	CHEMBL63760,TN,INACT,0.0	CHEMBL3645430,TP,ACT,0.9900000095367432	CHEMBL98038,TN,INACT,0.0	CHEMBL3701919,TP,ACT,1.0	CHEMBL275481,TN,INACT,0.0	CHEMBL3684808,TP,ACT,1.0	CHEMBL2436822,TN,INACT,0.0	CHEMBL3684917,TP,ACT,0.8999999761581421	CHEMBL43661,TN,INACT,0.0	CHEMBL104848,TN,INACT,0.0	CHEMBL2372076,TN,INACT,0.0	CHEMBL3663703,TP,ACT,0.9599999785423279	CHEMBL110695,TN,INACT,0.0	CHEMBL142295,TN,INACT,0.0	CHEMBL293874,TN,INACT,0.0	CHEMBL1669658,TP,ACT,0.9800000190734863	CHEMBL3641721,TP,ACT,1.0	CHEMBL3684759,TP,ACT,1.0	CHEMBL444128,TN,INACT,0.009999999776482582	CHEMBL2436722,TN,INACT,0.0	CHEMBL296245,TN,INACT,0.0	CHEMBL2206388,TP,ACT,0.38999998569488525	CHEMBL3656522,TP,ACT,0.9900000095367432	CHEMBL3663723,TP,ACT,0.8799999952316284	CHEMBL3652754,TP,ACT,1.0	CHEMBL3665444,TN,INACT,0.0	CHEMBL418658,TN,INACT,0.0	CHEMBL3701913,TP,ACT,1.0	CHEMBL283941,FP,INACT,0.5199999809265137	CHEMBL3673065,TP,ACT,1.0	CHEMBL608816,TN,INACT,0.0	CHEMBL3656499,TP,ACT,1.0	CHEMBL259131,TN,INACT,0.0	CHEMBL173059,TN,INACT,0.0	CHEMBL1669632,TP,ACT,0.9599999785423279	CHEMBL2163916,TN,INACT,0.0	CHEMBL98350,TN,INACT,0.0	CHEMBL3641695,FN,ACT,0.05000000074505806	CHEMBL3684912,TP,ACT,0.27000001072883606	CHEMBL501756,TN,INACT,0.0	CHEMBL3680166,TP,ACT,0.699999988079071	CHEMBL3677859,TP,ACT,0.9800000190734863	CHEMBL3656519,TP,ACT,0.9399999976158142	CHEMBL494093,TN,INACT,0.0	CHEMBL434,TN,INACT,0.019999999552965164	CHEMBL3641707,FN,ACT,0.07999999821186066	CHEMBL3652779,TP,ACT,1.0	CHEMBL68964,FP,INACT,0.18000000715255737	CHEMBL3673006,TP,ACT,1.0	CHEMBL3663720,TP,ACT,0.7799999713897705	CHEMBL1669634,TP,ACT,0.9900000095367432	CHEMBL26522,TN,INACT,0.0	CHEMBL3656502,TP,ACT,0.9599999785423279	CHEMBL320804,TN,INACT,0.0	CHEMBL3645455,TP,ACT,1.0	CHEMBL60435,TN,INACT,0.0	CHEMBL3684890,TP,ACT,0.4099999964237213	CHEMBL3701921,TP,ACT,1.0	CHEMBL3701943,TP,ACT,0.9900000095367432	CHEMBL3663727,TP,ACT,1.0	CHEMBL3580902,TP,ACT,1.0	CHEMBL2367888,TN,INACT,0.0	CHEMBL3680123,TP,ACT,1.0	CHEMBL3641753,TP,ACT,0.9900000095367432	CHEMBL241080,TN,INACT,0.0	CHEMBL103886,TN,INACT,0.0	CHEMBL1907665,TN,INACT,0.0	CHEMBL3652766,TP,ACT,0.8100000023841858	CHEMBL3701951,TP,ACT,0.8799999952316284	CHEMBL3684818,TP,ACT,1.0	CHEMBL3779993,TP,ACT,1.0	CHEMBL3701942,TP,ACT,1.0	CHEMBL964,TN,INACT,0.009999999776482582	CHEMBL66011,TN,INACT,0.009999999776482582	CHEMBL58617,TN,INACT,0.0	CHEMBL3641728,TP,ACT,0.6899999976158142	CHEMBL2158005,FP,INACT,0.5299999713897705	CHEMBL353088,FP,INACT,0.4000000059604645	CHEMBL330885,TN,INACT,0.0	CHEMBL166736,TN,INACT,0.009999999776482582	CHEMBL2164434,TN,INACT,0.03999999910593033	CHEMBL302886,TN,INACT,0.0	CHEMBL3684920,TP,ACT,1.0	CHEMBL3702023,TP,ACT,0.9900000095367432	CHEMBL296927,TN,INACT,0.0	CHEMBL452150,TN,INACT,0.019999999552965164	CHEMBL2335158,TN,INACT,0.009999999776482582	CHEMBL169553,TN,INACT,0.0	CHEMBL109926,TN,INACT,0.0	CHEMBL3641760,TP,ACT,0.9800000190734863	CHEMBL2436717,TN,INACT,0.0	CHEMBL160626,TN,INACT,0.0	CHEMBL336033,TN,INACT,0.0	CHEMBL417712,TN,INACT,0.0	CHEMBL3645445,TP,ACT,1.0	CHEMBL416069,TN,INACT,0.0	CHEMBL3642844,TP,ACT,1.0	CHEMBL3656511,TP,ACT,0.949999988079071	CHEMBL3645439,TP,ACT,1.0	CHEMBL3680167,TP,ACT,1.0	CHEMBL3677860,TP,ACT,0.9900000095367432	CHEMBL307659,TN,INACT,0.0	CHEMBL3672955,TP,ACT,1.0	CHEMBL3641718,TP,ACT,1.0	CHEMBL128360,TN,INACT,0.009999999776482582	CHEMBL3702021,TP,ACT,1.0	CHEMBL3641692,TP,ACT,0.9800000190734863	CHEMBL315096,TN,INACT,0.0	CHEMBL92318,TN,INACT,0.0	CHEMBL3701991,TP,ACT,1.0	CHEMBL2206402,TP,ACT,0.9900000095367432	CHEMBL3684810,TP,ACT,1.0	CHEMBL273410,TN,INACT,0.0	CHEMBL469855,TN,INACT,0.0	CHEMBL3660701,TP,ACT,1.0	CHEMBL3639837,TP,ACT,1.0	CHEMBL3684891,TP,ACT,1.0	CHEMBL3684909,TP,ACT,1.0	CHEMBL3684799,TP,ACT,1.0	CHEMBL175228,TN,INACT,0.0	CHEMBL1669670,TP,ACT,1.0	CHEMBL3684860,TP,ACT,1.0	CHEMBL323951,TN,INACT,0.0	CHEMBL170335,TN,INACT,0.0	CHEMBL3665440,TN,INACT,0.0	CHEMBL3652749,TP,ACT,1.0	CHEMBL294649,TN,INACT,0.019999999552965164	CHEMBL3652713,TP,ACT,0.9900000095367432	CHEMBL81923,TN,INACT,0.029999999329447746	CHEMBL1956200,FP,INACT,0.10999999940395355	CHEMBL3673003,TP,ACT,0.9900000095367432	CHEMBL3672999,TP,ACT,0.9900000095367432	CHEMBL3673067,TP,ACT,0.9300000071525574	CHEMBL3684798,TP,ACT,1.0	CHEMBL330674,TN,INACT,0.0	CHEMBL234771,TN,INACT,0.0	CHEMBL1576791,TN,INACT,0.0	CHEMBL2436817,TN,INACT,0.0	CHEMBL417719,TN,INACT,0.0	CHEMBL66789,TN,INACT,0.0	CHEMBL3672992,TP,ACT,1.0	CHEMBL251541,TN,INACT,0.029999999329447746	CHEMBL137483,FP,INACT,0.8799999952316284	CHEMBL3672974,TP,ACT,0.9599999785423279	CHEMBL7441,TN,INACT,0.0	CHEMBL78929,TN,INACT,0.009999999776482582	CHEMBL2373213,TN,INACT,0.0	CHEMBL3645435,FN,ACT,0.05000000074505806	CHEMBL3652751,TP,ACT,1.0	CHEMBL3645408,FN,ACT,0.0	CHEMBL3684841,TP,ACT,1.0	CHEMBL3410301,TN,INACT,0.0	CHEMBL603858,TN,INACT,0.0	CHEMBL89688,TN,INACT,0.0	CHEMBL3641740,FN,ACT,0.029999999329447746	CHEMBL3684896,TP,ACT,1.0	CHEMBL240888,TN,INACT,0.009999999776482582	CHEMBL352779,TN,INACT,0.0	CHEMBL76779,TN,INACT,0.009999999776482582	CHEMBL3656524,TP,ACT,1.0	CHEMBL3673049,TP,ACT,0.9900000095367432	CHEMBL296908,TN,INACT,0.0	CHEMBL3739820,TN,INACT,0.0	CHEMBL3641739,TP,ACT,0.9900000095367432	CHEMBL301826,TN,INACT,0.009999999776482582	CHEMBL104222,TN,INACT,0.0	CHEMBL515170,TN,INACT,0.0	CHEMBL3656493,TP,ACT,1.0	CHEMBL74342,TN,INACT,0.05999999865889549	CHEMBL3645395,FN,ACT,0.05000000074505806	CHEMBL275469,TN,INACT,0.009999999776482582	CHEMBL3684806,TP,ACT,1.0	CHEMBL3645396,TP,ACT,1.0	CHEMBL1669666,TP,ACT,1.0	CHEMBL251997,TN,INACT,0.009999999776482582	CHEMBL3684911,TP,ACT,1.0	CHEMBL3645389,TP,ACT,1.0	CHEMBL89203,TN,INACT,0.0	CHEMBL21508,TN,INACT,0.009999999776482582	CHEMBL1669638,TP,ACT,1.0	CHEMBL302468,TN,INACT,0.0	CHEMBL1791272,TN,INACT,0.0	CHEMBL3645453,TP,ACT,1.0	CHEMBL3684802,TP,ACT,1.0	CHEMBL589,TN,INACT,0.009999999776482582	CHEMBL3660708,TP,ACT,1.0	CHEMBL3663689,TP,ACT,0.9300000071525574	CHEMBL602269,TN,INACT,0.0	CHEMBL3290991,TN,INACT,0.009999999776482582	CHEMBL334933,TN,INACT,0.0	CHEMBL3701987,TP,ACT,0.9900000095367432	CHEMBL21937,TN,INACT,0.0	CHEMBL3701905,TP,ACT,1.0	CHEMBL3673043,TP,ACT,0.9100000262260437	CHEMBL3684848,TP,ACT,1.0	CHEMBL3645380,FN,ACT,0.009999999776482582	CHEMBL3684895,TP,ACT,1.0	CHEMBL158078,TN,INACT,0.0	CHEMBL276676,TN,INACT,0.0	CHEMBL257547,TN,INACT,0.0	CHEMBL1669672,TP,ACT,1.0	CHEMBL3645461,TP,ACT,0.949999988079071	CHEMBL101054,TN,INACT,0.0	CHEMBL3684768,TP,ACT,1.0	CHEMBL72295,TN,INACT,0.0	CHEMBL3701993,TP,ACT,1.0	CHEMBL165462,FP,INACT,0.699999988079071	CHEMBL3641719,TP,ACT,1.0	CHEMBL3684776,TP,ACT,1.0	CHEMBL3641720,TP,ACT,1.0	CHEMBL2206377,TP,ACT,0.7699999809265137	CHEMBL3684785,TP,ACT,1.0	CHEMBL3663729,TP,ACT,0.5899999737739563	CHEMBL48031,TN,INACT,0.0	CHEMBL3641732,TP,ACT,0.8899999856948853	CHEMBL3656495,TP,ACT,1.0	CHEMBL3680145,TP,ACT,1.0	CHEMBL3645407,TP,ACT,1.0	CHEMBL298203,TN,INACT,0.0	CHEMBL3656494,TP,ACT,1.0	CHEMBL15689,TN,INACT,0.0	CHEMBL3684778,TP,ACT,1.0	CHEMBL3680117,TP,ACT,1.0	CHEMBL1983100,TN,INACT,0.0	CHEMBL1161419,TN,INACT,0.0	CHEMBL294349,TN,INACT,0.0	CHEMBL552615,TN,INACT,0.0	CHEMBL70728,TN,INACT,0.0	CHEMBL3672956,TP,ACT,0.9900000095367432	CHEMBL3684854,TP,ACT,1.0	CHEMBL3673031,TP,ACT,0.9800000190734863	CHEMBL414570,TN,INACT,0.0	CHEMBL169675,TN,INACT,0.009999999776482582	CHEMBL3652686,TP,ACT,1.0	CHEMBL3350741,TN,INACT,0.0	CHEMBL3673038,TP,ACT,0.9800000190734863	CHEMBL79915,TN,INACT,0.0	CHEMBL3641694,TP,ACT,0.7200000286102295	CHEMBL3652752,TP,ACT,1.0	CHEMBL3684894,TP,ACT,0.75	CHEMBL3702015,TP,ACT,1.0	CHEMBL439335,TN,INACT,0.0	CHEMBL378173,TN,INACT,0.0	CHEMBL3673011,TP,ACT,1.0	CHEMBL3672982,TP,ACT,0.9900000095367432	CHEMBL461709,TN,INACT,0.009999999776482582	CHEMBL165012,FP,INACT,0.9700000286102295	CHEMBL595265,TN,INACT,0.009999999776482582	CHEMBL2112451,TN,INACT,0.0	CHEMBL3684821,TP,ACT,1.0	CHEMBL3663721,TP,ACT,1.0	CHEMBL3645458,TP,ACT,0.20999999344348907	CHEMBL1076554,TN,INACT,0.009999999776482582	CHEMBL3656498,TP,ACT,0.9200000166893005	CHEMBL3672975,TP,ACT,0.8199999928474426	CHEMBL461502,TN,INACT,0.009999999776482582	CHEMBL3652711,FN,ACT,0.0	CHEMBL404557,TN,INACT,0.009999999776482582	CHEMBL24781,TN,INACT,0.0	CHEMBL311455,TN,INACT,0.0	CHEMBL3673010,TP,ACT,0.9900000095367432	CHEMBL73096,TN,INACT,0.0	CHEMBL3217760,TN,INACT,0.0	CHEMBL3652764,FN,ACT,0.009999999776482582	CHEMBL78853,TN,INACT,0.0	CHEMBL3663704,TP,ACT,0.9900000095367432	CHEMBL3672948,TP,ACT,1.0	CHEMBL2163919,TN,INACT,0.0	CHEMBL3684872,TP,ACT,1.0	CHEMBL272853,TN,INACT,0.07999999821186066	

