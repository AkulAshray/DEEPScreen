CNNModel CHEMBL5023 adam 0.001 15 128 0 0.8 False True
Number of active compounds :	237
Number of inactive compounds :	158
---------------------------------
Run id: CNNModel_CHEMBL5023_adam_0.001_15_128_0_0.8_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL5023_adam_0.001_15_128_0.8_True/
---------------------------------
Training samples: 240
Validation samples: 76
--
Training Step: 1  | time: 1.522s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/240
[A[ATraining Step: 2  | total loss: [1m[32m0.62370[0m[0m | time: 2.353s
[2K
| Adam | epoch: 001 | loss: 0.62370 - acc: 0.4500 -- iter: 064/240
[A[ATraining Step: 3  | total loss: [1m[32m0.68680[0m[0m | time: 3.204s
[2K
| Adam | epoch: 001 | loss: 0.68680 - acc: 0.3375 -- iter: 096/240
[A[ATraining Step: 4  | total loss: [1m[32m0.69159[0m[0m | time: 4.057s
[2K
| Adam | epoch: 001 | loss: 0.69159 - acc: 0.4359 -- iter: 128/240
[A[ATraining Step: 5  | total loss: [1m[32m0.69244[0m[0m | time: 4.923s
[2K
| Adam | epoch: 001 | loss: 0.69244 - acc: 0.5019 -- iter: 160/240
[A[ATraining Step: 6  | total loss: [1m[32m0.69064[0m[0m | time: 5.810s
[2K
| Adam | epoch: 001 | loss: 0.69064 - acc: 0.6212 -- iter: 192/240
[A[ATraining Step: 7  | total loss: [1m[32m0.69081[0m[0m | time: 6.745s
[2K
| Adam | epoch: 001 | loss: 0.69081 - acc: 0.5860 -- iter: 224/240
[A[ATraining Step: 8  | total loss: [1m[32m0.68636[0m[0m | time: 8.291s
[2K
| Adam | epoch: 001 | loss: 0.68636 - acc: 0.6079 | val_loss: 0.65923 - val_acc: 0.6316 -- iter: 240/240
--
Training Step: 9  | total loss: [1m[32m0.68072[0m[0m | time: 0.449s
[2K
| Adam | epoch: 002 | loss: 0.68072 - acc: 0.6170 -- iter: 032/240
[A[ATraining Step: 10  | total loss: [1m[32m0.67028[0m[0m | time: 1.316s
[2K
| Adam | epoch: 002 | loss: 0.67028 - acc: 0.6210 -- iter: 064/240
[A[ATraining Step: 11  | total loss: [1m[32m0.66702[0m[0m | time: 2.212s
[2K
| Adam | epoch: 002 | loss: 0.66702 - acc: 0.6229 -- iter: 096/240
[A[ATraining Step: 12  | total loss: [1m[32m0.64542[0m[0m | time: 3.123s
[2K
| Adam | epoch: 002 | loss: 0.64542 - acc: 0.6520 -- iter: 128/240
[A[ATraining Step: 13  | total loss: [1m[32m0.67381[0m[0m | time: 4.087s
[2K
| Adam | epoch: 002 | loss: 0.67381 - acc: 0.6136 -- iter: 160/240
[A[ATraining Step: 14  | total loss: [1m[32m0.66855[0m[0m | time: 5.047s
[2K
| Adam | epoch: 002 | loss: 0.66855 - acc: 0.6183 -- iter: 192/240
[A[ATraining Step: 15  | total loss: [1m[32m0.65273[0m[0m | time: 5.888s
[2K
| Adam | epoch: 002 | loss: 0.65273 - acc: 0.6576 -- iter: 224/240
[A[ATraining Step: 16  | total loss: [1m[32m0.65340[0m[0m | time: 7.989s
[2K
| Adam | epoch: 002 | loss: 0.65340 - acc: 0.6571 | val_loss: 0.65747 - val_acc: 0.6316 -- iter: 240/240
--
Training Step: 17  | total loss: [1m[32m0.65359[0m[0m | time: 0.468s
[2K
| Adam | epoch: 003 | loss: 0.65359 - acc: 0.6568 -- iter: 032/240
[A[ATraining Step: 18  | total loss: [1m[32m0.66434[0m[0m | time: 0.924s
[2K
| Adam | epoch: 003 | loss: 0.66434 - acc: 0.6241 -- iter: 064/240
[A[ATraining Step: 19  | total loss: [1m[32m0.67356[0m[0m | time: 1.817s
[2K
| Adam | epoch: 003 | loss: 0.67356 - acc: 0.6036 -- iter: 096/240
[A[ATraining Step: 20  | total loss: [1m[32m0.67744[0m[0m | time: 2.772s
[2K
| Adam | epoch: 003 | loss: 0.67744 - acc: 0.5904 -- iter: 128/240
[A[ATraining Step: 21  | total loss: [1m[32m0.68684[0m[0m | time: 3.677s
[2K
| Adam | epoch: 003 | loss: 0.68684 - acc: 0.5623 -- iter: 160/240
[A[ATraining Step: 22  | total loss: [1m[32m0.67734[0m[0m | time: 4.528s
[2K
| Adam | epoch: 003 | loss: 0.67734 - acc: 0.5905 -- iter: 192/240
[A[ATraining Step: 23  | total loss: [1m[32m0.67079[0m[0m | time: 5.506s
[2K
| Adam | epoch: 003 | loss: 0.67079 - acc: 0.6096 -- iter: 224/240
[A[ATraining Step: 24  | total loss: [1m[32m0.67379[0m[0m | time: 7.502s
[2K
| Adam | epoch: 003 | loss: 0.67379 - acc: 0.5964 | val_loss: 0.65738 - val_acc: 0.6316 -- iter: 240/240
--
Training Step: 25  | total loss: [1m[32m0.66000[0m[0m | time: 0.939s
[2K
| Adam | epoch: 004 | loss: 0.66000 - acc: 0.6383 -- iter: 032/240
[A[ATraining Step: 26  | total loss: [1m[32m0.67681[0m[0m | time: 1.433s
[2K
| Adam | epoch: 004 | loss: 0.67681 - acc: 0.5934 -- iter: 064/240
[A[ATraining Step: 27  | total loss: [1m[32m0.65282[0m[0m | time: 1.903s
[2K
| Adam | epoch: 004 | loss: 0.65282 - acc: 0.6497 -- iter: 096/240
[A[ATraining Step: 28  | total loss: [1m[32m0.62985[0m[0m | time: 2.796s
[2K
| Adam | epoch: 004 | loss: 0.62985 - acc: 0.6904 -- iter: 128/240
[A[ATraining Step: 29  | total loss: [1m[32m0.65567[0m[0m | time: 3.618s
[2K
| Adam | epoch: 004 | loss: 0.65567 - acc: 0.6517 -- iter: 160/240
[A[ATraining Step: 30  | total loss: [1m[32m0.67520[0m[0m | time: 4.654s
[2K
| Adam | epoch: 004 | loss: 0.67520 - acc: 0.6232 -- iter: 192/240
[A[ATraining Step: 31  | total loss: [1m[32m0.67673[0m[0m | time: 5.686s
[2K
| Adam | epoch: 004 | loss: 0.67673 - acc: 0.6164 -- iter: 224/240
[A[ATraining Step: 32  | total loss: [1m[32m0.67236[0m[0m | time: 7.477s
[2K
| Adam | epoch: 004 | loss: 0.67236 - acc: 0.6183 | val_loss: 0.65482 - val_acc: 0.6316 -- iter: 240/240
--
Training Step: 33  | total loss: [1m[32m0.66517[0m[0m | time: 0.892s
[2K
| Adam | epoch: 005 | loss: 0.66517 - acc: 0.6266 -- iter: 032/240
[A[ATraining Step: 34  | total loss: [1m[32m0.66627[0m[0m | time: 1.914s
[2K
| Adam | epoch: 005 | loss: 0.66627 - acc: 0.6196 -- iter: 064/240
[A[ATraining Step: 35  | total loss: [1m[32m0.66259[0m[0m | time: 2.437s
[2K
| Adam | epoch: 005 | loss: 0.66259 - acc: 0.6273 -- iter: 096/240
[A[ATraining Step: 36  | total loss: [1m[32m0.65364[0m[0m | time: 2.895s
[2K
| Adam | epoch: 005 | loss: 0.65364 - acc: 0.6524 -- iter: 128/240
[A[ATraining Step: 37  | total loss: [1m[32m0.64509[0m[0m | time: 3.667s
[2K
| Adam | epoch: 005 | loss: 0.64509 - acc: 0.6719 -- iter: 160/240
[A[ATraining Step: 38  | total loss: [1m[32m0.64939[0m[0m | time: 4.519s
[2K
| Adam | epoch: 005 | loss: 0.64939 - acc: 0.6566 -- iter: 192/240
[A[ATraining Step: 39  | total loss: [1m[32m0.66603[0m[0m | time: 5.332s
[2K
| Adam | epoch: 005 | loss: 0.66603 - acc: 0.6206 -- iter: 224/240
[A[ATraining Step: 40  | total loss: [1m[32m0.66010[0m[0m | time: 7.178s
[2K
| Adam | epoch: 005 | loss: 0.66010 - acc: 0.6273 | val_loss: 0.65084 - val_acc: 0.6316 -- iter: 240/240
--
Training Step: 41  | total loss: [1m[32m0.66489[0m[0m | time: 0.837s
[2K
| Adam | epoch: 006 | loss: 0.66489 - acc: 0.6154 -- iter: 032/240
[A[ATraining Step: 42  | total loss: [1m[32m0.66504[0m[0m | time: 1.691s
[2K
| Adam | epoch: 006 | loss: 0.66504 - acc: 0.6115 -- iter: 064/240
[A[ATraining Step: 43  | total loss: [1m[32m0.67464[0m[0m | time: 2.578s
[2K
| Adam | epoch: 006 | loss: 0.67464 - acc: 0.5918 -- iter: 096/240
[A[ATraining Step: 44  | total loss: [1m[32m0.66845[0m[0m | time: 3.038s
[2K
| Adam | epoch: 006 | loss: 0.66845 - acc: 0.6030 -- iter: 128/240
[A[ATraining Step: 45  | total loss: [1m[32m0.68418[0m[0m | time: 3.482s
[2K
| Adam | epoch: 006 | loss: 0.68418 - acc: 0.5643 -- iter: 160/240
[A[ATraining Step: 46  | total loss: [1m[32m0.69330[0m[0m | time: 4.402s
[2K
| Adam | epoch: 006 | loss: 0.69330 - acc: 0.5327 -- iter: 192/240
[A[ATraining Step: 47  | total loss: [1m[32m0.68540[0m[0m | time: 5.381s
[2K
| Adam | epoch: 006 | loss: 0.68540 - acc: 0.5581 -- iter: 224/240
[A[ATraining Step: 48  | total loss: [1m[32m0.67876[0m[0m | time: 7.329s
[2K
| Adam | epoch: 006 | loss: 0.67876 - acc: 0.5839 | val_loss: 0.66663 - val_acc: 0.6316 -- iter: 240/240
--
Training Step: 49  | total loss: [1m[32m0.67432[0m[0m | time: 0.861s
[2K
| Adam | epoch: 007 | loss: 0.67432 - acc: 0.6002 -- iter: 032/240
[A[ATraining Step: 50  | total loss: [1m[32m0.67572[0m[0m | time: 1.740s
[2K
| Adam | epoch: 007 | loss: 0.67572 - acc: 0.5895 -- iter: 064/240
[A[ATraining Step: 51  | total loss: [1m[32m0.67833[0m[0m | time: 2.657s
[2K
| Adam | epoch: 007 | loss: 0.67833 - acc: 0.5759 -- iter: 096/240
[A[ATraining Step: 52  | total loss: [1m[32m0.68142[0m[0m | time: 3.618s
[2K
| Adam | epoch: 007 | loss: 0.68142 - acc: 0.5598 -- iter: 128/240
[A[ATraining Step: 53  | total loss: [1m[32m0.67914[0m[0m | time: 4.091s
[2K
| Adam | epoch: 007 | loss: 0.67914 - acc: 0.5694 -- iter: 160/240
[A[ATraining Step: 54  | total loss: [1m[32m0.67332[0m[0m | time: 4.593s
[2K
| Adam | epoch: 007 | loss: 0.67332 - acc: 0.5956 -- iter: 192/240
[A[ATraining Step: 55  | total loss: [1m[32m0.66761[0m[0m | time: 5.473s
[2K
| Adam | epoch: 007 | loss: 0.66761 - acc: 0.6177 -- iter: 224/240
[A[ATraining Step: 56  | total loss: [1m[32m0.66124[0m[0m | time: 7.553s
[2K
| Adam | epoch: 007 | loss: 0.66124 - acc: 0.6363 | val_loss: 0.64711 - val_acc: 0.6316 -- iter: 240/240
--
Training Step: 57  | total loss: [1m[32m0.65925[0m[0m | time: 0.908s
[2K
| Adam | epoch: 008 | loss: 0.65925 - acc: 0.6347 -- iter: 032/240
[A[ATraining Step: 58  | total loss: [1m[32m0.65101[0m[0m | time: 1.758s
[2K
| Adam | epoch: 008 | loss: 0.65101 - acc: 0.6462 -- iter: 064/240
[A[ATraining Step: 59  | total loss: [1m[32m0.64470[0m[0m | time: 2.715s
[2K
| Adam | epoch: 008 | loss: 0.64470 - acc: 0.6517 -- iter: 096/240
[A[ATraining Step: 60  | total loss: [1m[32m0.64747[0m[0m | time: 3.700s
[2K
| Adam | epoch: 008 | loss: 0.64747 - acc: 0.6482 -- iter: 128/240
[A[ATraining Step: 61  | total loss: [1m[32m0.64840[0m[0m | time: 4.602s
[2K
| Adam | epoch: 008 | loss: 0.64840 - acc: 0.6452 -- iter: 160/240
[A[ATraining Step: 62  | total loss: [1m[32m0.66208[0m[0m | time: 5.038s
[2K
| Adam | epoch: 008 | loss: 0.66208 - acc: 0.6305 -- iter: 192/240
[A[ATraining Step: 63  | total loss: [1m[32m0.65972[0m[0m | time: 5.507s
[2K
| Adam | epoch: 008 | loss: 0.65972 - acc: 0.6298 -- iter: 224/240
[A[ATraining Step: 64  | total loss: [1m[32m0.65646[0m[0m | time: 7.535s
[2K
| Adam | epoch: 008 | loss: 0.65646 - acc: 0.6292 | val_loss: 0.64951 - val_acc: 0.6316 -- iter: 240/240
--
Training Step: 65  | total loss: [1m[32m0.66015[0m[0m | time: 0.937s
[2K
| Adam | epoch: 009 | loss: 0.66015 - acc: 0.6171 -- iter: 032/240
[A[ATraining Step: 66  | total loss: [1m[32m0.66214[0m[0m | time: 1.813s
[2K
| Adam | epoch: 009 | loss: 0.66214 - acc: 0.6067 -- iter: 064/240
[A[ATraining Step: 67  | total loss: [1m[32m0.66226[0m[0m | time: 2.789s
[2K
| Adam | epoch: 009 | loss: 0.66226 - acc: 0.6051 -- iter: 096/240
[A[ATraining Step: 68  | total loss: [1m[32m0.66388[0m[0m | time: 3.737s
[2K
| Adam | epoch: 009 | loss: 0.66388 - acc: 0.5964 -- iter: 128/240
[A[ATraining Step: 69  | total loss: [1m[32m0.66517[0m[0m | time: 4.573s
[2K
| Adam | epoch: 009 | loss: 0.66517 - acc: 0.5924 -- iter: 160/240
[A[ATraining Step: 70  | total loss: [1m[32m0.66451[0m[0m | time: 5.545s
[2K
| Adam | epoch: 009 | loss: 0.66451 - acc: 0.5998 -- iter: 192/240
[A[ATraining Step: 71  | total loss: [1m[32m0.66485[0m[0m | time: 6.106s
[2K
| Adam | epoch: 009 | loss: 0.66485 - acc: 0.5991 -- iter: 224/240
[A[ATraining Step: 72  | total loss: [1m[32m0.66652[0m[0m | time: 7.643s
[2K
| Adam | epoch: 009 | loss: 0.66652 - acc: 0.5880 | val_loss: 0.65795 - val_acc: 0.6316 -- iter: 240/240
--
Training Step: 73  | total loss: [1m[32m0.66773[0m[0m | time: 0.953s
[2K
| Adam | epoch: 010 | loss: 0.66773 - acc: 0.5782 -- iter: 032/240
[A[ATraining Step: 74  | total loss: [1m[32m0.66464[0m[0m | time: 1.913s
[2K
| Adam | epoch: 010 | loss: 0.66464 - acc: 0.5868 -- iter: 064/240
[A[ATraining Step: 75  | total loss: [1m[32m0.65968[0m[0m | time: 2.816s
[2K
| Adam | epoch: 010 | loss: 0.65968 - acc: 0.6011 -- iter: 096/240
[A[ATraining Step: 76  | total loss: [1m[32m0.66118[0m[0m | time: 3.718s
[2K
| Adam | epoch: 010 | loss: 0.66118 - acc: 0.5936 -- iter: 128/240
[A[ATraining Step: 77  | total loss: [1m[32m0.65785[0m[0m | time: 4.762s
[2K
| Adam | epoch: 010 | loss: 0.65785 - acc: 0.5969 -- iter: 160/240
[A[ATraining Step: 78  | total loss: [1m[32m0.65087[0m[0m | time: 5.775s
[2K
| Adam | epoch: 010 | loss: 0.65087 - acc: 0.6064 -- iter: 192/240
[A[ATraining Step: 79  | total loss: [1m[32m0.64954[0m[0m | time: 6.542s
[2K
| Adam | epoch: 010 | loss: 0.64954 - acc: 0.6019 -- iter: 224/240
[A[ATraining Step: 80  | total loss: [1m[32m0.65398[0m[0m | time: 7.993s
[2K
| Adam | epoch: 010 | loss: 0.65398 - acc: 0.5914 | val_loss: 0.61835 - val_acc: 0.6316 -- iter: 240/240
--
Training Step: 81  | total loss: [1m[32m0.64345[0m[0m | time: 0.475s
[2K
| Adam | epoch: 011 | loss: 0.64345 - acc: 0.6011 -- iter: 032/240
[A[ATraining Step: 82  | total loss: [1m[32m0.63278[0m[0m | time: 1.327s
[2K
| Adam | epoch: 011 | loss: 0.63278 - acc: 0.6098 -- iter: 064/240
[A[ATraining Step: 83  | total loss: [1m[32m0.62292[0m[0m | time: 2.344s
[2K
| Adam | epoch: 011 | loss: 0.62292 - acc: 0.6207 -- iter: 096/240
[A[ATraining Step: 84  | total loss: [1m[32m0.61807[0m[0m | time: 3.414s
[2K
| Adam | epoch: 011 | loss: 0.61807 - acc: 0.6180 -- iter: 128/240
[A[ATraining Step: 85  | total loss: [1m[32m0.60895[0m[0m | time: 4.443s
[2K
| Adam | epoch: 011 | loss: 0.60895 - acc: 0.6218 -- iter: 160/240
[A[ATraining Step: 86  | total loss: [1m[32m0.60872[0m[0m | time: 5.209s
[2K
| Adam | epoch: 011 | loss: 0.60872 - acc: 0.6284 -- iter: 192/240
[A[ATraining Step: 87  | total loss: [1m[32m0.60630[0m[0m | time: 6.097s
[2K
| Adam | epoch: 011 | loss: 0.60630 - acc: 0.6405 -- iter: 224/240
[A[ATraining Step: 88  | total loss: [1m[32m0.58751[0m[0m | time: 7.971s
[2K
| Adam | epoch: 011 | loss: 0.58751 - acc: 0.6609 | val_loss: 0.70679 - val_acc: 0.6447 -- iter: 240/240
--
Training Step: 89  | total loss: [1m[32m0.59425[0m[0m | time: 0.575s
[2K
| Adam | epoch: 012 | loss: 0.59425 - acc: 0.6635 -- iter: 032/240
[A[ATraining Step: 90  | total loss: [1m[32m0.63667[0m[0m | time: 1.121s
[2K
| Adam | epoch: 012 | loss: 0.63667 - acc: 0.6472 -- iter: 064/240
[A[ATraining Step: 91  | total loss: [1m[32m0.64197[0m[0m | time: 1.858s
[2K
| Adam | epoch: 012 | loss: 0.64197 - acc: 0.6450 -- iter: 096/240
[A[ATraining Step: 92  | total loss: [1m[32m0.63824[0m[0m | time: 2.718s
[2K
| Adam | epoch: 012 | loss: 0.63824 - acc: 0.6492 -- iter: 128/240
[A[ATraining Step: 93  | total loss: [1m[32m0.64660[0m[0m | time: 3.557s
[2K
| Adam | epoch: 012 | loss: 0.64660 - acc: 0.6280 -- iter: 160/240
[A[ATraining Step: 94  | total loss: [1m[32m0.64698[0m[0m | time: 4.419s
[2K
| Adam | epoch: 012 | loss: 0.64698 - acc: 0.6309 -- iter: 192/240
[A[ATraining Step: 95  | total loss: [1m[32m0.64783[0m[0m | time: 5.289s
[2K
| Adam | epoch: 012 | loss: 0.64783 - acc: 0.6272 -- iter: 224/240
[A[ATraining Step: 96  | total loss: [1m[32m0.64168[0m[0m | time: 7.297s
[2K
| Adam | epoch: 012 | loss: 0.64168 - acc: 0.6457 | val_loss: 0.57969 - val_acc: 0.6842 -- iter: 240/240
--
Training Step: 97  | total loss: [1m[32m0.63169[0m[0m | time: 0.957s
[2K
| Adam | epoch: 013 | loss: 0.63169 - acc: 0.6655 -- iter: 032/240
[A[ATraining Step: 98  | total loss: [1m[32m0.62217[0m[0m | time: 1.408s
[2K
| Adam | epoch: 013 | loss: 0.62217 - acc: 0.6677 -- iter: 064/240
[A[ATraining Step: 99  | total loss: [1m[32m0.61373[0m[0m | time: 1.891s
[2K
| Adam | epoch: 013 | loss: 0.61373 - acc: 0.6697 -- iter: 096/240
[A[ATraining Step: 100  | total loss: [1m[32m0.60683[0m[0m | time: 2.840s
[2K
| Adam | epoch: 013 | loss: 0.60683 - acc: 0.6652 -- iter: 128/240
[A[ATraining Step: 101  | total loss: [1m[32m0.59200[0m[0m | time: 3.794s
[2K
| Adam | epoch: 013 | loss: 0.59200 - acc: 0.6737 -- iter: 160/240
[A[ATraining Step: 102  | total loss: [1m[32m0.58603[0m[0m | time: 4.673s
[2K
| Adam | epoch: 013 | loss: 0.58603 - acc: 0.6657 -- iter: 192/240
[A[ATraining Step: 103  | total loss: [1m[32m0.57959[0m[0m | time: 5.536s
[2K
| Adam | epoch: 013 | loss: 0.57959 - acc: 0.6616 -- iter: 224/240
[A[ATraining Step: 104  | total loss: [1m[32m0.57751[0m[0m | time: 7.490s
[2K
| Adam | epoch: 013 | loss: 0.57751 - acc: 0.6642 | val_loss: 0.55669 - val_acc: 0.8158 -- iter: 240/240
--
Training Step: 105  | total loss: [1m[32m0.56620[0m[0m | time: 0.890s
[2K
| Adam | epoch: 014 | loss: 0.56620 - acc: 0.6728 -- iter: 032/240
[A[ATraining Step: 106  | total loss: [1m[32m0.55898[0m[0m | time: 1.829s
[2K
| Adam | epoch: 014 | loss: 0.55898 - acc: 0.6868 -- iter: 064/240
[A[ATraining Step: 107  | total loss: [1m[32m0.55280[0m[0m | time: 2.343s
[2K
| Adam | epoch: 014 | loss: 0.55280 - acc: 0.6993 -- iter: 096/240
[A[ATraining Step: 108  | total loss: [1m[32m0.54687[0m[0m | time: 2.813s
[2K
| Adam | epoch: 014 | loss: 0.54687 - acc: 0.7169 -- iter: 128/240
[A[ATraining Step: 109  | total loss: [1m[32m0.53277[0m[0m | time: 3.651s
[2K
| Adam | epoch: 014 | loss: 0.53277 - acc: 0.7390 -- iter: 160/240
[A[ATraining Step: 110  | total loss: [1m[32m0.52370[0m[0m | time: 4.625s
[2K
| Adam | epoch: 014 | loss: 0.52370 - acc: 0.7432 -- iter: 192/240
[A[ATraining Step: 111  | total loss: [1m[32m0.52407[0m[0m | time: 5.636s
[2K
| Adam | epoch: 014 | loss: 0.52407 - acc: 0.7407 -- iter: 224/240
[A[ATraining Step: 112  | total loss: [1m[32m0.50778[0m[0m | time: 7.663s
[2K
| Adam | epoch: 014 | loss: 0.50778 - acc: 0.7510 | val_loss: 0.59329 - val_acc: 0.6974 -- iter: 240/240
--
Training Step: 113  | total loss: [1m[32m0.50057[0m[0m | time: 0.952s
[2K
| Adam | epoch: 015 | loss: 0.50057 - acc: 0.7572 -- iter: 032/240
[A[ATraining Step: 114  | total loss: [1m[32m0.46914[0m[0m | time: 1.854s
[2K
| Adam | epoch: 015 | loss: 0.46914 - acc: 0.7783 -- iter: 064/240
[A[ATraining Step: 115  | total loss: [1m[32m0.45329[0m[0m | time: 2.747s
[2K
| Adam | epoch: 015 | loss: 0.45329 - acc: 0.7880 -- iter: 096/240
[A[ATraining Step: 116  | total loss: [1m[32m0.43879[0m[0m | time: 3.167s
[2K
| Adam | epoch: 015 | loss: 0.43879 - acc: 0.7936 -- iter: 128/240
[A[ATraining Step: 117  | total loss: [1m[32m0.43743[0m[0m | time: 3.625s
[2K
| Adam | epoch: 015 | loss: 0.43743 - acc: 0.7955 -- iter: 160/240
[A[ATraining Step: 118  | total loss: [1m[32m0.42989[0m[0m | time: 4.610s
[2K
| Adam | epoch: 015 | loss: 0.42989 - acc: 0.7972 -- iter: 192/240
[A[ATraining Step: 119  | total loss: [1m[32m0.42344[0m[0m | time: 5.535s
[2K
| Adam | epoch: 015 | loss: 0.42344 - acc: 0.7987 -- iter: 224/240
[A[ATraining Step: 120  | total loss: [1m[32m0.42683[0m[0m | time: 7.499s
[2K
| Adam | epoch: 015 | loss: 0.42683 - acc: 0.8032 | val_loss: 0.69455 - val_acc: 0.6579 -- iter: 240/240
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.8385416666666666
Validation AUPRC:0.8941870225936792
Test AUC:0.9436222692036645
Test AUPRC:0.9471656562007194
BestTestF1Score	0.87	0.71	0.86	0.88	0.86	37	5	28	6	0.15
BestTestMCCScore	0.87	0.71	0.86	0.88	0.86	37	5	28	6	0.15
BestTestAccuracyScore	0.87	0.71	0.86	0.88	0.86	37	5	28	6	0.15
BestValidationF1Score	0.87	0.63	0.83	0.86	0.88	42	7	21	6	0.15
BestValidationMCC	0.87	0.63	0.83	0.86	0.88	42	7	21	6	0.15
BestValidationAccuracy	0.87	0.63	0.83	0.86	0.88	42	7	21	6	0.15
TestPredictions (Threshold:0.15)
CHEMBL181182,TN,INACT,0.05999999865889549	CHEMBL1322239,TN,INACT,0.05999999865889549	CHEMBL3647069,TP,ACT,0.8600000143051147	CHEMBL3650053,TP,ACT,0.46000000834465027	CHEMBL383471,FP,INACT,0.3799999952316284	CHEMBL611331,TN,INACT,0.05999999865889549	CHEMBL3650076,TP,ACT,0.6100000143051147	CHEMBL3696501,TN,INACT,0.10000000149011612	CHEMBL3220094,FN,ACT,0.11999999731779099	CHEMBL3683101,FP,INACT,0.7099999785423279	CHEMBL3653217,TP,ACT,0.4099999964237213	CHEMBL2089210,FN,ACT,0.14000000059604645	CHEMBL461462,FP,INACT,0.1599999964237213	CHEMBL3653195,TP,ACT,0.5400000214576721	CHEMBL1688152,TN,INACT,0.09000000357627869	CHEMBL515487,TP,ACT,0.3700000047683716	CHEMBL2089149,FN,ACT,0.14000000059604645	CHEMBL3683137,TP,ACT,0.6600000262260437	CHEMBL3675207,TP,ACT,0.41999998688697815	CHEMBL1405108,TN,INACT,0.10999999940395355	CHEMBL3649863,TP,ACT,0.9399999976158142	CHEMBL3653244,TP,ACT,0.7400000095367432	CHEMBL1455341,TN,INACT,0.07999999821186066	CHEMBL3752382,TP,ACT,0.7699999809265137	CHEMBL2396756,TN,INACT,0.09000000357627869	CHEMBL3657119,TP,ACT,0.8700000047683716	CHEMBL2431767,TN,INACT,0.10000000149011612	CHEMBL3657006,TP,ACT,0.17000000178813934	CHEMBL3649907,TP,ACT,0.9300000071525574	CHEMBL3600727,TN,INACT,0.05999999865889549	CHEMBL3800561,TP,ACT,0.5400000214576721	CHEMBL3691721,TP,ACT,0.5600000023841858	CHEMBL1442034,TN,INACT,0.05999999865889549	CHEMBL3657043,TP,ACT,0.38999998569488525	CHEMBL1568065,TN,INACT,0.05999999865889549	CHEMBL1688300,TN,INACT,0.09000000357627869	CHEMBL1617615,TN,INACT,0.05000000074505806	CHEMBL359675,TN,INACT,0.05999999865889549	CHEMBL3653215,TP,ACT,0.3100000023841858	CHEMBL3600729,TN,INACT,0.05999999865889549	CHEMBL2347398,TP,ACT,0.3100000023841858	CHEMBL3798014,TP,ACT,0.4099999964237213	CHEMBL3687304,TP,ACT,0.47999998927116394	CHEMBL3675221,FN,ACT,0.10999999940395355	CHEMBL1359629,TN,INACT,0.07000000029802322	CHEMBL3600728,TN,INACT,0.05999999865889549	CHEMBL1688263,TP,ACT,0.6899999976158142	CHEMBL1566168,TN,INACT,0.07000000029802322	CHEMBL3653165,TP,ACT,0.3499999940395355	CHEMBL1407893,TN,INACT,0.05999999865889549	CHEMBL3703668,TN,INACT,0.07000000029802322	CHEMBL3600720,TN,INACT,0.07999999821186066	CHEMBL393164,TN,INACT,0.14000000059604645	CHEMBL180741,TN,INACT,0.10999999940395355	CHEMBL3220089,FN,ACT,0.10000000149011612	CHEMBL3657057,TP,ACT,0.44999998807907104	CHEMBL3653201,TP,ACT,0.6800000071525574	CHEMBL3675220,TP,ACT,0.7099999785423279	CHEMBL1618026,TN,INACT,0.07000000029802322	CHEMBL3654186,TP,ACT,0.8999999761581421	CHEMBL3233135,TP,ACT,0.7900000214576721	CHEMBL3653338,FN,ACT,0.11999999731779099	CHEMBL3219393,FP,INACT,0.20000000298023224	CHEMBL3653235,TP,ACT,0.5400000214576721	CHEMBL3219402,FP,INACT,0.25999999046325684	CHEMBL359955,TP,ACT,0.4000000059604645	CHEMBL3703659,TP,ACT,0.18000000715255737	CHEMBL1527361,TN,INACT,0.05999999865889549	CHEMBL3649939,TP,ACT,0.8999999761581421	CHEMBL3600732,TN,INACT,0.07000000029802322	CHEMBL3797513,TP,ACT,0.4300000071525574	CHEMBL3236359,TP,ACT,0.15000000596046448	CHEMBL3657104,TP,ACT,0.8399999737739563	CHEMBL3220086,TP,ACT,0.33000001311302185	CHEMBL1605042,TN,INACT,0.03999999910593033	CHEMBL3665607,TP,ACT,0.7599999904632568	

