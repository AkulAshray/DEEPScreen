ImageNetInceptionV2 CHEMBL1293194 adam 0.0005 15 0 0 0.8 False True
Number of active compounds :	196
Number of inactive compounds :	131
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL1293194_adam_0.0005_15_0_0_0.8_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL1293194_adam_0.0005_15_0.8/
---------------------------------
Training samples: 208
Validation samples: 66
--
Training Step: 1  | time: 75.147s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/208
[A[ATraining Step: 2  | total loss: [1m[32m0.49785[0m[0m | time: 95.352s
[2K
| Adam | epoch: 001 | loss: 0.49785 - acc: 0.7875 -- iter: 064/208
[A[ATraining Step: 3  | total loss: [1m[32m0.78269[0m[0m | time: 114.565s
[2K
| Adam | epoch: 001 | loss: 0.78269 - acc: 0.7057 -- iter: 096/208
[A[ATraining Step: 4  | total loss: [1m[32m0.67675[0m[0m | time: 133.257s
[2K
| Adam | epoch: 001 | loss: 0.67675 - acc: 0.6686 -- iter: 128/208
[A[ATraining Step: 5  | total loss: [1m[32m0.57074[0m[0m | time: 161.891s
[2K
| Adam | epoch: 001 | loss: 0.57074 - acc: 0.6817 -- iter: 160/208
[A[ATraining Step: 6  | total loss: [1m[32m0.47101[0m[0m | time: 178.461s
[2K
| Adam | epoch: 001 | loss: 0.47101 - acc: 0.7859 -- iter: 192/208
[A[ATraining Step: 7  | total loss: [1m[32m0.41711[0m[0m | time: 203.082s
[2K
| Adam | epoch: 001 | loss: 0.41711 - acc: 0.7831 | val_loss: 0.80623 - val_acc: 0.5000 -- iter: 208/208
--
Training Step: 8  | total loss: [1m[32m0.32691[0m[0m | time: 15.705s
[2K
| Adam | epoch: 002 | loss: 0.32691 - acc: 0.8699 -- iter: 032/208
[A[ATraining Step: 9  | total loss: [1m[32m0.19687[0m[0m | time: 45.324s
[2K
| Adam | epoch: 002 | loss: 0.19687 - acc: 0.9388 -- iter: 064/208
[A[ATraining Step: 10  | total loss: [1m[32m0.21247[0m[0m | time: 74.567s
[2K
| Adam | epoch: 002 | loss: 0.21247 - acc: 0.9225 -- iter: 096/208
[A[ATraining Step: 11  | total loss: [1m[32m0.18134[0m[0m | time: 110.057s
[2K
| Adam | epoch: 002 | loss: 0.18134 - acc: 0.9296 -- iter: 128/208
[A[ATraining Step: 12  | total loss: [1m[32m0.21722[0m[0m | time: 137.256s
[2K
| Adam | epoch: 002 | loss: 0.21722 - acc: 0.9332 -- iter: 160/208
[A[ATraining Step: 13  | total loss: [1m[32m0.23242[0m[0m | time: 168.842s
[2K
| Adam | epoch: 002 | loss: 0.23242 - acc: 0.9216 -- iter: 192/208
[A[ATraining Step: 14  | total loss: [1m[32m0.19664[0m[0m | time: 209.487s
[2K
| Adam | epoch: 002 | loss: 0.19664 - acc: 0.9409 | val_loss: 3.29765 - val_acc: 0.5000 -- iter: 208/208
--
Training Step: 15  | total loss: [1m[32m0.26991[0m[0m | time: 8.500s
[2K
| Adam | epoch: 003 | loss: 0.26991 - acc: 0.9151 -- iter: 032/208
[A[ATraining Step: 16  | total loss: [1m[32m0.53322[0m[0m | time: 16.840s
[2K
| Adam | epoch: 003 | loss: 0.53322 - acc: 0.8766 -- iter: 064/208
[A[ATraining Step: 17  | total loss: [1m[32m0.35054[0m[0m | time: 43.198s
[2K
| Adam | epoch: 003 | loss: 0.35054 - acc: 0.9210 -- iter: 096/208
[A[ATraining Step: 18  | total loss: [1m[32m0.28984[0m[0m | time: 62.398s
[2K
| Adam | epoch: 003 | loss: 0.28984 - acc: 0.9267 -- iter: 128/208
[A[ATraining Step: 19  | total loss: [1m[32m0.24925[0m[0m | time: 81.485s
[2K
| Adam | epoch: 003 | loss: 0.24925 - acc: 0.9407 -- iter: 160/208
[A[ATraining Step: 20  | total loss: [1m[32m0.23012[0m[0m | time: 103.142s
[2K
| Adam | epoch: 003 | loss: 0.23012 - acc: 0.9397 -- iter: 192/208
[A[ATraining Step: 21  | total loss: [1m[32m0.21868[0m[0m | time: 126.440s
[2K
| Adam | epoch: 003 | loss: 0.21868 - acc: 0.9293 | val_loss: 0.89827 - val_acc: 0.5000 -- iter: 208/208
--
Training Step: 22  | total loss: [1m[32m0.20382[0m[0m | time: 23.266s
[2K
| Adam | epoch: 004 | loss: 0.20382 - acc: 0.9411 -- iter: 032/208
[A[ATraining Step: 23  | total loss: [1m[32m0.16373[0m[0m | time: 31.969s
[2K
| Adam | epoch: 004 | loss: 0.16373 - acc: 0.9582 -- iter: 064/208
[A[ATraining Step: 24  | total loss: [1m[32m0.18554[0m[0m | time: 40.025s
[2K
| Adam | epoch: 004 | loss: 0.18554 - acc: 0.9524 -- iter: 096/208
[A[ATraining Step: 25  | total loss: [1m[32m0.16429[0m[0m | time: 76.589s
[2K
| Adam | epoch: 004 | loss: 0.16429 - acc: 0.9654 -- iter: 128/208
[A[ATraining Step: 26  | total loss: [1m[32m0.13486[0m[0m | time: 90.716s
[2K
| Adam | epoch: 004 | loss: 0.13486 - acc: 0.9745 -- iter: 160/208
[A[ATraining Step: 27  | total loss: [1m[32m0.11748[0m[0m | time: 116.967s
[2K
| Adam | epoch: 004 | loss: 0.11748 - acc: 0.9731 -- iter: 192/208
[A[ATraining Step: 28  | total loss: [1m[32m0.11319[0m[0m | time: 150.073s
[2K
| Adam | epoch: 004 | loss: 0.11319 - acc: 0.9642 | val_loss: 2.24307 - val_acc: 0.5000 -- iter: 208/208
--
Training Step: 29  | total loss: [1m[32m0.10390[0m[0m | time: 19.701s
[2K
| Adam | epoch: 005 | loss: 0.10390 - acc: 0.9653 -- iter: 032/208
[A[ATraining Step: 30  | total loss: [1m[32m0.10897[0m[0m | time: 44.920s
[2K
| Adam | epoch: 005 | loss: 0.10897 - acc: 0.9587 -- iter: 064/208
[A[ATraining Step: 31  | total loss: [1m[32m0.15398[0m[0m | time: 52.933s
[2K
| Adam | epoch: 005 | loss: 0.15398 - acc: 0.9250 -- iter: 096/208
[A[ATraining Step: 32  | total loss: [1m[32m0.19824[0m[0m | time: 60.976s
[2K
| Adam | epoch: 005 | loss: 0.19824 - acc: 0.9278 -- iter: 128/208
[A[ATraining Step: 33  | total loss: [1m[32m0.15804[0m[0m | time: 77.481s
[2K
| Adam | epoch: 005 | loss: 0.15804 - acc: 0.9436 -- iter: 160/208
[A[ATraining Step: 34  | total loss: [1m[32m0.13759[0m[0m | time: 103.661s
[2K
| Adam | epoch: 005 | loss: 0.13759 - acc: 0.9490 -- iter: 192/208
[A[ATraining Step: 35  | total loss: [1m[32m0.12408[0m[0m | time: 122.396s
[2K
| Adam | epoch: 005 | loss: 0.12408 - acc: 0.9531 | val_loss: 1.09538 - val_acc: 0.5303 -- iter: 208/208
--
Training Step: 36  | total loss: [1m[32m0.12601[0m[0m | time: 17.345s
[2K
| Adam | epoch: 006 | loss: 0.12601 - acc: 0.9563 -- iter: 032/208
[A[ATraining Step: 37  | total loss: [1m[32m0.14100[0m[0m | time: 39.016s
[2K
| Adam | epoch: 006 | loss: 0.14100 - acc: 0.9526 -- iter: 064/208
[A[ATraining Step: 38  | total loss: [1m[32m0.11850[0m[0m | time: 55.186s
[2K
| Adam | epoch: 006 | loss: 0.11850 - acc: 0.9619 -- iter: 096/208
[A[ATraining Step: 39  | total loss: [1m[32m0.09923[0m[0m | time: 63.113s
[2K
| Adam | epoch: 006 | loss: 0.09923 - acc: 0.9692 -- iter: 128/208
[A[ATraining Step: 40  | total loss: [1m[32m0.09069[0m[0m | time: 71.394s
[2K
| Adam | epoch: 006 | loss: 0.09069 - acc: 0.9749 -- iter: 160/208
[A[ATraining Step: 41  | total loss: [1m[32m0.07820[0m[0m | time: 89.211s
[2K
| Adam | epoch: 006 | loss: 0.07820 - acc: 0.9795 -- iter: 192/208
[A[ATraining Step: 42  | total loss: [1m[32m0.07394[0m[0m | time: 108.732s
[2K
| Adam | epoch: 006 | loss: 0.07394 - acc: 0.9776 | val_loss: 0.88474 - val_acc: 0.4242 -- iter: 208/208
--
Training Step: 43  | total loss: [1m[32m0.07436[0m[0m | time: 23.534s
[2K
| Adam | epoch: 007 | loss: 0.07436 - acc: 0.9760 -- iter: 032/208
[A[ATraining Step: 44  | total loss: [1m[32m0.06552[0m[0m | time: 39.657s
[2K
| Adam | epoch: 007 | loss: 0.06552 - acc: 0.9802 -- iter: 064/208
[A[ATraining Step: 45  | total loss: [1m[32m0.06157[0m[0m | time: 57.085s
[2K
| Adam | epoch: 007 | loss: 0.06157 - acc: 0.9836 -- iter: 096/208
[A[ATraining Step: 46  | total loss: [1m[32m0.05200[0m[0m | time: 79.862s
[2K
| Adam | epoch: 007 | loss: 0.05200 - acc: 0.9863 -- iter: 128/208
[A[ATraining Step: 47  | total loss: [1m[32m0.05004[0m[0m | time: 88.185s
[2K
| Adam | epoch: 007 | loss: 0.05004 - acc: 0.9885 -- iter: 160/208
[A[ATraining Step: 48  | total loss: [1m[32m0.23940[0m[0m | time: 95.896s
[2K
| Adam | epoch: 007 | loss: 0.23940 - acc: 0.9602 -- iter: 192/208
[A[ATraining Step: 49  | total loss: [1m[32m0.20193[0m[0m | time: 114.524s
[2K
| Adam | epoch: 007 | loss: 0.20193 - acc: 0.9665 | val_loss: 1.59694 - val_acc: 0.5152 -- iter: 208/208
--
Training Step: 50  | total loss: [1m[32m0.18312[0m[0m | time: 31.998s
[2K
| Adam | epoch: 008 | loss: 0.18312 - acc: 0.9669 -- iter: 032/208
[A[ATraining Step: 51  | total loss: [1m[32m0.18839[0m[0m | time: 49.441s
[2K
| Adam | epoch: 008 | loss: 0.18839 - acc: 0.9624 -- iter: 064/208
[A[ATraining Step: 52  | total loss: [1m[32m0.16235[0m[0m | time: 73.325s
[2K
| Adam | epoch: 008 | loss: 0.16235 - acc: 0.9680 -- iter: 096/208
[A[ATraining Step: 53  | total loss: [1m[32m0.16598[0m[0m | time: 102.128s
[2K
| Adam | epoch: 008 | loss: 0.16598 - acc: 0.9681 -- iter: 128/208
[A[ATraining Step: 54  | total loss: [1m[32m0.17057[0m[0m | time: 116.778s
[2K
| Adam | epoch: 008 | loss: 0.17057 - acc: 0.9682 -- iter: 160/208
[A[ATraining Step: 55  | total loss: [1m[32m0.14803[0m[0m | time: 125.726s
[2K
| Adam | epoch: 008 | loss: 0.14803 - acc: 0.9728 -- iter: 192/208
[A[ATraining Step: 56  | total loss: [1m[32m0.19139[0m[0m | time: 140.824s
[2K
| Adam | epoch: 008 | loss: 0.19139 - acc: 0.9678 | val_loss: 1.54788 - val_acc: 0.6364 -- iter: 208/208
--
Training Step: 57  | total loss: [1m[32m0.16676[0m[0m | time: 56.837s
[2K
| Adam | epoch: 009 | loss: 0.16676 - acc: 0.9723 -- iter: 032/208
[A[ATraining Step: 58  | total loss: [1m[32m0.14620[0m[0m | time: 126.594s
[2K
| Adam | epoch: 009 | loss: 0.14620 - acc: 0.9760 -- iter: 064/208
[A[ATraining Step: 59  | total loss: [1m[32m0.13113[0m[0m | time: 159.674s
[2K
| Adam | epoch: 009 | loss: 0.13113 - acc: 0.9793 -- iter: 096/208
[A[ATraining Step: 60  | total loss: [1m[32m0.11694[0m[0m | time: 222.126s
[2K
| Adam | epoch: 009 | loss: 0.11694 - acc: 0.9820 -- iter: 128/208
[A[ATraining Step: 61  | total loss: [1m[32m0.12072[0m[0m | time: 254.283s
[2K
| Adam | epoch: 009 | loss: 0.12072 - acc: 0.9803 -- iter: 160/208
[A[ATraining Step: 62  | total loss: [1m[32m0.11736[0m[0m | time: 324.222s
[2K
| Adam | epoch: 009 | loss: 0.11736 - acc: 0.9788 -- iter: 192/208
[A[ATraining Step: 63  | total loss: [1m[32m0.10692[0m[0m | time: 339.722s
[2K
| Adam | epoch: 009 | loss: 0.10692 - acc: 0.9815 | val_loss: 0.98236 - val_acc: 0.7424 -- iter: 208/208
--
Training Step: 64  | total loss: [1m[32m0.13603[0m[0m | time: 9.651s
[2K
| Adam | epoch: 010 | loss: 0.13603 - acc: 0.9760 -- iter: 032/208
[A[ATraining Step: 65  | total loss: [1m[32m0.12246[0m[0m | time: 40.506s
[2K
| Adam | epoch: 010 | loss: 0.12246 - acc: 0.9789 -- iter: 064/208
[A[ATraining Step: 66  | total loss: [1m[32m0.10990[0m[0m | time: 84.066s
[2K
| Adam | epoch: 010 | loss: 0.10990 - acc: 0.9815 -- iter: 096/208
[A[ATraining Step: 67  | total loss: [1m[32m0.11444[0m[0m | time: 135.576s
[2K
| Adam | epoch: 010 | loss: 0.11444 - acc: 0.9800 -- iter: 128/208
[A[ATraining Step: 68  | total loss: [1m[32m0.11737[0m[0m | time: 175.031s
[2K
| Adam | epoch: 010 | loss: 0.11737 - acc: 0.9749 -- iter: 160/208
[A[ATraining Step: 69  | total loss: [1m[32m0.10660[0m[0m | time: 225.818s
[2K
| Adam | epoch: 010 | loss: 0.10660 - acc: 0.9779 -- iter: 192/208
[A[ATraining Step: 70  | total loss: [1m[32m0.09979[0m[0m | time: 258.714s
[2K
| Adam | epoch: 010 | loss: 0.09979 - acc: 0.9804 | val_loss: 0.35192 - val_acc: 0.8636 -- iter: 208/208
--
Training Step: 71  | total loss: [1m[32m0.09154[0m[0m | time: 9.166s
[2K
| Adam | epoch: 011 | loss: 0.09154 - acc: 0.9827 -- iter: 032/208
[A[ATraining Step: 72  | total loss: [1m[32m0.08335[0m[0m | time: 34.801s
[2K
| Adam | epoch: 011 | loss: 0.08335 - acc: 0.9846 -- iter: 064/208
[A[ATraining Step: 73  | total loss: [1m[32m0.07600[0m[0m | time: 56.659s
[2K
| Adam | epoch: 011 | loss: 0.07600 - acc: 0.9863 -- iter: 096/208
[A[ATraining Step: 74  | total loss: [1m[32m0.07331[0m[0m | time: 78.110s
[2K
| Adam | epoch: 011 | loss: 0.07331 - acc: 0.9878 -- iter: 128/208
[A[ATraining Step: 75  | total loss: [1m[32m0.07319[0m[0m | time: 131.150s
[2K
| Adam | epoch: 011 | loss: 0.07319 - acc: 0.9858 -- iter: 160/208
[A[ATraining Step: 76  | total loss: [1m[32m0.06729[0m[0m | time: 177.156s
[2K
| Adam | epoch: 011 | loss: 0.06729 - acc: 0.9873 -- iter: 192/208
[A[ATraining Step: 77  | total loss: [1m[32m0.06160[0m[0m | time: 234.185s
[2K
| Adam | epoch: 011 | loss: 0.06160 - acc: 0.9886 | val_loss: 0.90834 - val_acc: 0.7121 -- iter: 208/208
--
Training Step: 78  | total loss: [1m[32m0.05582[0m[0m | time: 49.835s
[2K
| Adam | epoch: 012 | loss: 0.05582 - acc: 0.9898 -- iter: 032/208
[A[ATraining Step: 79  | total loss: [1m[32m0.06023[0m[0m | time: 59.133s
[2K
| Adam | epoch: 012 | loss: 0.06023 - acc: 0.9876 -- iter: 064/208
[A[ATraining Step: 80  | total loss: [1m[32m0.09148[0m[0m | time: 68.260s
[2K
| Adam | epoch: 012 | loss: 0.09148 - acc: 0.9825 -- iter: 096/208
[A[ATraining Step: 81  | total loss: [1m[32m0.08256[0m[0m | time: 102.625s
[2K
| Adam | epoch: 012 | loss: 0.08256 - acc: 0.9843 -- iter: 128/208
[A[ATraining Step: 82  | total loss: [1m[32m0.07971[0m[0m | time: 118.063s
[2K
| Adam | epoch: 012 | loss: 0.07971 - acc: 0.9827 -- iter: 160/208
[A[ATraining Step: 83  | total loss: [1m[32m0.07210[0m[0m | time: 148.095s
[2K
| Adam | epoch: 012 | loss: 0.07210 - acc: 0.9845 -- iter: 192/208
[A[ATraining Step: 84  | total loss: [1m[32m0.06665[0m[0m | time: 174.893s
[2K
| Adam | epoch: 012 | loss: 0.06665 - acc: 0.9860 | val_loss: 0.81402 - val_acc: 0.7576 -- iter: 208/208
--
Training Step: 85  | total loss: [1m[32m0.06115[0m[0m | time: 51.842s
[2K
| Adam | epoch: 013 | loss: 0.06115 - acc: 0.9874 -- iter: 032/208
[A[ATraining Step: 86  | total loss: [1m[32m0.05597[0m[0m | time: 84.167s
[2K
| Adam | epoch: 013 | loss: 0.05597 - acc: 0.9887 -- iter: 064/208
[A[ATraining Step: 87  | total loss: [1m[32m0.06484[0m[0m | time: 93.271s
[2K
| Adam | epoch: 013 | loss: 0.06484 - acc: 0.9867 -- iter: 096/208
[A[ATraining Step: 88  | total loss: [1m[32m0.10741[0m[0m | time: 101.793s
[2K
| Adam | epoch: 013 | loss: 0.10741 - acc: 0.9818 -- iter: 128/208
[A[ATraining Step: 89  | total loss: [1m[32m0.09816[0m[0m | time: 117.822s
[2K
| Adam | epoch: 013 | loss: 0.09816 - acc: 0.9836 -- iter: 160/208
[A[ATraining Step: 90  | total loss: [1m[32m0.09024[0m[0m | time: 162.855s
[2K
| Adam | epoch: 013 | loss: 0.09024 - acc: 0.9852 -- iter: 192/208
[A[ATraining Step: 91  | total loss: [1m[32m0.08675[0m[0m | time: 210.619s
[2K
| Adam | epoch: 013 | loss: 0.08675 - acc: 0.9836 | val_loss: 1.08803 - val_acc: 0.6667 -- iter: 208/208
--
Training Step: 92  | total loss: [1m[32m0.08037[0m[0m | time: 27.627s
[2K
| Adam | epoch: 014 | loss: 0.08037 - acc: 0.9852 -- iter: 032/208
[A[ATraining Step: 93  | total loss: [1m[32m0.09186[0m[0m | time: 43.961s
[2K
| Adam | epoch: 014 | loss: 0.09186 - acc: 0.9836 -- iter: 064/208
[A[ATraining Step: 94  | total loss: [1m[32m0.08414[0m[0m | time: 88.158s
[2K
| Adam | epoch: 014 | loss: 0.08414 - acc: 0.9852 -- iter: 096/208
[A[ATraining Step: 95  | total loss: [1m[32m0.07745[0m[0m | time: 98.116s
[2K
| Adam | epoch: 014 | loss: 0.07745 - acc: 0.9867 -- iter: 128/208
[A[ATraining Step: 96  | total loss: [1m[32m0.12142[0m[0m | time: 107.576s
[2K
| Adam | epoch: 014 | loss: 0.12142 - acc: 0.9818 -- iter: 160/208
[A[ATraining Step: 97  | total loss: [1m[32m0.11310[0m[0m | time: 139.050s
[2K
| Adam | epoch: 014 | loss: 0.11310 - acc: 0.9836 -- iter: 192/208
[A[ATraining Step: 98  | total loss: [1m[32m0.10686[0m[0m | time: 207.290s
[2K
| Adam | epoch: 014 | loss: 0.10686 - acc: 0.9821 | val_loss: 0.21029 - val_acc: 0.9242 -- iter: 208/208
--
Training Step: 99  | total loss: [1m[32m0.09740[0m[0m | time: 24.383s
[2K
| Adam | epoch: 015 | loss: 0.09740 - acc: 0.9839 -- iter: 032/208
[A[ATraining Step: 100  | total loss: [1m[32m0.08853[0m[0m | time: 39.521s
[2K
| Adam | epoch: 015 | loss: 0.08853 - acc: 0.9855 -- iter: 064/208
[A[ATraining Step: 101  | total loss: [1m[32m0.08166[0m[0m | time: 68.545s
[2K
| Adam | epoch: 015 | loss: 0.08166 - acc: 0.9870 -- iter: 096/208
[A[ATraining Step: 102  | total loss: [1m[32m0.07976[0m[0m | time: 105.928s
[2K
| Adam | epoch: 015 | loss: 0.07976 - acc: 0.9851 -- iter: 128/208
[A[ATraining Step: 103  | total loss: [1m[32m0.07311[0m[0m | time: 115.319s
[2K
| Adam | epoch: 015 | loss: 0.07311 - acc: 0.9866 -- iter: 160/208
[A[ATraining Step: 104  | total loss: [1m[32m0.12060[0m[0m | time: 124.390s
[2K
| Adam | epoch: 015 | loss: 0.12060 - acc: 0.9755 -- iter: 192/208
[A[ATraining Step: 105  | total loss: [1m[32m0.10998[0m[0m | time: 146.330s
[2K
| Adam | epoch: 015 | loss: 0.10998 - acc: 0.9779 | val_loss: 0.23800 - val_acc: 0.9091 -- iter: 208/208
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.9724517906336088
Validation AUPRC:0.9718836635039896
Test AUC:0.9944444444444444
Test AUPRC:0.9939278937381404
BestTestF1Score	0.94	0.89	0.94	0.88	1.0	30	4	32	0	0.17
BestTestMCCScore	0.95	0.91	0.95	1.0	0.9	27	0	36	3	0.68
BestTestAccuracyScore	0.95	0.91	0.95	1.0	0.9	27	0	36	3	0.68
BestValidationF1Score	0.93	0.85	0.92	0.89	0.97	32	4	29	1	0.17
BestValidationMCC	0.92	0.85	0.92	0.97	0.88	29	1	32	4	0.68
BestValidationAccuracy	0.92	0.85	0.92	0.97	0.88	29	1	32	4	0.68
TestPredictions (Threshold:0.68)
CHEMBL3431775,TP,ACT,0.8799999952316284	CHEMBL3431828,TP,ACT,0.9100000262260437	CHEMBL19456,TN,INACT,0.05000000074505806	CHEMBL3335795,TN,INACT,0.05000000074505806	CHEMBL83103,TN,INACT,0.0	CHEMBL3559498,TN,INACT,0.009999999776482582	CHEMBL3763517,TN,INACT,0.0	CHEMBL2430728,TN,INACT,0.019999999552965164	CHEMBL752,TN,INACT,0.029999999329447746	CHEMBL3221026,TN,INACT,0.07000000029802322	CHEMBL3431786,TP,ACT,0.9700000286102295	CHEMBL3431859,TP,ACT,0.7799999713897705	CHEMBL2058999,TN,INACT,0.019999999552965164	CHEMBL2316890,TN,INACT,0.0	CHEMBL3817859,TN,INACT,0.0	CHEMBL3431767,TP,ACT,0.9200000166893005	CHEMBL2058998,TN,INACT,0.009999999776482582	CHEMBL3431853,FN,ACT,0.6600000262260437	CHEMBL2059000,TN,INACT,0.4099999964237213	CHEMBL3559497,TN,INACT,0.05999999865889549	CHEMBL2382400,TN,INACT,0.009999999776482582	CHEMBL3431873,TP,ACT,0.8299999833106995	CHEMBL3431467,TP,ACT,0.9300000071525574	CHEMBL3431543,FN,ACT,0.23999999463558197	CHEMBL3431887,FN,ACT,0.5699999928474426	CHEMBL3358869,TN,INACT,0.0	CHEMBL435995,TN,INACT,0.009999999776482582	CHEMBL127645,TN,INACT,0.0	CHEMBL3431676,TP,ACT,0.9599999785423279	CHEMBL19337,TN,INACT,0.15000000596046448	CHEMBL3431534,TP,ACT,0.8899999856948853	CHEMBL3431576,TP,ACT,0.9700000286102295	CHEMBL3431520,TP,ACT,0.9700000286102295	CHEMBL2430741,TN,INACT,0.0	CHEMBL2059002,TN,INACT,0.0	CHEMBL3431687,TP,ACT,0.9700000286102295	CHEMBL3431608,TP,ACT,0.9399999976158142	CHEMBL3431671,TP,ACT,0.8500000238418579	CHEMBL1232960,TN,INACT,0.0	CHEMBL3608327,TN,INACT,0.6499999761581421	CHEMBL3431703,TP,ACT,0.7900000214576721	CHEMBL3609809,TN,INACT,0.3199999928474426	CHEMBL2058997,TN,INACT,0.009999999776482582	CHEMBL3431897,TP,ACT,0.9800000190734863	CHEMBL2114397,TN,INACT,0.009999999776482582	CHEMBL3431812,TP,ACT,0.9700000286102295	CHEMBL2430736,TN,INACT,0.15000000596046448	CHEMBL3431525,TP,ACT,0.7799999713897705	CHEMBL2059006,TN,INACT,0.09000000357627869	CHEMBL3608328,TN,INACT,0.029999999329447746	CHEMBL3431779,TP,ACT,0.9100000262260437	CHEMBL3431570,TP,ACT,0.8799999952316284	CHEMBL3431806,TP,ACT,0.9200000166893005	CHEMBL1416049,TN,INACT,0.019999999552965164	CHEMBL3358885,TN,INACT,0.0	CHEMBL3431915,TP,ACT,0.9200000166893005	CHEMBL3431745,TP,ACT,0.9399999976158142	CHEMBL52639,TN,INACT,0.019999999552965164	CHEMBL3431792,TP,ACT,0.8899999856948853	CHEMBL3763160,TN,INACT,0.07000000029802322	CHEMBL3608326,TN,INACT,0.009999999776482582	CHEMBL3431670,TP,ACT,0.9100000262260437	CHEMBL3431840,TP,ACT,0.9800000190734863	CHEMBL3431726,TP,ACT,0.8999999761581421	CHEMBL2316893,TN,INACT,0.6100000143051147	CHEMBL3608330,TN,INACT,0.05000000074505806	

