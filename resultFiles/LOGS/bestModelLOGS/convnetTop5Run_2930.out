CNNModel CHEMBL322 adam 0.001 15 128 0 0.8 False True
Number of active compounds :	1574
Number of inactive compounds :	1574
---------------------------------
Run id: CNNModel_CHEMBL322_adam_0.001_15_128_0_0.8_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL322_adam_0.001_15_128_0.8_True/
---------------------------------
Training samples: 2005
Validation samples: 627
--
Training Step: 1  | time: 1.358s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 0032/2005
[A[ATraining Step: 2  | total loss: [1m[32m0.62405[0m[0m | time: 2.403s
[2K
| Adam | epoch: 001 | loss: 0.62405 - acc: 0.3656 -- iter: 0064/2005
[A[ATraining Step: 3  | total loss: [1m[32m0.68264[0m[0m | time: 3.488s
[2K
| Adam | epoch: 001 | loss: 0.68264 - acc: 0.4500 -- iter: 0096/2005
[A[ATraining Step: 4  | total loss: [1m[32m0.69038[0m[0m | time: 4.749s
[2K
| Adam | epoch: 001 | loss: 0.69038 - acc: 0.4875 -- iter: 0128/2005
[A[ATraining Step: 5  | total loss: [1m[32m0.69272[0m[0m | time: 5.899s
[2K
| Adam | epoch: 001 | loss: 0.69272 - acc: 0.4745 -- iter: 0160/2005
[A[ATraining Step: 6  | total loss: [1m[32m0.69291[0m[0m | time: 6.728s
[2K
| Adam | epoch: 001 | loss: 0.69291 - acc: 0.5512 -- iter: 0192/2005
[A[ATraining Step: 7  | total loss: [1m[32m0.69362[0m[0m | time: 8.045s
[2K
| Adam | epoch: 001 | loss: 0.69362 - acc: 0.4830 -- iter: 0224/2005
[A[ATraining Step: 8  | total loss: [1m[32m0.69433[0m[0m | time: 9.418s
[2K
| Adam | epoch: 001 | loss: 0.69433 - acc: 0.4398 -- iter: 0256/2005
[A[ATraining Step: 9  | total loss: [1m[32m0.69364[0m[0m | time: 10.665s
[2K
| Adam | epoch: 001 | loss: 0.69364 - acc: 0.4882 -- iter: 0288/2005
[A[ATraining Step: 10  | total loss: [1m[32m0.69346[0m[0m | time: 11.616s
[2K
| Adam | epoch: 001 | loss: 0.69346 - acc: 0.4472 -- iter: 0320/2005
[A[ATraining Step: 11  | total loss: [1m[32m0.69447[0m[0m | time: 12.545s
[2K
| Adam | epoch: 001 | loss: 0.69447 - acc: 0.3834 -- iter: 0352/2005
[A[ATraining Step: 12  | total loss: [1m[32m0.69404[0m[0m | time: 13.519s
[2K
| Adam | epoch: 001 | loss: 0.69404 - acc: 0.4359 -- iter: 0384/2005
[A[ATraining Step: 13  | total loss: [1m[32m0.69369[0m[0m | time: 14.465s
[2K
| Adam | epoch: 001 | loss: 0.69369 - acc: 0.4634 -- iter: 0416/2005
[A[ATraining Step: 14  | total loss: [1m[32m0.69349[0m[0m | time: 15.537s
[2K
| Adam | epoch: 001 | loss: 0.69349 - acc: 0.4783 -- iter: 0448/2005
[A[ATraining Step: 15  | total loss: [1m[32m0.69328[0m[0m | time: 16.545s
[2K
| Adam | epoch: 001 | loss: 0.69328 - acc: 0.5113 -- iter: 0480/2005
[A[ATraining Step: 16  | total loss: [1m[32m0.69328[0m[0m | time: 17.409s
[2K
| Adam | epoch: 001 | loss: 0.69328 - acc: 0.5070 -- iter: 0512/2005
[A[ATraining Step: 17  | total loss: [1m[32m0.69297[0m[0m | time: 18.508s
[2K
| Adam | epoch: 001 | loss: 0.69297 - acc: 0.5495 -- iter: 0544/2005
[A[ATraining Step: 18  | total loss: [1m[32m0.69255[0m[0m | time: 19.843s
[2K
| Adam | epoch: 001 | loss: 0.69255 - acc: 0.5648 -- iter: 0576/2005
[A[ATraining Step: 19  | total loss: [1m[32m0.69332[0m[0m | time: 21.222s
[2K
| Adam | epoch: 001 | loss: 0.69332 - acc: 0.5224 -- iter: 0608/2005
[A[ATraining Step: 20  | total loss: [1m[32m0.69194[0m[0m | time: 22.445s
[2K
| Adam | epoch: 001 | loss: 0.69194 - acc: 0.5654 -- iter: 0640/2005
[A[ATraining Step: 21  | total loss: [1m[32m0.69377[0m[0m | time: 23.379s
[2K
| Adam | epoch: 001 | loss: 0.69377 - acc: 0.5063 -- iter: 0672/2005
[A[ATraining Step: 22  | total loss: [1m[32m0.69202[0m[0m | time: 24.307s
[2K
| Adam | epoch: 001 | loss: 0.69202 - acc: 0.5419 -- iter: 0704/2005
[A[ATraining Step: 23  | total loss: [1m[32m0.69233[0m[0m | time: 25.247s
[2K
| Adam | epoch: 001 | loss: 0.69233 - acc: 0.5298 -- iter: 0736/2005
[A[ATraining Step: 24  | total loss: [1m[32m0.69379[0m[0m | time: 26.231s
[2K
| Adam | epoch: 001 | loss: 0.69379 - acc: 0.5038 -- iter: 0768/2005
[A[ATraining Step: 25  | total loss: [1m[32m0.69214[0m[0m | time: 27.305s
[2K
| Adam | epoch: 001 | loss: 0.69214 - acc: 0.5283 -- iter: 0800/2005
[A[ATraining Step: 26  | total loss: [1m[32m0.69487[0m[0m | time: 28.250s
[2K
| Adam | epoch: 001 | loss: 0.69487 - acc: 0.4877 -- iter: 0832/2005
[A[ATraining Step: 27  | total loss: [1m[32m0.69384[0m[0m | time: 29.218s
[2K
| Adam | epoch: 001 | loss: 0.69384 - acc: 0.4989 -- iter: 0864/2005
[A[ATraining Step: 28  | total loss: [1m[32m0.69335[0m[0m | time: 30.513s
[2K
| Adam | epoch: 001 | loss: 0.69335 - acc: 0.5070 -- iter: 0896/2005
[A[ATraining Step: 29  | total loss: [1m[32m0.69402[0m[0m | time: 31.873s
[2K
| Adam | epoch: 001 | loss: 0.69402 - acc: 0.4901 -- iter: 0928/2005
[A[ATraining Step: 30  | total loss: [1m[32m0.69249[0m[0m | time: 32.919s
[2K
| Adam | epoch: 001 | loss: 0.69249 - acc: 0.5221 -- iter: 0960/2005
[A[ATraining Step: 31  | total loss: [1m[32m0.69305[0m[0m | time: 33.811s
[2K
| Adam | epoch: 001 | loss: 0.69305 - acc: 0.5098 -- iter: 0992/2005
[A[ATraining Step: 32  | total loss: [1m[32m0.69159[0m[0m | time: 34.796s
[2K
| Adam | epoch: 001 | loss: 0.69159 - acc: 0.5427 -- iter: 1024/2005
[A[ATraining Step: 33  | total loss: [1m[32m0.69189[0m[0m | time: 35.800s
[2K
| Adam | epoch: 001 | loss: 0.69189 - acc: 0.5333 -- iter: 1056/2005
[A[ATraining Step: 34  | total loss: [1m[32m0.69261[0m[0m | time: 36.774s
[2K
| Adam | epoch: 001 | loss: 0.69261 - acc: 0.5195 -- iter: 1088/2005
[A[ATraining Step: 35  | total loss: [1m[32m0.69148[0m[0m | time: 37.837s
[2K
| Adam | epoch: 001 | loss: 0.69148 - acc: 0.5350 -- iter: 1120/2005
[A[ATraining Step: 36  | total loss: [1m[32m0.69171[0m[0m | time: 38.843s
[2K
| Adam | epoch: 001 | loss: 0.69171 - acc: 0.5343 -- iter: 1152/2005
[A[ATraining Step: 37  | total loss: [1m[32m0.69421[0m[0m | time: 39.622s
[2K
| Adam | epoch: 001 | loss: 0.69421 - acc: 0.5024 -- iter: 1184/2005
[A[ATraining Step: 38  | total loss: [1m[32m0.69368[0m[0m | time: 40.854s
[2K
| Adam | epoch: 001 | loss: 0.69368 - acc: 0.5081 -- iter: 1216/2005
[A[ATraining Step: 39  | total loss: [1m[32m0.69436[0m[0m | time: 42.186s
[2K
| Adam | epoch: 001 | loss: 0.69436 - acc: 0.4945 -- iter: 1248/2005
[A[ATraining Step: 40  | total loss: [1m[32m0.69481[0m[0m | time: 43.484s
[2K
| Adam | epoch: 001 | loss: 0.69481 - acc: 0.4838 -- iter: 1280/2005
[A[ATraining Step: 41  | total loss: [1m[32m0.69477[0m[0m | time: 44.408s
[2K
| Adam | epoch: 001 | loss: 0.69477 - acc: 0.4811 -- iter: 1312/2005
[A[ATraining Step: 42  | total loss: [1m[32m0.69467[0m[0m | time: 45.297s
[2K
| Adam | epoch: 001 | loss: 0.69467 - acc: 0.4789 -- iter: 1344/2005
[A[ATraining Step: 43  | total loss: [1m[32m0.69367[0m[0m | time: 46.250s
[2K
| Adam | epoch: 001 | loss: 0.69367 - acc: 0.5046 -- iter: 1376/2005
[A[ATraining Step: 44  | total loss: [1m[32m0.69282[0m[0m | time: 47.234s
[2K
| Adam | epoch: 001 | loss: 0.69282 - acc: 0.5309 -- iter: 1408/2005
[A[ATraining Step: 45  | total loss: [1m[32m0.69305[0m[0m | time: 48.261s
[2K
| Adam | epoch: 001 | loss: 0.69305 - acc: 0.5203 -- iter: 1440/2005
[A[ATraining Step: 46  | total loss: [1m[32m0.69385[0m[0m | time: 49.378s
[2K
| Adam | epoch: 001 | loss: 0.69385 - acc: 0.4909 -- iter: 1472/2005
[A[ATraining Step: 47  | total loss: [1m[32m0.69385[0m[0m | time: 50.290s
[2K
| Adam | epoch: 001 | loss: 0.69385 - acc: 0.4873 -- iter: 1504/2005
[A[ATraining Step: 48  | total loss: [1m[32m0.69357[0m[0m | time: 51.343s
[2K
| Adam | epoch: 001 | loss: 0.69357 - acc: 0.4943 -- iter: 1536/2005
[A[ATraining Step: 49  | total loss: [1m[32m0.69302[0m[0m | time: 52.743s
[2K
| Adam | epoch: 001 | loss: 0.69302 - acc: 0.5150 -- iter: 1568/2005
[A[ATraining Step: 50  | total loss: [1m[32m0.69302[0m[0m | time: 54.032s
[2K
| Adam | epoch: 001 | loss: 0.69302 - acc: 0.5127 -- iter: 1600/2005
[A[ATraining Step: 51  | total loss: [1m[32m0.69279[0m[0m | time: 54.940s
[2K
| Adam | epoch: 001 | loss: 0.69279 - acc: 0.5203 -- iter: 1632/2005
[A[ATraining Step: 52  | total loss: [1m[32m0.69297[0m[0m | time: 55.901s
[2K
| Adam | epoch: 001 | loss: 0.69297 - acc: 0.5125 -- iter: 1664/2005
[A[ATraining Step: 53  | total loss: [1m[32m0.69321[0m[0m | time: 56.885s
[2K
| Adam | epoch: 001 | loss: 0.69321 - acc: 0.5015 -- iter: 1696/2005
[A[ATraining Step: 54  | total loss: [1m[32m0.69297[0m[0m | time: 57.853s
[2K
| Adam | epoch: 001 | loss: 0.69297 - acc: 0.5103 -- iter: 1728/2005
[A[ATraining Step: 55  | total loss: [1m[32m0.69333[0m[0m | time: 58.937s
[2K
| Adam | epoch: 001 | loss: 0.69333 - acc: 0.4955 -- iter: 1760/2005
[A[ATraining Step: 56  | total loss: [1m[32m0.69341[0m[0m | time: 60.079s
[2K
| Adam | epoch: 001 | loss: 0.69341 - acc: 0.4917 -- iter: 1792/2005
[A[ATraining Step: 57  | total loss: [1m[32m0.69345[0m[0m | time: 61.005s
[2K
| Adam | epoch: 001 | loss: 0.69345 - acc: 0.4885 -- iter: 1824/2005
[A[ATraining Step: 58  | total loss: [1m[32m0.69310[0m[0m | time: 62.242s
[2K
| Adam | epoch: 001 | loss: 0.69310 - acc: 0.5071 -- iter: 1856/2005
[A[ATraining Step: 59  | total loss: [1m[32m0.69286[0m[0m | time: 63.539s
[2K
| Adam | epoch: 001 | loss: 0.69286 - acc: 0.5188 -- iter: 1888/2005
[A[ATraining Step: 60  | total loss: [1m[32m0.69310[0m[0m | time: 64.980s
[2K
| Adam | epoch: 001 | loss: 0.69310 - acc: 0.5039 -- iter: 1920/2005
[A[ATraining Step: 61  | total loss: [1m[32m0.69323[0m[0m | time: 65.873s
[2K
| Adam | epoch: 001 | loss: 0.69323 - acc: 0.4993 -- iter: 1952/2005
[A[ATraining Step: 62  | total loss: [1m[32m0.69338[0m[0m | time: 66.791s
[2K
| Adam | epoch: 001 | loss: 0.69338 - acc: 0.4913 -- iter: 1984/2005
[A[ATraining Step: 63  | total loss: [1m[32m0.69364[0m[0m | time: 70.745s
[2K
| Adam | epoch: 001 | loss: 0.69364 - acc: 0.4766 | val_loss: 0.69295 - val_acc: 0.4976 -- iter: 2005/2005
--
Training Step: 64  | total loss: [1m[32m0.69390[0m[0m | time: 0.613s
[2K
| Adam | epoch: 002 | loss: 0.69390 - acc: 0.4587 -- iter: 0032/2005
[A[ATraining Step: 65  | total loss: [1m[32m0.69401[0m[0m | time: 1.798s
[2K
| Adam | epoch: 002 | loss: 0.69401 - acc: 0.4432 -- iter: 0064/2005
[A[ATraining Step: 66  | total loss: [1m[32m0.69380[0m[0m | time: 3.156s
[2K
| Adam | epoch: 002 | loss: 0.69380 - acc: 0.4767 -- iter: 0096/2005
[A[ATraining Step: 67  | total loss: [1m[32m0.69377[0m[0m | time: 4.471s
[2K
| Adam | epoch: 002 | loss: 0.69377 - acc: 0.4608 -- iter: 0128/2005
[A[ATraining Step: 68  | total loss: [1m[32m0.69371[0m[0m | time: 5.410s
[2K
| Adam | epoch: 002 | loss: 0.69371 - acc: 0.4654 -- iter: 0160/2005
[A[ATraining Step: 69  | total loss: [1m[32m0.69361[0m[0m | time: 6.326s
[2K
| Adam | epoch: 002 | loss: 0.69361 - acc: 0.4731 -- iter: 0192/2005
[A[ATraining Step: 70  | total loss: [1m[32m0.69371[0m[0m | time: 7.247s
[2K
| Adam | epoch: 002 | loss: 0.69371 - acc: 0.4618 -- iter: 0224/2005
[A[ATraining Step: 71  | total loss: [1m[32m0.69371[0m[0m | time: 8.278s
[2K
| Adam | epoch: 002 | loss: 0.69371 - acc: 0.4590 -- iter: 0256/2005
[A[ATraining Step: 72  | total loss: [1m[32m0.69381[0m[0m | time: 9.257s
[2K
| Adam | epoch: 002 | loss: 0.69381 - acc: 0.4496 -- iter: 0288/2005
[A[ATraining Step: 73  | total loss: [1m[32m0.69379[0m[0m | time: 10.389s
[2K
| Adam | epoch: 002 | loss: 0.69379 - acc: 0.4482 -- iter: 0320/2005
[A[ATraining Step: 74  | total loss: [1m[32m0.69367[0m[0m | time: 11.353s
[2K
| Adam | epoch: 002 | loss: 0.69367 - acc: 0.4573 -- iter: 0352/2005
[A[ATraining Step: 75  | total loss: [1m[32m0.69370[0m[0m | time: 12.402s
[2K
| Adam | epoch: 002 | loss: 0.69370 - acc: 0.4484 -- iter: 0384/2005
[A[ATraining Step: 76  | total loss: [1m[32m0.69363[0m[0m | time: 13.702s
[2K
| Adam | epoch: 002 | loss: 0.69363 - acc: 0.4573 -- iter: 0416/2005
[A[ATraining Step: 77  | total loss: [1m[32m0.69350[0m[0m | time: 15.125s
[2K
| Adam | epoch: 002 | loss: 0.69350 - acc: 0.4850 -- iter: 0448/2005
[A[ATraining Step: 78  | total loss: [1m[32m0.69335[0m[0m | time: 16.200s
[2K
| Adam | epoch: 002 | loss: 0.69335 - acc: 0.4898 -- iter: 0480/2005
[A[ATraining Step: 79  | total loss: [1m[32m0.69328[0m[0m | time: 17.098s
[2K
| Adam | epoch: 002 | loss: 0.69328 - acc: 0.4909 -- iter: 0512/2005
[A[ATraining Step: 80  | total loss: [1m[32m0.69345[0m[0m | time: 18.118s
[2K
| Adam | epoch: 002 | loss: 0.69345 - acc: 0.4790 -- iter: 0544/2005
[A[ATraining Step: 81  | total loss: [1m[32m0.69324[0m[0m | time: 19.132s
[2K
| Adam | epoch: 002 | loss: 0.69324 - acc: 0.4811 -- iter: 0576/2005
[A[ATraining Step: 82  | total loss: [1m[32m0.69318[0m[0m | time: 20.171s
[2K
| Adam | epoch: 002 | loss: 0.69318 - acc: 0.4830 -- iter: 0608/2005
[A[ATraining Step: 83  | total loss: [1m[32m0.69304[0m[0m | time: 21.394s
[2K
| Adam | epoch: 002 | loss: 0.69304 - acc: 0.4941 -- iter: 0640/2005
[A[ATraining Step: 84  | total loss: [1m[32m0.69293[0m[0m | time: 22.507s
[2K
| Adam | epoch: 002 | loss: 0.69293 - acc: 0.5009 -- iter: 0672/2005
[A[ATraining Step: 85  | total loss: [1m[32m0.69281[0m[0m | time: 23.413s
[2K
| Adam | epoch: 002 | loss: 0.69281 - acc: 0.5040 -- iter: 0704/2005
[A[ATraining Step: 86  | total loss: [1m[32m0.69307[0m[0m | time: 24.908s
[2K
| Adam | epoch: 002 | loss: 0.69307 - acc: 0.4973 -- iter: 0736/2005
[A[ATraining Step: 87  | total loss: [1m[32m0.69308[0m[0m | time: 26.293s
[2K
| Adam | epoch: 002 | loss: 0.69308 - acc: 0.4913 -- iter: 0768/2005
[A[ATraining Step: 88  | total loss: [1m[32m0.69238[0m[0m | time: 27.437s
[2K
| Adam | epoch: 002 | loss: 0.69238 - acc: 0.4953 -- iter: 0800/2005
[A[ATraining Step: 89  | total loss: [1m[32m0.69192[0m[0m | time: 28.289s
[2K
| Adam | epoch: 002 | loss: 0.69192 - acc: 0.5208 -- iter: 0832/2005
[A[ATraining Step: 90  | total loss: [1m[32m0.69125[0m[0m | time: 29.255s
[2K
| Adam | epoch: 002 | loss: 0.69125 - acc: 0.5406 -- iter: 0864/2005
[A[ATraining Step: 91  | total loss: [1m[32m0.69078[0m[0m | time: 30.201s
[2K
| Adam | epoch: 002 | loss: 0.69078 - acc: 0.5490 -- iter: 0896/2005
[A[ATraining Step: 92  | total loss: [1m[32m0.69051[0m[0m | time: 31.185s
[2K
| Adam | epoch: 002 | loss: 0.69051 - acc: 0.5473 -- iter: 0928/2005
[A[ATraining Step: 93  | total loss: [1m[32m0.68859[0m[0m | time: 32.230s
[2K
| Adam | epoch: 002 | loss: 0.68859 - acc: 0.5488 -- iter: 0960/2005
[A[ATraining Step: 94  | total loss: [1m[32m0.68818[0m[0m | time: 33.348s
[2K
| Adam | epoch: 002 | loss: 0.68818 - acc: 0.5439 -- iter: 0992/2005
[A[ATraining Step: 95  | total loss: [1m[32m0.68534[0m[0m | time: 34.357s
[2K
| Adam | epoch: 002 | loss: 0.68534 - acc: 0.5489 -- iter: 1024/2005
[A[ATraining Step: 96  | total loss: [1m[32m0.68165[0m[0m | time: 35.489s
[2K
| Adam | epoch: 002 | loss: 0.68165 - acc: 0.5627 -- iter: 1056/2005
[A[ATraining Step: 97  | total loss: [1m[32m0.67759[0m[0m | time: 36.866s
[2K
| Adam | epoch: 002 | loss: 0.67759 - acc: 0.5752 -- iter: 1088/2005
[A[ATraining Step: 98  | total loss: [1m[32m0.68361[0m[0m | time: 38.293s
[2K
| Adam | epoch: 002 | loss: 0.68361 - acc: 0.5552 -- iter: 1120/2005
[A[ATraining Step: 99  | total loss: [1m[32m0.68953[0m[0m | time: 39.219s
[2K
| Adam | epoch: 002 | loss: 0.68953 - acc: 0.5403 -- iter: 1152/2005
[A[ATraining Step: 100  | total loss: [1m[32m0.68246[0m[0m | time: 40.133s
[2K
| Adam | epoch: 002 | loss: 0.68246 - acc: 0.5550 -- iter: 1184/2005
[A[ATraining Step: 101  | total loss: [1m[32m0.68768[0m[0m | time: 41.127s
[2K
| Adam | epoch: 002 | loss: 0.68768 - acc: 0.5495 -- iter: 1216/2005
[A[ATraining Step: 102  | total loss: [1m[32m0.68338[0m[0m | time: 42.121s
[2K
| Adam | epoch: 002 | loss: 0.68338 - acc: 0.5571 -- iter: 1248/2005
[A[ATraining Step: 103  | total loss: [1m[32m0.68652[0m[0m | time: 43.134s
[2K
| Adam | epoch: 002 | loss: 0.68652 - acc: 0.5545 -- iter: 1280/2005
[A[ATraining Step: 104  | total loss: [1m[32m0.68364[0m[0m | time: 44.280s
[2K
| Adam | epoch: 002 | loss: 0.68364 - acc: 0.5584 -- iter: 1312/2005
[A[ATraining Step: 105  | total loss: [1m[32m0.67935[0m[0m | time: 45.288s
[2K
| Adam | epoch: 002 | loss: 0.67935 - acc: 0.5713 -- iter: 1344/2005
[A[ATraining Step: 106  | total loss: [1m[32m0.67627[0m[0m | time: 46.238s
[2K
| Adam | epoch: 002 | loss: 0.67627 - acc: 0.5798 -- iter: 1376/2005
[A[ATraining Step: 107  | total loss: [1m[32m0.67489[0m[0m | time: 47.540s
[2K
| Adam | epoch: 002 | loss: 0.67489 - acc: 0.5875 -- iter: 1408/2005
[A[ATraining Step: 108  | total loss: [1m[32m0.67073[0m[0m | time: 48.918s
[2K
| Adam | epoch: 002 | loss: 0.67073 - acc: 0.5943 -- iter: 1440/2005
[A[ATraining Step: 109  | total loss: [1m[32m0.66925[0m[0m | time: 50.084s
[2K
| Adam | epoch: 002 | loss: 0.66925 - acc: 0.6005 -- iter: 1472/2005
[A[ATraining Step: 110  | total loss: [1m[32m0.66410[0m[0m | time: 50.962s
[2K
| Adam | epoch: 002 | loss: 0.66410 - acc: 0.6124 -- iter: 1504/2005
[A[ATraining Step: 111  | total loss: [1m[32m0.65932[0m[0m | time: 51.890s
[2K
| Adam | epoch: 002 | loss: 0.65932 - acc: 0.6167 -- iter: 1536/2005
[A[ATraining Step: 112  | total loss: [1m[32m0.65275[0m[0m | time: 52.872s
[2K
| Adam | epoch: 002 | loss: 0.65275 - acc: 0.6269 -- iter: 1568/2005
[A[ATraining Step: 113  | total loss: [1m[32m0.65965[0m[0m | time: 53.938s
[2K
| Adam | epoch: 002 | loss: 0.65965 - acc: 0.6267 -- iter: 1600/2005
[A[ATraining Step: 114  | total loss: [1m[32m0.65350[0m[0m | time: 55.092s
[2K
| Adam | epoch: 002 | loss: 0.65350 - acc: 0.6328 -- iter: 1632/2005
[A[ATraining Step: 115  | total loss: [1m[32m0.64995[0m[0m | time: 56.263s
[2K
| Adam | epoch: 002 | loss: 0.64995 - acc: 0.6289 -- iter: 1664/2005
[A[ATraining Step: 116  | total loss: [1m[32m0.64810[0m[0m | time: 57.253s
[2K
| Adam | epoch: 002 | loss: 0.64810 - acc: 0.6316 -- iter: 1696/2005
[A[ATraining Step: 117  | total loss: [1m[32m0.65043[0m[0m | time: 58.614s
[2K
| Adam | epoch: 002 | loss: 0.65043 - acc: 0.6310 -- iter: 1728/2005
[A[ATraining Step: 118  | total loss: [1m[32m0.65514[0m[0m | time: 59.991s
[2K
| Adam | epoch: 002 | loss: 0.65514 - acc: 0.6273 -- iter: 1760/2005
[A[ATraining Step: 119  | total loss: [1m[32m0.64981[0m[0m | time: 61.329s
[2K
| Adam | epoch: 002 | loss: 0.64981 - acc: 0.6333 -- iter: 1792/2005
[A[ATraining Step: 120  | total loss: [1m[32m0.65187[0m[0m | time: 62.292s
[2K
| Adam | epoch: 002 | loss: 0.65187 - acc: 0.6231 -- iter: 1824/2005
[A[ATraining Step: 121  | total loss: [1m[32m0.64692[0m[0m | time: 63.255s
[2K
| Adam | epoch: 002 | loss: 0.64692 - acc: 0.6326 -- iter: 1856/2005
[A[ATraining Step: 122  | total loss: [1m[32m0.65021[0m[0m | time: 64.189s
[2K
| Adam | epoch: 002 | loss: 0.65021 - acc: 0.6288 -- iter: 1888/2005
[A[ATraining Step: 123  | total loss: [1m[32m0.64909[0m[0m | time: 65.094s
[2K
| Adam | epoch: 002 | loss: 0.64909 - acc: 0.6315 -- iter: 1920/2005
[A[ATraining Step: 124  | total loss: [1m[32m0.64640[0m[0m | time: 66.061s
[2K
| Adam | epoch: 002 | loss: 0.64640 - acc: 0.6340 -- iter: 1952/2005
[A[ATraining Step: 125  | total loss: [1m[32m0.64705[0m[0m | time: 67.138s
[2K
| Adam | epoch: 002 | loss: 0.64705 - acc: 0.6300 -- iter: 1984/2005
[A[ATraining Step: 126  | total loss: [1m[32m0.64009[0m[0m | time: 71.650s
[2K
| Adam | epoch: 002 | loss: 0.64009 - acc: 0.6420 | val_loss: 0.60830 - val_acc: 0.7018 -- iter: 2005/2005
--
Training Step: 127  | total loss: [1m[32m0.64111[0m[0m | time: 0.671s
[2K
| Adam | epoch: 003 | loss: 0.64111 - acc: 0.6340 -- iter: 0032/2005
[A[ATraining Step: 128  | total loss: [1m[32m0.64050[0m[0m | time: 1.355s
[2K
| Adam | epoch: 003 | loss: 0.64050 - acc: 0.6420 -- iter: 0064/2005
[A[ATraining Step: 129  | total loss: [1m[32m0.63834[0m[0m | time: 2.326s
[2K
| Adam | epoch: 003 | loss: 0.63834 - acc: 0.6397 -- iter: 0096/2005
[A[ATraining Step: 130  | total loss: [1m[32m0.64523[0m[0m | time: 3.417s
[2K
| Adam | epoch: 003 | loss: 0.64523 - acc: 0.6320 -- iter: 0128/2005
[A[ATraining Step: 131  | total loss: [1m[32m0.63820[0m[0m | time: 4.536s
[2K
| Adam | epoch: 003 | loss: 0.63820 - acc: 0.6376 -- iter: 0160/2005
[A[ATraining Step: 132  | total loss: [1m[32m0.63594[0m[0m | time: 5.441s
[2K
| Adam | epoch: 003 | loss: 0.63594 - acc: 0.6426 -- iter: 0192/2005
[A[ATraining Step: 133  | total loss: [1m[32m0.64360[0m[0m | time: 6.683s
[2K
| Adam | epoch: 003 | loss: 0.64360 - acc: 0.6314 -- iter: 0224/2005
[A[ATraining Step: 134  | total loss: [1m[32m0.64632[0m[0m | time: 8.007s
[2K
| Adam | epoch: 003 | loss: 0.64632 - acc: 0.6245 -- iter: 0256/2005
[A[ATraining Step: 135  | total loss: [1m[32m0.64728[0m[0m | time: 9.349s
[2K
| Adam | epoch: 003 | loss: 0.64728 - acc: 0.6152 -- iter: 0288/2005
[A[ATraining Step: 136  | total loss: [1m[32m0.65183[0m[0m | time: 10.280s
[2K
| Adam | epoch: 003 | loss: 0.65183 - acc: 0.6131 -- iter: 0320/2005
[A[ATraining Step: 137  | total loss: [1m[32m0.64942[0m[0m | time: 11.177s
[2K
| Adam | epoch: 003 | loss: 0.64942 - acc: 0.6205 -- iter: 0352/2005
[A[ATraining Step: 138  | total loss: [1m[32m0.64140[0m[0m | time: 12.084s
[2K
| Adam | epoch: 003 | loss: 0.64140 - acc: 0.6366 -- iter: 0384/2005
[A[ATraining Step: 139  | total loss: [1m[32m0.63375[0m[0m | time: 13.042s
[2K
| Adam | epoch: 003 | loss: 0.63375 - acc: 0.6448 -- iter: 0416/2005
[A[ATraining Step: 140  | total loss: [1m[32m0.63724[0m[0m | time: 14.073s
[2K
| Adam | epoch: 003 | loss: 0.63724 - acc: 0.6334 -- iter: 0448/2005
[A[ATraining Step: 141  | total loss: [1m[32m0.63610[0m[0m | time: 15.176s
[2K
| Adam | epoch: 003 | loss: 0.63610 - acc: 0.6357 -- iter: 0480/2005
[A[ATraining Step: 142  | total loss: [1m[32m0.63688[0m[0m | time: 16.057s
[2K
| Adam | epoch: 003 | loss: 0.63688 - acc: 0.6378 -- iter: 0512/2005
[A[ATraining Step: 143  | total loss: [1m[32m0.63062[0m[0m | time: 17.099s
[2K
| Adam | epoch: 003 | loss: 0.63062 - acc: 0.6427 -- iter: 0544/2005
[A[ATraining Step: 144  | total loss: [1m[32m0.62774[0m[0m | time: 18.073s
[2K
| Adam | epoch: 003 | loss: 0.62774 - acc: 0.6503 -- iter: 0576/2005
[A[ATraining Step: 145  | total loss: [1m[32m0.62770[0m[0m | time: 19.128s
[2K
| Adam | epoch: 003 | loss: 0.62770 - acc: 0.6509 -- iter: 0608/2005
[A[ATraining Step: 146  | total loss: [1m[32m0.62908[0m[0m | time: 20.147s
[2K
| Adam | epoch: 003 | loss: 0.62908 - acc: 0.6483 -- iter: 0640/2005
[A[ATraining Step: 147  | total loss: [1m[32m0.62232[0m[0m | time: 21.197s
[2K
| Adam | epoch: 003 | loss: 0.62232 - acc: 0.6585 -- iter: 0672/2005
[A[ATraining Step: 148  | total loss: [1m[32m0.62149[0m[0m | time: 22.226s
[2K
| Adam | epoch: 003 | loss: 0.62149 - acc: 0.6552 -- iter: 0704/2005
[A[ATraining Step: 149  | total loss: [1m[32m0.60139[0m[0m | time: 23.271s
[2K
| Adam | epoch: 003 | loss: 0.60139 - acc: 0.6709 -- iter: 0736/2005
[A[ATraining Step: 150  | total loss: [1m[32m0.59986[0m[0m | time: 24.279s
[2K
| Adam | epoch: 003 | loss: 0.59986 - acc: 0.6694 -- iter: 0768/2005
[A[ATraining Step: 151  | total loss: [1m[32m0.62226[0m[0m | time: 25.345s
[2K
| Adam | epoch: 003 | loss: 0.62226 - acc: 0.6556 -- iter: 0800/2005
[A[ATraining Step: 152  | total loss: [1m[32m0.62663[0m[0m | time: 26.312s
[2K
| Adam | epoch: 003 | loss: 0.62663 - acc: 0.6432 -- iter: 0832/2005
[A[ATraining Step: 153  | total loss: [1m[32m0.62295[0m[0m | time: 27.334s
[2K
| Adam | epoch: 003 | loss: 0.62295 - acc: 0.6414 -- iter: 0864/2005
[A[ATraining Step: 154  | total loss: [1m[32m0.61577[0m[0m | time: 28.351s
[2K
| Adam | epoch: 003 | loss: 0.61577 - acc: 0.6522 -- iter: 0896/2005
[A[ATraining Step: 155  | total loss: [1m[32m0.61984[0m[0m | time: 29.433s
[2K
| Adam | epoch: 003 | loss: 0.61984 - acc: 0.6464 -- iter: 0928/2005
[A[ATraining Step: 156  | total loss: [1m[32m0.62124[0m[0m | time: 30.415s
[2K
| Adam | epoch: 003 | loss: 0.62124 - acc: 0.6474 -- iter: 0960/2005
[A[ATraining Step: 157  | total loss: [1m[32m0.62175[0m[0m | time: 31.454s
[2K
| Adam | epoch: 003 | loss: 0.62175 - acc: 0.6483 -- iter: 0992/2005
[A[ATraining Step: 158  | total loss: [1m[32m0.62135[0m[0m | time: 32.494s
[2K
| Adam | epoch: 003 | loss: 0.62135 - acc: 0.6584 -- iter: 1024/2005
[A[ATraining Step: 159  | total loss: [1m[32m0.62469[0m[0m | time: 33.475s
[2K
| Adam | epoch: 003 | loss: 0.62469 - acc: 0.6551 -- iter: 1056/2005
[A[ATraining Step: 160  | total loss: [1m[32m0.61855[0m[0m | time: 34.419s
[2K
| Adam | epoch: 003 | loss: 0.61855 - acc: 0.6646 -- iter: 1088/2005
[A[ATraining Step: 161  | total loss: [1m[32m0.62085[0m[0m | time: 35.403s
[2K
| Adam | epoch: 003 | loss: 0.62085 - acc: 0.6606 -- iter: 1120/2005
[A[ATraining Step: 162  | total loss: [1m[32m0.61635[0m[0m | time: 36.464s
[2K
| Adam | epoch: 003 | loss: 0.61635 - acc: 0.6633 -- iter: 1152/2005
[A[ATraining Step: 163  | total loss: [1m[32m0.62102[0m[0m | time: 37.491s
[2K
| Adam | epoch: 003 | loss: 0.62102 - acc: 0.6657 -- iter: 1184/2005
[A[ATraining Step: 164  | total loss: [1m[32m0.62287[0m[0m | time: 38.586s
[2K
| Adam | epoch: 003 | loss: 0.62287 - acc: 0.6648 -- iter: 1216/2005
[A[ATraining Step: 165  | total loss: [1m[32m0.61689[0m[0m | time: 39.614s
[2K
| Adam | epoch: 003 | loss: 0.61689 - acc: 0.6733 -- iter: 1248/2005
[A[ATraining Step: 166  | total loss: [1m[32m0.60918[0m[0m | time: 40.662s
[2K
| Adam | epoch: 003 | loss: 0.60918 - acc: 0.6810 -- iter: 1280/2005
[A[ATraining Step: 167  | total loss: [1m[32m0.60935[0m[0m | time: 41.713s
[2K
| Adam | epoch: 003 | loss: 0.60935 - acc: 0.6816 -- iter: 1312/2005
[A[ATraining Step: 168  | total loss: [1m[32m0.60469[0m[0m | time: 42.678s
[2K
| Adam | epoch: 003 | loss: 0.60469 - acc: 0.6885 -- iter: 1344/2005
[A[ATraining Step: 169  | total loss: [1m[32m0.60622[0m[0m | time: 43.291s
[2K
| Adam | epoch: 003 | loss: 0.60622 - acc: 0.6821 -- iter: 1376/2005
[A[ATraining Step: 170  | total loss: [1m[32m0.60879[0m[0m | time: 43.922s
[2K
| Adam | epoch: 003 | loss: 0.60879 - acc: 0.6795 -- iter: 1408/2005
[A[ATraining Step: 171  | total loss: [1m[32m0.61073[0m[0m | time: 44.531s
[2K
| Adam | epoch: 003 | loss: 0.61073 - acc: 0.6803 -- iter: 1440/2005
[A[ATraining Step: 172  | total loss: [1m[32m0.60646[0m[0m | time: 45.140s
[2K
| Adam | epoch: 003 | loss: 0.60646 - acc: 0.6779 -- iter: 1472/2005
[A[ATraining Step: 173  | total loss: [1m[32m0.60900[0m[0m | time: 45.738s
[2K
| Adam | epoch: 003 | loss: 0.60900 - acc: 0.6695 -- iter: 1504/2005
[A[ATraining Step: 174  | total loss: [1m[32m0.60246[0m[0m | time: 46.369s
[2K
| Adam | epoch: 003 | loss: 0.60246 - acc: 0.6713 -- iter: 1536/2005
[A[ATraining Step: 175  | total loss: [1m[32m0.60417[0m[0m | time: 46.975s
[2K
| Adam | epoch: 003 | loss: 0.60417 - acc: 0.6698 -- iter: 1568/2005
[A[ATraining Step: 176  | total loss: [1m[32m0.59699[0m[0m | time: 47.582s
[2K
| Adam | epoch: 003 | loss: 0.59699 - acc: 0.6778 -- iter: 1600/2005
[A[ATraining Step: 177  | total loss: [1m[32m0.59909[0m[0m | time: 48.183s
[2K
| Adam | epoch: 003 | loss: 0.59909 - acc: 0.6694 -- iter: 1632/2005
[A[ATraining Step: 178  | total loss: [1m[32m0.60504[0m[0m | time: 48.798s
[2K
| Adam | epoch: 003 | loss: 0.60504 - acc: 0.6493 -- iter: 1664/2005
[A[ATraining Step: 179  | total loss: [1m[32m0.60237[0m[0m | time: 49.401s
[2K
| Adam | epoch: 003 | loss: 0.60237 - acc: 0.6532 -- iter: 1696/2005
[A[ATraining Step: 180  | total loss: [1m[32m0.59779[0m[0m | time: 50.011s
[2K
| Adam | epoch: 003 | loss: 0.59779 - acc: 0.6628 -- iter: 1728/2005
[A[ATraining Step: 181  | total loss: [1m[32m0.59529[0m[0m | time: 50.625s
[2K
| Adam | epoch: 003 | loss: 0.59529 - acc: 0.6653 -- iter: 1760/2005
[A[ATraining Step: 182  | total loss: [1m[32m0.58692[0m[0m | time: 51.227s
[2K
| Adam | epoch: 003 | loss: 0.58692 - acc: 0.6738 -- iter: 1792/2005
[A[ATraining Step: 183  | total loss: [1m[32m0.57827[0m[0m | time: 51.845s
[2K
| Adam | epoch: 003 | loss: 0.57827 - acc: 0.6783 -- iter: 1824/2005
[A[ATraining Step: 184  | total loss: [1m[32m0.58839[0m[0m | time: 52.474s
[2K
| Adam | epoch: 003 | loss: 0.58839 - acc: 0.6604 -- iter: 1856/2005
[A[ATraining Step: 185  | total loss: [1m[32m0.59061[0m[0m | time: 53.086s
[2K
| Adam | epoch: 003 | loss: 0.59061 - acc: 0.6538 -- iter: 1888/2005
[A[ATraining Step: 186  | total loss: [1m[32m0.57841[0m[0m | time: 53.687s
[2K
| Adam | epoch: 003 | loss: 0.57841 - acc: 0.6634 -- iter: 1920/2005
[A[ATraining Step: 187  | total loss: [1m[32m0.57765[0m[0m | time: 54.292s
[2K
| Adam | epoch: 003 | loss: 0.57765 - acc: 0.6658 -- iter: 1952/2005
[A[ATraining Step: 188  | total loss: [1m[32m0.58699[0m[0m | time: 54.922s
[2K
| Adam | epoch: 003 | loss: 0.58699 - acc: 0.6586 -- iter: 1984/2005
[A[ATraining Step: 189  | total loss: [1m[32m0.57647[0m[0m | time: 57.514s
[2K
| Adam | epoch: 003 | loss: 0.57647 - acc: 0.6740 | val_loss: 0.51465 - val_acc: 0.7496 -- iter: 2005/2005
--
Training Step: 190  | total loss: [1m[32m0.58005[0m[0m | time: 0.599s
[2K
| Adam | epoch: 004 | loss: 0.58005 - acc: 0.6816 -- iter: 0032/2005
[A[ATraining Step: 191  | total loss: [1m[32m0.57381[0m[0m | time: 1.020s
[2K
| Adam | epoch: 004 | loss: 0.57381 - acc: 0.6822 -- iter: 0064/2005
[A[ATraining Step: 192  | total loss: [1m[32m0.58006[0m[0m | time: 1.578s
[2K
| Adam | epoch: 004 | loss: 0.58006 - acc: 0.6806 -- iter: 0096/2005
[A[ATraining Step: 193  | total loss: [1m[32m0.58337[0m[0m | time: 3.004s
[2K
| Adam | epoch: 004 | loss: 0.58337 - acc: 0.6792 -- iter: 0128/2005
[A[ATraining Step: 194  | total loss: [1m[32m0.57312[0m[0m | time: 4.408s
[2K
| Adam | epoch: 004 | loss: 0.57312 - acc: 0.6863 -- iter: 0160/2005
[A[ATraining Step: 195  | total loss: [1m[32m0.56327[0m[0m | time: 5.511s
[2K
| Adam | epoch: 004 | loss: 0.56327 - acc: 0.7021 -- iter: 0192/2005
[A[ATraining Step: 196  | total loss: [1m[32m0.56105[0m[0m | time: 6.348s
[2K
| Adam | epoch: 004 | loss: 0.56105 - acc: 0.7037 -- iter: 0224/2005
[A[ATraining Step: 197  | total loss: [1m[32m0.55628[0m[0m | time: 7.264s
[2K
| Adam | epoch: 004 | loss: 0.55628 - acc: 0.7084 -- iter: 0256/2005
[A[ATraining Step: 198  | total loss: [1m[32m0.57071[0m[0m | time: 8.195s
[2K
| Adam | epoch: 004 | loss: 0.57071 - acc: 0.6969 -- iter: 0288/2005
[A[ATraining Step: 199  | total loss: [1m[32m0.57260[0m[0m | time: 9.134s
[2K
| Adam | epoch: 004 | loss: 0.57260 - acc: 0.6960 -- iter: 0320/2005
[A[ATraining Step: 200  | total loss: [1m[32m0.56656[0m[0m | time: 13.109s
[2K
| Adam | epoch: 004 | loss: 0.56656 - acc: 0.7045 | val_loss: 0.51854 - val_acc: 0.7560 -- iter: 0352/2005
--
Training Step: 201  | total loss: [1m[32m0.54885[0m[0m | time: 14.459s
[2K
| Adam | epoch: 004 | loss: 0.54885 - acc: 0.7184 -- iter: 0384/2005
[A[ATraining Step: 202  | total loss: [1m[32m0.55345[0m[0m | time: 15.299s
[2K
| Adam | epoch: 004 | loss: 0.55345 - acc: 0.7153 -- iter: 0416/2005
[A[ATraining Step: 203  | total loss: [1m[32m0.57094[0m[0m | time: 16.183s
[2K
| Adam | epoch: 004 | loss: 0.57094 - acc: 0.7063 -- iter: 0448/2005
[A[ATraining Step: 204  | total loss: [1m[32m0.56399[0m[0m | time: 17.103s
[2K
| Adam | epoch: 004 | loss: 0.56399 - acc: 0.7107 -- iter: 0480/2005
[A[ATraining Step: 205  | total loss: [1m[32m0.55471[0m[0m | time: 18.045s
[2K
| Adam | epoch: 004 | loss: 0.55471 - acc: 0.7177 -- iter: 0512/2005
[A[ATraining Step: 206  | total loss: [1m[32m0.54296[0m[0m | time: 19.008s
[2K
| Adam | epoch: 004 | loss: 0.54296 - acc: 0.7303 -- iter: 0544/2005
[A[ATraining Step: 207  | total loss: [1m[32m0.52669[0m[0m | time: 20.050s
[2K
| Adam | epoch: 004 | loss: 0.52669 - acc: 0.7479 -- iter: 0576/2005
[A[ATraining Step: 208  | total loss: [1m[32m0.53729[0m[0m | time: 21.031s
[2K
| Adam | epoch: 004 | loss: 0.53729 - acc: 0.7325 -- iter: 0608/2005
[A[ATraining Step: 209  | total loss: [1m[32m0.52892[0m[0m | time: 22.034s
[2K
| Adam | epoch: 004 | loss: 0.52892 - acc: 0.7405 -- iter: 0640/2005
[A[ATraining Step: 210  | total loss: [1m[32m0.52417[0m[0m | time: 23.449s
[2K
| Adam | epoch: 004 | loss: 0.52417 - acc: 0.7446 -- iter: 0672/2005
[A[ATraining Step: 211  | total loss: [1m[32m0.54280[0m[0m | time: 24.841s
[2K
| Adam | epoch: 004 | loss: 0.54280 - acc: 0.7232 -- iter: 0704/2005
[A[ATraining Step: 212  | total loss: [1m[32m0.53528[0m[0m | time: 25.764s
[2K
| Adam | epoch: 004 | loss: 0.53528 - acc: 0.7290 -- iter: 0736/2005
[A[ATraining Step: 213  | total loss: [1m[32m0.54842[0m[0m | time: 26.677s
[2K
| Adam | epoch: 004 | loss: 0.54842 - acc: 0.7218 -- iter: 0768/2005
[A[ATraining Step: 214  | total loss: [1m[32m0.55605[0m[0m | time: 27.641s
[2K
| Adam | epoch: 004 | loss: 0.55605 - acc: 0.7058 -- iter: 0800/2005
[A[ATraining Step: 215  | total loss: [1m[32m0.55030[0m[0m | time: 28.577s
[2K
| Adam | epoch: 004 | loss: 0.55030 - acc: 0.7165 -- iter: 0832/2005
[A[ATraining Step: 216  | total loss: [1m[32m0.54057[0m[0m | time: 29.466s
[2K
| Adam | epoch: 004 | loss: 0.54057 - acc: 0.7230 -- iter: 0864/2005
[A[ATraining Step: 217  | total loss: [1m[32m0.52491[0m[0m | time: 30.474s
[2K
| Adam | epoch: 004 | loss: 0.52491 - acc: 0.7444 -- iter: 0896/2005
[A[ATraining Step: 218  | total loss: [1m[32m0.51554[0m[0m | time: 31.421s
[2K
| Adam | epoch: 004 | loss: 0.51554 - acc: 0.7512 -- iter: 0928/2005
[A[ATraining Step: 219  | total loss: [1m[32m0.51427[0m[0m | time: 32.341s
[2K
| Adam | epoch: 004 | loss: 0.51427 - acc: 0.7511 -- iter: 0960/2005
[A[ATraining Step: 220  | total loss: [1m[32m0.52411[0m[0m | time: 33.692s
[2K
| Adam | epoch: 004 | loss: 0.52411 - acc: 0.7448 -- iter: 0992/2005
[A[ATraining Step: 221  | total loss: [1m[32m0.51716[0m[0m | time: 35.067s
[2K
| Adam | epoch: 004 | loss: 0.51716 - acc: 0.7515 -- iter: 1024/2005
[A[ATraining Step: 222  | total loss: [1m[32m0.52058[0m[0m | time: 36.164s
[2K
| Adam | epoch: 004 | loss: 0.52058 - acc: 0.7420 -- iter: 1056/2005
[A[ATraining Step: 223  | total loss: [1m[32m0.51525[0m[0m | time: 37.070s
[2K
| Adam | epoch: 004 | loss: 0.51525 - acc: 0.7459 -- iter: 1088/2005
[A[ATraining Step: 224  | total loss: [1m[32m0.51222[0m[0m | time: 37.977s
[2K
| Adam | epoch: 004 | loss: 0.51222 - acc: 0.7495 -- iter: 1120/2005
[A[ATraining Step: 225  | total loss: [1m[32m0.52745[0m[0m | time: 38.877s
[2K
| Adam | epoch: 004 | loss: 0.52745 - acc: 0.7339 -- iter: 1152/2005
[A[ATraining Step: 226  | total loss: [1m[32m0.51312[0m[0m | time: 39.772s
[2K
| Adam | epoch: 004 | loss: 0.51312 - acc: 0.7417 -- iter: 1184/2005
[A[ATraining Step: 227  | total loss: [1m[32m0.49921[0m[0m | time: 40.836s
[2K
| Adam | epoch: 004 | loss: 0.49921 - acc: 0.7519 -- iter: 1216/2005
[A[ATraining Step: 228  | total loss: [1m[32m0.49162[0m[0m | time: 41.842s
[2K
| Adam | epoch: 004 | loss: 0.49162 - acc: 0.7580 -- iter: 1248/2005
[A[ATraining Step: 229  | total loss: [1m[32m0.49764[0m[0m | time: 42.717s
[2K
| Adam | epoch: 004 | loss: 0.49764 - acc: 0.7510 -- iter: 1280/2005
[A[ATraining Step: 230  | total loss: [1m[32m0.48702[0m[0m | time: 43.707s
[2K
| Adam | epoch: 004 | loss: 0.48702 - acc: 0.7571 -- iter: 1312/2005
[A[ATraining Step: 231  | total loss: [1m[32m0.47543[0m[0m | time: 44.673s
[2K
| Adam | epoch: 004 | loss: 0.47543 - acc: 0.7658 -- iter: 1344/2005
[A[ATraining Step: 232  | total loss: [1m[32m0.47367[0m[0m | time: 45.489s
[2K
| Adam | epoch: 004 | loss: 0.47367 - acc: 0.7673 -- iter: 1376/2005
[A[ATraining Step: 233  | total loss: [1m[32m0.50202[0m[0m | time: 46.305s
[2K
| Adam | epoch: 004 | loss: 0.50202 - acc: 0.7593 -- iter: 1408/2005
[A[ATraining Step: 234  | total loss: [1m[32m0.50022[0m[0m | time: 47.173s
[2K
| Adam | epoch: 004 | loss: 0.50022 - acc: 0.7647 -- iter: 1440/2005
[A[ATraining Step: 235  | total loss: [1m[32m0.48549[0m[0m | time: 48.023s
[2K
| Adam | epoch: 004 | loss: 0.48549 - acc: 0.7726 -- iter: 1472/2005
[A[ATraining Step: 236  | total loss: [1m[32m0.48149[0m[0m | time: 49.025s
[2K
| Adam | epoch: 004 | loss: 0.48149 - acc: 0.7766 -- iter: 1504/2005
[A[ATraining Step: 237  | total loss: [1m[32m0.48873[0m[0m | time: 50.011s
[2K
| Adam | epoch: 004 | loss: 0.48873 - acc: 0.7677 -- iter: 1536/2005
[A[ATraining Step: 238  | total loss: [1m[32m0.47893[0m[0m | time: 50.825s
[2K
| Adam | epoch: 004 | loss: 0.47893 - acc: 0.7659 -- iter: 1568/2005
[A[ATraining Step: 239  | total loss: [1m[32m0.47577[0m[0m | time: 51.709s
[2K
| Adam | epoch: 004 | loss: 0.47577 - acc: 0.7705 -- iter: 1600/2005
[A[ATraining Step: 240  | total loss: [1m[32m0.46700[0m[0m | time: 52.636s
[2K
| Adam | epoch: 004 | loss: 0.46700 - acc: 0.7716 -- iter: 1632/2005
[A[ATraining Step: 241  | total loss: [1m[32m0.46678[0m[0m | time: 53.672s
[2K
| Adam | epoch: 004 | loss: 0.46678 - acc: 0.7726 -- iter: 1664/2005
[A[ATraining Step: 242  | total loss: [1m[32m0.47617[0m[0m | time: 54.950s
[2K
| Adam | epoch: 004 | loss: 0.47617 - acc: 0.7672 -- iter: 1696/2005
[A[ATraining Step: 243  | total loss: [1m[32m0.46841[0m[0m | time: 56.327s
[2K
| Adam | epoch: 004 | loss: 0.46841 - acc: 0.7655 -- iter: 1728/2005
[A[ATraining Step: 244  | total loss: [1m[32m0.46906[0m[0m | time: 57.303s
[2K
| Adam | epoch: 004 | loss: 0.46906 - acc: 0.7671 -- iter: 1760/2005
[A[ATraining Step: 245  | total loss: [1m[32m0.48225[0m[0m | time: 58.248s
[2K
| Adam | epoch: 004 | loss: 0.48225 - acc: 0.7653 -- iter: 1792/2005
[A[ATraining Step: 246  | total loss: [1m[32m0.49170[0m[0m | time: 59.185s
[2K
| Adam | epoch: 004 | loss: 0.49170 - acc: 0.7638 -- iter: 1824/2005
[A[ATraining Step: 247  | total loss: [1m[32m0.50101[0m[0m | time: 60.128s
[2K
| Adam | epoch: 004 | loss: 0.50101 - acc: 0.7562 -- iter: 1856/2005
[A[ATraining Step: 248  | total loss: [1m[32m0.50326[0m[0m | time: 61.134s
[2K
| Adam | epoch: 004 | loss: 0.50326 - acc: 0.7618 -- iter: 1888/2005
[A[ATraining Step: 249  | total loss: [1m[32m0.50579[0m[0m | time: 62.219s
[2K
| Adam | epoch: 004 | loss: 0.50579 - acc: 0.7606 -- iter: 1920/2005
[A[ATraining Step: 250  | total loss: [1m[32m0.50255[0m[0m | time: 63.109s
[2K
| Adam | epoch: 004 | loss: 0.50255 - acc: 0.7627 -- iter: 1952/2005
[A[ATraining Step: 251  | total loss: [1m[32m0.49824[0m[0m | time: 64.100s
[2K
| Adam | epoch: 004 | loss: 0.49824 - acc: 0.7646 -- iter: 1984/2005
[A[ATraining Step: 252  | total loss: [1m[32m0.49475[0m[0m | time: 68.810s
[2K
| Adam | epoch: 004 | loss: 0.49475 - acc: 0.7693 | val_loss: 0.49242 - val_acc: 0.7799 -- iter: 2005/2005
--
Training Step: 253  | total loss: [1m[32m0.49159[0m[0m | time: 0.951s
[2K
| Adam | epoch: 005 | loss: 0.49159 - acc: 0.7674 -- iter: 0032/2005
[A[ATraining Step: 254  | total loss: [1m[32m0.49518[0m[0m | time: 1.961s
[2K
| Adam | epoch: 005 | loss: 0.49518 - acc: 0.7657 -- iter: 0064/2005
[A[ATraining Step: 255  | total loss: [1m[32m0.48853[0m[0m | time: 2.677s
[2K
| Adam | epoch: 005 | loss: 0.48853 - acc: 0.7735 -- iter: 0096/2005
[A[ATraining Step: 256  | total loss: [1m[32m0.48429[0m[0m | time: 3.239s
[2K
| Adam | epoch: 005 | loss: 0.48429 - acc: 0.7771 -- iter: 0128/2005
[A[ATraining Step: 257  | total loss: [1m[32m0.47672[0m[0m | time: 4.328s
[2K
| Adam | epoch: 005 | loss: 0.47672 - acc: 0.7851 -- iter: 0160/2005
[A[ATraining Step: 258  | total loss: [1m[32m0.46976[0m[0m | time: 5.609s
[2K
| Adam | epoch: 005 | loss: 0.46976 - acc: 0.7878 -- iter: 0192/2005
[A[ATraining Step: 259  | total loss: [1m[32m0.47310[0m[0m | time: 6.910s
[2K
| Adam | epoch: 005 | loss: 0.47310 - acc: 0.7809 -- iter: 0224/2005
[A[ATraining Step: 260  | total loss: [1m[32m0.47933[0m[0m | time: 7.924s
[2K
| Adam | epoch: 005 | loss: 0.47933 - acc: 0.7747 -- iter: 0256/2005
[A[ATraining Step: 261  | total loss: [1m[32m0.46813[0m[0m | time: 8.829s
[2K
| Adam | epoch: 005 | loss: 0.46813 - acc: 0.7816 -- iter: 0288/2005
[A[ATraining Step: 262  | total loss: [1m[32m0.47053[0m[0m | time: 9.817s
[2K
| Adam | epoch: 005 | loss: 0.47053 - acc: 0.7784 -- iter: 0320/2005
[A[ATraining Step: 263  | total loss: [1m[32m0.47772[0m[0m | time: 10.786s
[2K
| Adam | epoch: 005 | loss: 0.47772 - acc: 0.7756 -- iter: 0352/2005
[A[ATraining Step: 264  | total loss: [1m[32m0.47207[0m[0m | time: 11.748s
[2K
| Adam | epoch: 005 | loss: 0.47207 - acc: 0.7762 -- iter: 0384/2005
[A[ATraining Step: 265  | total loss: [1m[32m0.47942[0m[0m | time: 12.735s
[2K
| Adam | epoch: 005 | loss: 0.47942 - acc: 0.7704 -- iter: 0416/2005
[A[ATraining Step: 266  | total loss: [1m[32m0.48489[0m[0m | time: 13.663s
[2K
| Adam | epoch: 005 | loss: 0.48489 - acc: 0.7684 -- iter: 0448/2005
[A[ATraining Step: 267  | total loss: [1m[32m0.48342[0m[0m | time: 14.768s
[2K
| Adam | epoch: 005 | loss: 0.48342 - acc: 0.7665 -- iter: 0480/2005
[A[ATraining Step: 268  | total loss: [1m[32m0.48314[0m[0m | time: 16.101s
[2K
| Adam | epoch: 005 | loss: 0.48314 - acc: 0.7618 -- iter: 0512/2005
[A[ATraining Step: 269  | total loss: [1m[32m0.50778[0m[0m | time: 17.432s
[2K
| Adam | epoch: 005 | loss: 0.50778 - acc: 0.7450 -- iter: 0544/2005
[A[ATraining Step: 270  | total loss: [1m[32m0.49794[0m[0m | time: 18.321s
[2K
| Adam | epoch: 005 | loss: 0.49794 - acc: 0.7517 -- iter: 0576/2005
[A[ATraining Step: 271  | total loss: [1m[32m0.49452[0m[0m | time: 19.218s
[2K
| Adam | epoch: 005 | loss: 0.49452 - acc: 0.7547 -- iter: 0608/2005
[A[ATraining Step: 272  | total loss: [1m[32m0.50533[0m[0m | time: 20.112s
[2K
| Adam | epoch: 005 | loss: 0.50533 - acc: 0.7480 -- iter: 0640/2005
[A[ATraining Step: 273  | total loss: [1m[32m0.50965[0m[0m | time: 21.060s
[2K
| Adam | epoch: 005 | loss: 0.50965 - acc: 0.7513 -- iter: 0672/2005
[A[ATraining Step: 274  | total loss: [1m[32m0.51553[0m[0m | time: 22.080s
[2K
| Adam | epoch: 005 | loss: 0.51553 - acc: 0.7418 -- iter: 0704/2005
[A[ATraining Step: 275  | total loss: [1m[32m0.51836[0m[0m | time: 23.144s
[2K
| Adam | epoch: 005 | loss: 0.51836 - acc: 0.7332 -- iter: 0736/2005
[A[ATraining Step: 276  | total loss: [1m[32m0.51538[0m[0m | time: 24.028s
[2K
| Adam | epoch: 005 | loss: 0.51538 - acc: 0.7349 -- iter: 0768/2005
[A[ATraining Step: 277  | total loss: [1m[32m0.51630[0m[0m | time: 25.021s
[2K
| Adam | epoch: 005 | loss: 0.51630 - acc: 0.7364 -- iter: 0800/2005
[A[ATraining Step: 278  | total loss: [1m[32m0.51913[0m[0m | time: 26.330s
[2K
| Adam | epoch: 005 | loss: 0.51913 - acc: 0.7503 -- iter: 0832/2005
[A[ATraining Step: 279  | total loss: [1m[32m0.51955[0m[0m | time: 27.691s
[2K
| Adam | epoch: 005 | loss: 0.51955 - acc: 0.7565 -- iter: 0864/2005
[A[ATraining Step: 280  | total loss: [1m[32m0.54531[0m[0m | time: 28.650s
[2K
| Adam | epoch: 005 | loss: 0.54531 - acc: 0.7308 -- iter: 0896/2005
[A[ATraining Step: 281  | total loss: [1m[32m0.55802[0m[0m | time: 29.563s
[2K
| Adam | epoch: 005 | loss: 0.55802 - acc: 0.7171 -- iter: 0928/2005
[A[ATraining Step: 282  | total loss: [1m[32m0.54000[0m[0m | time: 30.502s
[2K
| Adam | epoch: 005 | loss: 0.54000 - acc: 0.7329 -- iter: 0960/2005
[A[ATraining Step: 283  | total loss: [1m[32m0.53648[0m[0m | time: 31.500s
[2K
| Adam | epoch: 005 | loss: 0.53648 - acc: 0.7346 -- iter: 0992/2005
[A[ATraining Step: 284  | total loss: [1m[32m0.53389[0m[0m | time: 32.574s
[2K
| Adam | epoch: 005 | loss: 0.53389 - acc: 0.7330 -- iter: 1024/2005
[A[ATraining Step: 285  | total loss: [1m[32m0.53262[0m[0m | time: 33.636s
[2K
| Adam | epoch: 005 | loss: 0.53262 - acc: 0.7316 -- iter: 1056/2005
[A[ATraining Step: 286  | total loss: [1m[32m0.53048[0m[0m | time: 34.481s
[2K
| Adam | epoch: 005 | loss: 0.53048 - acc: 0.7241 -- iter: 1088/2005
[A[ATraining Step: 287  | total loss: [1m[32m0.52671[0m[0m | time: 35.541s
[2K
| Adam | epoch: 005 | loss: 0.52671 - acc: 0.7267 -- iter: 1120/2005
[A[ATraining Step: 288  | total loss: [1m[32m0.51808[0m[0m | time: 36.876s
[2K
| Adam | epoch: 005 | loss: 0.51808 - acc: 0.7321 -- iter: 1152/2005
[A[ATraining Step: 289  | total loss: [1m[32m0.51924[0m[0m | time: 38.261s
[2K
| Adam | epoch: 005 | loss: 0.51924 - acc: 0.7370 -- iter: 1184/2005
[A[ATraining Step: 290  | total loss: [1m[32m0.50601[0m[0m | time: 39.141s
[2K
| Adam | epoch: 005 | loss: 0.50601 - acc: 0.7508 -- iter: 1216/2005
[A[ATraining Step: 291  | total loss: [1m[32m0.49862[0m[0m | time: 40.026s
[2K
| Adam | epoch: 005 | loss: 0.49862 - acc: 0.7539 -- iter: 1248/2005
[A[ATraining Step: 292  | total loss: [1m[32m0.49183[0m[0m | time: 40.960s
[2K
| Adam | epoch: 005 | loss: 0.49183 - acc: 0.7566 -- iter: 1280/2005
[A[ATraining Step: 293  | total loss: [1m[32m0.49999[0m[0m | time: 41.908s
[2K
| Adam | epoch: 005 | loss: 0.49999 - acc: 0.7497 -- iter: 1312/2005
[A[ATraining Step: 294  | total loss: [1m[32m0.49310[0m[0m | time: 42.839s
[2K
| Adam | epoch: 005 | loss: 0.49310 - acc: 0.7529 -- iter: 1344/2005
[A[ATraining Step: 295  | total loss: [1m[32m0.48222[0m[0m | time: 43.878s
[2K
| Adam | epoch: 005 | loss: 0.48222 - acc: 0.7619 -- iter: 1376/2005
[A[ATraining Step: 296  | total loss: [1m[32m0.47051[0m[0m | time: 44.850s
[2K
| Adam | epoch: 005 | loss: 0.47051 - acc: 0.7733 -- iter: 1408/2005
[A[ATraining Step: 297  | total loss: [1m[32m0.45576[0m[0m | time: 45.797s
[2K
| Adam | epoch: 005 | loss: 0.45576 - acc: 0.7803 -- iter: 1440/2005
[A[ATraining Step: 298  | total loss: [1m[32m0.44058[0m[0m | time: 47.131s
[2K
| Adam | epoch: 005 | loss: 0.44058 - acc: 0.7898 -- iter: 1472/2005
[A[ATraining Step: 299  | total loss: [1m[32m0.43280[0m[0m | time: 48.501s
[2K
| Adam | epoch: 005 | loss: 0.43280 - acc: 0.7920 -- iter: 1504/2005
[A[ATraining Step: 300  | total loss: [1m[32m0.41330[0m[0m | time: 49.569s
[2K
| Adam | epoch: 005 | loss: 0.41330 - acc: 0.8097 -- iter: 1536/2005
[A[ATraining Step: 301  | total loss: [1m[32m0.40197[0m[0m | time: 50.435s
[2K
| Adam | epoch: 005 | loss: 0.40197 - acc: 0.8162 -- iter: 1568/2005
[A[ATraining Step: 302  | total loss: [1m[32m0.41555[0m[0m | time: 51.359s
[2K
| Adam | epoch: 005 | loss: 0.41555 - acc: 0.8034 -- iter: 1600/2005
[A[ATraining Step: 303  | total loss: [1m[32m0.41205[0m[0m | time: 52.306s
[2K
| Adam | epoch: 005 | loss: 0.41205 - acc: 0.8105 -- iter: 1632/2005
[A[ATraining Step: 304  | total loss: [1m[32m0.43056[0m[0m | time: 53.238s
[2K
| Adam | epoch: 005 | loss: 0.43056 - acc: 0.8076 -- iter: 1664/2005
[A[ATraining Step: 305  | total loss: [1m[32m0.42455[0m[0m | time: 54.382s
[2K
| Adam | epoch: 005 | loss: 0.42455 - acc: 0.8112 -- iter: 1696/2005
[A[ATraining Step: 306  | total loss: [1m[32m0.42063[0m[0m | time: 55.343s
[2K
| Adam | epoch: 005 | loss: 0.42063 - acc: 0.8113 -- iter: 1728/2005
[A[ATraining Step: 307  | total loss: [1m[32m0.42066[0m[0m | time: 56.209s
[2K
| Adam | epoch: 005 | loss: 0.42066 - acc: 0.8146 -- iter: 1760/2005
[A[ATraining Step: 308  | total loss: [1m[32m0.43058[0m[0m | time: 57.582s
[2K
| Adam | epoch: 005 | loss: 0.43058 - acc: 0.8081 -- iter: 1792/2005
[A[ATraining Step: 309  | total loss: [1m[32m0.43223[0m[0m | time: 58.940s
[2K
| Adam | epoch: 005 | loss: 0.43223 - acc: 0.8117 -- iter: 1824/2005
[A[ATraining Step: 310  | total loss: [1m[32m0.42260[0m[0m | time: 60.156s
[2K
| Adam | epoch: 005 | loss: 0.42260 - acc: 0.8118 -- iter: 1856/2005
[A[ATraining Step: 311  | total loss: [1m[32m0.41759[0m[0m | time: 60.976s
[2K
| Adam | epoch: 005 | loss: 0.41759 - acc: 0.8118 -- iter: 1888/2005
[A[ATraining Step: 312  | total loss: [1m[32m0.42703[0m[0m | time: 61.911s
[2K
| Adam | epoch: 005 | loss: 0.42703 - acc: 0.8119 -- iter: 1920/2005
[A[ATraining Step: 313  | total loss: [1m[32m0.41111[0m[0m | time: 62.892s
[2K
| Adam | epoch: 005 | loss: 0.41111 - acc: 0.8245 -- iter: 1952/2005
[A[ATraining Step: 314  | total loss: [1m[32m0.40769[0m[0m | time: 63.859s
[2K
| Adam | epoch: 005 | loss: 0.40769 - acc: 0.8264 -- iter: 1984/2005
[A[ATraining Step: 315  | total loss: [1m[32m0.42019[0m[0m | time: 68.006s
[2K
| Adam | epoch: 005 | loss: 0.42019 - acc: 0.8156 | val_loss: 0.41059 - val_acc: 0.8086 -- iter: 2005/2005
--
Training Step: 316  | total loss: [1m[32m0.42978[0m[0m | time: 1.205s
[2K
| Adam | epoch: 006 | loss: 0.42978 - acc: 0.8091 -- iter: 0032/2005
[A[ATraining Step: 317  | total loss: [1m[32m0.42689[0m[0m | time: 2.051s
[2K
| Adam | epoch: 006 | loss: 0.42689 - acc: 0.8094 -- iter: 0064/2005
[A[ATraining Step: 318  | total loss: [1m[32m0.43265[0m[0m | time: 2.969s
[2K
| Adam | epoch: 006 | loss: 0.43265 - acc: 0.8003 -- iter: 0096/2005
[A[ATraining Step: 319  | total loss: [1m[32m0.44295[0m[0m | time: 3.619s
[2K
| Adam | epoch: 006 | loss: 0.44295 - acc: 0.7828 -- iter: 0128/2005
[A[ATraining Step: 320  | total loss: [1m[32m0.44090[0m[0m | time: 4.271s
[2K
| Adam | epoch: 006 | loss: 0.44090 - acc: 0.7807 -- iter: 0160/2005
[A[ATraining Step: 321  | total loss: [1m[32m0.43404[0m[0m | time: 5.221s
[2K
| Adam | epoch: 006 | loss: 0.43404 - acc: 0.7884 -- iter: 0192/2005
[A[ATraining Step: 322  | total loss: [1m[32m0.42054[0m[0m | time: 6.330s
[2K
| Adam | epoch: 006 | loss: 0.42054 - acc: 0.8002 -- iter: 0224/2005
[A[ATraining Step: 323  | total loss: [1m[32m0.40776[0m[0m | time: 7.366s
[2K
| Adam | epoch: 006 | loss: 0.40776 - acc: 0.8108 -- iter: 0256/2005
[A[ATraining Step: 324  | total loss: [1m[32m0.41869[0m[0m | time: 8.253s
[2K
| Adam | epoch: 006 | loss: 0.41869 - acc: 0.8047 -- iter: 0288/2005
[A[ATraining Step: 325  | total loss: [1m[32m0.41745[0m[0m | time: 9.677s
[2K
| Adam | epoch: 006 | loss: 0.41745 - acc: 0.8086 -- iter: 0320/2005
[A[ATraining Step: 326  | total loss: [1m[32m0.42630[0m[0m | time: 11.108s
[2K
| Adam | epoch: 006 | loss: 0.42630 - acc: 0.8059 -- iter: 0352/2005
[A[ATraining Step: 327  | total loss: [1m[32m0.43439[0m[0m | time: 12.166s
[2K
| Adam | epoch: 006 | loss: 0.43439 - acc: 0.7940 -- iter: 0384/2005
[A[ATraining Step: 328  | total loss: [1m[32m0.42586[0m[0m | time: 13.027s
[2K
| Adam | epoch: 006 | loss: 0.42586 - acc: 0.7927 -- iter: 0416/2005
[A[ATraining Step: 329  | total loss: [1m[32m0.42317[0m[0m | time: 13.972s
[2K
| Adam | epoch: 006 | loss: 0.42317 - acc: 0.7916 -- iter: 0448/2005
[A[ATraining Step: 330  | total loss: [1m[32m0.43044[0m[0m | time: 14.909s
[2K
| Adam | epoch: 006 | loss: 0.43044 - acc: 0.7906 -- iter: 0480/2005
[A[ATraining Step: 331  | total loss: [1m[32m0.41198[0m[0m | time: 15.862s
[2K
| Adam | epoch: 006 | loss: 0.41198 - acc: 0.8021 -- iter: 0512/2005
[A[ATraining Step: 332  | total loss: [1m[32m0.41042[0m[0m | time: 16.917s
[2K
| Adam | epoch: 006 | loss: 0.41042 - acc: 0.8000 -- iter: 0544/2005
[A[ATraining Step: 333  | total loss: [1m[32m0.42051[0m[0m | time: 17.983s
[2K
| Adam | epoch: 006 | loss: 0.42051 - acc: 0.7982 -- iter: 0576/2005
[A[ATraining Step: 334  | total loss: [1m[32m0.42658[0m[0m | time: 18.849s
[2K
| Adam | epoch: 006 | loss: 0.42658 - acc: 0.7933 -- iter: 0608/2005
[A[ATraining Step: 335  | total loss: [1m[32m0.44104[0m[0m | time: 20.158s
[2K
| Adam | epoch: 006 | loss: 0.44104 - acc: 0.7921 -- iter: 0640/2005
[A[ATraining Step: 336  | total loss: [1m[32m0.44808[0m[0m | time: 21.393s
[2K
| Adam | epoch: 006 | loss: 0.44808 - acc: 0.7817 -- iter: 0672/2005
[A[ATraining Step: 337  | total loss: [1m[32m0.44372[0m[0m | time: 22.745s
[2K
| Adam | epoch: 006 | loss: 0.44372 - acc: 0.7816 -- iter: 0704/2005
[A[ATraining Step: 338  | total loss: [1m[32m0.44339[0m[0m | time: 23.608s
[2K
| Adam | epoch: 006 | loss: 0.44339 - acc: 0.7847 -- iter: 0736/2005
[A[ATraining Step: 339  | total loss: [1m[32m0.42245[0m[0m | time: 24.575s
[2K
| Adam | epoch: 006 | loss: 0.42245 - acc: 0.8000 -- iter: 0768/2005
[A[ATraining Step: 340  | total loss: [1m[32m0.41780[0m[0m | time: 25.530s
[2K
| Adam | epoch: 006 | loss: 0.41780 - acc: 0.8012 -- iter: 0800/2005
[A[ATraining Step: 341  | total loss: [1m[32m0.42319[0m[0m | time: 26.467s
[2K
| Adam | epoch: 006 | loss: 0.42319 - acc: 0.8024 -- iter: 0832/2005
[A[ATraining Step: 342  | total loss: [1m[32m0.42159[0m[0m | time: 27.535s
[2K
| Adam | epoch: 006 | loss: 0.42159 - acc: 0.8034 -- iter: 0864/2005
[A[ATraining Step: 343  | total loss: [1m[32m0.41161[0m[0m | time: 28.611s
[2K
| Adam | epoch: 006 | loss: 0.41161 - acc: 0.8043 -- iter: 0896/2005
[A[ATraining Step: 344  | total loss: [1m[32m0.42188[0m[0m | time: 29.508s
[2K
| Adam | epoch: 006 | loss: 0.42188 - acc: 0.7864 -- iter: 0928/2005
[A[ATraining Step: 345  | total loss: [1m[32m0.41517[0m[0m | time: 30.646s
[2K
| Adam | epoch: 006 | loss: 0.41517 - acc: 0.7890 -- iter: 0960/2005
[A[ATraining Step: 346  | total loss: [1m[32m0.40038[0m[0m | time: 31.948s
[2K
| Adam | epoch: 006 | loss: 0.40038 - acc: 0.8007 -- iter: 0992/2005
[A[ATraining Step: 347  | total loss: [1m[32m0.38545[0m[0m | time: 33.246s
[2K
| Adam | epoch: 006 | loss: 0.38545 - acc: 0.8113 -- iter: 1024/2005
[A[ATraining Step: 348  | total loss: [1m[32m0.39284[0m[0m | time: 34.162s
[2K
| Adam | epoch: 006 | loss: 0.39284 - acc: 0.8051 -- iter: 1056/2005
[A[ATraining Step: 349  | total loss: [1m[32m0.40848[0m[0m | time: 35.150s
[2K
| Adam | epoch: 006 | loss: 0.40848 - acc: 0.7996 -- iter: 1088/2005
[A[ATraining Step: 350  | total loss: [1m[32m0.39282[0m[0m | time: 36.135s
[2K
| Adam | epoch: 006 | loss: 0.39282 - acc: 0.8103 -- iter: 1120/2005
[A[ATraining Step: 351  | total loss: [1m[32m0.39022[0m[0m | time: 37.127s
[2K
| Adam | epoch: 006 | loss: 0.39022 - acc: 0.8074 -- iter: 1152/2005
[A[ATraining Step: 352  | total loss: [1m[32m0.40596[0m[0m | time: 38.182s
[2K
| Adam | epoch: 006 | loss: 0.40596 - acc: 0.8079 -- iter: 1184/2005
[A[ATraining Step: 353  | total loss: [1m[32m0.39141[0m[0m | time: 39.224s
[2K
| Adam | epoch: 006 | loss: 0.39141 - acc: 0.8146 -- iter: 1216/2005
[A[ATraining Step: 354  | total loss: [1m[32m0.41143[0m[0m | time: 40.178s
[2K
| Adam | epoch: 006 | loss: 0.41143 - acc: 0.8081 -- iter: 1248/2005
[A[ATraining Step: 355  | total loss: [1m[32m0.41178[0m[0m | time: 41.448s
[2K
| Adam | epoch: 006 | loss: 0.41178 - acc: 0.8055 -- iter: 1280/2005
[A[ATraining Step: 356  | total loss: [1m[32m0.40711[0m[0m | time: 42.703s
[2K
| Adam | epoch: 006 | loss: 0.40711 - acc: 0.8093 -- iter: 1312/2005
[A[ATraining Step: 357  | total loss: [1m[32m0.39621[0m[0m | time: 44.007s
[2K
| Adam | epoch: 006 | loss: 0.39621 - acc: 0.8096 -- iter: 1344/2005
[A[ATraining Step: 358  | total loss: [1m[32m0.39371[0m[0m | time: 44.857s
[2K
| Adam | epoch: 006 | loss: 0.39371 - acc: 0.8130 -- iter: 1376/2005
[A[ATraining Step: 359  | total loss: [1m[32m0.42079[0m[0m | time: 45.786s
[2K
| Adam | epoch: 006 | loss: 0.42079 - acc: 0.8005 -- iter: 1408/2005
[A[ATraining Step: 360  | total loss: [1m[32m0.42422[0m[0m | time: 46.764s
[2K
| Adam | epoch: 006 | loss: 0.42422 - acc: 0.8079 -- iter: 1440/2005
[A[ATraining Step: 361  | total loss: [1m[32m0.43672[0m[0m | time: 47.719s
[2K
| Adam | epoch: 006 | loss: 0.43672 - acc: 0.8053 -- iter: 1472/2005
[A[ATraining Step: 362  | total loss: [1m[32m0.42639[0m[0m | time: 48.772s
[2K
| Adam | epoch: 006 | loss: 0.42639 - acc: 0.8154 -- iter: 1504/2005
[A[ATraining Step: 363  | total loss: [1m[32m0.41220[0m[0m | time: 49.825s
[2K
| Adam | epoch: 006 | loss: 0.41220 - acc: 0.8244 -- iter: 1536/2005
[A[ATraining Step: 364  | total loss: [1m[32m0.40131[0m[0m | time: 50.712s
[2K
| Adam | epoch: 006 | loss: 0.40131 - acc: 0.8295 -- iter: 1568/2005
[A[ATraining Step: 365  | total loss: [1m[32m0.42778[0m[0m | time: 51.928s
[2K
| Adam | epoch: 006 | loss: 0.42778 - acc: 0.8122 -- iter: 1600/2005
[A[ATraining Step: 366  | total loss: [1m[32m0.41514[0m[0m | time: 53.154s
[2K
| Adam | epoch: 006 | loss: 0.41514 - acc: 0.8185 -- iter: 1632/2005
[A[ATraining Step: 367  | total loss: [1m[32m0.40557[0m[0m | time: 54.536s
[2K
| Adam | epoch: 006 | loss: 0.40557 - acc: 0.8241 -- iter: 1664/2005
[A[ATraining Step: 368  | total loss: [1m[32m0.39956[0m[0m | time: 55.431s
[2K
| Adam | epoch: 006 | loss: 0.39956 - acc: 0.8292 -- iter: 1696/2005
[A[ATraining Step: 369  | total loss: [1m[32m0.39722[0m[0m | time: 56.336s
[2K
| Adam | epoch: 006 | loss: 0.39722 - acc: 0.8275 -- iter: 1728/2005
[A[ATraining Step: 370  | total loss: [1m[32m0.40536[0m[0m | time: 57.242s
[2K
| Adam | epoch: 006 | loss: 0.40536 - acc: 0.8198 -- iter: 1760/2005
[A[ATraining Step: 371  | total loss: [1m[32m0.39614[0m[0m | time: 58.184s
[2K
| Adam | epoch: 006 | loss: 0.39614 - acc: 0.8253 -- iter: 1792/2005
[A[ATraining Step: 372  | total loss: [1m[32m0.39458[0m[0m | time: 59.215s
[2K
| Adam | epoch: 006 | loss: 0.39458 - acc: 0.8334 -- iter: 1824/2005
[A[ATraining Step: 373  | total loss: [1m[32m0.37955[0m[0m | time: 60.272s
[2K
| Adam | epoch: 006 | loss: 0.37955 - acc: 0.8438 -- iter: 1856/2005
[A[ATraining Step: 374  | total loss: [1m[32m0.38094[0m[0m | time: 61.233s
[2K
| Adam | epoch: 006 | loss: 0.38094 - acc: 0.8407 -- iter: 1888/2005
[A[ATraining Step: 375  | total loss: [1m[32m0.39781[0m[0m | time: 62.206s
[2K
| Adam | epoch: 006 | loss: 0.39781 - acc: 0.8285 -- iter: 1920/2005
[A[ATraining Step: 376  | total loss: [1m[32m0.37355[0m[0m | time: 63.487s
[2K
| Adam | epoch: 006 | loss: 0.37355 - acc: 0.8363 -- iter: 1952/2005
[A[ATraining Step: 377  | total loss: [1m[32m0.38243[0m[0m | time: 64.845s
[2K
| Adam | epoch: 006 | loss: 0.38243 - acc: 0.8339 -- iter: 1984/2005
[A[ATraining Step: 378  | total loss: [1m[32m0.37647[0m[0m | time: 68.607s
[2K
| Adam | epoch: 006 | loss: 0.37647 - acc: 0.8411 | val_loss: 0.41126 - val_acc: 0.8246 -- iter: 2005/2005
--
Training Step: 379  | total loss: [1m[32m0.36146[0m[0m | time: 0.946s
[2K
| Adam | epoch: 007 | loss: 0.36146 - acc: 0.8539 -- iter: 0032/2005
[A[ATraining Step: 380  | total loss: [1m[32m0.35862[0m[0m | time: 2.215s
[2K
| Adam | epoch: 007 | loss: 0.35862 - acc: 0.8591 -- iter: 0064/2005
[A[ATraining Step: 381  | total loss: [1m[32m0.36490[0m[0m | time: 3.565s
[2K
| Adam | epoch: 007 | loss: 0.36490 - acc: 0.8482 -- iter: 0096/2005
[A[ATraining Step: 382  | total loss: [1m[32m0.34374[0m[0m | time: 4.679s
[2K
| Adam | epoch: 007 | loss: 0.34374 - acc: 0.8634 -- iter: 0128/2005
[A[ATraining Step: 383  | total loss: [1m[32m0.33932[0m[0m | time: 5.251s
[2K
| Adam | epoch: 007 | loss: 0.33932 - acc: 0.8677 -- iter: 0160/2005
[A[ATraining Step: 384  | total loss: [1m[32m0.33602[0m[0m | time: 5.922s
[2K
| Adam | epoch: 007 | loss: 0.33602 - acc: 0.8714 -- iter: 0192/2005
[A[ATraining Step: 385  | total loss: [1m[32m0.32563[0m[0m | time: 6.905s
[2K
| Adam | epoch: 007 | loss: 0.32563 - acc: 0.8652 -- iter: 0224/2005
[A[ATraining Step: 386  | total loss: [1m[32m0.35048[0m[0m | time: 7.854s
[2K
| Adam | epoch: 007 | loss: 0.35048 - acc: 0.8537 -- iter: 0256/2005
[A[ATraining Step: 387  | total loss: [1m[32m0.35244[0m[0m | time: 8.830s
[2K
| Adam | epoch: 007 | loss: 0.35244 - acc: 0.8527 -- iter: 0288/2005
[A[ATraining Step: 388  | total loss: [1m[32m0.34809[0m[0m | time: 9.936s
[2K
| Adam | epoch: 007 | loss: 0.34809 - acc: 0.8518 -- iter: 0320/2005
[A[ATraining Step: 389  | total loss: [1m[32m0.34105[0m[0m | time: 10.864s
[2K
| Adam | epoch: 007 | loss: 0.34105 - acc: 0.8541 -- iter: 0352/2005
[A[ATraining Step: 390  | total loss: [1m[32m0.33750[0m[0m | time: 11.935s
[2K
| Adam | epoch: 007 | loss: 0.33750 - acc: 0.8531 -- iter: 0384/2005
[A[ATraining Step: 391  | total loss: [1m[32m0.34751[0m[0m | time: 13.187s
[2K
| Adam | epoch: 007 | loss: 0.34751 - acc: 0.8490 -- iter: 0416/2005
[A[ATraining Step: 392  | total loss: [1m[32m0.35178[0m[0m | time: 14.536s
[2K
| Adam | epoch: 007 | loss: 0.35178 - acc: 0.8485 -- iter: 0448/2005
[A[ATraining Step: 393  | total loss: [1m[32m0.35663[0m[0m | time: 15.527s
[2K
| Adam | epoch: 007 | loss: 0.35663 - acc: 0.8324 -- iter: 0480/2005
[A[ATraining Step: 394  | total loss: [1m[32m0.34185[0m[0m | time: 16.387s
[2K
| Adam | epoch: 007 | loss: 0.34185 - acc: 0.8429 -- iter: 0512/2005
[A[ATraining Step: 395  | total loss: [1m[32m0.33528[0m[0m | time: 17.365s
[2K
| Adam | epoch: 007 | loss: 0.33528 - acc: 0.8430 -- iter: 0544/2005
[A[ATraining Step: 396  | total loss: [1m[32m0.32970[0m[0m | time: 18.321s
[2K
| Adam | epoch: 007 | loss: 0.32970 - acc: 0.8462 -- iter: 0576/2005
[A[ATraining Step: 397  | total loss: [1m[32m0.34024[0m[0m | time: 19.308s
[2K
| Adam | epoch: 007 | loss: 0.34024 - acc: 0.8366 -- iter: 0608/2005
[A[ATraining Step: 398  | total loss: [1m[32m0.33767[0m[0m | time: 20.351s
[2K
| Adam | epoch: 007 | loss: 0.33767 - acc: 0.8373 -- iter: 0640/2005
[A[ATraining Step: 399  | total loss: [1m[32m0.33803[0m[0m | time: 21.285s
[2K
| Adam | epoch: 007 | loss: 0.33803 - acc: 0.8379 -- iter: 0672/2005
[A[ATraining Step: 400  | total loss: [1m[32m0.33263[0m[0m | time: 25.942s
[2K
| Adam | epoch: 007 | loss: 0.33263 - acc: 0.8416 | val_loss: 0.44204 - val_acc: 0.8038 -- iter: 0704/2005
--
Training Step: 401  | total loss: [1m[32m0.32447[0m[0m | time: 26.928s
[2K
| Adam | epoch: 007 | loss: 0.32447 - acc: 0.8450 -- iter: 0736/2005
[A[ATraining Step: 402  | total loss: [1m[32m0.33355[0m[0m | time: 27.871s
[2K
| Adam | epoch: 007 | loss: 0.33355 - acc: 0.8386 -- iter: 0768/2005
[A[ATraining Step: 403  | total loss: [1m[32m0.31822[0m[0m | time: 28.860s
[2K
| Adam | epoch: 007 | loss: 0.31822 - acc: 0.8485 -- iter: 0800/2005
[A[ATraining Step: 404  | total loss: [1m[32m0.31927[0m[0m | time: 29.880s
[2K
| Adam | epoch: 007 | loss: 0.31927 - acc: 0.8511 -- iter: 0832/2005
[A[ATraining Step: 405  | total loss: [1m[32m0.33696[0m[0m | time: 30.931s
[2K
| Adam | epoch: 007 | loss: 0.33696 - acc: 0.8442 -- iter: 0864/2005
[A[ATraining Step: 406  | total loss: [1m[32m0.33737[0m[0m | time: 32.042s
[2K
| Adam | epoch: 007 | loss: 0.33737 - acc: 0.8347 -- iter: 0896/2005
[A[ATraining Step: 407  | total loss: [1m[32m0.33368[0m[0m | time: 33.001s
[2K
| Adam | epoch: 007 | loss: 0.33368 - acc: 0.8388 -- iter: 0928/2005
[A[ATraining Step: 408  | total loss: [1m[32m0.32980[0m[0m | time: 33.966s
[2K
| Adam | epoch: 007 | loss: 0.32980 - acc: 0.8455 -- iter: 0960/2005
[A[ATraining Step: 409  | total loss: [1m[32m0.34024[0m[0m | time: 35.270s
[2K
| Adam | epoch: 007 | loss: 0.34024 - acc: 0.8422 -- iter: 0992/2005
[A[ATraining Step: 410  | total loss: [1m[32m0.34149[0m[0m | time: 36.641s
[2K
| Adam | epoch: 007 | loss: 0.34149 - acc: 0.8424 -- iter: 1024/2005
[A[ATraining Step: 411  | total loss: [1m[32m0.34287[0m[0m | time: 37.727s
[2K
| Adam | epoch: 007 | loss: 0.34287 - acc: 0.8425 -- iter: 1056/2005
[A[ATraining Step: 412  | total loss: [1m[32m0.32908[0m[0m | time: 38.591s
[2K
| Adam | epoch: 007 | loss: 0.32908 - acc: 0.8520 -- iter: 1088/2005
[A[ATraining Step: 413  | total loss: [1m[32m0.33815[0m[0m | time: 39.517s
[2K
| Adam | epoch: 007 | loss: 0.33815 - acc: 0.8387 -- iter: 1120/2005
[A[ATraining Step: 414  | total loss: [1m[32m0.35249[0m[0m | time: 40.486s
[2K
| Adam | epoch: 007 | loss: 0.35249 - acc: 0.8329 -- iter: 1152/2005
[A[ATraining Step: 415  | total loss: [1m[32m0.33049[0m[0m | time: 41.412s
[2K
| Adam | epoch: 007 | loss: 0.33049 - acc: 0.8496 -- iter: 1184/2005
[A[ATraining Step: 416  | total loss: [1m[32m0.32226[0m[0m | time: 42.568s
[2K
| Adam | epoch: 007 | loss: 0.32226 - acc: 0.8522 -- iter: 1216/2005
[A[ATraining Step: 417  | total loss: [1m[32m0.30429[0m[0m | time: 43.579s
[2K
| Adam | epoch: 007 | loss: 0.30429 - acc: 0.8670 -- iter: 1248/2005
[A[ATraining Step: 418  | total loss: [1m[32m0.30394[0m[0m | time: 44.499s
[2K
| Adam | epoch: 007 | loss: 0.30394 - acc: 0.8646 -- iter: 1280/2005
[A[ATraining Step: 419  | total loss: [1m[32m0.30189[0m[0m | time: 45.848s
[2K
| Adam | epoch: 007 | loss: 0.30189 - acc: 0.8688 -- iter: 1312/2005
[A[ATraining Step: 420  | total loss: [1m[32m0.29020[0m[0m | time: 47.241s
[2K
| Adam | epoch: 007 | loss: 0.29020 - acc: 0.8725 -- iter: 1344/2005
[A[ATraining Step: 421  | total loss: [1m[32m0.28525[0m[0m | time: 48.523s
[2K
| Adam | epoch: 007 | loss: 0.28525 - acc: 0.8790 -- iter: 1376/2005
[A[ATraining Step: 422  | total loss: [1m[32m0.27323[0m[0m | time: 49.386s
[2K
| Adam | epoch: 007 | loss: 0.27323 - acc: 0.8849 -- iter: 1408/2005
[A[ATraining Step: 423  | total loss: [1m[32m0.29383[0m[0m | time: 50.343s
[2K
| Adam | epoch: 007 | loss: 0.29383 - acc: 0.8839 -- iter: 1440/2005
[A[ATraining Step: 424  | total loss: [1m[32m0.30075[0m[0m | time: 51.345s
[2K
| Adam | epoch: 007 | loss: 0.30075 - acc: 0.8861 -- iter: 1472/2005
[A[ATraining Step: 425  | total loss: [1m[32m0.28538[0m[0m | time: 52.335s
[2K
| Adam | epoch: 007 | loss: 0.28538 - acc: 0.8913 -- iter: 1504/2005
[A[ATraining Step: 426  | total loss: [1m[32m0.28753[0m[0m | time: 53.415s
[2K
| Adam | epoch: 007 | loss: 0.28753 - acc: 0.8865 -- iter: 1536/2005
[A[ATraining Step: 427  | total loss: [1m[32m0.27417[0m[0m | time: 54.505s
[2K
| Adam | epoch: 007 | loss: 0.27417 - acc: 0.8916 -- iter: 1568/2005
[A[ATraining Step: 428  | total loss: [1m[32m0.26268[0m[0m | time: 55.402s
[2K
| Adam | epoch: 007 | loss: 0.26268 - acc: 0.8993 -- iter: 1600/2005
[A[ATraining Step: 429  | total loss: [1m[32m0.26329[0m[0m | time: 56.630s
[2K
| Adam | epoch: 007 | loss: 0.26329 - acc: 0.8906 -- iter: 1632/2005
[A[ATraining Step: 430  | total loss: [1m[32m0.27568[0m[0m | time: 57.925s
[2K
| Adam | epoch: 007 | loss: 0.27568 - acc: 0.8860 -- iter: 1664/2005
[A[ATraining Step: 431  | total loss: [1m[32m0.29631[0m[0m | time: 59.206s
[2K
| Adam | epoch: 007 | loss: 0.29631 - acc: 0.8724 -- iter: 1696/2005
[A[ATraining Step: 432  | total loss: [1m[32m0.29131[0m[0m | time: 60.179s
[2K
| Adam | epoch: 007 | loss: 0.29131 - acc: 0.8726 -- iter: 1728/2005
[A[ATraining Step: 433  | total loss: [1m[32m0.27453[0m[0m | time: 61.053s
[2K
| Adam | epoch: 007 | loss: 0.27453 - acc: 0.8854 -- iter: 1760/2005
[A[ATraining Step: 434  | total loss: [1m[32m0.26817[0m[0m | time: 62.031s
[2K
| Adam | epoch: 007 | loss: 0.26817 - acc: 0.8906 -- iter: 1792/2005
[A[ATraining Step: 435  | total loss: [1m[32m0.26458[0m[0m | time: 63.044s
[2K
| Adam | epoch: 007 | loss: 0.26458 - acc: 0.8921 -- iter: 1824/2005
[A[ATraining Step: 436  | total loss: [1m[32m0.26282[0m[0m | time: 64.078s
[2K
| Adam | epoch: 007 | loss: 0.26282 - acc: 0.8936 -- iter: 1856/2005
[A[ATraining Step: 437  | total loss: [1m[32m0.26877[0m[0m | time: 65.186s
[2K
| Adam | epoch: 007 | loss: 0.26877 - acc: 0.8917 -- iter: 1888/2005
[A[ATraining Step: 438  | total loss: [1m[32m0.26505[0m[0m | time: 66.188s
[2K
| Adam | epoch: 007 | loss: 0.26505 - acc: 0.8932 -- iter: 1920/2005
[A[ATraining Step: 439  | total loss: [1m[32m0.26078[0m[0m | time: 67.237s
[2K
| Adam | epoch: 007 | loss: 0.26078 - acc: 0.8913 -- iter: 1952/2005
[A[ATraining Step: 440  | total loss: [1m[32m0.24005[0m[0m | time: 68.670s
[2K
| Adam | epoch: 007 | loss: 0.24005 - acc: 0.9022 -- iter: 1984/2005
[A[ATraining Step: 441  | total loss: [1m[32m0.22863[0m[0m | time: 72.911s
[2K
| Adam | epoch: 007 | loss: 0.22863 - acc: 0.9089 | val_loss: 0.39733 - val_acc: 0.8246 -- iter: 2005/2005
--
Training Step: 442  | total loss: [1m[32m0.23802[0m[0m | time: 1.125s
[2K
| Adam | epoch: 008 | loss: 0.23802 - acc: 0.8961 -- iter: 0032/2005
[A[ATraining Step: 443  | total loss: [1m[32m0.23708[0m[0m | time: 2.164s
[2K
| Adam | epoch: 008 | loss: 0.23708 - acc: 0.9002 -- iter: 0064/2005
[A[ATraining Step: 444  | total loss: [1m[32m0.25075[0m[0m | time: 3.063s
[2K
| Adam | epoch: 008 | loss: 0.25075 - acc: 0.9008 -- iter: 0096/2005
[A[ATraining Step: 445  | total loss: [1m[32m0.25553[0m[0m | time: 4.252s
[2K
| Adam | epoch: 008 | loss: 0.25553 - acc: 0.8983 -- iter: 0128/2005
[A[ATraining Step: 446  | total loss: [1m[32m0.24667[0m[0m | time: 5.540s
[2K
| Adam | epoch: 008 | loss: 0.24667 - acc: 0.9022 -- iter: 0160/2005
[A[ATraining Step: 447  | total loss: [1m[32m0.24777[0m[0m | time: 6.452s
[2K
| Adam | epoch: 008 | loss: 0.24777 - acc: 0.9026 -- iter: 0192/2005
[A[ATraining Step: 448  | total loss: [1m[32m0.24387[0m[0m | time: 7.209s
[2K
| Adam | epoch: 008 | loss: 0.24387 - acc: 0.9028 -- iter: 0224/2005
[A[ATraining Step: 449  | total loss: [1m[32m0.23466[0m[0m | time: 8.052s
[2K
| Adam | epoch: 008 | loss: 0.23466 - acc: 0.9078 -- iter: 0256/2005
[A[ATraining Step: 450  | total loss: [1m[32m0.22778[0m[0m | time: 8.950s
[2K
| Adam | epoch: 008 | loss: 0.22778 - acc: 0.9107 -- iter: 0288/2005
[A[ATraining Step: 451  | total loss: [1m[32m0.23083[0m[0m | time: 9.869s
[2K
| Adam | epoch: 008 | loss: 0.23083 - acc: 0.9103 -- iter: 0320/2005
[A[ATraining Step: 452  | total loss: [1m[32m0.23390[0m[0m | time: 10.787s
[2K
| Adam | epoch: 008 | loss: 0.23390 - acc: 0.9068 -- iter: 0352/2005
[A[ATraining Step: 453  | total loss: [1m[32m0.26672[0m[0m | time: 11.812s
[2K
| Adam | epoch: 008 | loss: 0.26672 - acc: 0.8942 -- iter: 0384/2005
[A[ATraining Step: 454  | total loss: [1m[32m0.28316[0m[0m | time: 12.954s
[2K
| Adam | epoch: 008 | loss: 0.28316 - acc: 0.8892 -- iter: 0416/2005
[A[ATraining Step: 455  | total loss: [1m[32m0.29002[0m[0m | time: 13.851s
[2K
| Adam | epoch: 008 | loss: 0.29002 - acc: 0.8846 -- iter: 0448/2005
[A[ATraining Step: 456  | total loss: [1m[32m0.30771[0m[0m | time: 14.974s
[2K
| Adam | epoch: 008 | loss: 0.30771 - acc: 0.8743 -- iter: 0480/2005
[A[ATraining Step: 457  | total loss: [1m[32m0.30387[0m[0m | time: 16.208s
[2K
| Adam | epoch: 008 | loss: 0.30387 - acc: 0.8775 -- iter: 0512/2005
[A[ATraining Step: 458  | total loss: [1m[32m0.29864[0m[0m | time: 17.525s
[2K
| Adam | epoch: 008 | loss: 0.29864 - acc: 0.8835 -- iter: 0544/2005
[A[ATraining Step: 459  | total loss: [1m[32m0.28684[0m[0m | time: 18.608s
[2K
| Adam | epoch: 008 | loss: 0.28684 - acc: 0.8858 -- iter: 0576/2005
[A[ATraining Step: 460  | total loss: [1m[32m0.29222[0m[0m | time: 19.482s
[2K
| Adam | epoch: 008 | loss: 0.29222 - acc: 0.8878 -- iter: 0608/2005
[A[ATraining Step: 461  | total loss: [1m[32m0.29694[0m[0m | time: 20.470s
[2K
| Adam | epoch: 008 | loss: 0.29694 - acc: 0.8803 -- iter: 0640/2005
[A[ATraining Step: 462  | total loss: [1m[32m0.28420[0m[0m | time: 21.452s
[2K
| Adam | epoch: 008 | loss: 0.28420 - acc: 0.8891 -- iter: 0672/2005
[A[ATraining Step: 463  | total loss: [1m[32m0.26939[0m[0m | time: 22.422s
[2K
| Adam | epoch: 008 | loss: 0.26939 - acc: 0.8971 -- iter: 0704/2005
[A[ATraining Step: 464  | total loss: [1m[32m0.26293[0m[0m | time: 23.444s
[2K
| Adam | epoch: 008 | loss: 0.26293 - acc: 0.9011 -- iter: 0736/2005
[A[ATraining Step: 465  | total loss: [1m[32m0.27358[0m[0m | time: 24.493s
[2K
| Adam | epoch: 008 | loss: 0.27358 - acc: 0.8954 -- iter: 0768/2005
[A[ATraining Step: 466  | total loss: [1m[32m0.27935[0m[0m | time: 25.353s
[2K
| Adam | epoch: 008 | loss: 0.27935 - acc: 0.8934 -- iter: 0800/2005
[A[ATraining Step: 467  | total loss: [1m[32m0.27287[0m[0m | time: 26.725s
[2K
| Adam | epoch: 008 | loss: 0.27287 - acc: 0.8978 -- iter: 0832/2005
[A[ATraining Step: 468  | total loss: [1m[32m0.28004[0m[0m | time: 28.148s
[2K
| Adam | epoch: 008 | loss: 0.28004 - acc: 0.8892 -- iter: 0864/2005
[A[ATraining Step: 469  | total loss: [1m[32m0.28128[0m[0m | time: 29.352s
[2K
| Adam | epoch: 008 | loss: 0.28128 - acc: 0.8909 -- iter: 0896/2005
[A[ATraining Step: 470  | total loss: [1m[32m0.29160[0m[0m | time: 30.181s
[2K
| Adam | epoch: 008 | loss: 0.29160 - acc: 0.8862 -- iter: 0928/2005
[A[ATraining Step: 471  | total loss: [1m[32m0.29129[0m[0m | time: 31.104s
[2K
| Adam | epoch: 008 | loss: 0.29129 - acc: 0.8820 -- iter: 0960/2005
[A[ATraining Step: 472  | total loss: [1m[32m0.28607[0m[0m | time: 32.041s
[2K
| Adam | epoch: 008 | loss: 0.28607 - acc: 0.8907 -- iter: 0992/2005
[A[ATraining Step: 473  | total loss: [1m[32m0.30402[0m[0m | time: 33.073s
[2K
| Adam | epoch: 008 | loss: 0.30402 - acc: 0.8828 -- iter: 1024/2005
[A[ATraining Step: 474  | total loss: [1m[32m0.29356[0m[0m | time: 34.159s
[2K
| Adam | epoch: 008 | loss: 0.29356 - acc: 0.8852 -- iter: 1056/2005
[A[ATraining Step: 475  | total loss: [1m[32m0.29602[0m[0m | time: 35.238s
[2K
| Adam | epoch: 008 | loss: 0.29602 - acc: 0.8779 -- iter: 1088/2005
[A[ATraining Step: 476  | total loss: [1m[32m0.28754[0m[0m | time: 36.137s
[2K
| Adam | epoch: 008 | loss: 0.28754 - acc: 0.8870 -- iter: 1120/2005
[A[ATraining Step: 477  | total loss: [1m[32m0.28688[0m[0m | time: 37.332s
[2K
| Adam | epoch: 008 | loss: 0.28688 - acc: 0.8827 -- iter: 1152/2005
[A[ATraining Step: 478  | total loss: [1m[32m0.28274[0m[0m | time: 38.519s
[2K
| Adam | epoch: 008 | loss: 0.28274 - acc: 0.8850 -- iter: 1184/2005
[A[ATraining Step: 479  | total loss: [1m[32m0.27720[0m[0m | time: 39.894s
[2K
| Adam | epoch: 008 | loss: 0.27720 - acc: 0.8903 -- iter: 1216/2005
[A[ATraining Step: 480  | total loss: [1m[32m0.31989[0m[0m | time: 40.860s
[2K
| Adam | epoch: 008 | loss: 0.31989 - acc: 0.8794 -- iter: 1248/2005
[A[ATraining Step: 481  | total loss: [1m[32m0.30084[0m[0m | time: 41.784s
[2K
| Adam | epoch: 008 | loss: 0.30084 - acc: 0.8883 -- iter: 1280/2005
[A[ATraining Step: 482  | total loss: [1m[32m0.28307[0m[0m | time: 42.744s
[2K
| Adam | epoch: 008 | loss: 0.28307 - acc: 0.8964 -- iter: 1312/2005
[A[ATraining Step: 483  | total loss: [1m[32m0.28371[0m[0m | time: 43.753s
[2K
| Adam | epoch: 008 | loss: 0.28371 - acc: 0.8942 -- iter: 1344/2005
[A[ATraining Step: 484  | total loss: [1m[32m0.28214[0m[0m | time: 44.757s
[2K
| Adam | epoch: 008 | loss: 0.28214 - acc: 0.8923 -- iter: 1376/2005
[A[ATraining Step: 485  | total loss: [1m[32m0.26447[0m[0m | time: 45.866s
[2K
| Adam | epoch: 008 | loss: 0.26447 - acc: 0.8999 -- iter: 1408/2005
[A[ATraining Step: 486  | total loss: [1m[32m0.26888[0m[0m | time: 46.844s
[2K
| Adam | epoch: 008 | loss: 0.26888 - acc: 0.9037 -- iter: 1440/2005
[A[ATraining Step: 487  | total loss: [1m[32m0.26458[0m[0m | time: 47.902s
[2K
| Adam | epoch: 008 | loss: 0.26458 - acc: 0.9008 -- iter: 1472/2005
[A[ATraining Step: 488  | total loss: [1m[32m0.26719[0m[0m | time: 49.201s
[2K
| Adam | epoch: 008 | loss: 0.26719 - acc: 0.8951 -- iter: 1504/2005
[A[ATraining Step: 489  | total loss: [1m[32m0.25709[0m[0m | time: 50.565s
[2K
| Adam | epoch: 008 | loss: 0.25709 - acc: 0.8962 -- iter: 1536/2005
[A[ATraining Step: 490  | total loss: [1m[32m0.23652[0m[0m | time: 51.646s
[2K
| Adam | epoch: 008 | loss: 0.23652 - acc: 0.9066 -- iter: 1568/2005
[A[ATraining Step: 491  | total loss: [1m[32m0.25548[0m[0m | time: 52.508s
[2K
| Adam | epoch: 008 | loss: 0.25548 - acc: 0.9034 -- iter: 1600/2005
[A[ATraining Step: 492  | total loss: [1m[32m0.25414[0m[0m | time: 53.473s
[2K
| Adam | epoch: 008 | loss: 0.25414 - acc: 0.9037 -- iter: 1632/2005
[A[ATraining Step: 493  | total loss: [1m[32m0.23969[0m[0m | time: 54.504s
[2K
| Adam | epoch: 008 | loss: 0.23969 - acc: 0.9102 -- iter: 1664/2005
[A[ATraining Step: 494  | total loss: [1m[32m0.23484[0m[0m | time: 55.463s
[2K
| Adam | epoch: 008 | loss: 0.23484 - acc: 0.9098 -- iter: 1696/2005
[A[ATraining Step: 495  | total loss: [1m[32m0.22748[0m[0m | time: 56.622s
[2K
| Adam | epoch: 008 | loss: 0.22748 - acc: 0.9157 -- iter: 1728/2005
[A[ATraining Step: 496  | total loss: [1m[32m0.23338[0m[0m | time: 57.666s
[2K
| Adam | epoch: 008 | loss: 0.23338 - acc: 0.9085 -- iter: 1760/2005
[A[ATraining Step: 497  | total loss: [1m[32m0.23217[0m[0m | time: 58.591s
[2K
| Adam | epoch: 008 | loss: 0.23217 - acc: 0.9145 -- iter: 1792/2005
[A[ATraining Step: 498  | total loss: [1m[32m0.22865[0m[0m | time: 59.930s
[2K
| Adam | epoch: 008 | loss: 0.22865 - acc: 0.9168 -- iter: 1824/2005
[A[ATraining Step: 499  | total loss: [1m[32m0.21666[0m[0m | time: 61.222s
[2K
| Adam | epoch: 008 | loss: 0.21666 - acc: 0.9220 -- iter: 1856/2005
[A[ATraining Step: 500  | total loss: [1m[32m0.21238[0m[0m | time: 62.447s
[2K
| Adam | epoch: 008 | loss: 0.21238 - acc: 0.9267 -- iter: 1888/2005
[A[ATraining Step: 501  | total loss: [1m[32m0.20376[0m[0m | time: 63.356s
[2K
| Adam | epoch: 008 | loss: 0.20376 - acc: 0.9309 -- iter: 1920/2005
[A[ATraining Step: 502  | total loss: [1m[32m0.18713[0m[0m | time: 64.341s
[2K
| Adam | epoch: 008 | loss: 0.18713 - acc: 0.9378 -- iter: 1952/2005
[A[ATraining Step: 503  | total loss: [1m[32m0.19771[0m[0m | time: 65.320s
[2K
| Adam | epoch: 008 | loss: 0.19771 - acc: 0.9315 -- iter: 1984/2005
[A[ATraining Step: 504  | total loss: [1m[32m0.19740[0m[0m | time: 70.008s
[2K
| Adam | epoch: 008 | loss: 0.19740 - acc: 0.9321 | val_loss: 0.36903 - val_acc: 0.8421 -- iter: 2005/2005
--
Training Step: 505  | total loss: [1m[32m0.19469[0m[0m | time: 1.030s
[2K
| Adam | epoch: 009 | loss: 0.19469 - acc: 0.9327 -- iter: 0032/2005
[A[ATraining Step: 506  | total loss: [1m[32m0.18599[0m[0m | time: 1.945s
[2K
| Adam | epoch: 009 | loss: 0.18599 - acc: 0.9363 -- iter: 0064/2005
[A[ATraining Step: 507  | total loss: [1m[32m0.18059[0m[0m | time: 2.971s
[2K
| Adam | epoch: 009 | loss: 0.18059 - acc: 0.9395 -- iter: 0096/2005
[A[ATraining Step: 508  | total loss: [1m[32m0.17335[0m[0m | time: 3.973s
[2K
| Adam | epoch: 009 | loss: 0.17335 - acc: 0.9393 -- iter: 0128/2005
[A[ATraining Step: 509  | total loss: [1m[32m0.17266[0m[0m | time: 5.004s
[2K
| Adam | epoch: 009 | loss: 0.17266 - acc: 0.9360 -- iter: 0160/2005
[A[ATraining Step: 510  | total loss: [1m[32m0.18579[0m[0m | time: 6.170s
[2K
| Adam | epoch: 009 | loss: 0.18579 - acc: 0.9330 -- iter: 0192/2005
[A[ATraining Step: 511  | total loss: [1m[32m0.19307[0m[0m | time: 6.871s
[2K
| Adam | epoch: 009 | loss: 0.19307 - acc: 0.9335 -- iter: 0224/2005
[A[ATraining Step: 512  | total loss: [1m[32m0.20722[0m[0m | time: 7.488s
[2K
| Adam | epoch: 009 | loss: 0.20722 - acc: 0.9211 -- iter: 0256/2005
[A[ATraining Step: 513  | total loss: [1m[32m0.21282[0m[0m | time: 8.905s
[2K
| Adam | epoch: 009 | loss: 0.21282 - acc: 0.9147 -- iter: 0288/2005
[A[ATraining Step: 514  | total loss: [1m[32m0.19745[0m[0m | time: 10.264s
[2K
| Adam | epoch: 009 | loss: 0.19745 - acc: 0.9201 -- iter: 0320/2005
[A[ATraining Step: 515  | total loss: [1m[32m0.19633[0m[0m | time: 11.557s
[2K
| Adam | epoch: 009 | loss: 0.19633 - acc: 0.9218 -- iter: 0352/2005
[A[ATraining Step: 516  | total loss: [1m[32m0.19561[0m[0m | time: 12.372s
[2K
| Adam | epoch: 009 | loss: 0.19561 - acc: 0.9234 -- iter: 0384/2005
[A[ATraining Step: 517  | total loss: [1m[32m0.18417[0m[0m | time: 13.301s
[2K
| Adam | epoch: 009 | loss: 0.18417 - acc: 0.9311 -- iter: 0416/2005
[A[ATraining Step: 518  | total loss: [1m[32m0.17933[0m[0m | time: 14.257s
[2K
| Adam | epoch: 009 | loss: 0.17933 - acc: 0.9286 -- iter: 0448/2005
[A[ATraining Step: 519  | total loss: [1m[32m0.16944[0m[0m | time: 15.214s
[2K
| Adam | epoch: 009 | loss: 0.16944 - acc: 0.9357 -- iter: 0480/2005
[A[ATraining Step: 520  | total loss: [1m[32m0.18920[0m[0m | time: 16.253s
[2K
| Adam | epoch: 009 | loss: 0.18920 - acc: 0.9265 -- iter: 0512/2005
[A[ATraining Step: 521  | total loss: [1m[32m0.19720[0m[0m | time: 17.355s
[2K
| Adam | epoch: 009 | loss: 0.19720 - acc: 0.9214 -- iter: 0544/2005
[A[ATraining Step: 522  | total loss: [1m[32m0.18752[0m[0m | time: 18.364s
[2K
| Adam | epoch: 009 | loss: 0.18752 - acc: 0.9261 -- iter: 0576/2005
[A[ATraining Step: 523  | total loss: [1m[32m0.21521[0m[0m | time: 19.383s
[2K
| Adam | epoch: 009 | loss: 0.21521 - acc: 0.9148 -- iter: 0608/2005
[A[ATraining Step: 524  | total loss: [1m[32m0.32760[0m[0m | time: 20.765s
[2K
| Adam | epoch: 009 | loss: 0.32760 - acc: 0.8702 -- iter: 0640/2005
[A[ATraining Step: 525  | total loss: [1m[32m0.35434[0m[0m | time: 21.977s
[2K
| Adam | epoch: 009 | loss: 0.35434 - acc: 0.8613 -- iter: 0672/2005
[A[ATraining Step: 526  | total loss: [1m[32m0.32967[0m[0m | time: 23.046s
[2K
| Adam | epoch: 009 | loss: 0.32967 - acc: 0.8720 -- iter: 0704/2005
[A[ATraining Step: 527  | total loss: [1m[32m0.32071[0m[0m | time: 23.968s
[2K
| Adam | epoch: 009 | loss: 0.32071 - acc: 0.8723 -- iter: 0736/2005
[A[ATraining Step: 528  | total loss: [1m[32m0.30505[0m[0m | time: 24.942s
[2K
| Adam | epoch: 009 | loss: 0.30505 - acc: 0.8788 -- iter: 0768/2005
[A[ATraining Step: 529  | total loss: [1m[32m0.29499[0m[0m | time: 25.878s
[2K
| Adam | epoch: 009 | loss: 0.29499 - acc: 0.8816 -- iter: 0800/2005
[A[ATraining Step: 530  | total loss: [1m[32m0.27969[0m[0m | time: 26.888s
[2K
| Adam | epoch: 009 | loss: 0.27969 - acc: 0.8872 -- iter: 0832/2005
[A[ATraining Step: 531  | total loss: [1m[32m0.26467[0m[0m | time: 28.013s
[2K
| Adam | epoch: 009 | loss: 0.26467 - acc: 0.8922 -- iter: 0864/2005
[A[ATraining Step: 532  | total loss: [1m[32m0.25586[0m[0m | time: 29.079s
[2K
| Adam | epoch: 009 | loss: 0.25586 - acc: 0.8999 -- iter: 0896/2005
[A[ATraining Step: 533  | total loss: [1m[32m0.27901[0m[0m | time: 29.959s
[2K
| Adam | epoch: 009 | loss: 0.27901 - acc: 0.8880 -- iter: 0928/2005
[A[ATraining Step: 534  | total loss: [1m[32m0.26166[0m[0m | time: 31.312s
[2K
| Adam | epoch: 009 | loss: 0.26166 - acc: 0.8961 -- iter: 0960/2005
[A[ATraining Step: 535  | total loss: [1m[32m0.25489[0m[0m | time: 32.679s
[2K
| Adam | epoch: 009 | loss: 0.25489 - acc: 0.9002 -- iter: 0992/2005
[A[ATraining Step: 536  | total loss: [1m[32m0.24412[0m[0m | time: 33.897s
[2K
| Adam | epoch: 009 | loss: 0.24412 - acc: 0.9039 -- iter: 1024/2005
[A[ATraining Step: 537  | total loss: [1m[32m0.23153[0m[0m | time: 34.728s
[2K
| Adam | epoch: 009 | loss: 0.23153 - acc: 0.9104 -- iter: 1056/2005
[A[ATraining Step: 538  | total loss: [1m[32m0.22947[0m[0m | time: 35.686s
[2K
| Adam | epoch: 009 | loss: 0.22947 - acc: 0.9163 -- iter: 1088/2005
[A[ATraining Step: 539  | total loss: [1m[32m0.24084[0m[0m | time: 36.646s
[2K
| Adam | epoch: 009 | loss: 0.24084 - acc: 0.9121 -- iter: 1120/2005
[A[ATraining Step: 540  | total loss: [1m[32m0.23586[0m[0m | time: 37.674s
[2K
| Adam | epoch: 009 | loss: 0.23586 - acc: 0.9147 -- iter: 1152/2005
[A[ATraining Step: 541  | total loss: [1m[32m0.23506[0m[0m | time: 38.721s
[2K
| Adam | epoch: 009 | loss: 0.23506 - acc: 0.9169 -- iter: 1184/2005
[A[ATraining Step: 542  | total loss: [1m[32m0.24904[0m[0m | time: 39.835s
[2K
| Adam | epoch: 009 | loss: 0.24904 - acc: 0.9034 -- iter: 1216/2005
[A[ATraining Step: 543  | total loss: [1m[32m0.26539[0m[0m | time: 40.838s
[2K
| Adam | epoch: 009 | loss: 0.26539 - acc: 0.8974 -- iter: 1248/2005
[A[ATraining Step: 544  | total loss: [1m[32m0.26222[0m[0m | time: 41.892s
[2K
| Adam | epoch: 009 | loss: 0.26222 - acc: 0.8983 -- iter: 1280/2005
[A[ATraining Step: 545  | total loss: [1m[32m0.26941[0m[0m | time: 43.222s
[2K
| Adam | epoch: 009 | loss: 0.26941 - acc: 0.8928 -- iter: 1312/2005
[A[ATraining Step: 546  | total loss: [1m[32m0.26528[0m[0m | time: 44.482s
[2K
| Adam | epoch: 009 | loss: 0.26528 - acc: 0.8911 -- iter: 1344/2005
[A[ATraining Step: 547  | total loss: [1m[32m0.27426[0m[0m | time: 45.621s
[2K
| Adam | epoch: 009 | loss: 0.27426 - acc: 0.8832 -- iter: 1376/2005
[A[ATraining Step: 548  | total loss: [1m[32m0.26587[0m[0m | time: 46.516s
[2K
| Adam | epoch: 009 | loss: 0.26587 - acc: 0.8824 -- iter: 1408/2005
[A[ATraining Step: 549  | total loss: [1m[32m0.27508[0m[0m | time: 47.539s
[2K
| Adam | epoch: 009 | loss: 0.27508 - acc: 0.8785 -- iter: 1440/2005
[A[ATraining Step: 550  | total loss: [1m[32m0.27530[0m[0m | time: 48.535s
[2K
| Adam | epoch: 009 | loss: 0.27530 - acc: 0.8844 -- iter: 1472/2005
[A[ATraining Step: 551  | total loss: [1m[32m0.26194[0m[0m | time: 49.484s
[2K
| Adam | epoch: 009 | loss: 0.26194 - acc: 0.8929 -- iter: 1504/2005
[A[ATraining Step: 552  | total loss: [1m[32m0.25738[0m[0m | time: 50.801s
[2K
| Adam | epoch: 009 | loss: 0.25738 - acc: 0.8942 -- iter: 1536/2005
[A[ATraining Step: 553  | total loss: [1m[32m0.25533[0m[0m | time: 51.901s
[2K
| Adam | epoch: 009 | loss: 0.25533 - acc: 0.8954 -- iter: 1568/2005
[A[ATraining Step: 554  | total loss: [1m[32m0.24034[0m[0m | time: 52.753s
[2K
| Adam | epoch: 009 | loss: 0.24034 - acc: 0.8996 -- iter: 1600/2005
[A[ATraining Step: 555  | total loss: [1m[32m0.23577[0m[0m | time: 54.113s
[2K
| Adam | epoch: 009 | loss: 0.23577 - acc: 0.9034 -- iter: 1632/2005
[A[ATraining Step: 556  | total loss: [1m[32m0.22727[0m[0m | time: 55.438s
[2K
| Adam | epoch: 009 | loss: 0.22727 - acc: 0.9099 -- iter: 1664/2005
[A[ATraining Step: 557  | total loss: [1m[32m0.22020[0m[0m | time: 56.693s
[2K
| Adam | epoch: 009 | loss: 0.22020 - acc: 0.9127 -- iter: 1696/2005
[A[ATraining Step: 558  | total loss: [1m[32m0.21578[0m[0m | time: 57.507s
[2K
| Adam | epoch: 009 | loss: 0.21578 - acc: 0.9183 -- iter: 1728/2005
[A[ATraining Step: 559  | total loss: [1m[32m0.22138[0m[0m | time: 58.445s
[2K
| Adam | epoch: 009 | loss: 0.22138 - acc: 0.9202 -- iter: 1760/2005
[A[ATraining Step: 560  | total loss: [1m[32m0.21482[0m[0m | time: 59.388s
[2K
| Adam | epoch: 009 | loss: 0.21482 - acc: 0.9251 -- iter: 1792/2005
[A[ATraining Step: 561  | total loss: [1m[32m0.20631[0m[0m | time: 60.337s
[2K
| Adam | epoch: 009 | loss: 0.20631 - acc: 0.9294 -- iter: 1824/2005
[A[ATraining Step: 562  | total loss: [1m[32m0.19785[0m[0m | time: 61.340s
[2K
| Adam | epoch: 009 | loss: 0.19785 - acc: 0.9302 -- iter: 1856/2005
[A[ATraining Step: 563  | total loss: [1m[32m0.18707[0m[0m | time: 62.461s
[2K
| Adam | epoch: 009 | loss: 0.18707 - acc: 0.9310 -- iter: 1888/2005
[A[ATraining Step: 564  | total loss: [1m[32m0.18928[0m[0m | time: 63.526s
[2K
| Adam | epoch: 009 | loss: 0.18928 - acc: 0.9316 -- iter: 1920/2005
[A[ATraining Step: 565  | total loss: [1m[32m0.18336[0m[0m | time: 64.393s
[2K
| Adam | epoch: 009 | loss: 0.18336 - acc: 0.9353 -- iter: 1952/2005
[A[ATraining Step: 566  | total loss: [1m[32m0.17293[0m[0m | time: 65.708s
[2K
| Adam | epoch: 009 | loss: 0.17293 - acc: 0.9387 -- iter: 1984/2005
[A[ATraining Step: 567  | total loss: [1m[32m0.18320[0m[0m | time: 70.078s
[2K
| Adam | epoch: 009 | loss: 0.18320 - acc: 0.9261 | val_loss: 0.47330 - val_acc: 0.8134 -- iter: 2005/2005
--
Training Step: 568  | total loss: [1m[32m0.19973[0m[0m | time: 0.944s
[2K
| Adam | epoch: 010 | loss: 0.19973 - acc: 0.9210 -- iter: 0032/2005
[A[ATraining Step: 569  | total loss: [1m[32m0.18538[0m[0m | time: 1.878s
[2K
| Adam | epoch: 010 | loss: 0.18538 - acc: 0.9289 -- iter: 0064/2005
[A[ATraining Step: 570  | total loss: [1m[32m0.18243[0m[0m | time: 2.945s
[2K
| Adam | epoch: 010 | loss: 0.18243 - acc: 0.9297 -- iter: 0096/2005
[A[ATraining Step: 571  | total loss: [1m[32m0.16767[0m[0m | time: 4.073s
[2K
| Adam | epoch: 010 | loss: 0.16767 - acc: 0.9367 -- iter: 0128/2005
[A[ATraining Step: 572  | total loss: [1m[32m0.16654[0m[0m | time: 4.923s
[2K
| Adam | epoch: 010 | loss: 0.16654 - acc: 0.9368 -- iter: 0160/2005
[A[ATraining Step: 573  | total loss: [1m[32m0.19796[0m[0m | time: 6.340s
[2K
| Adam | epoch: 010 | loss: 0.19796 - acc: 0.9244 -- iter: 0192/2005
[A[ATraining Step: 574  | total loss: [1m[32m0.22837[0m[0m | time: 7.628s
[2K
| Adam | epoch: 010 | loss: 0.22837 - acc: 0.9163 -- iter: 0224/2005
[A[ATraining Step: 575  | total loss: [1m[32m0.21681[0m[0m | time: 8.540s
[2K
| Adam | epoch: 010 | loss: 0.21681 - acc: 0.9184 -- iter: 0256/2005
[A[ATraining Step: 576  | total loss: [1m[32m0.23126[0m[0m | time: 9.226s
[2K
| Adam | epoch: 010 | loss: 0.23126 - acc: 0.9123 -- iter: 0288/2005
[A[ATraining Step: 577  | total loss: [1m[32m0.25349[0m[0m | time: 10.089s
[2K
| Adam | epoch: 010 | loss: 0.25349 - acc: 0.9020 -- iter: 0320/2005
[A[ATraining Step: 578  | total loss: [1m[32m0.26034[0m[0m | time: 11.047s
[2K
| Adam | epoch: 010 | loss: 0.26034 - acc: 0.9025 -- iter: 0352/2005
[A[ATraining Step: 579  | total loss: [1m[32m0.27534[0m[0m | time: 12.030s
[2K
| Adam | epoch: 010 | loss: 0.27534 - acc: 0.8935 -- iter: 0384/2005
[A[ATraining Step: 580  | total loss: [1m[32m0.26692[0m[0m | time: 13.015s
[2K
| Adam | epoch: 010 | loss: 0.26692 - acc: 0.8979 -- iter: 0416/2005
[A[ATraining Step: 581  | total loss: [1m[32m0.25849[0m[0m | time: 14.109s
[2K
| Adam | epoch: 010 | loss: 0.25849 - acc: 0.9018 -- iter: 0448/2005
[A[ATraining Step: 582  | total loss: [1m[32m0.25267[0m[0m | time: 15.125s
[2K
| Adam | epoch: 010 | loss: 0.25267 - acc: 0.9054 -- iter: 0480/2005
[A[ATraining Step: 583  | total loss: [1m[32m0.24433[0m[0m | time: 16.137s
[2K
| Adam | epoch: 010 | loss: 0.24433 - acc: 0.9086 -- iter: 0512/2005
[A[ATraining Step: 584  | total loss: [1m[32m0.23763[0m[0m | time: 17.567s
[2K
| Adam | epoch: 010 | loss: 0.23763 - acc: 0.9146 -- iter: 0544/2005
[A[ATraining Step: 585  | total loss: [1m[32m0.24127[0m[0m | time: 18.905s
[2K
| Adam | epoch: 010 | loss: 0.24127 - acc: 0.9138 -- iter: 0576/2005
[A[ATraining Step: 586  | total loss: [1m[32m0.22777[0m[0m | time: 19.951s
[2K
| Adam | epoch: 010 | loss: 0.22777 - acc: 0.9224 -- iter: 0608/2005
[A[ATraining Step: 587  | total loss: [1m[32m0.22295[0m[0m | time: 20.831s
[2K
| Adam | epoch: 010 | loss: 0.22295 - acc: 0.9239 -- iter: 0640/2005
[A[ATraining Step: 588  | total loss: [1m[32m0.23077[0m[0m | time: 21.779s
[2K
| Adam | epoch: 010 | loss: 0.23077 - acc: 0.9190 -- iter: 0672/2005
[A[ATraining Step: 589  | total loss: [1m[32m0.21969[0m[0m | time: 22.721s
[2K
| Adam | epoch: 010 | loss: 0.21969 - acc: 0.9240 -- iter: 0704/2005
[A[ATraining Step: 590  | total loss: [1m[32m0.22789[0m[0m | time: 23.743s
[2K
| Adam | epoch: 010 | loss: 0.22789 - acc: 0.9222 -- iter: 0736/2005
[A[ATraining Step: 591  | total loss: [1m[32m0.21832[0m[0m | time: 24.733s
[2K
| Adam | epoch: 010 | loss: 0.21832 - acc: 0.9269 -- iter: 0768/2005
[A[ATraining Step: 592  | total loss: [1m[32m0.22151[0m[0m | time: 25.641s
[2K
| Adam | epoch: 010 | loss: 0.22151 - acc: 0.9248 -- iter: 0800/2005
[A[ATraining Step: 593  | total loss: [1m[32m0.21771[0m[0m | time: 26.577s
[2K
| Adam | epoch: 010 | loss: 0.21771 - acc: 0.9292 -- iter: 0832/2005
[A[ATraining Step: 594  | total loss: [1m[32m0.20894[0m[0m | time: 27.856s
[2K
| Adam | epoch: 010 | loss: 0.20894 - acc: 0.9332 -- iter: 0864/2005
[A[ATraining Step: 595  | total loss: [1m[32m0.21402[0m[0m | time: 29.135s
[2K
| Adam | epoch: 010 | loss: 0.21402 - acc: 0.9336 -- iter: 0896/2005
[A[ATraining Step: 596  | total loss: [1m[32m0.20839[0m[0m | time: 30.198s
[2K
| Adam | epoch: 010 | loss: 0.20839 - acc: 0.9277 -- iter: 0928/2005
[A[ATraining Step: 597  | total loss: [1m[32m0.19344[0m[0m | time: 31.027s
[2K
| Adam | epoch: 010 | loss: 0.19344 - acc: 0.9350 -- iter: 0960/2005
[A[ATraining Step: 598  | total loss: [1m[32m0.18256[0m[0m | time: 31.970s
[2K
| Adam | epoch: 010 | loss: 0.18256 - acc: 0.9383 -- iter: 0992/2005
[A[ATraining Step: 599  | total loss: [1m[32m0.17340[0m[0m | time: 32.906s
[2K
| Adam | epoch: 010 | loss: 0.17340 - acc: 0.9445 -- iter: 1024/2005
[A[ATraining Step: 600  | total loss: [1m[32m0.17402[0m[0m | time: 37.583s
[2K
| Adam | epoch: 010 | loss: 0.17402 - acc: 0.9376 | val_loss: 0.38494 - val_acc: 0.8612 -- iter: 1056/2005
--
Training Step: 601  | total loss: [1m[32m0.17196[0m[0m | time: 38.951s
[2K
| Adam | epoch: 010 | loss: 0.17196 - acc: 0.9407 -- iter: 1088/2005
[A[ATraining Step: 602  | total loss: [1m[32m0.16547[0m[0m | time: 39.853s
[2K
| Adam | epoch: 010 | loss: 0.16547 - acc: 0.9435 -- iter: 1120/2005
[A[ATraining Step: 603  | total loss: [1m[32m0.15693[0m[0m | time: 40.768s
[2K
| Adam | epoch: 010 | loss: 0.15693 - acc: 0.9460 -- iter: 1152/2005
[A[ATraining Step: 604  | total loss: [1m[32m0.15735[0m[0m | time: 41.792s
[2K
| Adam | epoch: 010 | loss: 0.15735 - acc: 0.9452 -- iter: 1184/2005
[A[ATraining Step: 605  | total loss: [1m[32m0.14847[0m[0m | time: 42.761s
[2K
| Adam | epoch: 010 | loss: 0.14847 - acc: 0.9475 -- iter: 1216/2005
[A[ATraining Step: 606  | total loss: [1m[32m0.14410[0m[0m | time: 43.786s
[2K
| Adam | epoch: 010 | loss: 0.14410 - acc: 0.9496 -- iter: 1248/2005
[A[ATraining Step: 607  | total loss: [1m[32m0.13930[0m[0m | time: 44.852s
[2K
| Adam | epoch: 010 | loss: 0.13930 - acc: 0.9516 -- iter: 1280/2005
[A[ATraining Step: 608  | total loss: [1m[32m0.16059[0m[0m | time: 45.755s
[2K
| Adam | epoch: 010 | loss: 0.16059 - acc: 0.9439 -- iter: 1312/2005
[A[ATraining Step: 609  | total loss: [1m[32m0.16979[0m[0m | time: 46.796s
[2K
| Adam | epoch: 010 | loss: 0.16979 - acc: 0.9433 -- iter: 1344/2005
[A[ATraining Step: 610  | total loss: [1m[32m0.15666[0m[0m | time: 48.035s
[2K
| Adam | epoch: 010 | loss: 0.15666 - acc: 0.9489 -- iter: 1376/2005
[A[ATraining Step: 611  | total loss: [1m[32m0.15919[0m[0m | time: 49.355s
[2K
| Adam | epoch: 010 | loss: 0.15919 - acc: 0.9509 -- iter: 1408/2005
[A[ATraining Step: 612  | total loss: [1m[32m0.14793[0m[0m | time: 50.334s
[2K
| Adam | epoch: 010 | loss: 0.14793 - acc: 0.9558 -- iter: 1440/2005
[A[ATraining Step: 613  | total loss: [1m[32m0.13891[0m[0m | time: 51.248s
[2K
| Adam | epoch: 010 | loss: 0.13891 - acc: 0.9602 -- iter: 1472/2005
[A[ATraining Step: 614  | total loss: [1m[32m0.13909[0m[0m | time: 52.231s
[2K
| Adam | epoch: 010 | loss: 0.13909 - acc: 0.9580 -- iter: 1504/2005
[A[ATraining Step: 615  | total loss: [1m[32m0.13691[0m[0m | time: 53.177s
[2K
| Adam | epoch: 010 | loss: 0.13691 - acc: 0.9590 -- iter: 1536/2005
[A[ATraining Step: 616  | total loss: [1m[32m0.13180[0m[0m | time: 54.194s
[2K
| Adam | epoch: 010 | loss: 0.13180 - acc: 0.9600 -- iter: 1568/2005
[A[ATraining Step: 617  | total loss: [1m[32m0.12263[0m[0m | time: 55.300s
[2K
| Adam | epoch: 010 | loss: 0.12263 - acc: 0.9640 -- iter: 1600/2005
[A[ATraining Step: 618  | total loss: [1m[32m0.12780[0m[0m | time: 56.294s
[2K
| Adam | epoch: 010 | loss: 0.12780 - acc: 0.9582 -- iter: 1632/2005
[A[ATraining Step: 619  | total loss: [1m[32m0.11999[0m[0m | time: 57.309s
[2K
| Adam | epoch: 010 | loss: 0.11999 - acc: 0.9624 -- iter: 1664/2005
[A[ATraining Step: 620  | total loss: [1m[32m0.13059[0m[0m | time: 58.585s
[2K
| Adam | epoch: 010 | loss: 0.13059 - acc: 0.9568 -- iter: 1696/2005
[A[ATraining Step: 621  | total loss: [1m[32m0.12572[0m[0m | time: 59.919s
[2K
| Adam | epoch: 010 | loss: 0.12572 - acc: 0.9580 -- iter: 1728/2005
[A[ATraining Step: 622  | total loss: [1m[32m0.13877[0m[0m | time: 61.072s
[2K
| Adam | epoch: 010 | loss: 0.13877 - acc: 0.9528 -- iter: 1760/2005
[A[ATraining Step: 623  | total loss: [1m[32m0.13451[0m[0m | time: 61.948s
[2K
| Adam | epoch: 010 | loss: 0.13451 - acc: 0.9513 -- iter: 1792/2005
[A[ATraining Step: 624  | total loss: [1m[32m0.14574[0m[0m | time: 62.848s
[2K
| Adam | epoch: 010 | loss: 0.14574 - acc: 0.9499 -- iter: 1824/2005
[A[ATraining Step: 625  | total loss: [1m[32m0.13733[0m[0m | time: 63.895s
[2K
| Adam | epoch: 010 | loss: 0.13733 - acc: 0.9518 -- iter: 1856/2005
[A[ATraining Step: 626  | total loss: [1m[32m0.13262[0m[0m | time: 64.921s
[2K
| Adam | epoch: 010 | loss: 0.13262 - acc: 0.9535 -- iter: 1888/2005
[A[ATraining Step: 627  | total loss: [1m[32m0.15086[0m[0m | time: 66.091s
[2K
| Adam | epoch: 010 | loss: 0.15086 - acc: 0.9456 -- iter: 1920/2005
[A[ATraining Step: 628  | total loss: [1m[32m0.16710[0m[0m | time: 67.212s
[2K
| Adam | epoch: 010 | loss: 0.16710 - acc: 0.9386 -- iter: 1952/2005
[A[ATraining Step: 629  | total loss: [1m[32m0.20746[0m[0m | time: 68.162s
[2K
| Adam | epoch: 010 | loss: 0.20746 - acc: 0.9228 -- iter: 1984/2005
[A[ATraining Step: 630  | total loss: [1m[32m0.23025[0m[0m | time: 73.266s
[2K
| Adam | epoch: 010 | loss: 0.23025 - acc: 0.9181 | val_loss: 0.49236 - val_acc: 0.8198 -- iter: 2005/2005
--
Training Step: 631  | total loss: [1m[32m0.22489[0m[0m | time: 0.930s
[2K
| Adam | epoch: 011 | loss: 0.22489 - acc: 0.9231 -- iter: 0032/2005
[A[ATraining Step: 632  | total loss: [1m[32m0.22106[0m[0m | time: 1.899s
[2K
| Adam | epoch: 011 | loss: 0.22106 - acc: 0.9246 -- iter: 0064/2005
[A[ATraining Step: 633  | total loss: [1m[32m0.24778[0m[0m | time: 2.909s
[2K
| Adam | epoch: 011 | loss: 0.24778 - acc: 0.9134 -- iter: 0096/2005
[A[ATraining Step: 634  | total loss: [1m[32m0.25740[0m[0m | time: 3.953s
[2K
| Adam | epoch: 011 | loss: 0.25740 - acc: 0.9126 -- iter: 0128/2005
[A[ATraining Step: 635  | total loss: [1m[32m0.25773[0m[0m | time: 4.856s
[2K
| Adam | epoch: 011 | loss: 0.25773 - acc: 0.9151 -- iter: 0160/2005
[A[ATraining Step: 636  | total loss: [1m[32m0.24079[0m[0m | time: 5.842s
[2K
| Adam | epoch: 011 | loss: 0.24079 - acc: 0.9205 -- iter: 0192/2005
[A[ATraining Step: 637  | total loss: [1m[32m0.22664[0m[0m | time: 6.828s
[2K
| Adam | epoch: 011 | loss: 0.22664 - acc: 0.9253 -- iter: 0224/2005
[A[ATraining Step: 638  | total loss: [1m[32m0.21896[0m[0m | time: 7.929s
[2K
| Adam | epoch: 011 | loss: 0.21896 - acc: 0.9234 -- iter: 0256/2005
[A[ATraining Step: 639  | total loss: [1m[32m0.21674[0m[0m | time: 8.639s
[2K
| Adam | epoch: 011 | loss: 0.21674 - acc: 0.9186 -- iter: 0288/2005
[A[ATraining Step: 640  | total loss: [1m[32m0.20447[0m[0m | time: 9.424s
[2K
| Adam | epoch: 011 | loss: 0.20447 - acc: 0.9220 -- iter: 0320/2005
[A[ATraining Step: 641  | total loss: [1m[32m0.19072[0m[0m | time: 10.449s
[2K
| Adam | epoch: 011 | loss: 0.19072 - acc: 0.9298 -- iter: 0352/2005
[A[ATraining Step: 642  | total loss: [1m[32m0.17927[0m[0m | time: 11.528s
[2K
| Adam | epoch: 011 | loss: 0.17927 - acc: 0.9368 -- iter: 0384/2005
[A[ATraining Step: 643  | total loss: [1m[32m0.17842[0m[0m | time: 12.575s
[2K
| Adam | epoch: 011 | loss: 0.17842 - acc: 0.9400 -- iter: 0416/2005
[A[ATraining Step: 644  | total loss: [1m[32m0.16898[0m[0m | time: 13.618s
[2K
| Adam | epoch: 011 | loss: 0.16898 - acc: 0.9429 -- iter: 0448/2005
[A[ATraining Step: 645  | total loss: [1m[32m0.15678[0m[0m | time: 14.671s
[2K
| Adam | epoch: 011 | loss: 0.15678 - acc: 0.9486 -- iter: 0480/2005
[A[ATraining Step: 646  | total loss: [1m[32m0.16821[0m[0m | time: 15.696s
[2K
| Adam | epoch: 011 | loss: 0.16821 - acc: 0.9412 -- iter: 0512/2005
[A[ATraining Step: 647  | total loss: [1m[32m0.16274[0m[0m | time: 16.801s
[2K
| Adam | epoch: 011 | loss: 0.16274 - acc: 0.9440 -- iter: 0544/2005
[A[ATraining Step: 648  | total loss: [1m[32m0.17826[0m[0m | time: 17.836s
[2K
| Adam | epoch: 011 | loss: 0.17826 - acc: 0.9433 -- iter: 0576/2005
[A[ATraining Step: 649  | total loss: [1m[32m0.16715[0m[0m | time: 18.947s
[2K
| Adam | epoch: 011 | loss: 0.16715 - acc: 0.9490 -- iter: 0608/2005
[A[ATraining Step: 650  | total loss: [1m[32m0.16320[0m[0m | time: 20.034s
[2K
| Adam | epoch: 011 | loss: 0.16320 - acc: 0.9447 -- iter: 0640/2005
[A[ATraining Step: 651  | total loss: [1m[32m0.17300[0m[0m | time: 21.077s
[2K
| Adam | epoch: 011 | loss: 0.17300 - acc: 0.9409 -- iter: 0672/2005
[A[ATraining Step: 652  | total loss: [1m[32m0.17172[0m[0m | time: 22.129s
[2K
| Adam | epoch: 011 | loss: 0.17172 - acc: 0.9405 -- iter: 0704/2005
[A[ATraining Step: 653  | total loss: [1m[32m0.16345[0m[0m | time: 23.244s
[2K
| Adam | epoch: 011 | loss: 0.16345 - acc: 0.9434 -- iter: 0736/2005
[A[ATraining Step: 654  | total loss: [1m[32m0.17396[0m[0m | time: 24.327s
[2K
| Adam | epoch: 011 | loss: 0.17396 - acc: 0.9396 -- iter: 0768/2005
[A[ATraining Step: 655  | total loss: [1m[32m0.16834[0m[0m | time: 25.355s
[2K
| Adam | epoch: 011 | loss: 0.16834 - acc: 0.9394 -- iter: 0800/2005
[A[ATraining Step: 656  | total loss: [1m[32m0.16304[0m[0m | time: 26.527s
[2K
| Adam | epoch: 011 | loss: 0.16304 - acc: 0.9424 -- iter: 0832/2005
[A[ATraining Step: 657  | total loss: [1m[32m0.15374[0m[0m | time: 27.608s
[2K
| Adam | epoch: 011 | loss: 0.15374 - acc: 0.9481 -- iter: 0864/2005
[A[ATraining Step: 658  | total loss: [1m[32m0.14540[0m[0m | time: 28.610s
[2K
| Adam | epoch: 011 | loss: 0.14540 - acc: 0.9533 -- iter: 0896/2005
[A[ATraining Step: 659  | total loss: [1m[32m0.14111[0m[0m | time: 29.646s
[2K
| Adam | epoch: 011 | loss: 0.14111 - acc: 0.9549 -- iter: 0928/2005
[A[ATraining Step: 660  | total loss: [1m[32m0.13858[0m[0m | time: 30.689s
[2K
| Adam | epoch: 011 | loss: 0.13858 - acc: 0.9531 -- iter: 0960/2005
[A[ATraining Step: 661  | total loss: [1m[32m0.13016[0m[0m | time: 31.721s
[2K
| Adam | epoch: 011 | loss: 0.13016 - acc: 0.9547 -- iter: 0992/2005
[A[ATraining Step: 662  | total loss: [1m[32m0.12339[0m[0m | time: 32.642s
[2K
| Adam | epoch: 011 | loss: 0.12339 - acc: 0.9592 -- iter: 1024/2005
[A[ATraining Step: 663  | total loss: [1m[32m0.11601[0m[0m | time: 33.254s
[2K
| Adam | epoch: 011 | loss: 0.11601 - acc: 0.9633 -- iter: 1056/2005
[A[ATraining Step: 664  | total loss: [1m[32m0.10745[0m[0m | time: 33.874s
[2K
| Adam | epoch: 011 | loss: 0.10745 - acc: 0.9670 -- iter: 1088/2005
[A[ATraining Step: 665  | total loss: [1m[32m0.13011[0m[0m | time: 34.518s
[2K
| Adam | epoch: 011 | loss: 0.13011 - acc: 0.9578 -- iter: 1120/2005
[A[ATraining Step: 666  | total loss: [1m[32m0.12554[0m[0m | time: 35.166s
[2K
| Adam | epoch: 011 | loss: 0.12554 - acc: 0.9589 -- iter: 1152/2005
[A[ATraining Step: 667  | total loss: [1m[32m0.11866[0m[0m | time: 35.782s
[2K
| Adam | epoch: 011 | loss: 0.11866 - acc: 0.9630 -- iter: 1184/2005
[A[ATraining Step: 668  | total loss: [1m[32m0.12282[0m[0m | time: 36.414s
[2K
| Adam | epoch: 011 | loss: 0.12282 - acc: 0.9604 -- iter: 1216/2005
[A[ATraining Step: 669  | total loss: [1m[32m0.12260[0m[0m | time: 37.028s
[2K
| Adam | epoch: 011 | loss: 0.12260 - acc: 0.9550 -- iter: 1248/2005
[A[ATraining Step: 670  | total loss: [1m[32m0.11702[0m[0m | time: 37.623s
[2K
| Adam | epoch: 011 | loss: 0.11702 - acc: 0.9564 -- iter: 1280/2005
[A[ATraining Step: 671  | total loss: [1m[32m0.13506[0m[0m | time: 38.285s
[2K
| Adam | epoch: 011 | loss: 0.13506 - acc: 0.9451 -- iter: 1312/2005
[A[ATraining Step: 672  | total loss: [1m[32m0.16118[0m[0m | time: 38.907s
[2K
| Adam | epoch: 011 | loss: 0.16118 - acc: 0.9412 -- iter: 1344/2005
[A[ATraining Step: 673  | total loss: [1m[32m0.16370[0m[0m | time: 39.503s
[2K
| Adam | epoch: 011 | loss: 0.16370 - acc: 0.9409 -- iter: 1376/2005
[A[ATraining Step: 674  | total loss: [1m[32m0.15310[0m[0m | time: 40.102s
[2K
| Adam | epoch: 011 | loss: 0.15310 - acc: 0.9437 -- iter: 1408/2005
[A[ATraining Step: 675  | total loss: [1m[32m0.15827[0m[0m | time: 40.713s
[2K
| Adam | epoch: 011 | loss: 0.15827 - acc: 0.9399 -- iter: 1440/2005
[A[ATraining Step: 676  | total loss: [1m[32m0.15194[0m[0m | time: 41.334s
[2K
| Adam | epoch: 011 | loss: 0.15194 - acc: 0.9428 -- iter: 1472/2005
[A[ATraining Step: 677  | total loss: [1m[32m0.15207[0m[0m | time: 41.940s
[2K
| Adam | epoch: 011 | loss: 0.15207 - acc: 0.9454 -- iter: 1504/2005
[A[ATraining Step: 678  | total loss: [1m[32m0.13968[0m[0m | time: 42.545s
[2K
| Adam | epoch: 011 | loss: 0.13968 - acc: 0.9509 -- iter: 1536/2005
[A[ATraining Step: 679  | total loss: [1m[32m0.13754[0m[0m | time: 43.177s
[2K
| Adam | epoch: 011 | loss: 0.13754 - acc: 0.9495 -- iter: 1568/2005
[A[ATraining Step: 680  | total loss: [1m[32m0.13000[0m[0m | time: 43.787s
[2K
| Adam | epoch: 011 | loss: 0.13000 - acc: 0.9546 -- iter: 1600/2005
[A[ATraining Step: 681  | total loss: [1m[32m0.13305[0m[0m | time: 44.381s
[2K
| Adam | epoch: 011 | loss: 0.13305 - acc: 0.9529 -- iter: 1632/2005
[A[ATraining Step: 682  | total loss: [1m[32m0.12226[0m[0m | time: 45.026s
[2K
| Adam | epoch: 011 | loss: 0.12226 - acc: 0.9576 -- iter: 1664/2005
[A[ATraining Step: 683  | total loss: [1m[32m0.12574[0m[0m | time: 45.637s
[2K
| Adam | epoch: 011 | loss: 0.12574 - acc: 0.9556 -- iter: 1696/2005
[A[ATraining Step: 684  | total loss: [1m[32m0.12284[0m[0m | time: 46.238s
[2K
| Adam | epoch: 011 | loss: 0.12284 - acc: 0.9569 -- iter: 1728/2005
[A[ATraining Step: 685  | total loss: [1m[32m0.12308[0m[0m | time: 46.847s
[2K
| Adam | epoch: 011 | loss: 0.12308 - acc: 0.9549 -- iter: 1760/2005
[A[ATraining Step: 686  | total loss: [1m[32m0.12032[0m[0m | time: 47.437s
[2K
| Adam | epoch: 011 | loss: 0.12032 - acc: 0.9563 -- iter: 1792/2005
[A[ATraining Step: 687  | total loss: [1m[32m0.11684[0m[0m | time: 48.024s
[2K
| Adam | epoch: 011 | loss: 0.11684 - acc: 0.9576 -- iter: 1824/2005
[A[ATraining Step: 688  | total loss: [1m[32m0.10965[0m[0m | time: 48.663s
[2K
| Adam | epoch: 011 | loss: 0.10965 - acc: 0.9618 -- iter: 1856/2005
[A[ATraining Step: 689  | total loss: [1m[32m0.10958[0m[0m | time: 49.257s
[2K
| Adam | epoch: 011 | loss: 0.10958 - acc: 0.9625 -- iter: 1888/2005
[A[ATraining Step: 690  | total loss: [1m[32m0.10341[0m[0m | time: 49.896s
[2K
| Adam | epoch: 011 | loss: 0.10341 - acc: 0.9631 -- iter: 1920/2005
[A[ATraining Step: 691  | total loss: [1m[32m0.10505[0m[0m | time: 50.532s
[2K
| Adam | epoch: 011 | loss: 0.10505 - acc: 0.9637 -- iter: 1952/2005
[A[ATraining Step: 692  | total loss: [1m[32m0.10022[0m[0m | time: 51.157s
[2K
| Adam | epoch: 011 | loss: 0.10022 - acc: 0.9673 -- iter: 1984/2005
[A[ATraining Step: 693  | total loss: [1m[32m0.10168[0m[0m | time: 53.808s
[2K
| Adam | epoch: 011 | loss: 0.10168 - acc: 0.9643 | val_loss: 0.45371 - val_acc: 0.8565 -- iter: 2005/2005
--
Training Step: 694  | total loss: [1m[32m0.10715[0m[0m | time: 0.611s
[2K
| Adam | epoch: 012 | loss: 0.10715 - acc: 0.9617 -- iter: 0032/2005
[A[ATraining Step: 695  | total loss: [1m[32m0.11217[0m[0m | time: 1.213s
[2K
| Adam | epoch: 012 | loss: 0.11217 - acc: 0.9592 -- iter: 0064/2005
[A[ATraining Step: 696  | total loss: [1m[32m0.10253[0m[0m | time: 1.812s
[2K
| Adam | epoch: 012 | loss: 0.10253 - acc: 0.9633 -- iter: 0096/2005
[A[ATraining Step: 697  | total loss: [1m[32m0.09788[0m[0m | time: 2.427s
[2K
| Adam | epoch: 012 | loss: 0.09788 - acc: 0.9639 -- iter: 0128/2005
[A[ATraining Step: 698  | total loss: [1m[32m0.09011[0m[0m | time: 3.059s
[2K
| Adam | epoch: 012 | loss: 0.09011 - acc: 0.9675 -- iter: 0160/2005
[A[ATraining Step: 699  | total loss: [1m[32m0.08278[0m[0m | time: 3.674s
[2K
| Adam | epoch: 012 | loss: 0.08278 - acc: 0.9707 -- iter: 0192/2005
[A[ATraining Step: 700  | total loss: [1m[32m0.08599[0m[0m | time: 4.273s
[2K
| Adam | epoch: 012 | loss: 0.08599 - acc: 0.9674 -- iter: 0224/2005
[A[ATraining Step: 701  | total loss: [1m[32m0.08211[0m[0m | time: 4.903s
[2K
| Adam | epoch: 012 | loss: 0.08211 - acc: 0.9707 -- iter: 0256/2005
[A[ATraining Step: 702  | total loss: [1m[32m0.08448[0m[0m | time: 6.113s
[2K
| Adam | epoch: 012 | loss: 0.08448 - acc: 0.9642 -- iter: 0288/2005
[A[ATraining Step: 703  | total loss: [1m[32m0.07729[0m[0m | time: 7.001s
[2K
| Adam | epoch: 012 | loss: 0.07729 - acc: 0.9678 -- iter: 0320/2005
[A[ATraining Step: 704  | total loss: [1m[32m0.07467[0m[0m | time: 7.934s
[2K
| Adam | epoch: 012 | loss: 0.07467 - acc: 0.9663 -- iter: 0352/2005
[A[ATraining Step: 705  | total loss: [1m[32m0.06928[0m[0m | time: 9.068s
[2K
| Adam | epoch: 012 | loss: 0.06928 - acc: 0.9696 -- iter: 0384/2005
[A[ATraining Step: 706  | total loss: [1m[32m0.06723[0m[0m | time: 9.928s
[2K
| Adam | epoch: 012 | loss: 0.06723 - acc: 0.9695 -- iter: 0416/2005
[A[ATraining Step: 707  | total loss: [1m[32m0.07687[0m[0m | time: 10.887s
[2K
| Adam | epoch: 012 | loss: 0.07687 - acc: 0.9632 -- iter: 0448/2005
[A[ATraining Step: 708  | total loss: [1m[32m0.09894[0m[0m | time: 11.907s
[2K
| Adam | epoch: 012 | loss: 0.09894 - acc: 0.9575 -- iter: 0480/2005
[A[ATraining Step: 709  | total loss: [1m[32m0.09061[0m[0m | time: 12.963s
[2K
| Adam | epoch: 012 | loss: 0.09061 - acc: 0.9618 -- iter: 0512/2005
[A[ATraining Step: 710  | total loss: [1m[32m0.09184[0m[0m | time: 14.004s
[2K
| Adam | epoch: 012 | loss: 0.09184 - acc: 0.9593 -- iter: 0544/2005
[A[ATraining Step: 711  | total loss: [1m[32m0.09506[0m[0m | time: 14.948s
[2K
| Adam | epoch: 012 | loss: 0.09506 - acc: 0.9540 -- iter: 0576/2005
[A[ATraining Step: 712  | total loss: [1m[32m0.08645[0m[0m | time: 15.917s
[2K
| Adam | epoch: 012 | loss: 0.08645 - acc: 0.9586 -- iter: 0608/2005
[A[ATraining Step: 713  | total loss: [1m[32m0.08415[0m[0m | time: 17.141s
[2K
| Adam | epoch: 012 | loss: 0.08415 - acc: 0.9596 -- iter: 0640/2005
[A[ATraining Step: 714  | total loss: [1m[32m0.08556[0m[0m | time: 18.417s
[2K
| Adam | epoch: 012 | loss: 0.08556 - acc: 0.9574 -- iter: 0672/2005
[A[ATraining Step: 715  | total loss: [1m[32m0.08468[0m[0m | time: 19.541s
[2K
| Adam | epoch: 012 | loss: 0.08468 - acc: 0.9554 -- iter: 0704/2005
[A[ATraining Step: 716  | total loss: [1m[32m0.07828[0m[0m | time: 20.449s
[2K
| Adam | epoch: 012 | loss: 0.07828 - acc: 0.9599 -- iter: 0736/2005
[A[ATraining Step: 717  | total loss: [1m[32m0.07434[0m[0m | time: 21.366s
[2K
| Adam | epoch: 012 | loss: 0.07434 - acc: 0.9608 -- iter: 0768/2005
[A[ATraining Step: 718  | total loss: [1m[32m0.07799[0m[0m | time: 22.325s
[2K
| Adam | epoch: 012 | loss: 0.07799 - acc: 0.9553 -- iter: 0800/2005
[A[ATraining Step: 719  | total loss: [1m[32m0.10179[0m[0m | time: 23.313s
[2K
| Adam | epoch: 012 | loss: 0.10179 - acc: 0.9473 -- iter: 0832/2005
[A[ATraining Step: 720  | total loss: [1m[32m0.10141[0m[0m | time: 24.333s
[2K
| Adam | epoch: 012 | loss: 0.10141 - acc: 0.9432 -- iter: 0864/2005
[A[ATraining Step: 721  | total loss: [1m[32m0.10164[0m[0m | time: 25.298s
[2K
| Adam | epoch: 012 | loss: 0.10164 - acc: 0.9457 -- iter: 0896/2005
[A[ATraining Step: 722  | total loss: [1m[32m0.10604[0m[0m | time: 26.129s
[2K
| Adam | epoch: 012 | loss: 0.10604 - acc: 0.9480 -- iter: 0928/2005
[A[ATraining Step: 723  | total loss: [1m[32m0.10085[0m[0m | time: 27.178s
[2K
| Adam | epoch: 012 | loss: 0.10085 - acc: 0.9501 -- iter: 0960/2005
[A[ATraining Step: 724  | total loss: [1m[32m0.10010[0m[0m | time: 28.454s
[2K
| Adam | epoch: 012 | loss: 0.10010 - acc: 0.9520 -- iter: 0992/2005
[A[ATraining Step: 725  | total loss: [1m[32m0.09235[0m[0m | time: 29.728s
[2K
| Adam | epoch: 012 | loss: 0.09235 - acc: 0.9568 -- iter: 1024/2005
[A[ATraining Step: 726  | total loss: [1m[32m0.09718[0m[0m | time: 30.585s
[2K
| Adam | epoch: 012 | loss: 0.09718 - acc: 0.9517 -- iter: 1056/2005
[A[ATraining Step: 727  | total loss: [1m[32m0.08922[0m[0m | time: 31.502s
[2K
| Adam | epoch: 012 | loss: 0.08922 - acc: 0.9566 -- iter: 1088/2005
[A[ATraining Step: 728  | total loss: [1m[32m0.08301[0m[0m | time: 32.509s
[2K
| Adam | epoch: 012 | loss: 0.08301 - acc: 0.9609 -- iter: 1120/2005
[A[ATraining Step: 729  | total loss: [1m[32m0.08743[0m[0m | time: 33.419s
[2K
| Adam | epoch: 012 | loss: 0.08743 - acc: 0.9554 -- iter: 1152/2005
[A[ATraining Step: 730  | total loss: [1m[32m0.08733[0m[0m | time: 34.361s
[2K
| Adam | epoch: 012 | loss: 0.08733 - acc: 0.9568 -- iter: 1184/2005
[A[ATraining Step: 731  | total loss: [1m[32m0.11352[0m[0m | time: 35.407s
[2K
| Adam | epoch: 012 | loss: 0.11352 - acc: 0.9548 -- iter: 1216/2005
[A[ATraining Step: 732  | total loss: [1m[32m0.11374[0m[0m | time: 36.327s
[2K
| Adam | epoch: 012 | loss: 0.11374 - acc: 0.9531 -- iter: 1248/2005
[A[ATraining Step: 733  | total loss: [1m[32m0.10936[0m[0m | time: 37.180s
[2K
| Adam | epoch: 012 | loss: 0.10936 - acc: 0.9515 -- iter: 1280/2005
[A[ATraining Step: 734  | total loss: [1m[32m0.13631[0m[0m | time: 38.432s
[2K
| Adam | epoch: 012 | loss: 0.13631 - acc: 0.9470 -- iter: 1312/2005
[A[ATraining Step: 735  | total loss: [1m[32m0.14710[0m[0m | time: 39.779s
[2K
| Adam | epoch: 012 | loss: 0.14710 - acc: 0.9429 -- iter: 1344/2005
[A[ATraining Step: 736  | total loss: [1m[32m0.13568[0m[0m | time: 40.842s
[2K
| Adam | epoch: 012 | loss: 0.13568 - acc: 0.9486 -- iter: 1376/2005
[A[ATraining Step: 737  | total loss: [1m[32m0.13258[0m[0m | time: 41.775s
[2K
| Adam | epoch: 012 | loss: 0.13258 - acc: 0.9507 -- iter: 1408/2005
[A[ATraining Step: 738  | total loss: [1m[32m0.12300[0m[0m | time: 42.714s
[2K
| Adam | epoch: 012 | loss: 0.12300 - acc: 0.9556 -- iter: 1440/2005
[A[ATraining Step: 739  | total loss: [1m[32m0.12743[0m[0m | time: 43.640s
[2K
| Adam | epoch: 012 | loss: 0.12743 - acc: 0.9538 -- iter: 1472/2005
[A[ATraining Step: 740  | total loss: [1m[32m0.14300[0m[0m | time: 44.553s
[2K
| Adam | epoch: 012 | loss: 0.14300 - acc: 0.9459 -- iter: 1504/2005
[A[ATraining Step: 741  | total loss: [1m[32m0.14447[0m[0m | time: 45.562s
[2K
| Adam | epoch: 012 | loss: 0.14447 - acc: 0.9419 -- iter: 1536/2005
[A[ATraining Step: 742  | total loss: [1m[32m0.13514[0m[0m | time: 46.564s
[2K
| Adam | epoch: 012 | loss: 0.13514 - acc: 0.9446 -- iter: 1568/2005
[A[ATraining Step: 743  | total loss: [1m[32m0.12588[0m[0m | time: 47.428s
[2K
| Adam | epoch: 012 | loss: 0.12588 - acc: 0.9470 -- iter: 1600/2005
[A[ATraining Step: 744  | total loss: [1m[32m0.13050[0m[0m | time: 48.582s
[2K
| Adam | epoch: 012 | loss: 0.13050 - acc: 0.9461 -- iter: 1632/2005
[A[ATraining Step: 745  | total loss: [1m[32m0.12198[0m[0m | time: 49.864s
[2K
| Adam | epoch: 012 | loss: 0.12198 - acc: 0.9515 -- iter: 1664/2005
[A[ATraining Step: 746  | total loss: [1m[32m0.12199[0m[0m | time: 51.143s
[2K
| Adam | epoch: 012 | loss: 0.12199 - acc: 0.9501 -- iter: 1696/2005
[A[ATraining Step: 747  | total loss: [1m[32m0.11319[0m[0m | time: 51.992s
[2K
| Adam | epoch: 012 | loss: 0.11319 - acc: 0.9551 -- iter: 1728/2005
[A[ATraining Step: 748  | total loss: [1m[32m0.11540[0m[0m | time: 52.879s
[2K
| Adam | epoch: 012 | loss: 0.11540 - acc: 0.9502 -- iter: 1760/2005
[A[ATraining Step: 749  | total loss: [1m[32m0.10555[0m[0m | time: 53.869s
[2K
| Adam | epoch: 012 | loss: 0.10555 - acc: 0.9552 -- iter: 1792/2005
[A[ATraining Step: 750  | total loss: [1m[32m0.10489[0m[0m | time: 54.766s
[2K
| Adam | epoch: 012 | loss: 0.10489 - acc: 0.9534 -- iter: 1824/2005
[A[ATraining Step: 751  | total loss: [1m[32m0.09948[0m[0m | time: 55.767s
[2K
| Adam | epoch: 012 | loss: 0.09948 - acc: 0.9549 -- iter: 1856/2005
[A[ATraining Step: 752  | total loss: [1m[32m0.09224[0m[0m | time: 56.791s
[2K
| Adam | epoch: 012 | loss: 0.09224 - acc: 0.9594 -- iter: 1888/2005
[A[ATraining Step: 753  | total loss: [1m[32m0.08490[0m[0m | time: 57.749s
[2K
| Adam | epoch: 012 | loss: 0.08490 - acc: 0.9635 -- iter: 1920/2005
[A[ATraining Step: 754  | total loss: [1m[32m0.07807[0m[0m | time: 58.611s
[2K
| Adam | epoch: 012 | loss: 0.07807 - acc: 0.9671 -- iter: 1952/2005
[A[ATraining Step: 755  | total loss: [1m[32m0.07306[0m[0m | time: 59.637s
[2K
| Adam | epoch: 012 | loss: 0.07306 - acc: 0.9704 -- iter: 1984/2005
[A[ATraining Step: 756  | total loss: [1m[32m0.07190[0m[0m | time: 63.054s
[2K
| Adam | epoch: 012 | loss: 0.07190 - acc: 0.9703 | val_loss: 0.53548 - val_acc: 0.8293 -- iter: 2005/2005
--
Training Step: 757  | total loss: [1m[32m0.06827[0m[0m | time: 1.041s
[2K
| Adam | epoch: 013 | loss: 0.06827 - acc: 0.9732 -- iter: 0032/2005
[A[ATraining Step: 758  | total loss: [1m[32m0.07832[0m[0m | time: 1.872s
[2K
| Adam | epoch: 013 | loss: 0.07832 - acc: 0.9665 -- iter: 0064/2005
[A[ATraining Step: 759  | total loss: [1m[32m0.07592[0m[0m | time: 2.701s
[2K
| Adam | epoch: 013 | loss: 0.07592 - acc: 0.9668 -- iter: 0096/2005
[A[ATraining Step: 760  | total loss: [1m[32m0.08356[0m[0m | time: 3.563s
[2K
| Adam | epoch: 013 | loss: 0.08356 - acc: 0.9670 -- iter: 0128/2005
[A[ATraining Step: 761  | total loss: [1m[32m0.08635[0m[0m | time: 4.481s
[2K
| Adam | epoch: 013 | loss: 0.08635 - acc: 0.9640 -- iter: 0160/2005
[A[ATraining Step: 762  | total loss: [1m[32m0.08141[0m[0m | time: 5.743s
[2K
| Adam | epoch: 013 | loss: 0.08141 - acc: 0.9676 -- iter: 0192/2005
[A[ATraining Step: 763  | total loss: [1m[32m0.07774[0m[0m | time: 7.086s
[2K
| Adam | epoch: 013 | loss: 0.07774 - acc: 0.9709 -- iter: 0224/2005
[A[ATraining Step: 764  | total loss: [1m[32m0.07695[0m[0m | time: 8.213s
[2K
| Adam | epoch: 013 | loss: 0.07695 - acc: 0.9706 -- iter: 0256/2005
[A[ATraining Step: 765  | total loss: [1m[32m0.07103[0m[0m | time: 9.070s
[2K
| Adam | epoch: 013 | loss: 0.07103 - acc: 0.9736 -- iter: 0288/2005
[A[ATraining Step: 766  | total loss: [1m[32m0.08538[0m[0m | time: 10.032s
[2K
| Adam | epoch: 013 | loss: 0.08538 - acc: 0.9731 -- iter: 0320/2005
[A[ATraining Step: 767  | total loss: [1m[32m0.08769[0m[0m | time: 10.690s
[2K
| Adam | epoch: 013 | loss: 0.08769 - acc: 0.9664 -- iter: 0352/2005
[A[ATraining Step: 768  | total loss: [1m[32m0.08703[0m[0m | time: 11.354s
[2K
| Adam | epoch: 013 | loss: 0.08703 - acc: 0.9650 -- iter: 0384/2005
[A[ATraining Step: 769  | total loss: [1m[32m0.08103[0m[0m | time: 12.411s
[2K
| Adam | epoch: 013 | loss: 0.08103 - acc: 0.9685 -- iter: 0416/2005
[A[ATraining Step: 770  | total loss: [1m[32m0.07587[0m[0m | time: 13.450s
[2K
| Adam | epoch: 013 | loss: 0.07587 - acc: 0.9685 -- iter: 0448/2005
[A[ATraining Step: 771  | total loss: [1m[32m0.07178[0m[0m | time: 14.380s
[2K
| Adam | epoch: 013 | loss: 0.07178 - acc: 0.9717 -- iter: 0480/2005
[A[ATraining Step: 772  | total loss: [1m[32m0.06984[0m[0m | time: 15.522s
[2K
| Adam | epoch: 013 | loss: 0.06984 - acc: 0.9714 -- iter: 0512/2005
[A[ATraining Step: 773  | total loss: [1m[32m0.06703[0m[0m | time: 16.784s
[2K
| Adam | epoch: 013 | loss: 0.06703 - acc: 0.9742 -- iter: 0544/2005
[A[ATraining Step: 774  | total loss: [1m[32m0.06853[0m[0m | time: 18.016s
[2K
| Adam | epoch: 013 | loss: 0.06853 - acc: 0.9706 -- iter: 0576/2005
[A[ATraining Step: 775  | total loss: [1m[32m0.06290[0m[0m | time: 18.890s
[2K
| Adam | epoch: 013 | loss: 0.06290 - acc: 0.9735 -- iter: 0608/2005
[A[ATraining Step: 776  | total loss: [1m[32m0.07369[0m[0m | time: 19.838s
[2K
| Adam | epoch: 013 | loss: 0.07369 - acc: 0.9730 -- iter: 0640/2005
[A[ATraining Step: 777  | total loss: [1m[32m0.10991[0m[0m | time: 20.783s
[2K
| Adam | epoch: 013 | loss: 0.10991 - acc: 0.9632 -- iter: 0672/2005
[A[ATraining Step: 778  | total loss: [1m[32m0.12174[0m[0m | time: 21.675s
[2K
| Adam | epoch: 013 | loss: 0.12174 - acc: 0.9607 -- iter: 0704/2005
[A[ATraining Step: 779  | total loss: [1m[32m0.11138[0m[0m | time: 22.649s
[2K
| Adam | epoch: 013 | loss: 0.11138 - acc: 0.9646 -- iter: 0736/2005
[A[ATraining Step: 780  | total loss: [1m[32m0.10229[0m[0m | time: 23.786s
[2K
| Adam | epoch: 013 | loss: 0.10229 - acc: 0.9681 -- iter: 0768/2005
[A[ATraining Step: 781  | total loss: [1m[32m0.10145[0m[0m | time: 24.743s
[2K
| Adam | epoch: 013 | loss: 0.10145 - acc: 0.9651 -- iter: 0800/2005
[A[ATraining Step: 782  | total loss: [1m[32m0.09291[0m[0m | time: 25.662s
[2K
| Adam | epoch: 013 | loss: 0.09291 - acc: 0.9686 -- iter: 0832/2005
[A[ATraining Step: 783  | total loss: [1m[32m0.08621[0m[0m | time: 26.915s
[2K
| Adam | epoch: 013 | loss: 0.08621 - acc: 0.9717 -- iter: 0864/2005
[A[ATraining Step: 784  | total loss: [1m[32m0.08229[0m[0m | time: 28.238s
[2K
| Adam | epoch: 013 | loss: 0.08229 - acc: 0.9714 -- iter: 0896/2005
[A[ATraining Step: 785  | total loss: [1m[32m0.07494[0m[0m | time: 29.267s
[2K
| Adam | epoch: 013 | loss: 0.07494 - acc: 0.9743 -- iter: 0928/2005
[A[ATraining Step: 786  | total loss: [1m[32m0.06961[0m[0m | time: 30.148s
[2K
| Adam | epoch: 013 | loss: 0.06961 - acc: 0.9768 -- iter: 0960/2005
[A[ATraining Step: 787  | total loss: [1m[32m0.06490[0m[0m | time: 31.126s
[2K
| Adam | epoch: 013 | loss: 0.06490 - acc: 0.9792 -- iter: 0992/2005
[A[ATraining Step: 788  | total loss: [1m[32m0.06702[0m[0m | time: 32.150s
[2K
| Adam | epoch: 013 | loss: 0.06702 - acc: 0.9781 -- iter: 1024/2005
[A[ATraining Step: 789  | total loss: [1m[32m0.06214[0m[0m | time: 33.139s
[2K
| Adam | epoch: 013 | loss: 0.06214 - acc: 0.9803 -- iter: 1056/2005
[A[ATraining Step: 790  | total loss: [1m[32m0.05747[0m[0m | time: 34.212s
[2K
| Adam | epoch: 013 | loss: 0.05747 - acc: 0.9823 -- iter: 1088/2005
[A[ATraining Step: 791  | total loss: [1m[32m0.06782[0m[0m | time: 35.118s
[2K
| Adam | epoch: 013 | loss: 0.06782 - acc: 0.9809 -- iter: 1120/2005
[A[ATraining Step: 792  | total loss: [1m[32m0.07151[0m[0m | time: 36.111s
[2K
| Adam | epoch: 013 | loss: 0.07151 - acc: 0.9766 -- iter: 1152/2005
[A[ATraining Step: 793  | total loss: [1m[32m0.06605[0m[0m | time: 37.424s
[2K
| Adam | epoch: 013 | loss: 0.06605 - acc: 0.9789 -- iter: 1184/2005
[A[ATraining Step: 794  | total loss: [1m[32m0.06438[0m[0m | time: 38.779s
[2K
| Adam | epoch: 013 | loss: 0.06438 - acc: 0.9779 -- iter: 1216/2005
[A[ATraining Step: 795  | total loss: [1m[32m0.06775[0m[0m | time: 39.667s
[2K
| Adam | epoch: 013 | loss: 0.06775 - acc: 0.9739 -- iter: 1248/2005
[A[ATraining Step: 796  | total loss: [1m[32m0.08554[0m[0m | time: 40.553s
[2K
| Adam | epoch: 013 | loss: 0.08554 - acc: 0.9702 -- iter: 1280/2005
[A[ATraining Step: 797  | total loss: [1m[32m0.08595[0m[0m | time: 41.518s
[2K
| Adam | epoch: 013 | loss: 0.08595 - acc: 0.9701 -- iter: 1312/2005
[A[ATraining Step: 798  | total loss: [1m[32m0.07804[0m[0m | time: 42.521s
[2K
| Adam | epoch: 013 | loss: 0.07804 - acc: 0.9731 -- iter: 1344/2005
[A[ATraining Step: 799  | total loss: [1m[32m0.07107[0m[0m | time: 43.446s
[2K
| Adam | epoch: 013 | loss: 0.07107 - acc: 0.9758 -- iter: 1376/2005
[A[ATraining Step: 800  | total loss: [1m[32m0.08757[0m[0m | time: 47.439s
[2K
| Adam | epoch: 013 | loss: 0.08757 - acc: 0.9657 | val_loss: 0.69900 - val_acc: 0.8198 -- iter: 1408/2005
--
Training Step: 801  | total loss: [1m[32m0.10455[0m[0m | time: 48.820s
[2K
| Adam | epoch: 013 | loss: 0.10455 - acc: 0.9629 -- iter: 1440/2005
[A[ATraining Step: 802  | total loss: [1m[32m0.11088[0m[0m | time: 49.724s
[2K
| Adam | epoch: 013 | loss: 0.11088 - acc: 0.9603 -- iter: 1472/2005
[A[ATraining Step: 803  | total loss: [1m[32m0.11033[0m[0m | time: 50.599s
[2K
| Adam | epoch: 013 | loss: 0.11033 - acc: 0.9580 -- iter: 1504/2005
[A[ATraining Step: 804  | total loss: [1m[32m0.10509[0m[0m | time: 51.524s
[2K
| Adam | epoch: 013 | loss: 0.10509 - acc: 0.9591 -- iter: 1536/2005
[A[ATraining Step: 805  | total loss: [1m[32m0.09727[0m[0m | time: 52.484s
[2K
| Adam | epoch: 013 | loss: 0.09727 - acc: 0.9632 -- iter: 1568/2005
[A[ATraining Step: 806  | total loss: [1m[32m0.09383[0m[0m | time: 53.428s
[2K
| Adam | epoch: 013 | loss: 0.09383 - acc: 0.9638 -- iter: 1600/2005
[A[ATraining Step: 807  | total loss: [1m[32m0.09306[0m[0m | time: 54.487s
[2K
| Adam | epoch: 013 | loss: 0.09306 - acc: 0.9643 -- iter: 1632/2005
[A[ATraining Step: 808  | total loss: [1m[32m0.10315[0m[0m | time: 55.434s
[2K
| Adam | epoch: 013 | loss: 0.10315 - acc: 0.9616 -- iter: 1664/2005
[A[ATraining Step: 809  | total loss: [1m[32m0.09687[0m[0m | time: 56.323s
[2K
| Adam | epoch: 013 | loss: 0.09687 - acc: 0.9623 -- iter: 1696/2005
[A[ATraining Step: 810  | total loss: [1m[32m0.08960[0m[0m | time: 57.638s
[2K
| Adam | epoch: 013 | loss: 0.08960 - acc: 0.9661 -- iter: 1728/2005
[A[ATraining Step: 811  | total loss: [1m[32m0.08276[0m[0m | time: 58.969s
[2K
| Adam | epoch: 013 | loss: 0.08276 - acc: 0.9695 -- iter: 1760/2005
[A[ATraining Step: 812  | total loss: [1m[32m0.07536[0m[0m | time: 60.089s
[2K
| Adam | epoch: 013 | loss: 0.07536 - acc: 0.9725 -- iter: 1792/2005
[A[ATraining Step: 813  | total loss: [1m[32m0.06859[0m[0m | time: 60.927s
[2K
| Adam | epoch: 013 | loss: 0.06859 - acc: 0.9753 -- iter: 1824/2005
[A[ATraining Step: 814  | total loss: [1m[32m0.07154[0m[0m | time: 61.862s
[2K
| Adam | epoch: 013 | loss: 0.07154 - acc: 0.9715 -- iter: 1856/2005
[A[ATraining Step: 815  | total loss: [1m[32m0.06602[0m[0m | time: 62.783s
[2K
| Adam | epoch: 013 | loss: 0.06602 - acc: 0.9743 -- iter: 1888/2005
[A[ATraining Step: 816  | total loss: [1m[32m0.06070[0m[0m | time: 63.801s
[2K
| Adam | epoch: 013 | loss: 0.06070 - acc: 0.9769 -- iter: 1920/2005
[A[ATraining Step: 817  | total loss: [1m[32m0.05671[0m[0m | time: 64.926s
[2K
| Adam | epoch: 013 | loss: 0.05671 - acc: 0.9792 -- iter: 1952/2005
[A[ATraining Step: 818  | total loss: [1m[32m0.05166[0m[0m | time: 65.891s
[2K
| Adam | epoch: 013 | loss: 0.05166 - acc: 0.9813 -- iter: 1984/2005
[A[ATraining Step: 819  | total loss: [1m[32m0.06598[0m[0m | time: 70.583s
[2K
| Adam | epoch: 013 | loss: 0.06598 - acc: 0.9800 | val_loss: 0.45439 - val_acc: 0.8660 -- iter: 2005/2005
--
Training Step: 820  | total loss: [1m[32m0.06778[0m[0m | time: 0.941s
[2K
| Adam | epoch: 014 | loss: 0.06778 - acc: 0.9789 -- iter: 0032/2005
[A[ATraining Step: 821  | total loss: [1m[32m0.06151[0m[0m | time: 1.844s
[2K
| Adam | epoch: 014 | loss: 0.06151 - acc: 0.9810 -- iter: 0064/2005
[A[ATraining Step: 822  | total loss: [1m[32m0.05698[0m[0m | time: 2.928s
[2K
| Adam | epoch: 014 | loss: 0.05698 - acc: 0.9829 -- iter: 0096/2005
[A[ATraining Step: 823  | total loss: [1m[32m0.05820[0m[0m | time: 3.900s
[2K
| Adam | epoch: 014 | loss: 0.05820 - acc: 0.9815 -- iter: 0128/2005
[A[ATraining Step: 824  | total loss: [1m[32m0.05298[0m[0m | time: 4.779s
[2K
| Adam | epoch: 014 | loss: 0.05298 - acc: 0.9834 -- iter: 0160/2005
[A[ATraining Step: 825  | total loss: [1m[32m0.05853[0m[0m | time: 6.034s
[2K
| Adam | epoch: 014 | loss: 0.05853 - acc: 0.9788 -- iter: 0192/2005
[A[ATraining Step: 826  | total loss: [1m[32m0.05321[0m[0m | time: 7.365s
[2K
| Adam | epoch: 014 | loss: 0.05321 - acc: 0.9809 -- iter: 0224/2005
[A[ATraining Step: 827  | total loss: [1m[32m0.04974[0m[0m | time: 8.495s
[2K
| Adam | epoch: 014 | loss: 0.04974 - acc: 0.9828 -- iter: 0256/2005
[A[ATraining Step: 828  | total loss: [1m[32m0.04538[0m[0m | time: 9.305s
[2K
| Adam | epoch: 014 | loss: 0.04538 - acc: 0.9845 -- iter: 0288/2005
[A[ATraining Step: 829  | total loss: [1m[32m0.04170[0m[0m | time: 10.222s
[2K
| Adam | epoch: 014 | loss: 0.04170 - acc: 0.9861 -- iter: 0320/2005
[A[ATraining Step: 830  | total loss: [1m[32m0.04006[0m[0m | time: 11.094s
[2K
| Adam | epoch: 014 | loss: 0.04006 - acc: 0.9875 -- iter: 0352/2005
[A[ATraining Step: 831  | total loss: [1m[32m0.04279[0m[0m | time: 11.721s
[2K
| Adam | epoch: 014 | loss: 0.04279 - acc: 0.9856 -- iter: 0384/2005
[A[ATraining Step: 832  | total loss: [1m[32m0.04170[0m[0m | time: 12.373s
[2K
| Adam | epoch: 014 | loss: 0.04170 - acc: 0.9870 -- iter: 0416/2005
[A[ATraining Step: 833  | total loss: [1m[32m0.03919[0m[0m | time: 13.429s
[2K
| Adam | epoch: 014 | loss: 0.03919 - acc: 0.9883 -- iter: 0448/2005
[A[ATraining Step: 834  | total loss: [1m[32m0.03603[0m[0m | time: 14.479s
[2K
| Adam | epoch: 014 | loss: 0.03603 - acc: 0.9895 -- iter: 0480/2005
[A[ATraining Step: 835  | total loss: [1m[32m0.04255[0m[0m | time: 15.261s
[2K
| Adam | epoch: 014 | loss: 0.04255 - acc: 0.9843 -- iter: 0512/2005
[A[ATraining Step: 836  | total loss: [1m[32m0.04492[0m[0m | time: 16.374s
[2K
| Adam | epoch: 014 | loss: 0.04492 - acc: 0.9827 -- iter: 0544/2005
[A[ATraining Step: 837  | total loss: [1m[32m0.04079[0m[0m | time: 17.571s
[2K
| Adam | epoch: 014 | loss: 0.04079 - acc: 0.9845 -- iter: 0576/2005
[A[ATraining Step: 838  | total loss: [1m[32m0.04156[0m[0m | time: 18.857s
[2K
| Adam | epoch: 014 | loss: 0.04156 - acc: 0.9829 -- iter: 0608/2005
[A[ATraining Step: 839  | total loss: [1m[32m0.04219[0m[0m | time: 19.663s
[2K
| Adam | epoch: 014 | loss: 0.04219 - acc: 0.9846 -- iter: 0640/2005
[A[ATraining Step: 840  | total loss: [1m[32m0.03864[0m[0m | time: 20.593s
[2K
| Adam | epoch: 014 | loss: 0.03864 - acc: 0.9861 -- iter: 0672/2005
[A[ATraining Step: 841  | total loss: [1m[32m0.05070[0m[0m | time: 21.514s
[2K
| Adam | epoch: 014 | loss: 0.05070 - acc: 0.9782 -- iter: 0704/2005
[A[ATraining Step: 842  | total loss: [1m[32m0.04848[0m[0m | time: 22.430s
[2K
| Adam | epoch: 014 | loss: 0.04848 - acc: 0.9803 -- iter: 0736/2005
[A[ATraining Step: 843  | total loss: [1m[32m0.04529[0m[0m | time: 23.414s
[2K
| Adam | epoch: 014 | loss: 0.04529 - acc: 0.9823 -- iter: 0768/2005
[A[ATraining Step: 844  | total loss: [1m[32m0.04928[0m[0m | time: 24.456s
[2K
| Adam | epoch: 014 | loss: 0.04928 - acc: 0.9810 -- iter: 0800/2005
[A[ATraining Step: 845  | total loss: [1m[32m0.06008[0m[0m | time: 25.379s
[2K
| Adam | epoch: 014 | loss: 0.06008 - acc: 0.9797 -- iter: 0832/2005
[A[ATraining Step: 846  | total loss: [1m[32m0.05694[0m[0m | time: 26.361s
[2K
| Adam | epoch: 014 | loss: 0.05694 - acc: 0.9818 -- iter: 0864/2005
[A[ATraining Step: 847  | total loss: [1m[32m0.05359[0m[0m | time: 27.575s
[2K
| Adam | epoch: 014 | loss: 0.05359 - acc: 0.9836 -- iter: 0896/2005
[A[ATraining Step: 848  | total loss: [1m[32m0.04929[0m[0m | time: 28.882s
[2K
| Adam | epoch: 014 | loss: 0.04929 - acc: 0.9852 -- iter: 0928/2005
[A[ATraining Step: 849  | total loss: [1m[32m0.04514[0m[0m | time: 29.889s
[2K
| Adam | epoch: 014 | loss: 0.04514 - acc: 0.9867 -- iter: 0960/2005
[A[ATraining Step: 850  | total loss: [1m[32m0.04487[0m[0m | time: 30.774s
[2K
| Adam | epoch: 014 | loss: 0.04487 - acc: 0.9849 -- iter: 0992/2005
[A[ATraining Step: 851  | total loss: [1m[32m0.04125[0m[0m | time: 31.729s
[2K
| Adam | epoch: 014 | loss: 0.04125 - acc: 0.9864 -- iter: 1024/2005
[A[ATraining Step: 852  | total loss: [1m[32m0.04261[0m[0m | time: 32.639s
[2K
| Adam | epoch: 014 | loss: 0.04261 - acc: 0.9846 -- iter: 1056/2005
[A[ATraining Step: 853  | total loss: [1m[32m0.03873[0m[0m | time: 33.541s
[2K
| Adam | epoch: 014 | loss: 0.03873 - acc: 0.9862 -- iter: 1088/2005
[A[ATraining Step: 854  | total loss: [1m[32m0.03881[0m[0m | time: 34.629s
[2K
| Adam | epoch: 014 | loss: 0.03881 - acc: 0.9876 -- iter: 1120/2005
[A[ATraining Step: 855  | total loss: [1m[32m0.03630[0m[0m | time: 35.632s
[2K
| Adam | epoch: 014 | loss: 0.03630 - acc: 0.9888 -- iter: 1152/2005
[A[ATraining Step: 856  | total loss: [1m[32m0.03925[0m[0m | time: 36.533s
[2K
| Adam | epoch: 014 | loss: 0.03925 - acc: 0.9868 -- iter: 1184/2005
[A[ATraining Step: 857  | total loss: [1m[32m0.04236[0m[0m | time: 37.901s
[2K
| Adam | epoch: 014 | loss: 0.04236 - acc: 0.9850 -- iter: 1216/2005
[A[ATraining Step: 858  | total loss: [1m[32m0.05285[0m[0m | time: 39.262s
[2K
| Adam | epoch: 014 | loss: 0.05285 - acc: 0.9834 -- iter: 1248/2005
[A[ATraining Step: 859  | total loss: [1m[32m0.05662[0m[0m | time: 40.272s
[2K
| Adam | epoch: 014 | loss: 0.05662 - acc: 0.9819 -- iter: 1280/2005
[A[ATraining Step: 860  | total loss: [1m[32m0.05237[0m[0m | time: 41.177s
[2K
| Adam | epoch: 014 | loss: 0.05237 - acc: 0.9837 -- iter: 1312/2005
[A[ATraining Step: 861  | total loss: [1m[32m0.04858[0m[0m | time: 42.111s
[2K
| Adam | epoch: 014 | loss: 0.04858 - acc: 0.9853 -- iter: 1344/2005
[A[ATraining Step: 862  | total loss: [1m[32m0.04896[0m[0m | time: 43.128s
[2K
| Adam | epoch: 014 | loss: 0.04896 - acc: 0.9837 -- iter: 1376/2005
[A[ATraining Step: 863  | total loss: [1m[32m0.06367[0m[0m | time: 44.116s
[2K
| Adam | epoch: 014 | loss: 0.06367 - acc: 0.9791 -- iter: 1408/2005
[A[ATraining Step: 864  | total loss: [1m[32m0.06872[0m[0m | time: 45.144s
[2K
| Adam | epoch: 014 | loss: 0.06872 - acc: 0.9780 -- iter: 1440/2005
[A[ATraining Step: 865  | total loss: [1m[32m0.09060[0m[0m | time: 46.088s
[2K
| Adam | epoch: 014 | loss: 0.09060 - acc: 0.9740 -- iter: 1472/2005
[A[ATraining Step: 866  | total loss: [1m[32m0.08275[0m[0m | time: 46.988s
[2K
| Adam | epoch: 014 | loss: 0.08275 - acc: 0.9766 -- iter: 1504/2005
[A[ATraining Step: 867  | total loss: [1m[32m0.07473[0m[0m | time: 48.268s
[2K
| Adam | epoch: 014 | loss: 0.07473 - acc: 0.9789 -- iter: 1536/2005
[A[ATraining Step: 868  | total loss: [1m[32m0.06788[0m[0m | time: 49.638s
[2K
| Adam | epoch: 014 | loss: 0.06788 - acc: 0.9810 -- iter: 1568/2005
[A[ATraining Step: 869  | total loss: [1m[32m0.06271[0m[0m | time: 50.738s
[2K
| Adam | epoch: 014 | loss: 0.06271 - acc: 0.9829 -- iter: 1600/2005
[A[ATraining Step: 870  | total loss: [1m[32m0.06273[0m[0m | time: 51.595s
[2K
| Adam | epoch: 014 | loss: 0.06273 - acc: 0.9784 -- iter: 1632/2005
[A[ATraining Step: 871  | total loss: [1m[32m0.05688[0m[0m | time: 52.523s
[2K
| Adam | epoch: 014 | loss: 0.05688 - acc: 0.9805 -- iter: 1664/2005
[A[ATraining Step: 872  | total loss: [1m[32m0.05751[0m[0m | time: 53.438s
[2K
| Adam | epoch: 014 | loss: 0.05751 - acc: 0.9794 -- iter: 1696/2005
[A[ATraining Step: 873  | total loss: [1m[32m0.06752[0m[0m | time: 54.378s
[2K
| Adam | epoch: 014 | loss: 0.06752 - acc: 0.9752 -- iter: 1728/2005
[A[ATraining Step: 874  | total loss: [1m[32m0.06162[0m[0m | time: 55.459s
[2K
| Adam | epoch: 014 | loss: 0.06162 - acc: 0.9777 -- iter: 1760/2005
[A[ATraining Step: 875  | total loss: [1m[32m0.05610[0m[0m | time: 56.468s
[2K
| Adam | epoch: 014 | loss: 0.05610 - acc: 0.9799 -- iter: 1792/2005
[A[ATraining Step: 876  | total loss: [1m[32m0.05215[0m[0m | time: 57.332s
[2K
| Adam | epoch: 014 | loss: 0.05215 - acc: 0.9819 -- iter: 1824/2005
[A[ATraining Step: 877  | total loss: [1m[32m0.05109[0m[0m | time: 58.622s
[2K
| Adam | epoch: 014 | loss: 0.05109 - acc: 0.9806 -- iter: 1856/2005
[A[ATraining Step: 878  | total loss: [1m[32m0.04888[0m[0m | time: 59.922s
[2K
| Adam | epoch: 014 | loss: 0.04888 - acc: 0.9794 -- iter: 1888/2005
[A[ATraining Step: 879  | total loss: [1m[32m0.04562[0m[0m | time: 61.014s
[2K
| Adam | epoch: 014 | loss: 0.04562 - acc: 0.9815 -- iter: 1920/2005
[A[ATraining Step: 880  | total loss: [1m[32m0.04635[0m[0m | time: 61.852s
[2K
| Adam | epoch: 014 | loss: 0.04635 - acc: 0.9802 -- iter: 1952/2005
[A[ATraining Step: 881  | total loss: [1m[32m0.04234[0m[0m | time: 62.784s
[2K
| Adam | epoch: 014 | loss: 0.04234 - acc: 0.9822 -- iter: 1984/2005
[A[ATraining Step: 882  | total loss: [1m[32m0.03898[0m[0m | time: 66.858s
[2K
| Adam | epoch: 014 | loss: 0.03898 - acc: 0.9840 | val_loss: 0.60911 - val_acc: 0.8421 -- iter: 2005/2005
--
Training Step: 883  | total loss: [1m[32m0.03625[0m[0m | time: 0.903s
[2K
| Adam | epoch: 015 | loss: 0.03625 - acc: 0.9856 -- iter: 0032/2005
[A[ATraining Step: 884  | total loss: [1m[32m0.03458[0m[0m | time: 2.170s
[2K
| Adam | epoch: 015 | loss: 0.03458 - acc: 0.9870 -- iter: 0064/2005
[A[ATraining Step: 885  | total loss: [1m[32m0.05119[0m[0m | time: 3.529s
[2K
| Adam | epoch: 015 | loss: 0.05119 - acc: 0.9789 -- iter: 0096/2005
[A[ATraining Step: 886  | total loss: [1m[32m0.05117[0m[0m | time: 4.645s
[2K
| Adam | epoch: 015 | loss: 0.05117 - acc: 0.9779 -- iter: 0128/2005
[A[ATraining Step: 887  | total loss: [1m[32m0.04694[0m[0m | time: 5.493s
[2K
| Adam | epoch: 015 | loss: 0.04694 - acc: 0.9801 -- iter: 0160/2005
[A[ATraining Step: 888  | total loss: [1m[32m0.04283[0m[0m | time: 6.426s
[2K
| Adam | epoch: 015 | loss: 0.04283 - acc: 0.9821 -- iter: 0192/2005
[A[ATraining Step: 889  | total loss: [1m[32m0.04244[0m[0m | time: 7.342s
[2K
| Adam | epoch: 015 | loss: 0.04244 - acc: 0.9839 -- iter: 0224/2005
[A[ATraining Step: 890  | total loss: [1m[32m0.04060[0m[0m | time: 8.258s
[2K
| Adam | epoch: 015 | loss: 0.04060 - acc: 0.9855 -- iter: 0256/2005
[A[ATraining Step: 891  | total loss: [1m[32m0.03722[0m[0m | time: 9.262s
[2K
| Adam | epoch: 015 | loss: 0.03722 - acc: 0.9870 -- iter: 0288/2005
[A[ATraining Step: 892  | total loss: [1m[32m0.03404[0m[0m | time: 10.264s
[2K
| Adam | epoch: 015 | loss: 0.03404 - acc: 0.9883 -- iter: 0320/2005
[A[ATraining Step: 893  | total loss: [1m[32m0.04770[0m[0m | time: 11.152s
[2K
| Adam | epoch: 015 | loss: 0.04770 - acc: 0.9863 -- iter: 0352/2005
[A[ATraining Step: 894  | total loss: [1m[32m0.04881[0m[0m | time: 12.296s
[2K
| Adam | epoch: 015 | loss: 0.04881 - acc: 0.9846 -- iter: 0384/2005
[A[ATraining Step: 895  | total loss: [1m[32m0.04428[0m[0m | time: 13.208s
[2K
| Adam | epoch: 015 | loss: 0.04428 - acc: 0.9861 -- iter: 0416/2005
[A[ATraining Step: 896  | total loss: [1m[32m0.05383[0m[0m | time: 14.118s
[2K
| Adam | epoch: 015 | loss: 0.05383 - acc: 0.9827 -- iter: 0448/2005
[A[ATraining Step: 897  | total loss: [1m[32m0.05290[0m[0m | time: 15.334s
[2K
| Adam | epoch: 015 | loss: 0.05290 - acc: 0.9797 -- iter: 0480/2005
[A[ATraining Step: 898  | total loss: [1m[32m0.05067[0m[0m | time: 16.224s
[2K
| Adam | epoch: 015 | loss: 0.05067 - acc: 0.9817 -- iter: 0512/2005
[A[ATraining Step: 899  | total loss: [1m[32m0.04665[0m[0m | time: 17.164s
[2K
| Adam | epoch: 015 | loss: 0.04665 - acc: 0.9836 -- iter: 0544/2005
[A[ATraining Step: 900  | total loss: [1m[32m0.04350[0m[0m | time: 18.060s
[2K
| Adam | epoch: 015 | loss: 0.04350 - acc: 0.9852 -- iter: 0576/2005
[A[ATraining Step: 901  | total loss: [1m[32m0.04023[0m[0m | time: 18.940s
[2K
| Adam | epoch: 015 | loss: 0.04023 - acc: 0.9867 -- iter: 0608/2005
[A[ATraining Step: 902  | total loss: [1m[32m0.04795[0m[0m | time: 19.914s
[2K
| Adam | epoch: 015 | loss: 0.04795 - acc: 0.9818 -- iter: 0640/2005
[A[ATraining Step: 903  | total loss: [1m[32m0.04701[0m[0m | time: 20.947s
[2K
| Adam | epoch: 015 | loss: 0.04701 - acc: 0.9805 -- iter: 0672/2005
[A[ATraining Step: 904  | total loss: [1m[32m0.04686[0m[0m | time: 21.821s
[2K
| Adam | epoch: 015 | loss: 0.04686 - acc: 0.9824 -- iter: 0704/2005
[A[ATraining Step: 905  | total loss: [1m[32m0.04835[0m[0m | time: 22.844s
[2K
| Adam | epoch: 015 | loss: 0.04835 - acc: 0.9810 -- iter: 0736/2005
[A[ATraining Step: 906  | total loss: [1m[32m0.08026[0m[0m | time: 24.166s
[2K
| Adam | epoch: 015 | loss: 0.08026 - acc: 0.9704 -- iter: 0768/2005
[A[ATraining Step: 907  | total loss: [1m[32m0.09848[0m[0m | time: 25.536s
[2K
| Adam | epoch: 015 | loss: 0.09848 - acc: 0.9609 -- iter: 0800/2005
[A[ATraining Step: 908  | total loss: [1m[32m0.10877[0m[0m | time: 26.493s
[2K
| Adam | epoch: 015 | loss: 0.10877 - acc: 0.9554 -- iter: 0832/2005
[A[ATraining Step: 909  | total loss: [1m[32m0.10615[0m[0m | time: 27.413s
[2K
| Adam | epoch: 015 | loss: 0.10615 - acc: 0.9568 -- iter: 0864/2005
[A[ATraining Step: 910  | total loss: [1m[32m0.10087[0m[0m | time: 28.359s
[2K
| Adam | epoch: 015 | loss: 0.10087 - acc: 0.9611 -- iter: 0896/2005
[A[ATraining Step: 911  | total loss: [1m[32m0.12990[0m[0m | time: 29.306s
[2K
| Adam | epoch: 015 | loss: 0.12990 - acc: 0.9494 -- iter: 0928/2005
[A[ATraining Step: 912  | total loss: [1m[32m0.16753[0m[0m | time: 30.212s
[2K
| Adam | epoch: 015 | loss: 0.16753 - acc: 0.9357 -- iter: 0960/2005
[A[ATraining Step: 913  | total loss: [1m[32m0.16961[0m[0m | time: 31.246s
[2K
| Adam | epoch: 015 | loss: 0.16961 - acc: 0.9327 -- iter: 0992/2005
[A[ATraining Step: 914  | total loss: [1m[32m0.20061[0m[0m | time: 32.199s
[2K
| Adam | epoch: 015 | loss: 0.20061 - acc: 0.9238 -- iter: 1024/2005
[A[ATraining Step: 915  | total loss: [1m[32m0.18915[0m[0m | time: 33.263s
[2K
| Adam | epoch: 015 | loss: 0.18915 - acc: 0.9283 -- iter: 1056/2005
[A[ATraining Step: 916  | total loss: [1m[32m0.17285[0m[0m | time: 34.570s
[2K
| Adam | epoch: 015 | loss: 0.17285 - acc: 0.9355 -- iter: 1088/2005
[A[ATraining Step: 917  | total loss: [1m[32m0.16646[0m[0m | time: 35.966s
[2K
| Adam | epoch: 015 | loss: 0.16646 - acc: 0.9357 -- iter: 1120/2005
[A[ATraining Step: 918  | total loss: [1m[32m0.16344[0m[0m | time: 36.862s
[2K
| Adam | epoch: 015 | loss: 0.16344 - acc: 0.9359 -- iter: 1152/2005
[A[ATraining Step: 919  | total loss: [1m[32m0.15820[0m[0m | time: 37.770s
[2K
| Adam | epoch: 015 | loss: 0.15820 - acc: 0.9360 -- iter: 1184/2005
[A[ATraining Step: 920  | total loss: [1m[32m0.15610[0m[0m | time: 38.706s
[2K
| Adam | epoch: 015 | loss: 0.15610 - acc: 0.9362 -- iter: 1216/2005
[A[ATraining Step: 921  | total loss: [1m[32m0.14885[0m[0m | time: 39.670s
[2K
| Adam | epoch: 015 | loss: 0.14885 - acc: 0.9363 -- iter: 1248/2005
[A[ATraining Step: 922  | total loss: [1m[32m0.14225[0m[0m | time: 40.598s
[2K
| Adam | epoch: 015 | loss: 0.14225 - acc: 0.9396 -- iter: 1280/2005
[A[ATraining Step: 923  | total loss: [1m[32m0.13783[0m[0m | time: 41.639s
[2K
| Adam | epoch: 015 | loss: 0.13783 - acc: 0.9394 -- iter: 1312/2005
[A[ATraining Step: 924  | total loss: [1m[32m0.14738[0m[0m | time: 42.650s
[2K
| Adam | epoch: 015 | loss: 0.14738 - acc: 0.9329 -- iter: 1344/2005
[A[ATraining Step: 925  | total loss: [1m[32m0.16379[0m[0m | time: 43.568s
[2K
| Adam | epoch: 015 | loss: 0.16379 - acc: 0.9271 -- iter: 1376/2005
[A[ATraining Step: 926  | total loss: [1m[32m0.15820[0m[0m | time: 44.983s
[2K
| Adam | epoch: 015 | loss: 0.15820 - acc: 0.9313 -- iter: 1408/2005
[A[ATraining Step: 927  | total loss: [1m[32m0.15057[0m[0m | time: 46.330s
[2K
| Adam | epoch: 015 | loss: 0.15057 - acc: 0.9319 -- iter: 1440/2005
[A[ATraining Step: 928  | total loss: [1m[32m0.15096[0m[0m | time: 47.295s
[2K
| Adam | epoch: 015 | loss: 0.15096 - acc: 0.9325 -- iter: 1472/2005
[A[ATraining Step: 929  | total loss: [1m[32m0.14579[0m[0m | time: 48.173s
[2K
| Adam | epoch: 015 | loss: 0.14579 - acc: 0.9330 -- iter: 1504/2005
[A[ATraining Step: 930  | total loss: [1m[32m0.15122[0m[0m | time: 49.062s
[2K
| Adam | epoch: 015 | loss: 0.15122 - acc: 0.9303 -- iter: 1536/2005
[A[ATraining Step: 931  | total loss: [1m[32m0.17708[0m[0m | time: 49.966s
[2K
| Adam | epoch: 015 | loss: 0.17708 - acc: 0.9279 -- iter: 1568/2005
[A[ATraining Step: 932  | total loss: [1m[32m0.17419[0m[0m | time: 50.978s
[2K
| Adam | epoch: 015 | loss: 0.17419 - acc: 0.9289 -- iter: 1600/2005
[A[ATraining Step: 933  | total loss: [1m[32m0.15766[0m[0m | time: 52.006s
[2K
| Adam | epoch: 015 | loss: 0.15766 - acc: 0.9360 -- iter: 1632/2005
[A[ATraining Step: 934  | total loss: [1m[32m0.15136[0m[0m | time: 52.933s
[2K
| Adam | epoch: 015 | loss: 0.15136 - acc: 0.9361 -- iter: 1664/2005
[A[ATraining Step: 935  | total loss: [1m[32m0.15121[0m[0m | time: 53.910s
[2K
| Adam | epoch: 015 | loss: 0.15121 - acc: 0.9394 -- iter: 1696/2005
[A[ATraining Step: 936  | total loss: [1m[32m0.13909[0m[0m | time: 55.104s
[2K
| Adam | epoch: 015 | loss: 0.13909 - acc: 0.9454 -- iter: 1728/2005
[A[ATraining Step: 937  | total loss: [1m[32m0.12885[0m[0m | time: 56.355s
[2K
| Adam | epoch: 015 | loss: 0.12885 - acc: 0.9478 -- iter: 1760/2005
[A[ATraining Step: 938  | total loss: [1m[32m0.12220[0m[0m | time: 57.414s
[2K
| Adam | epoch: 015 | loss: 0.12220 - acc: 0.9499 -- iter: 1792/2005
[A[ATraining Step: 939  | total loss: [1m[32m0.11479[0m[0m | time: 58.243s
[2K
| Adam | epoch: 015 | loss: 0.11479 - acc: 0.9549 -- iter: 1824/2005
[A[ATraining Step: 940  | total loss: [1m[32m0.11093[0m[0m | time: 59.162s
[2K
| Adam | epoch: 015 | loss: 0.11093 - acc: 0.9563 -- iter: 1856/2005
[A[ATraining Step: 941  | total loss: [1m[32m0.11836[0m[0m | time: 60.095s
[2K
| Adam | epoch: 015 | loss: 0.11836 - acc: 0.9544 -- iter: 1888/2005
[A[ATraining Step: 942  | total loss: [1m[32m0.11304[0m[0m | time: 60.991s
[2K
| Adam | epoch: 015 | loss: 0.11304 - acc: 0.9558 -- iter: 1920/2005
[A[ATraining Step: 943  | total loss: [1m[32m0.11045[0m[0m | time: 61.920s
[2K
| Adam | epoch: 015 | loss: 0.11045 - acc: 0.9540 -- iter: 1952/2005
[A[ATraining Step: 944  | total loss: [1m[32m0.10310[0m[0m | time: 62.886s
[2K
| Adam | epoch: 015 | loss: 0.10310 - acc: 0.9586 -- iter: 1984/2005
[A[ATraining Step: 945  | total loss: [1m[32m0.09435[0m[0m | time: 67.083s
[2K
| Adam | epoch: 015 | loss: 0.09435 - acc: 0.9627 | val_loss: 0.41183 - val_acc: 0.8596 -- iter: 2005/2005
--
2018-08-02 03:54:06.467562: W tensorflow/core/framework/allocator.cc:101] Allocation of 5678593536 exceeds 10% of system memory.
2018-08-02 03:54:09.975428: W tensorflow/core/framework/allocator.cc:101] Allocation of 5678593536 exceeds 10% of system memory.
Validation AUC:0.9375356125356127
Validation AUPRC:0.9366745596207497
Test AUC:0.933568003264973
Test AUPRC:0.9365537060167385
BestTestF1Score	0.88	0.74	0.87	0.88	0.87	288	38	259	42	0.77
BestTestMCCScore	0.88	0.75	0.87	0.89	0.87	288	37	260	42	0.79
BestTestAccuracyScore	0.88	0.75	0.87	0.89	0.87	288	37	260	42	0.79
BestValidationF1Score	0.89	0.78	0.89	0.9	0.88	278	32	280	37	0.77
BestValidationMCC	0.89	0.78	0.89	0.9	0.88	277	31	281	38	0.79
BestValidationAccuracy	0.89	0.78	0.89	0.9	0.88	277	31	281	38	0.79
TestPredictions (Threshold:0.79)
CHEMBL370859,TP,ACT,0.9599999785423279	CHEMBL275481,FP,INACT,0.9800000190734863	CHEMBL508030,TN,INACT,0.009999999776482582	CHEMBL168632,TN,INACT,0.7300000190734863	CHEMBL234647,TP,ACT,0.9900000095367432	CHEMBL146983,TN,INACT,0.019999999552965164	CHEMBL61304,TP,ACT,0.9900000095367432	CHEMBL168372,TN,INACT,0.0	CHEMBL594717,TP,ACT,0.9900000095367432	CHEMBL274318,TN,INACT,0.7400000095367432	CHEMBL393466,FN,ACT,0.0	CHEMBL513277,TN,INACT,0.009999999776482582	CHEMBL348766,TN,INACT,0.6299999952316284	CHEMBL419912,TN,INACT,0.0	CHEMBL321471,FN,ACT,0.07999999821186066	CHEMBL148413,TP,ACT,0.9900000095367432	CHEMBL422701,TN,INACT,0.0	CHEMBL436451,TN,INACT,0.0	CHEMBL210457,TN,INACT,0.14000000059604645	CHEMBL2387227,TP,ACT,1.0	CHEMBL273410,TN,INACT,0.0	CHEMBL3360997,TP,ACT,0.9800000190734863	CHEMBL3764080,TP,ACT,1.0	CHEMBL461236,TP,ACT,1.0	CHEMBL46195,TN,INACT,0.0	CHEMBL1096620,TP,ACT,0.9900000095367432	CHEMBL283941,FP,INACT,0.8299999833106995	CHEMBL3084425,TP,ACT,0.949999988079071	CHEMBL417215,TN,INACT,0.009999999776482582	CHEMBL342536,TP,ACT,0.9700000286102295	CHEMBL73272,TN,INACT,0.0	CHEMBL236929,TP,ACT,0.9800000190734863	CHEMBL10801,FP,INACT,0.9800000190734863	CHEMBL412876,TP,ACT,0.9800000190734863	CHEMBL104551,TN,INACT,0.009999999776482582	CHEMBL2387260,TP,ACT,1.0	CHEMBL134065,TP,ACT,0.8799999952316284	CHEMBL25976,TN,INACT,0.0	CHEMBL92364,TN,INACT,0.029999999329447746	CHEMBL129163,TP,ACT,0.9599999785423279	CHEMBL147368,FN,ACT,0.019999999552965164	CHEMBL295651,TN,INACT,0.019999999552965164	CHEMBL322512,TN,INACT,0.0	CHEMBL390734,TP,ACT,0.9900000095367432	CHEMBL3740256,TP,ACT,0.9599999785423279	CHEMBL2237145,FN,ACT,0.0	CHEMBL104210,TN,INACT,0.0	CHEMBL126138,TP,ACT,0.949999988079071	CHEMBL95590,TN,INACT,0.009999999776482582	CHEMBL3775378,TN,INACT,0.009999999776482582	CHEMBL3752465,FN,ACT,0.009999999776482582	CHEMBL18905,TP,ACT,1.0	CHEMBL3590087,TP,ACT,1.0	CHEMBL21172,TN,INACT,0.009999999776482582	CHEMBL3818301,TN,INACT,0.0	CHEMBL326565,TP,ACT,0.9900000095367432	CHEMBL123094,TP,ACT,1.0	CHEMBL229179,TP,ACT,0.9900000095367432	CHEMBL285380,TN,INACT,0.0	CHEMBL3360995,TP,ACT,0.9900000095367432	CHEMBL225284,TP,ACT,1.0	CHEMBL240279,TN,INACT,0.05999999865889549	CHEMBL45305,FP,INACT,0.8500000238418579	CHEMBL28946,TP,ACT,0.9900000095367432	CHEMBL453822,FP,INACT,0.8600000143051147	CHEMBL273953,TN,INACT,0.009999999776482582	CHEMBL282032,TP,ACT,0.9800000190734863	CHEMBL357077,TN,INACT,0.0	CHEMBL89953,TN,INACT,0.03999999910593033	CHEMBL147820,FN,ACT,0.4099999964237213	CHEMBL373245,TP,ACT,0.949999988079071	CHEMBL330885,TN,INACT,0.009999999776482582	CHEMBL99331,FP,INACT,0.949999988079071	CHEMBL489624,TP,ACT,0.9900000095367432	CHEMBL249492,TN,INACT,0.009999999776482582	CHEMBL74330,TN,INACT,0.5899999737739563	CHEMBL59517,TN,INACT,0.7599999904632568	CHEMBL97422,TP,ACT,1.0	CHEMBL356993,TP,ACT,1.0	CHEMBL469855,TN,INACT,0.009999999776482582	CHEMBL20674,FN,ACT,0.7200000286102295	CHEMBL74852,TN,INACT,0.009999999776482582	CHEMBL228686,TN,INACT,0.009999999776482582	CHEMBL7828,TP,ACT,1.0	CHEMBL276676,TN,INACT,0.0	CHEMBL27065,FP,INACT,0.8500000238418579	CHEMBL3741596,TP,ACT,1.0	CHEMBL141048,TN,INACT,0.6000000238418579	CHEMBL332405,TN,INACT,0.23999999463558197	CHEMBL3087709,TN,INACT,0.0	CHEMBL2158722,TP,ACT,0.9800000190734863	CHEMBL226759,TP,ACT,1.0	CHEMBL78326,TN,INACT,0.0	CHEMBL1916704,TN,INACT,0.009999999776482582	CHEMBL191915,TN,INACT,0.009999999776482582	CHEMBL6857,TP,ACT,0.9800000190734863	CHEMBL344202,TP,ACT,1.0	CHEMBL119323,TP,ACT,0.9700000286102295	CHEMBL366864,TN,INACT,0.0	CHEMBL3741516,TP,ACT,0.9800000190734863	CHEMBL161423,TP,ACT,0.949999988079071	CHEMBL294349,TN,INACT,0.0	CHEMBL593545,TP,ACT,0.9700000286102295	CHEMBL461087,TN,INACT,0.15000000596046448	CHEMBL147363,TP,ACT,0.9800000190734863	CHEMBL239232,TN,INACT,0.7200000286102295	CHEMBL1098616,TP,ACT,1.0	CHEMBL3699820,TP,ACT,0.9800000190734863	CHEMBL330603,TP,ACT,0.9800000190734863	CHEMBL1173143,TP,ACT,0.9700000286102295	CHEMBL1277935,TP,ACT,0.9800000190734863	CHEMBL2448068,TP,ACT,0.9900000095367432	CHEMBL3423403,TN,INACT,0.0	CHEMBL100810,TN,INACT,0.009999999776482582	CHEMBL317333,TP,ACT,0.9900000095367432	CHEMBL148358,TP,ACT,0.8100000023841858	CHEMBL527880,FP,INACT,0.8899999856948853	CHEMBL336033,TN,INACT,0.0	CHEMBL2112362,TP,ACT,0.9800000190734863	CHEMBL328187,TP,ACT,0.9900000095367432	CHEMBL2158727,TP,ACT,0.9599999785423279	CHEMBL593443,TN,INACT,0.029999999329447746	CHEMBL218217,TP,ACT,1.0	CHEMBL71726,TN,INACT,0.029999999329447746	CHEMBL101076,TP,ACT,1.0	CHEMBL29534,TN,INACT,0.4000000059604645	CHEMBL430566,TP,ACT,0.9900000095367432	CHEMBL29471,TN,INACT,0.019999999552965164	CHEMBL115556,TN,INACT,0.019999999552965164	CHEMBL561750,FN,ACT,0.44999998807907104	CHEMBL334813,TN,INACT,0.019999999552965164	CHEMBL3742362,TP,ACT,0.9399999976158142	CHEMBL38704,TN,INACT,0.0	CHEMBL604965,TN,INACT,0.009999999776482582	CHEMBL291516,TN,INACT,0.0	CHEMBL152408,TP,ACT,0.9900000095367432	CHEMBL3590097,TP,ACT,0.9599999785423279	CHEMBL3233868,TP,ACT,0.9599999785423279	CHEMBL408492,TN,INACT,0.0	CHEMBL118889,TP,ACT,0.9900000095367432	CHEMBL108417,TN,INACT,0.0	CHEMBL3087714,TN,INACT,0.0	CHEMBL413040,TN,INACT,0.0	CHEMBL2436814,TN,INACT,0.0	CHEMBL423034,FN,ACT,0.25	CHEMBL422498,TP,ACT,0.9800000190734863	CHEMBL208311,FP,INACT,0.9900000095367432	CHEMBL78668,FP,INACT,0.7900000214576721	CHEMBL221691,TN,INACT,0.0	CHEMBL3114163,TN,INACT,0.0	CHEMBL171310,TN,INACT,0.0	CHEMBL143341,TN,INACT,0.0	CHEMBL393622,TP,ACT,0.9800000190734863	CHEMBL150412,FN,ACT,0.7300000190734863	CHEMBL246585,TN,INACT,0.019999999552965164	CHEMBL964,TN,INACT,0.0	CHEMBL67323,FP,INACT,0.9200000166893005	CHEMBL319534,TN,INACT,0.009999999776482582	CHEMBL543613,TP,ACT,1.0	CHEMBL2414355,TP,ACT,0.9399999976158142	CHEMBL9666,TN,INACT,0.05999999865889549	CHEMBL78853,TN,INACT,0.009999999776482582	CHEMBL76933,TN,INACT,0.03999999910593033	CHEMBL143027,TP,ACT,0.9900000095367432	CHEMBL296245,TN,INACT,0.1599999964237213	CHEMBL121307,TN,INACT,0.009999999776482582	CHEMBL233721,TN,INACT,0.15000000596046448	CHEMBL559260,TP,ACT,0.9900000095367432	CHEMBL149510,TP,ACT,0.9900000095367432	CHEMBL3314929,TN,INACT,0.0	CHEMBL25688,TN,INACT,0.0	CHEMBL621,TP,ACT,1.0	CHEMBL58617,TN,INACT,0.0	CHEMBL314959,TN,INACT,0.03999999910593033	CHEMBL332425,TP,ACT,0.9900000095367432	CHEMBL3741292,TP,ACT,0.9900000095367432	CHEMBL477428,TP,ACT,0.9800000190734863	CHEMBL128099,TP,ACT,0.9900000095367432	CHEMBL19615,TP,ACT,1.0	CHEMBL422310,TP,ACT,0.9900000095367432	CHEMBL2387253,TP,ACT,1.0	CHEMBL370758,TP,ACT,1.0	CHEMBL104593,TP,ACT,0.9900000095367432	CHEMBL3735265,TN,INACT,0.0	CHEMBL39843,TN,INACT,0.0	CHEMBL444590,TP,ACT,0.9900000095367432	CHEMBL73933,TN,INACT,0.0	CHEMBL3634809,TP,ACT,0.9599999785423279	CHEMBL355851,TN,INACT,0.009999999776482582	CHEMBL64784,TN,INACT,0.09000000357627869	CHEMBL433345,TP,ACT,0.9399999976158142	CHEMBL176343,TP,ACT,0.9900000095367432	CHEMBL268258,TP,ACT,0.8299999833106995	CHEMBL3093268,TP,ACT,0.9900000095367432	CHEMBL110749,TN,INACT,0.0	CHEMBL594802,TN,INACT,0.03999999910593033	CHEMBL233581,TP,ACT,0.949999988079071	CHEMBL103419,TN,INACT,0.6200000047683716	CHEMBL3084424,FN,ACT,0.09000000357627869	CHEMBL3764761,TP,ACT,0.9599999785423279	CHEMBL331545,FN,ACT,0.30000001192092896	CHEMBL365309,TP,ACT,0.9900000095367432	CHEMBL8600,TP,ACT,0.9700000286102295	CHEMBL43661,TN,INACT,0.019999999552965164	CHEMBL3589940,TN,INACT,0.2800000011920929	CHEMBL357870,TP,ACT,0.9599999785423279	CHEMBL394325,TP,ACT,0.9700000286102295	CHEMBL159466,TP,ACT,0.9800000190734863	CHEMBL2414358,TP,ACT,0.9399999976158142	CHEMBL199413,TN,INACT,0.05000000074505806	CHEMBL2436713,TN,INACT,0.0	CHEMBL228939,TP,ACT,0.9800000190734863	CHEMBL150162,TP,ACT,0.949999988079071	CHEMBL14563,FN,ACT,0.7400000095367432	CHEMBL88506,TN,INACT,0.0	CHEMBL114131,TP,ACT,0.9700000286102295	CHEMBL3084427,TP,ACT,0.9700000286102295	CHEMBL78830,TN,INACT,0.05999999865889549	CHEMBL1223055,FP,INACT,0.8500000238418579	CHEMBL2093084,TN,INACT,0.0	CHEMBL312670,TN,INACT,0.0	CHEMBL177328,FN,ACT,0.6100000143051147	CHEMBL439960,TP,ACT,0.9900000095367432	CHEMBL332817,TP,ACT,0.9900000095367432	CHEMBL71385,TN,INACT,0.0	CHEMBL609043,TP,ACT,0.9700000286102295	CHEMBL19338,TP,ACT,0.9800000190734863	CHEMBL229024,TP,ACT,1.0	CHEMBL93923,TP,ACT,1.0	CHEMBL2387258,TP,ACT,1.0	CHEMBL14,TN,INACT,0.0	CHEMBL50456,TN,INACT,0.47999998927116394	CHEMBL2042551,FP,INACT,0.9800000190734863	CHEMBL109778,TN,INACT,0.0	CHEMBL219111,TP,ACT,0.9900000095367432	CHEMBL416069,TN,INACT,0.0	CHEMBL60859,TP,ACT,1.0	CHEMBL593620,TN,INACT,0.009999999776482582	CHEMBL390298,TN,INACT,0.0	CHEMBL241083,TN,INACT,0.7200000286102295	CHEMBL377153,TP,ACT,1.0	CHEMBL3634811,FN,ACT,0.3700000047683716	CHEMBL439335,TN,INACT,0.0	CHEMBL76874,TN,INACT,0.0	CHEMBL224883,TP,ACT,1.0	CHEMBL72738,TN,INACT,0.019999999552965164	CHEMBL446693,TN,INACT,0.009999999776482582	CHEMBL475497,TN,INACT,0.009999999776482582	CHEMBL479026,TP,ACT,0.9100000262260437	CHEMBL365958,TP,ACT,0.9800000190734863	CHEMBL133455,TP,ACT,0.9900000095367432	CHEMBL1949978,TP,ACT,0.9700000286102295	CHEMBL344602,FP,INACT,0.8500000238418579	CHEMBL25373,FP,INACT,0.9399999976158142	CHEMBL2237148,TP,ACT,0.9900000095367432	CHEMBL373654,TN,INACT,0.0	CHEMBL197653,TP,ACT,0.9399999976158142	CHEMBL3393993,TN,INACT,0.0	CHEMBL2163568,TN,INACT,0.019999999552965164	CHEMBL445331,TP,ACT,0.9800000190734863	CHEMBL308243,TN,INACT,0.0	CHEMBL136936,TP,ACT,0.9900000095367432	CHEMBL905,TP,ACT,0.9900000095367432	CHEMBL145867,TP,ACT,1.0	CHEMBL118553,TN,INACT,0.009999999776482582	CHEMBL275469,FP,INACT,0.9900000095367432	CHEMBL228940,TP,ACT,0.9800000190734863	CHEMBL374261,TP,ACT,0.9900000095367432	CHEMBL99846,TP,ACT,0.9599999785423279	CHEMBL296291,TN,INACT,0.0	CHEMBL11262,TN,INACT,0.07000000029802322	CHEMBL1907840,TN,INACT,0.0	CHEMBL2387249,TP,ACT,1.0	CHEMBL600610,TN,INACT,0.009999999776482582	CHEMBL287047,TP,ACT,0.9300000071525574	CHEMBL2062848,TN,INACT,0.0	CHEMBL110740,TP,ACT,0.8899999856948853	CHEMBL598845,TP,ACT,0.949999988079071	CHEMBL2112411,FP,INACT,0.9200000166893005	CHEMBL261623,TN,INACT,0.0	CHEMBL1098614,TP,ACT,1.0	CHEMBL6626,TP,ACT,0.9399999976158142	CHEMBL2207493,TN,INACT,0.18000000715255737	CHEMBL149592,TN,INACT,0.0	CHEMBL240895,TN,INACT,0.009999999776482582	CHEMBL274822,TN,INACT,0.1599999964237213	CHEMBL2237149,TP,ACT,0.9900000095367432	CHEMBL120278,TN,INACT,0.009999999776482582	CHEMBL390667,TN,INACT,0.07999999821186066	CHEMBL143728,TP,ACT,1.0	CHEMBL58228,TN,INACT,0.20000000298023224	CHEMBL2436819,TN,INACT,0.0	CHEMBL3742391,TP,ACT,1.0	CHEMBL12529,FP,INACT,1.0	CHEMBL431000,TP,ACT,0.9900000095367432	CHEMBL304636,TP,ACT,0.9700000286102295	CHEMBL191907,TP,ACT,0.9800000190734863	CHEMBL75358,TN,INACT,0.28999999165534973	CHEMBL211301,TP,ACT,0.949999988079071	CHEMBL3360996,TP,ACT,0.9800000190734863	CHEMBL3290986,FP,INACT,1.0	CHEMBL54265,TP,ACT,0.9700000286102295	CHEMBL39852,TN,INACT,0.009999999776482582	CHEMBL514606,TN,INACT,0.009999999776482582	CHEMBL100832,TN,INACT,0.019999999552965164	CHEMBL715,TP,ACT,0.9800000190734863	CHEMBL397166,TP,ACT,0.8399999737739563	CHEMBL556969,TP,ACT,0.9900000095367432	CHEMBL554670,TP,ACT,1.0	CHEMBL293218,TP,ACT,0.9800000190734863	CHEMBL127411,FN,ACT,0.009999999776482582	CHEMBL240657,TN,INACT,0.7200000286102295	CHEMBL117899,TN,INACT,0.009999999776482582	CHEMBL391196,TP,ACT,0.9800000190734863	CHEMBL19262,TP,ACT,1.0	CHEMBL307770,TP,ACT,0.9800000190734863	CHEMBL3634804,FN,ACT,0.11999999731779099	CHEMBL476010,TP,ACT,0.9900000095367432	CHEMBL514202,TP,ACT,0.8600000143051147	CHEMBL404557,TN,INACT,0.019999999552965164	CHEMBL136873,TP,ACT,1.0	CHEMBL145939,TP,ACT,0.9900000095367432	CHEMBL593547,FN,ACT,0.2800000011920929	CHEMBL3361003,FP,INACT,0.9900000095367432	CHEMBL118,TN,INACT,0.6000000238418579	CHEMBL451363,TN,INACT,0.009999999776482582	CHEMBL3741933,TP,ACT,0.9599999785423279	CHEMBL75141,TN,INACT,0.0	CHEMBL3634817,TP,ACT,0.9800000190734863	CHEMBL64079,TP,ACT,0.9900000095367432	CHEMBL3590092,TP,ACT,0.9900000095367432	CHEMBL426537,TN,INACT,0.029999999329447746	CHEMBL3604300,TN,INACT,0.009999999776482582	CHEMBL98241,TP,ACT,0.9100000262260437	CHEMBL43788,TN,INACT,0.4000000059604645	CHEMBL277079,FP,INACT,0.9599999785423279	CHEMBL558766,TN,INACT,0.05000000074505806	CHEMBL3764306,FP,INACT,0.9900000095367432	CHEMBL3085215,TN,INACT,0.03999999910593033	CHEMBL63024,TP,ACT,0.9900000095367432	CHEMBL576586,TP,ACT,1.0	CHEMBL3349057,TP,ACT,0.8700000047683716	CHEMBL96504,TP,ACT,1.0	CHEMBL193867,TP,ACT,0.9900000095367432	CHEMBL1275791,TN,INACT,0.75	CHEMBL356149,TP,ACT,0.7900000214576721	CHEMBL340511,TP,ACT,1.0	CHEMBL2321893,TN,INACT,0.009999999776482582	CHEMBL515472,TP,ACT,0.9100000262260437	CHEMBL295698,TN,INACT,0.0	CHEMBL477266,TP,ACT,0.9800000190734863	CHEMBL3349065,TP,ACT,0.9599999785423279	CHEMBL148826,TN,INACT,0.0	CHEMBL241279,TN,INACT,0.029999999329447746	CHEMBL297785,TP,ACT,0.9800000190734863	CHEMBL364845,TN,INACT,0.0	CHEMBL151043,TP,ACT,0.9700000286102295	CHEMBL2112077,TN,INACT,0.0	CHEMBL191286,TP,ACT,0.9900000095367432	CHEMBL333246,TP,ACT,0.9900000095367432	CHEMBL281811,TN,INACT,0.009999999776482582	CHEMBL3326905,TN,INACT,0.0	CHEMBL3742070,TP,ACT,1.0	CHEMBL3233867,TP,ACT,0.9300000071525574	CHEMBL3360998,TP,ACT,0.9900000095367432	CHEMBL44262,TN,INACT,0.0	CHEMBL164471,TP,ACT,1.0	CHEMBL514965,TN,INACT,0.0	CHEMBL352107,TP,ACT,0.9700000286102295	CHEMBL336126,FN,ACT,0.019999999552965164	CHEMBL328048,TP,ACT,1.0	CHEMBL344568,FP,INACT,0.949999988079071	CHEMBL418375,TN,INACT,0.0	CHEMBL299213,TP,ACT,0.9800000190734863	CHEMBL154787,TP,ACT,1.0	CHEMBL143937,TP,ACT,0.9900000095367432	CHEMBL424890,TP,ACT,0.9800000190734863	CHEMBL589,TN,INACT,0.03999999910593033	CHEMBL3349062,TP,ACT,0.9800000190734863	CHEMBL369367,TP,ACT,0.9800000190734863	CHEMBL1927098,TP,ACT,0.9900000095367432	CHEMBL541424,TN,INACT,0.0	CHEMBL511555,TP,ACT,0.9800000190734863	CHEMBL146998,TP,ACT,1.0	CHEMBL31115,TP,ACT,0.9599999785423279	CHEMBL2436817,TN,INACT,0.07999999821186066	CHEMBL123654,TN,INACT,0.0	CHEMBL240021,TN,INACT,0.009999999776482582	CHEMBL2111789,TN,INACT,0.0	CHEMBL3264204,FP,INACT,0.9200000166893005	CHEMBL112877,TN,INACT,0.0	CHEMBL243523,TP,ACT,1.0	CHEMBL163605,TP,ACT,0.9800000190734863	CHEMBL285357,FP,INACT,0.8999999761581421	CHEMBL3120691,TP,ACT,1.0	CHEMBL555362,TP,ACT,0.9900000095367432	CHEMBL337552,TN,INACT,0.029999999329447746	CHEMBL1381098,FP,INACT,0.9900000095367432	CHEMBL110695,TN,INACT,0.0	CHEMBL105457,TN,INACT,0.009999999776482582	CHEMBL392401,FP,INACT,0.9800000190734863	CHEMBL64344,TP,ACT,0.9599999785423279	CHEMBL306907,FP,INACT,0.9100000262260437	CHEMBL63003,FP,INACT,0.9200000166893005	CHEMBL3349059,TP,ACT,0.949999988079071	CHEMBL293879,TN,INACT,0.0	CHEMBL67700,TN,INACT,0.009999999776482582	CHEMBL321682,TP,ACT,1.0	CHEMBL129014,TN,INACT,0.019999999552965164	CHEMBL608530,TP,ACT,1.0	CHEMBL114478,TN,INACT,0.07999999821186066	CHEMBL199385,TP,ACT,0.9399999976158142	CHEMBL490633,TP,ACT,0.9599999785423279	CHEMBL3590091,TP,ACT,0.9900000095367432	CHEMBL140495,TN,INACT,0.20000000298023224	CHEMBL3325705,TN,INACT,0.0	CHEMBL3763834,TP,ACT,1.0	CHEMBL2391352,TN,INACT,0.009999999776482582	CHEMBL287605,TP,ACT,0.9800000190734863	CHEMBL142295,TN,INACT,0.12999999523162842	CHEMBL18929,TP,ACT,0.9900000095367432	CHEMBL142316,TP,ACT,1.0	CHEMBL146620,TP,ACT,1.0	CHEMBL76406,TP,ACT,1.0	CHEMBL343078,FN,ACT,0.019999999552965164	CHEMBL70319,FN,ACT,0.009999999776482582	CHEMBL2058703,FN,ACT,0.6899999976158142	CHEMBL293364,FN,ACT,0.10999999940395355	CHEMBL379780,TP,ACT,0.9300000071525574	CHEMBL435810,TN,INACT,0.0	CHEMBL92673,FN,ACT,0.10000000149011612	CHEMBL608330,TN,INACT,0.009999999776482582	CHEMBL424258,TP,ACT,0.9800000190734863	CHEMBL1171823,TP,ACT,0.9900000095367432	CHEMBL458947,TN,INACT,0.009999999776482582	CHEMBL319231,TN,INACT,0.009999999776482582	CHEMBL3634825,TP,ACT,0.9900000095367432	CHEMBL229021,TP,ACT,0.9800000190734863	CHEMBL356075,TP,ACT,0.9200000166893005	CHEMBL15809,TP,ACT,0.9100000262260437	CHEMBL163743,TN,INACT,0.0	CHEMBL541164,FP,INACT,0.8999999761581421	CHEMBL2312346,TN,INACT,0.0	CHEMBL495572,TP,ACT,1.0	CHEMBL116931,TP,ACT,0.9900000095367432	CHEMBL234646,TP,ACT,0.949999988079071	CHEMBL312150,TN,INACT,0.0	CHEMBL332077,TP,ACT,1.0	CHEMBL222301,TP,ACT,0.9300000071525574	CHEMBL3764420,TP,ACT,1.0	CHEMBL3699819,TP,ACT,0.9599999785423279	CHEMBL3349052,FN,ACT,0.03999999910593033	CHEMBL267476,TN,INACT,0.10000000149011612	CHEMBL100513,FP,INACT,0.9399999976158142	CHEMBL2323443,TN,INACT,0.3499999940395355	CHEMBL105152,FN,ACT,0.36000001430511475	CHEMBL98398,TN,INACT,0.009999999776482582	CHEMBL308756,TN,INACT,0.0	CHEMBL150743,TN,INACT,0.0	CHEMBL148348,TP,ACT,0.9800000190734863	CHEMBL147164,TP,ACT,0.9599999785423279	CHEMBL344187,TN,INACT,0.0	CHEMBL383805,TN,INACT,0.0	CHEMBL2112360,TP,ACT,1.0	CHEMBL25303,TN,INACT,0.0	CHEMBL60620,TN,INACT,0.0	CHEMBL241514,TN,INACT,0.7599999904632568	CHEMBL135076,TP,ACT,0.9900000095367432	CHEMBL418994,TP,ACT,0.9599999785423279	CHEMBL110948,TP,ACT,1.0	CHEMBL3753681,FN,ACT,0.0	CHEMBL67313,TN,INACT,0.009999999776482582	CHEMBL450729,TN,INACT,0.0	CHEMBL432974,TN,INACT,0.0	CHEMBL565484,TP,ACT,0.9599999785423279	CHEMBL114074,TN,INACT,0.0	CHEMBL142368,FN,ACT,0.029999999329447746	CHEMBL3665441,TN,INACT,0.23999999463558197	CHEMBL94612,TP,ACT,1.0	CHEMBL1223275,TN,INACT,0.0	CHEMBL279586,TP,ACT,1.0	CHEMBL475496,TN,INACT,0.0	CHEMBL3361001,TP,ACT,0.9900000095367432	CHEMBL62,FN,ACT,0.23999999463558197	CHEMBL610372,TN,INACT,0.009999999776482582	CHEMBL3120678,TP,ACT,1.0	CHEMBL2112361,TP,ACT,0.9800000190734863	CHEMBL146410,TP,ACT,0.9800000190734863	CHEMBL19589,TP,ACT,0.9900000095367432	CHEMBL396271,TN,INACT,0.0	CHEMBL226858,FN,ACT,0.44999998807907104	CHEMBL294849,TN,INACT,0.03999999910593033	CHEMBL3120682,TP,ACT,1.0	CHEMBL452861,TP,ACT,0.949999988079071	CHEMBL345357,TN,INACT,0.75	CHEMBL226707,TP,ACT,0.9900000095367432	CHEMBL1170027,TN,INACT,0.0	CHEMBL89457,TN,INACT,0.009999999776482582	CHEMBL352925,TN,INACT,0.029999999329447746	CHEMBL284855,TN,INACT,0.0	CHEMBL342119,TP,ACT,1.0	CHEMBL316247,TN,INACT,0.7200000286102295	CHEMBL421349,FP,INACT,0.9300000071525574	CHEMBL3590093,TP,ACT,0.9599999785423279	CHEMBL43330,TN,INACT,0.0	CHEMBL537834,FP,INACT,0.9700000286102295	CHEMBL73792,TP,ACT,1.0	CHEMBL305558,TN,INACT,0.0	CHEMBL2387245,FN,ACT,0.1599999964237213	CHEMBL195437,TN,INACT,0.7400000095367432	CHEMBL169178,TN,INACT,0.0	CHEMBL438297,TN,INACT,0.4399999976158142	CHEMBL45160,TN,INACT,0.0	CHEMBL420359,TN,INACT,0.0	CHEMBL490845,TP,ACT,0.9300000071525574	CHEMBL144445,TP,ACT,1.0	CHEMBL1173018,TP,ACT,0.9900000095367432	CHEMBL3634802,TP,ACT,0.9800000190734863	CHEMBL3590090,TP,ACT,1.0	CHEMBL121314,TN,INACT,0.019999999552965164	CHEMBL106163,TN,INACT,0.0	CHEMBL91588,TP,ACT,0.9900000095367432	CHEMBL109548,TP,ACT,0.949999988079071	CHEMBL423405,TN,INACT,0.009999999776482582	CHEMBL228865,FN,ACT,0.5799999833106995	CHEMBL89358,FP,INACT,0.8600000143051147	CHEMBL1927090,TP,ACT,0.9900000095367432	CHEMBL544089,TP,ACT,0.9900000095367432	CHEMBL89494,TN,INACT,0.0	CHEMBL286214,TN,INACT,0.0	CHEMBL308090,TP,ACT,0.9900000095367432	CHEMBL455493,TN,INACT,0.18000000715255737	CHEMBL147387,TP,ACT,0.9900000095367432	CHEMBL2414354,TP,ACT,1.0	CHEMBL79030,TN,INACT,0.20000000298023224	CHEMBL229940,TP,ACT,0.9800000190734863	CHEMBL3763633,TP,ACT,0.9900000095367432	CHEMBL152622,TP,ACT,0.8999999761581421	CHEMBL64146,FN,ACT,0.12999999523162842	CHEMBL197664,FN,ACT,0.6600000262260437	CHEMBL17875,TN,INACT,0.6399999856948853	CHEMBL48194,TP,ACT,1.0	CHEMBL390842,TN,INACT,0.009999999776482582	CHEMBL514701,TN,INACT,0.009999999776482582	CHEMBL42586,TN,INACT,0.0	CHEMBL392115,TN,INACT,0.05000000074505806	CHEMBL94644,TP,ACT,0.9599999785423279	CHEMBL134394,TP,ACT,0.8100000023841858	CHEMBL390580,TP,ACT,0.9800000190734863	CHEMBL2062861,TN,INACT,0.0	CHEMBL2158704,FN,ACT,0.7400000095367432	CHEMBL2113072,TN,INACT,0.0	CHEMBL112314,TN,INACT,0.009999999776482582	CHEMBL172369,TP,ACT,0.9900000095367432	CHEMBL3349058,TP,ACT,0.8600000143051147	CHEMBL3233865,TP,ACT,0.9900000095367432	CHEMBL177546,TN,INACT,0.0	CHEMBL321178,TN,INACT,0.009999999776482582	CHEMBL217002,TN,INACT,0.0	CHEMBL3326222,FP,INACT,0.8199999928474426	CHEMBL134351,TP,ACT,0.9900000095367432	CHEMBL77049,TP,ACT,1.0	CHEMBL76480,FN,ACT,0.0	CHEMBL103388,FN,ACT,0.019999999552965164	CHEMBL21328,TN,INACT,0.03999999910593033	CHEMBL519145,TP,ACT,0.949999988079071	CHEMBL43355,TP,ACT,0.9900000095367432	CHEMBL120216,TP,ACT,0.8299999833106995	CHEMBL462650,TN,INACT,0.7799999713897705	CHEMBL80180,TN,INACT,0.6600000262260437	CHEMBL593685,TN,INACT,0.009999999776482582	CHEMBL111352,TP,ACT,1.0	CHEMBL2112363,TP,ACT,0.9800000190734863	CHEMBL267599,TP,ACT,0.9700000286102295	CHEMBL536044,TN,INACT,0.03999999910593033	CHEMBL443926,TN,INACT,0.20999999344348907	CHEMBL334933,TN,INACT,0.07999999821186066	CHEMBL331193,FN,ACT,0.0	CHEMBL112590,TP,ACT,0.8600000143051147	CHEMBL575027,TN,INACT,0.0	CHEMBL393077,TP,ACT,0.9599999785423279	CHEMBL279841,TP,ACT,0.9800000190734863	CHEMBL1097970,TP,ACT,1.0	CHEMBL489621,TP,ACT,0.9800000190734863	CHEMBL483469,TN,INACT,0.029999999329447746	CHEMBL1259241,TN,INACT,0.009999999776482582	CHEMBL62716,TN,INACT,0.0	CHEMBL220808,TP,ACT,0.9700000286102295	CHEMBL178055,FN,ACT,0.6800000071525574	CHEMBL172947,TP,ACT,0.9900000095367432	CHEMBL1154,TP,ACT,0.8600000143051147	CHEMBL478413,TP,ACT,0.949999988079071	CHEMBL351183,FP,INACT,0.9700000286102295	CHEMBL3084422,FN,ACT,0.10000000149011612	CHEMBL103371,TP,ACT,0.9800000190734863	CHEMBL163,TN,INACT,0.0	CHEMBL2387254,TP,ACT,0.9900000095367432	CHEMBL432144,TN,INACT,0.0	CHEMBL342256,TN,INACT,0.07000000029802322	CHEMBL2436714,TN,INACT,0.019999999552965164	CHEMBL105567,TN,INACT,0.0	CHEMBL3741290,TN,INACT,0.019999999552965164	CHEMBL143110,TP,ACT,1.0	CHEMBL3114145,TN,INACT,0.0	CHEMBL1086959,TP,ACT,0.9900000095367432	CHEMBL408589,TP,ACT,0.9900000095367432	CHEMBL7617,TP,ACT,0.9900000095367432	CHEMBL123027,TP,ACT,1.0	CHEMBL45158,TP,ACT,0.8500000238418579	CHEMBL341031,TN,INACT,0.0	CHEMBL233579,TP,ACT,0.9900000095367432	CHEMBL436180,TN,INACT,0.009999999776482582	CHEMBL85,TP,ACT,0.8600000143051147	CHEMBL3649661,TP,ACT,0.9900000095367432	CHEMBL377542,TN,INACT,0.7400000095367432	CHEMBL2391836,FP,INACT,0.9700000286102295	CHEMBL315096,TN,INACT,0.6000000238418579	CHEMBL90977,FN,ACT,0.019999999552965164	CHEMBL416019,TN,INACT,0.699999988079071	CHEMBL343455,FN,ACT,0.1899999976158142	CHEMBL2387261,TP,ACT,1.0	CHEMBL128185,TP,ACT,0.9900000095367432	CHEMBL306141,TP,ACT,1.0	CHEMBL172163,TN,INACT,0.5199999809265137	CHEMBL61792,TN,INACT,0.0	CHEMBL320178,TN,INACT,0.0	CHEMBL3410297,TN,INACT,0.28999999165534973	

