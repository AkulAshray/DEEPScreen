CNNModel CHEMBL3411 adam 0.0005 30 32 0 0.6 False True
Number of active compounds :	105
Number of inactive compounds :	104
---------------------------------
Run id: CNNModel_CHEMBL3411_adam_0.0005_30_32_0_0.6_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL3411_adam_0.0005_30_32_0.6_True/
---------------------------------
Training samples: 128
Validation samples: 41
--
Training Step: 1  | time: 1.216s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/128
[A[ATraining Step: 2  | total loss: [1m[32m0.62397[0m[0m | time: 2.144s
[2K
| Adam | epoch: 001 | loss: 0.62397 - acc: 0.4219 -- iter: 064/128
[A[ATraining Step: 3  | total loss: [1m[32m0.68068[0m[0m | time: 3.142s
[2K
| Adam | epoch: 001 | loss: 0.68068 - acc: 0.4347 -- iter: 096/128
[A[ATraining Step: 4  | total loss: [1m[32m0.68957[0m[0m | time: 5.246s
[2K
| Adam | epoch: 001 | loss: 0.68957 - acc: 0.5540 | val_loss: 0.69188 - val_acc: 0.5610 -- iter: 128/128
--
Training Step: 5  | total loss: [1m[32m0.69278[0m[0m | time: 8.961s
[2K
| Adam | epoch: 002 | loss: 0.69278 - acc: 0.4950 -- iter: 032/128
[A[ATraining Step: 6  | total loss: [1m[32m0.69338[0m[0m | time: 14.245s
[2K
| Adam | epoch: 002 | loss: 0.69338 - acc: 0.4781 -- iter: 064/128
[A[ATraining Step: 7  | total loss: [1m[32m0.69289[0m[0m | time: 15.320s
[2K
| Adam | epoch: 002 | loss: 0.69289 - acc: 0.5100 -- iter: 096/128
[A[ATraining Step: 8  | total loss: [1m[32m0.69361[0m[0m | time: 17.428s
[2K
| Adam | epoch: 002 | loss: 0.69361 - acc: 0.4692 | val_loss: 0.69223 - val_acc: 0.5610 -- iter: 128/128
--
Training Step: 9  | total loss: [1m[32m0.69287[0m[0m | time: 1.320s
[2K
| Adam | epoch: 003 | loss: 0.69287 - acc: 0.5351 -- iter: 032/128
[A[ATraining Step: 10  | total loss: [1m[32m0.69290[0m[0m | time: 2.484s
[2K
| Adam | epoch: 003 | loss: 0.69290 - acc: 0.5176 -- iter: 064/128
[A[ATraining Step: 11  | total loss: [1m[32m0.69299[0m[0m | time: 6.883s
[2K
| Adam | epoch: 003 | loss: 0.69299 - acc: 0.5092 -- iter: 096/128
[A[ATraining Step: 12  | total loss: [1m[32m0.69231[0m[0m | time: 32.252s
[2K
| Adam | epoch: 003 | loss: 0.69231 - acc: 0.5473 | val_loss: 0.69065 - val_acc: 0.5610 -- iter: 128/128
--
Training Step: 13  | total loss: [1m[32m0.69183[0m[0m | time: 36.026s
[2K
| Adam | epoch: 004 | loss: 0.69183 - acc: 0.5538 -- iter: 032/128
[A[ATraining Step: 14  | total loss: [1m[32m0.69079[0m[0m | time: 62.597s
[2K
| Adam | epoch: 004 | loss: 0.69079 - acc: 0.5701 -- iter: 064/128
[A[ATraining Step: 15  | total loss: [1m[32m0.69162[0m[0m | time: 98.090s
[2K
| Adam | epoch: 004 | loss: 0.69162 - acc: 0.5427 -- iter: 096/128
[A[ATraining Step: 16  | total loss: [1m[32m0.69258[0m[0m | time: 118.041s
[2K
| Adam | epoch: 004 | loss: 0.69258 - acc: 0.5267 | val_loss: 0.68817 - val_acc: 0.5610 -- iter: 128/128
--
Training Step: 17  | total loss: [1m[32m0.69430[0m[0m | time: 1.060s
[2K
| Adam | epoch: 005 | loss: 0.69430 - acc: 0.5058 -- iter: 032/128
[A[ATraining Step: 18  | total loss: [1m[32m0.69636[0m[0m | time: 2.184s
[2K
| Adam | epoch: 005 | loss: 0.69636 - acc: 0.4822 -- iter: 064/128
[A[ATraining Step: 19  | total loss: [1m[32m0.69101[0m[0m | time: 3.513s
[2K
| Adam | epoch: 005 | loss: 0.69101 - acc: 0.5402 -- iter: 096/128
[A[ATraining Step: 20  | total loss: [1m[32m0.69073[0m[0m | time: 5.671s
[2K
| Adam | epoch: 005 | loss: 0.69073 - acc: 0.5373 | val_loss: 0.68819 - val_acc: 0.5610 -- iter: 128/128
--
Training Step: 21  | total loss: [1m[32m0.69076[0m[0m | time: 3.786s
[2K
| Adam | epoch: 006 | loss: 0.69076 - acc: 0.5354 -- iter: 032/128
[A[ATraining Step: 22  | total loss: [1m[32m0.69163[0m[0m | time: 15.425s
[2K
| Adam | epoch: 006 | loss: 0.69163 - acc: 0.5248 -- iter: 064/128
[A[ATraining Step: 23  | total loss: [1m[32m0.69238[0m[0m | time: 33.616s
[2K
| Adam | epoch: 006 | loss: 0.69238 - acc: 0.5176 -- iter: 096/128
[A[ATraining Step: 24  | total loss: [1m[32m0.69335[0m[0m | time: 56.530s
[2K
| Adam | epoch: 006 | loss: 0.69335 - acc: 0.5039 | val_loss: 0.68797 - val_acc: 0.5610 -- iter: 128/128
--
Training Step: 25  | total loss: [1m[32m0.69109[0m[0m | time: 17.479s
[2K
| Adam | epoch: 007 | loss: 0.69109 - acc: 0.5284 -- iter: 032/128
[A[ATraining Step: 26  | total loss: [1m[32m0.68915[0m[0m | time: 18.564s
[2K
| Adam | epoch: 007 | loss: 0.68915 - acc: 0.5457 -- iter: 064/128
[A[ATraining Step: 27  | total loss: [1m[32m0.68798[0m[0m | time: 19.770s
[2K
| Adam | epoch: 007 | loss: 0.68798 - acc: 0.5500 -- iter: 096/128
[A[ATraining Step: 28  | total loss: [1m[32m0.68856[0m[0m | time: 21.990s
[2K
| Adam | epoch: 007 | loss: 0.68856 - acc: 0.5453 | val_loss: 0.68609 - val_acc: 0.5610 -- iter: 128/128
--
Training Step: 29  | total loss: [1m[32m0.69589[0m[0m | time: 1.972s
[2K
| Adam | epoch: 008 | loss: 0.69589 - acc: 0.5039 -- iter: 032/128
[A[ATraining Step: 30  | total loss: [1m[32m0.69061[0m[0m | time: 16.226s
[2K
| Adam | epoch: 008 | loss: 0.69061 - acc: 0.5326 -- iter: 064/128
[A[ATraining Step: 31  | total loss: [1m[32m0.68696[0m[0m | time: 31.539s
[2K
| Adam | epoch: 008 | loss: 0.68696 - acc: 0.5539 -- iter: 096/128
[A[ATraining Step: 32  | total loss: [1m[32m0.68480[0m[0m | time: 56.692s
[2K
| Adam | epoch: 008 | loss: 0.68480 - acc: 0.5629 | val_loss: 0.68480 - val_acc: 0.5610 -- iter: 128/128
--
Training Step: 33  | total loss: [1m[32m0.68414[0m[0m | time: 17.795s
[2K
| Adam | epoch: 009 | loss: 0.68414 - acc: 0.5628 -- iter: 032/128
[A[ATraining Step: 34  | total loss: [1m[32m0.68717[0m[0m | time: 23.667s
[2K
| Adam | epoch: 009 | loss: 0.68717 - acc: 0.5493 -- iter: 064/128
[A[ATraining Step: 35  | total loss: [1m[32m0.68415[0m[0m | time: 34.312s
[2K
| Adam | epoch: 009 | loss: 0.68415 - acc: 0.5586 -- iter: 096/128
[A[ATraining Step: 36  | total loss: [1m[32m0.68086[0m[0m | time: 36.471s
[2K
| Adam | epoch: 009 | loss: 0.68086 - acc: 0.5658 | val_loss: 0.68472 - val_acc: 0.5610 -- iter: 128/128
--
Training Step: 37  | total loss: [1m[32m0.68759[0m[0m | time: 1.312s
[2K
| Adam | epoch: 010 | loss: 0.68759 - acc: 0.5464 -- iter: 032/128
[A[ATraining Step: 38  | total loss: [1m[32m0.68520[0m[0m | time: 2.612s
[2K
| Adam | epoch: 010 | loss: 0.68520 - acc: 0.5496 -- iter: 064/128
[A[ATraining Step: 39  | total loss: [1m[32m0.68841[0m[0m | time: 4.793s
[2K
| Adam | epoch: 010 | loss: 0.68841 - acc: 0.5401 -- iter: 096/128
[A[ATraining Step: 40  | total loss: [1m[32m0.68784[0m[0m | time: 42.491s
[2K
| Adam | epoch: 010 | loss: 0.68784 - acc: 0.5384 | val_loss: 0.68499 - val_acc: 0.5610 -- iter: 128/128
--
Training Step: 41  | total loss: [1m[32m0.68738[0m[0m | time: 14.548s
[2K
| Adam | epoch: 011 | loss: 0.68738 - acc: 0.5371 -- iter: 032/128
[A[ATraining Step: 42  | total loss: [1m[32m0.68731[0m[0m | time: 26.949s
[2K
| Adam | epoch: 011 | loss: 0.68731 - acc: 0.5360 -- iter: 064/128
[A[ATraining Step: 43  | total loss: [1m[32m0.68531[0m[0m | time: 33.338s
[2K
| Adam | epoch: 011 | loss: 0.68531 - acc: 0.5407 -- iter: 096/128
[A[ATraining Step: 44  | total loss: [1m[32m0.68656[0m[0m | time: 35.380s
[2K
| Adam | epoch: 011 | loss: 0.68656 - acc: 0.5283 | val_loss: 0.68398 - val_acc: 0.5610 -- iter: 128/128
--
Training Step: 45  | total loss: [1m[32m0.68545[0m[0m | time: 2.987s
[2K
| Adam | epoch: 012 | loss: 0.68545 - acc: 0.5341 -- iter: 032/128
[A[ATraining Step: 46  | total loss: [1m[32m0.68455[0m[0m | time: 6.299s
[2K
| Adam | epoch: 012 | loss: 0.68455 - acc: 0.5388 -- iter: 064/128
[A[ATraining Step: 47  | total loss: [1m[32m0.68370[0m[0m | time: 9.468s
[2K
| Adam | epoch: 012 | loss: 0.68370 - acc: 0.5325 -- iter: 096/128
[A[ATraining Step: 48  | total loss: [1m[32m0.68322[0m[0m | time: 11.625s
[2K
| Adam | epoch: 012 | loss: 0.68322 - acc: 0.5323 | val_loss: 0.68075 - val_acc: 0.5610 -- iter: 128/128
--
Training Step: 49  | total loss: [1m[32m0.68384[0m[0m | time: 1.414s
[2K
| Adam | epoch: 013 | loss: 0.68384 - acc: 0.5173 -- iter: 032/128
[A[ATraining Step: 50  | total loss: [1m[32m0.68454[0m[0m | time: 2.780s
[2K
| Adam | epoch: 013 | loss: 0.68454 - acc: 0.5098 -- iter: 064/128
[A[ATraining Step: 51  | total loss: [1m[32m0.68475[0m[0m | time: 8.437s
[2K
| Adam | epoch: 013 | loss: 0.68475 - acc: 0.5035 -- iter: 096/128
[A[ATraining Step: 52  | total loss: [1m[32m0.68016[0m[0m | time: 31.809s
[2K
| Adam | epoch: 013 | loss: 0.68016 - acc: 0.5358 | val_loss: 0.67617 - val_acc: 0.5610 -- iter: 128/128
--
Training Step: 53  | total loss: [1m[32m0.67391[0m[0m | time: 4.095s
[2K
| Adam | epoch: 014 | loss: 0.67391 - acc: 0.5582 -- iter: 032/128
[A[ATraining Step: 54  | total loss: [1m[32m0.66954[0m[0m | time: 9.549s
[2K
| Adam | epoch: 014 | loss: 0.66954 - acc: 0.5588 -- iter: 064/128
[A[ATraining Step: 55  | total loss: [1m[32m0.66920[0m[0m | time: 16.279s
[2K
| Adam | epoch: 014 | loss: 0.66920 - acc: 0.5504 -- iter: 096/128
[A[ATraining Step: 56  | total loss: [1m[32m0.66797[0m[0m | time: 34.269s
[2K
| Adam | epoch: 014 | loss: 0.66797 - acc: 0.5433 | val_loss: 0.67082 - val_acc: 0.5610 -- iter: 128/128
--
Training Step: 57  | total loss: [1m[32m0.67604[0m[0m | time: 1.191s
[2K
| Adam | epoch: 015 | loss: 0.67604 - acc: 0.5330 -- iter: 032/128
[A[ATraining Step: 58  | total loss: [1m[32m0.67359[0m[0m | time: 2.578s
[2K
| Adam | epoch: 015 | loss: 0.67359 - acc: 0.5455 -- iter: 064/128
[A[ATraining Step: 59  | total loss: [1m[32m0.66996[0m[0m | time: 3.710s
[2K
| Adam | epoch: 015 | loss: 0.66996 - acc: 0.5562 -- iter: 096/128
[A[ATraining Step: 60  | total loss: [1m[32m0.67068[0m[0m | time: 5.880s
[2K
| Adam | epoch: 015 | loss: 0.67068 - acc: 0.5777 | val_loss: 0.67884 - val_acc: 0.5610 -- iter: 128/128
--
Training Step: 61  | total loss: [1m[32m0.67045[0m[0m | time: 6.343s
[2K
| Adam | epoch: 016 | loss: 0.67045 - acc: 0.6002 -- iter: 032/128
[A[ATraining Step: 62  | total loss: [1m[32m0.66921[0m[0m | time: 10.944s
[2K
| Adam | epoch: 016 | loss: 0.66921 - acc: 0.6114 -- iter: 064/128
[A[ATraining Step: 63  | total loss: [1m[32m0.66413[0m[0m | time: 19.949s
[2K
| Adam | epoch: 016 | loss: 0.66413 - acc: 0.6369 -- iter: 096/128
[A[ATraining Step: 64  | total loss: [1m[32m0.66076[0m[0m | time: 23.901s
[2K
| Adam | epoch: 016 | loss: 0.66076 - acc: 0.6354 | val_loss: 0.67340 - val_acc: 0.5854 -- iter: 128/128
--
Training Step: 65  | total loss: [1m[32m0.65989[0m[0m | time: 1.990s
[2K
| Adam | epoch: 017 | loss: 0.65989 - acc: 0.6264 -- iter: 032/128
[A[ATraining Step: 66  | total loss: [1m[32m0.65997[0m[0m | time: 12.618s
[2K
| Adam | epoch: 017 | loss: 0.65997 - acc: 0.6149 -- iter: 064/128
[A[ATraining Step: 67  | total loss: [1m[32m0.64934[0m[0m | time: 18.538s
[2K
| Adam | epoch: 017 | loss: 0.64934 - acc: 0.6348 -- iter: 096/128
[A[ATraining Step: 68  | total loss: [1m[32m0.65023[0m[0m | time: 20.715s
[2K
| Adam | epoch: 017 | loss: 0.65023 - acc: 0.6300 | val_loss: 0.67620 - val_acc: 0.5610 -- iter: 128/128
--
Training Step: 69  | total loss: [1m[32m0.64520[0m[0m | time: 1.161s
[2K
| Adam | epoch: 018 | loss: 0.64520 - acc: 0.6367 -- iter: 032/128
[A[ATraining Step: 70  | total loss: [1m[32m0.63751[0m[0m | time: 2.434s
[2K
| Adam | epoch: 018 | loss: 0.63751 - acc: 0.6534 -- iter: 064/128
[A[ATraining Step: 71  | total loss: [1m[32m0.62473[0m[0m | time: 3.662s
[2K
| Adam | epoch: 018 | loss: 0.62473 - acc: 0.6715 -- iter: 096/128
[A[ATraining Step: 72  | total loss: [1m[32m0.61668[0m[0m | time: 6.806s
[2K
| Adam | epoch: 018 | loss: 0.61668 - acc: 0.6698 | val_loss: 0.78498 - val_acc: 0.5854 -- iter: 128/128
--
Training Step: 73  | total loss: [1m[32m0.60964[0m[0m | time: 1.366s
[2K
| Adam | epoch: 019 | loss: 0.60964 - acc: 0.6717 -- iter: 032/128
[A[ATraining Step: 74  | total loss: [1m[32m0.61639[0m[0m | time: 2.602s
[2K
| Adam | epoch: 019 | loss: 0.61639 - acc: 0.6700 -- iter: 064/128
[A[ATraining Step: 75  | total loss: [1m[32m0.60691[0m[0m | time: 3.829s
[2K
| Adam | epoch: 019 | loss: 0.60691 - acc: 0.6719 -- iter: 096/128
[A[ATraining Step: 76  | total loss: [1m[32m0.59199[0m[0m | time: 6.124s
[2K
| Adam | epoch: 019 | loss: 0.59199 - acc: 0.6937 | val_loss: 0.75071 - val_acc: 0.4878 -- iter: 128/128
--
Training Step: 77  | total loss: [1m[32m0.58521[0m[0m | time: 5.484s
[2K
| Adam | epoch: 020 | loss: 0.58521 - acc: 0.7096 -- iter: 032/128
[A[ATraining Step: 78  | total loss: [1m[32m0.60253[0m[0m | time: 18.149s
[2K
| Adam | epoch: 020 | loss: 0.60253 - acc: 0.6942 -- iter: 064/128
[A[ATraining Step: 79  | total loss: [1m[32m0.61672[0m[0m | time: 29.785s
[2K
| Adam | epoch: 020 | loss: 0.61672 - acc: 0.6676 -- iter: 096/128
[A[ATraining Step: 80  | total loss: [1m[32m0.61294[0m[0m | time: 49.434s
[2K
| Adam | epoch: 020 | loss: 0.61294 - acc: 0.6697 | val_loss: 0.66412 - val_acc: 0.6341 -- iter: 128/128
--
Training Step: 81  | total loss: [1m[32m0.60497[0m[0m | time: 5.244s
[2K
| Adam | epoch: 021 | loss: 0.60497 - acc: 0.6810 -- iter: 032/128
[A[ATraining Step: 82  | total loss: [1m[32m0.59825[0m[0m | time: 14.857s
[2K
| Adam | epoch: 021 | loss: 0.59825 - acc: 0.6879 -- iter: 064/128
[A[ATraining Step: 83  | total loss: [1m[32m0.58851[0m[0m | time: 15.920s
[2K
| Adam | epoch: 021 | loss: 0.58851 - acc: 0.7003 -- iter: 096/128
[A[ATraining Step: 84  | total loss: [1m[32m0.58810[0m[0m | time: 17.975s
[2K
| Adam | epoch: 021 | loss: 0.58810 - acc: 0.6959 | val_loss: 0.69160 - val_acc: 0.6098 -- iter: 128/128
--
Training Step: 85  | total loss: [1m[32m0.59634[0m[0m | time: 1.053s
[2K
| Adam | epoch: 022 | loss: 0.59634 - acc: 0.6826 -- iter: 032/128
[A[ATraining Step: 86  | total loss: [1m[32m0.60168[0m[0m | time: 2.406s
[2K
| Adam | epoch: 022 | loss: 0.60168 - acc: 0.6768 -- iter: 064/128
[A[ATraining Step: 87  | total loss: [1m[32m0.58809[0m[0m | time: 3.718s
[2K
| Adam | epoch: 022 | loss: 0.58809 - acc: 0.6873 -- iter: 096/128
[A[ATraining Step: 88  | total loss: [1m[32m0.58380[0m[0m | time: 8.181s
[2K
| Adam | epoch: 022 | loss: 0.58380 - acc: 0.6935 | val_loss: 0.65202 - val_acc: 0.5610 -- iter: 128/128
--
Training Step: 89  | total loss: [1m[32m0.57093[0m[0m | time: 5.592s
[2K
| Adam | epoch: 023 | loss: 0.57093 - acc: 0.7179 -- iter: 032/128
[A[ATraining Step: 90  | total loss: [1m[32m0.56677[0m[0m | time: 20.381s
[2K
| Adam | epoch: 023 | loss: 0.56677 - acc: 0.7180 -- iter: 064/128
[A[ATraining Step: 91  | total loss: [1m[32m0.56169[0m[0m | time: 23.557s
[2K
| Adam | epoch: 023 | loss: 0.56169 - acc: 0.7118 -- iter: 096/128
[A[ATraining Step: 92  | total loss: [1m[32m0.54993[0m[0m | time: 32.294s
[2K
| Adam | epoch: 023 | loss: 0.54993 - acc: 0.7313 | val_loss: 0.68691 - val_acc: 0.6098 -- iter: 128/128
--
Training Step: 93  | total loss: [1m[32m0.54300[0m[0m | time: 1.155s
[2K
| Adam | epoch: 024 | loss: 0.54300 - acc: 0.7425 -- iter: 032/128
[A[ATraining Step: 94  | total loss: [1m[32m0.53302[0m[0m | time: 2.173s
[2K
| Adam | epoch: 024 | loss: 0.53302 - acc: 0.7495 -- iter: 064/128
[A[ATraining Step: 95  | total loss: [1m[32m0.52877[0m[0m | time: 3.270s
[2K
| Adam | epoch: 024 | loss: 0.52877 - acc: 0.7558 -- iter: 096/128
[A[ATraining Step: 96  | total loss: [1m[32m0.52372[0m[0m | time: 5.541s
[2K
| Adam | epoch: 024 | loss: 0.52372 - acc: 0.7584 | val_loss: 0.74262 - val_acc: 0.6098 -- iter: 128/128
--
Training Step: 97  | total loss: [1m[32m0.50730[0m[0m | time: 7.705s
[2K
| Adam | epoch: 025 | loss: 0.50730 - acc: 0.7700 -- iter: 032/128
[A[ATraining Step: 98  | total loss: [1m[32m0.50905[0m[0m | time: 18.401s
[2K
| Adam | epoch: 025 | loss: 0.50905 - acc: 0.7743 -- iter: 064/128
[A[ATraining Step: 99  | total loss: [1m[32m0.50331[0m[0m | time: 24.117s
[2K
| Adam | epoch: 025 | loss: 0.50331 - acc: 0.7718 -- iter: 096/128
[A[ATraining Step: 100  | total loss: [1m[32m0.49571[0m[0m | time: 32.932s
[2K
| Adam | epoch: 025 | loss: 0.49571 - acc: 0.7790 | val_loss: 0.75119 - val_acc: 0.6341 -- iter: 128/128
--
Training Step: 101  | total loss: [1m[32m0.48619[0m[0m | time: 1.343s
[2K
| Adam | epoch: 026 | loss: 0.48619 - acc: 0.7855 -- iter: 032/128
[A[ATraining Step: 102  | total loss: [1m[32m0.47270[0m[0m | time: 2.505s
[2K
| Adam | epoch: 026 | loss: 0.47270 - acc: 0.7913 -- iter: 064/128
[A[ATraining Step: 103  | total loss: [1m[32m0.47353[0m[0m | time: 3.617s
[2K
| Adam | epoch: 026 | loss: 0.47353 - acc: 0.7872 -- iter: 096/128
[A[ATraining Step: 104  | total loss: [1m[32m0.46282[0m[0m | time: 6.062s
[2K
| Adam | epoch: 026 | loss: 0.46282 - acc: 0.7929 | val_loss: 0.77689 - val_acc: 0.6098 -- iter: 128/128
--
Training Step: 105  | total loss: [1m[32m0.44219[0m[0m | time: 11.931s
[2K
| Adam | epoch: 027 | loss: 0.44219 - acc: 0.8073 -- iter: 032/128
[A[ATraining Step: 106  | total loss: [1m[32m0.42503[0m[0m | time: 22.454s
[2K
| Adam | epoch: 027 | loss: 0.42503 - acc: 0.8203 -- iter: 064/128
[A[ATraining Step: 107  | total loss: [1m[32m0.42322[0m[0m | time: 32.047s
[2K
| Adam | epoch: 027 | loss: 0.42322 - acc: 0.8258 -- iter: 096/128
[A[ATraining Step: 108  | total loss: [1m[32m0.41544[0m[0m | time: 40.521s
[2K
| Adam | epoch: 027 | loss: 0.41544 - acc: 0.8307 | val_loss: 0.77494 - val_acc: 0.6829 -- iter: 128/128
--
Training Step: 109  | total loss: [1m[32m0.40074[0m[0m | time: 1.285s
[2K
| Adam | epoch: 028 | loss: 0.40074 - acc: 0.8414 -- iter: 032/128
[A[ATraining Step: 110  | total loss: [1m[32m0.39540[0m[0m | time: 2.399s
[2K
| Adam | epoch: 028 | loss: 0.39540 - acc: 0.8416 -- iter: 064/128
[A[ATraining Step: 111  | total loss: [1m[32m0.38404[0m[0m | time: 3.811s
[2K
| Adam | epoch: 028 | loss: 0.38404 - acc: 0.8543 -- iter: 096/128
[A[ATraining Step: 112  | total loss: [1m[32m0.36498[0m[0m | time: 8.855s
[2K
| Adam | epoch: 028 | loss: 0.36498 - acc: 0.8627 | val_loss: 0.73178 - val_acc: 0.6829 -- iter: 128/128
--
Training Step: 113  | total loss: [1m[32m0.37410[0m[0m | time: 6.574s
[2K
| Adam | epoch: 029 | loss: 0.37410 - acc: 0.8576 -- iter: 032/128
[A[ATraining Step: 114  | total loss: [1m[32m0.34775[0m[0m | time: 7.785s
[2K
| Adam | epoch: 029 | loss: 0.34775 - acc: 0.8719 -- iter: 064/128
[A[ATraining Step: 115  | total loss: [1m[32m0.33625[0m[0m | time: 9.011s
[2K
| Adam | epoch: 029 | loss: 0.33625 - acc: 0.8784 -- iter: 096/128
[A[ATraining Step: 116  | total loss: [1m[32m0.32342[0m[0m | time: 11.253s
[2K
| Adam | epoch: 029 | loss: 0.32342 - acc: 0.8843 | val_loss: 0.77258 - val_acc: 0.6341 -- iter: 128/128
--
Training Step: 117  | total loss: [1m[32m0.30751[0m[0m | time: 0.906s
[2K
| Adam | epoch: 030 | loss: 0.30751 - acc: 0.8897 -- iter: 032/128
[A[ATraining Step: 118  | total loss: [1m[32m0.29271[0m[0m | time: 2.009s
[2K
| Adam | epoch: 030 | loss: 0.29271 - acc: 0.8944 -- iter: 064/128
[A[ATraining Step: 119  | total loss: [1m[32m0.28227[0m[0m | time: 3.242s
[2K
| Adam | epoch: 030 | loss: 0.28227 - acc: 0.8988 -- iter: 096/128
[A[ATraining Step: 120  | total loss: [1m[32m0.26620[0m[0m | time: 5.622s
[2K
| Adam | epoch: 030 | loss: 0.26620 - acc: 0.9058 | val_loss: 0.80507 - val_acc: 0.6341 -- iter: 128/128
--
Validation AUC:0.7463768115942029
Validation AUPRC:0.7745094473888183
Test AUC:0.7050000000000001
Test AUPRC:0.5605552255080287
BestTestF1Score	0.64	0.36	0.56	0.47	1.0	16	18	7	0	0.1
BestTestMCCScore	0.64	0.36	0.56	0.47	1.0	16	18	7	0	0.1
BestTestAccuracyScore	0.64	0.36	0.56	0.47	1.0	16	18	7	0	0.1
BestValidationF1Score	0.81	0.53	0.76	0.71	0.96	22	9	9	1	0.1
BestValidationMCC	0.81	0.53	0.76	0.71	0.96	22	9	9	1	0.1
BestValidationAccuracy	0.81	0.53	0.76	0.71	0.96	22	9	9	1	0.1
TestPredictions (Threshold:0.1)
CHEMBL1602800,FP,INACT,0.23000000417232513	CHEMBL1419182,TP,ACT,0.9100000262260437	CHEMBL1468861,FP,INACT,0.6399999856948853	CHEMBL1704776,FP,INACT,0.4300000071525574	CHEMBL297453,FP,INACT,0.12999999523162842	CHEMBL3740962,TP,ACT,0.6399999856948853	CHEMBL1509494,TP,ACT,0.18000000715255737	CHEMBL1405851,TP,ACT,0.5099999904632568	CHEMBL1828984,TN,INACT,0.019999999552965164	CHEMBL1335846,TN,INACT,0.09000000357627869	CHEMBL1546194,FP,INACT,1.0	CHEMBL3654301,FP,INACT,0.12999999523162842	CHEMBL3771157,FP,INACT,0.30000001192092896	CHEMBL1441135,TP,ACT,0.5400000214576721	CHEMBL539027,FP,INACT,0.9900000095367432	CHEMBL423248,TN,INACT,0.029999999329447746	CHEMBL1310798,TN,INACT,0.07000000029802322	CHEMBL1607592,TP,ACT,0.3499999940395355	CHEMBL3197063,TP,ACT,0.6600000262260437	CHEMBL1464388,TP,ACT,1.0	CHEMBL1299499,FP,INACT,0.9599999785423279	CHEMBL1867074,FP,INACT,0.1899999976158142	CHEMBL3742211,TP,ACT,0.5600000023841858	CHEMBL1601256,FP,INACT,0.10000000149011612	CHEMBL1363179,FP,INACT,0.30000001192092896	CHEMBL1560459,TP,ACT,0.25999999046325684	CHEMBL3754735,FP,INACT,0.8700000047683716	CHEMBL1480899,TP,ACT,0.9900000095367432	CHEMBL1303255,TP,ACT,0.25	CHEMBL1442299,TP,ACT,0.8500000238418579	CHEMBL3741673,TP,ACT,0.6000000238418579	CHEMBL1538873,FP,INACT,0.9900000095367432	CHEMBL1600573,FP,INACT,0.6700000166893005	CHEMBL1349802,TP,ACT,0.7799999713897705	CHEMBL2142704,TN,INACT,0.009999999776482582	CHEMBL3221336,FP,INACT,0.10000000149011612	CHEMBL3194374,FP,INACT,0.6200000047683716	CHEMBL3810303,FP,INACT,0.8299999833106995	CHEMBL1828980,TN,INACT,0.019999999552965164	CHEMBL1527578,TP,ACT,0.8899999856948853	CHEMBL159196,TN,INACT,0.009999999776482582	

