ImageNetInceptionV2 CHEMBL4234 adam 0.0001 15 0 0 0.8 False True
Number of active compounds :	173
Number of inactive compounds :	126
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL4234_adam_0.0001_15_0_0_0.8_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL4234_adam_0.0001_15_0.8/
---------------------------------
Training samples: 140
Validation samples: 44
--
Training Step: 1  | time: 39.555s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/140
[A[ATraining Step: 2  | total loss: [1m[32m0.61463[0m[0m | time: 50.219s
[2K
| Adam | epoch: 001 | loss: 0.61463 - acc: 0.3937 -- iter: 064/140
[A[ATraining Step: 3  | total loss: [1m[32m0.71196[0m[0m | time: 60.628s
[2K
| Adam | epoch: 001 | loss: 0.71196 - acc: 0.4040 -- iter: 096/140
[A[ATraining Step: 4  | total loss: [1m[32m0.66105[0m[0m | time: 71.324s
[2K
| Adam | epoch: 001 | loss: 0.66105 - acc: 0.4994 -- iter: 128/140
[A[ATraining Step: 5  | total loss: [1m[32m0.57249[0m[0m | time: 83.849s
[2K
| Adam | epoch: 001 | loss: 0.57249 - acc: 0.6945 | val_loss: 0.70278 - val_acc: 0.4773 -- iter: 140/140
--
Training Step: 6  | total loss: [1m[32m0.58577[0m[0m | time: 4.890s
[2K
| Adam | epoch: 002 | loss: 0.58577 - acc: 0.6766 -- iter: 032/140
[A[ATraining Step: 7  | total loss: [1m[32m0.44375[0m[0m | time: 15.337s
[2K
| Adam | epoch: 002 | loss: 0.44375 - acc: 0.8706 -- iter: 064/140
[A[ATraining Step: 8  | total loss: [1m[32m0.48190[0m[0m | time: 25.841s
[2K
| Adam | epoch: 002 | loss: 0.48190 - acc: 0.8028 -- iter: 096/140
[A[ATraining Step: 9  | total loss: [1m[32m0.48106[0m[0m | time: 36.661s
[2K
| Adam | epoch: 002 | loss: 0.48106 - acc: 0.7583 -- iter: 128/140
[A[ATraining Step: 10  | total loss: [1m[32m0.39562[0m[0m | time: 49.542s
[2K
| Adam | epoch: 002 | loss: 0.39562 - acc: 0.8323 | val_loss: 0.83918 - val_acc: 0.5227 -- iter: 140/140
--
Training Step: 11  | total loss: [1m[32m0.33388[0m[0m | time: 4.607s
[2K
| Adam | epoch: 003 | loss: 0.33388 - acc: 0.9117 -- iter: 032/140
[A[ATraining Step: 12  | total loss: [1m[32m0.27427[0m[0m | time: 9.069s
[2K
| Adam | epoch: 003 | loss: 0.27427 - acc: 0.9514 -- iter: 064/140
[A[ATraining Step: 13  | total loss: [1m[32m0.21362[0m[0m | time: 19.338s
[2K
| Adam | epoch: 003 | loss: 0.21362 - acc: 0.9723 -- iter: 096/140
[A[ATraining Step: 14  | total loss: [1m[32m0.22259[0m[0m | time: 29.672s
[2K
| Adam | epoch: 003 | loss: 0.22259 - acc: 0.9453 -- iter: 128/140
[A[ATraining Step: 15  | total loss: [1m[32m0.23207[0m[0m | time: 42.414s
[2K
| Adam | epoch: 003 | loss: 0.23207 - acc: 0.9300 | val_loss: 1.01847 - val_acc: 0.5227 -- iter: 140/140
--
Training Step: 16  | total loss: [1m[32m0.18099[0m[0m | time: 10.344s
[2K
| Adam | epoch: 004 | loss: 0.18099 - acc: 0.9562 -- iter: 032/140
[A[ATraining Step: 17  | total loss: [1m[32m0.15400[0m[0m | time: 14.813s
[2K
| Adam | epoch: 004 | loss: 0.15400 - acc: 0.9720 -- iter: 064/140
[A[ATraining Step: 18  | total loss: [1m[32m0.12737[0m[0m | time: 19.210s
[2K
| Adam | epoch: 004 | loss: 0.12737 - acc: 0.9817 -- iter: 096/140
[A[ATraining Step: 19  | total loss: [1m[32m0.09724[0m[0m | time: 29.571s
[2K
| Adam | epoch: 004 | loss: 0.09724 - acc: 0.9878 -- iter: 128/140
[A[ATraining Step: 20  | total loss: [1m[32m0.07980[0m[0m | time: 42.257s
[2K
| Adam | epoch: 004 | loss: 0.07980 - acc: 0.9917 | val_loss: 1.06809 - val_acc: 0.5227 -- iter: 140/140
--
Training Step: 21  | total loss: [1m[32m0.06814[0m[0m | time: 10.260s
[2K
| Adam | epoch: 005 | loss: 0.06814 - acc: 0.9943 -- iter: 032/140
[A[ATraining Step: 22  | total loss: [1m[32m0.08763[0m[0m | time: 20.645s
[2K
| Adam | epoch: 005 | loss: 0.08763 - acc: 0.9866 -- iter: 064/140
[A[ATraining Step: 23  | total loss: [1m[32m0.06762[0m[0m | time: 25.016s
[2K
| Adam | epoch: 005 | loss: 0.06762 - acc: 0.9905 -- iter: 096/140
[A[ATraining Step: 24  | total loss: [1m[32m0.05809[0m[0m | time: 29.293s
[2K
| Adam | epoch: 005 | loss: 0.05809 - acc: 0.9932 -- iter: 128/140
[A[ATraining Step: 25  | total loss: [1m[32m0.04818[0m[0m | time: 41.880s
[2K
| Adam | epoch: 005 | loss: 0.04818 - acc: 0.9950 | val_loss: 0.74552 - val_acc: 0.5227 -- iter: 140/140
--
Training Step: 26  | total loss: [1m[32m0.03880[0m[0m | time: 10.216s
[2K
| Adam | epoch: 006 | loss: 0.03880 - acc: 0.9964 -- iter: 032/140
[A[ATraining Step: 27  | total loss: [1m[32m0.03190[0m[0m | time: 20.056s
[2K
| Adam | epoch: 006 | loss: 0.03190 - acc: 0.9973 -- iter: 064/140
[A[ATraining Step: 28  | total loss: [1m[32m0.02518[0m[0m | time: 30.196s
[2K
| Adam | epoch: 006 | loss: 0.02518 - acc: 0.9980 -- iter: 096/140
[A[ATraining Step: 29  | total loss: [1m[32m0.02185[0m[0m | time: 34.613s
[2K
| Adam | epoch: 006 | loss: 0.02185 - acc: 0.9985 -- iter: 128/140
[A[ATraining Step: 30  | total loss: [1m[32m0.12892[0m[0m | time: 41.371s
[2K
| Adam | epoch: 006 | loss: 0.12892 - acc: 0.9791 | val_loss: 0.71426 - val_acc: 0.5682 -- iter: 140/140
--
Training Step: 31  | total loss: [1m[32m0.12693[0m[0m | time: 10.139s
[2K
| Adam | epoch: 007 | loss: 0.12693 - acc: 0.9647 -- iter: 032/140
[A[ATraining Step: 32  | total loss: [1m[32m0.10087[0m[0m | time: 20.213s
[2K
| Adam | epoch: 007 | loss: 0.10087 - acc: 0.9726 -- iter: 064/140
[A[ATraining Step: 33  | total loss: [1m[32m0.07964[0m[0m | time: 30.411s
[2K
| Adam | epoch: 007 | loss: 0.07964 - acc: 0.9786 -- iter: 096/140
[A[ATraining Step: 34  | total loss: [1m[32m0.06388[0m[0m | time: 40.453s
[2K
| Adam | epoch: 007 | loss: 0.06388 - acc: 0.9832 -- iter: 128/140
[A[ATraining Step: 35  | total loss: [1m[32m0.05175[0m[0m | time: 47.083s
[2K
| Adam | epoch: 007 | loss: 0.05175 - acc: 0.9867 | val_loss: 0.74146 - val_acc: 0.5682 -- iter: 140/140
--
Training Step: 36  | total loss: [1m[32m0.07509[0m[0m | time: 4.515s
[2K
| Adam | epoch: 008 | loss: 0.07509 - acc: 0.9724 -- iter: 032/140
[A[ATraining Step: 37  | total loss: [1m[32m0.06839[0m[0m | time: 14.490s
[2K
| Adam | epoch: 008 | loss: 0.06839 - acc: 0.9779 -- iter: 064/140
[A[ATraining Step: 38  | total loss: [1m[32m0.06182[0m[0m | time: 24.380s
[2K
| Adam | epoch: 008 | loss: 0.06182 - acc: 0.9822 -- iter: 096/140
[A[ATraining Step: 39  | total loss: [1m[32m0.07962[0m[0m | time: 34.462s
[2K
| Adam | epoch: 008 | loss: 0.07962 - acc: 0.9677 -- iter: 128/140
[A[ATraining Step: 40  | total loss: [1m[32m0.06543[0m[0m | time: 47.020s
[2K
| Adam | epoch: 008 | loss: 0.06543 - acc: 0.9737 | val_loss: 1.00915 - val_acc: 0.5227 -- iter: 140/140
--
Training Step: 41  | total loss: [1m[32m0.05430[0m[0m | time: 4.353s
[2K
| Adam | epoch: 009 | loss: 0.05430 - acc: 0.9786 -- iter: 032/140
[A[ATraining Step: 42  | total loss: [1m[32m0.04549[0m[0m | time: 8.540s
[2K
| Adam | epoch: 009 | loss: 0.04549 - acc: 0.9824 -- iter: 064/140
[A[ATraining Step: 43  | total loss: [1m[32m0.03850[0m[0m | time: 18.521s
[2K
| Adam | epoch: 009 | loss: 0.03850 - acc: 0.9855 -- iter: 096/140
[A[ATraining Step: 44  | total loss: [1m[32m0.03686[0m[0m | time: 28.439s
[2K
| Adam | epoch: 009 | loss: 0.03686 - acc: 0.9880 -- iter: 128/140
[A[ATraining Step: 45  | total loss: [1m[32m0.03128[0m[0m | time: 40.741s
[2K
| Adam | epoch: 009 | loss: 0.03128 - acc: 0.9901 | val_loss: 0.79176 - val_acc: 0.5682 -- iter: 140/140
--
Training Step: 46  | total loss: [1m[32m0.06796[0m[0m | time: 10.170s
[2K
| Adam | epoch: 010 | loss: 0.06796 - acc: 0.9865 -- iter: 032/140
[A[ATraining Step: 47  | total loss: [1m[32m0.05763[0m[0m | time: 14.486s
[2K
| Adam | epoch: 010 | loss: 0.05763 - acc: 0.9887 -- iter: 064/140
[A[ATraining Step: 48  | total loss: [1m[32m0.04990[0m[0m | time: 18.752s
[2K
| Adam | epoch: 010 | loss: 0.04990 - acc: 0.9905 -- iter: 096/140
[A[ATraining Step: 49  | total loss: [1m[32m0.04280[0m[0m | time: 28.623s
[2K
| Adam | epoch: 010 | loss: 0.04280 - acc: 0.9920 -- iter: 128/140
[A[ATraining Step: 50  | total loss: [1m[32m0.03747[0m[0m | time: 40.691s
[2K
| Adam | epoch: 010 | loss: 0.03747 - acc: 0.9933 | val_loss: 0.54998 - val_acc: 0.7273 -- iter: 140/140
--
Training Step: 51  | total loss: [1m[32m0.07294[0m[0m | time: 9.984s
[2K
| Adam | epoch: 011 | loss: 0.07294 - acc: 0.9895 -- iter: 032/140
[A[ATraining Step: 52  | total loss: [1m[32m0.08629[0m[0m | time: 19.931s
[2K
| Adam | epoch: 011 | loss: 0.08629 - acc: 0.9864 -- iter: 064/140
[A[ATraining Step: 53  | total loss: [1m[32m0.07626[0m[0m | time: 24.119s
[2K
| Adam | epoch: 011 | loss: 0.07626 - acc: 0.9884 -- iter: 096/140
[A[ATraining Step: 54  | total loss: [1m[32m0.06583[0m[0m | time: 28.317s
[2K
| Adam | epoch: 011 | loss: 0.06583 - acc: 0.9901 -- iter: 128/140
[A[ATraining Step: 55  | total loss: [1m[32m0.05702[0m[0m | time: 40.600s
[2K
| Adam | epoch: 011 | loss: 0.05702 - acc: 0.9915 | val_loss: 0.55950 - val_acc: 0.7500 -- iter: 140/140
--
Training Step: 56  | total loss: [1m[32m0.06675[0m[0m | time: 9.883s
[2K
| Adam | epoch: 012 | loss: 0.06675 - acc: 0.9883 -- iter: 032/140
[A[ATraining Step: 57  | total loss: [1m[32m0.05856[0m[0m | time: 19.882s
[2K
| Adam | epoch: 012 | loss: 0.05856 - acc: 0.9899 -- iter: 064/140
[A[ATraining Step: 58  | total loss: [1m[32m0.06997[0m[0m | time: 29.905s
[2K
| Adam | epoch: 012 | loss: 0.06997 - acc: 0.9870 -- iter: 096/140
[A[ATraining Step: 59  | total loss: [1m[32m0.09711[0m[0m | time: 34.171s
[2K
| Adam | epoch: 012 | loss: 0.09711 - acc: 0.9804 -- iter: 128/140
[A[ATraining Step: 60  | total loss: [1m[32m0.08611[0m[0m | time: 40.689s
[2K
| Adam | epoch: 012 | loss: 0.08611 - acc: 0.9830 | val_loss: 0.75008 - val_acc: 0.6818 -- iter: 140/140
--
Training Step: 61  | total loss: [1m[32m0.07645[0m[0m | time: 10.002s
[2K
| Adam | epoch: 013 | loss: 0.07645 - acc: 0.9852 -- iter: 032/140
[A[ATraining Step: 62  | total loss: [1m[32m0.06743[0m[0m | time: 20.027s
[2K
| Adam | epoch: 013 | loss: 0.06743 - acc: 0.9871 -- iter: 064/140
[A[ATraining Step: 63  | total loss: [1m[32m0.05989[0m[0m | time: 29.627s
[2K
| Adam | epoch: 013 | loss: 0.05989 - acc: 0.9887 -- iter: 096/140
[A[ATraining Step: 64  | total loss: [1m[32m0.13731[0m[0m | time: 39.660s
[2K
| Adam | epoch: 013 | loss: 0.13731 - acc: 0.9745 -- iter: 128/140
[A[ATraining Step: 65  | total loss: [1m[32m0.13723[0m[0m | time: 46.147s
[2K
| Adam | epoch: 013 | loss: 0.13723 - acc: 0.9738 | val_loss: 0.72536 - val_acc: 0.7045 -- iter: 140/140
--
Training Step: 66  | total loss: [1m[32m0.12342[0m[0m | time: 4.423s
[2K
| Adam | epoch: 014 | loss: 0.12342 - acc: 0.9770 -- iter: 032/140
[A[ATraining Step: 67  | total loss: [1m[32m0.11025[0m[0m | time: 14.298s
[2K
| Adam | epoch: 014 | loss: 0.11025 - acc: 0.9798 -- iter: 064/140
[A[ATraining Step: 68  | total loss: [1m[32m0.09880[0m[0m | time: 24.139s
[2K
| Adam | epoch: 014 | loss: 0.09880 - acc: 0.9822 -- iter: 096/140
[A[ATraining Step: 69  | total loss: [1m[32m0.10211[0m[0m | time: 34.004s
[2K
| Adam | epoch: 014 | loss: 0.10211 - acc: 0.9806 -- iter: 128/140
[A[ATraining Step: 70  | total loss: [1m[32m0.10948[0m[0m | time: 45.891s
[2K
| Adam | epoch: 014 | loss: 0.10948 - acc: 0.9792 | val_loss: 0.70547 - val_acc: 0.7500 -- iter: 140/140
--
Training Step: 71  | total loss: [1m[32m0.10636[0m[0m | time: 4.246s
[2K
| Adam | epoch: 015 | loss: 0.10636 - acc: 0.9816 -- iter: 032/140
[A[ATraining Step: 72  | total loss: [1m[32m0.13113[0m[0m | time: 8.364s
[2K
| Adam | epoch: 015 | loss: 0.13113 - acc: 0.9743 -- iter: 064/140
[A[ATraining Step: 73  | total loss: [1m[32m0.12928[0m[0m | time: 17.836s
[2K
| Adam | epoch: 015 | loss: 0.12928 - acc: 0.9679 -- iter: 096/140
[A[ATraining Step: 74  | total loss: [1m[32m0.11754[0m[0m | time: 27.447s
[2K
| Adam | epoch: 015 | loss: 0.11754 - acc: 0.9714 -- iter: 128/140
[A[ATraining Step: 75  | total loss: [1m[32m0.10645[0m[0m | time: 39.308s
[2K
| Adam | epoch: 015 | loss: 0.10645 - acc: 0.9745 | val_loss: 1.40379 - val_acc: 0.5682 -- iter: 140/140
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.8302277432712215
Validation AUPRC:0.8296230641947839
Test AUC:0.9256198347107438
Test AUPRC:0.9032952972458319
BestTestF1Score	0.84	0.68	0.84	0.83	0.86	19	4	18	3	0.97
BestTestMCCScore	0.84	0.68	0.84	0.83	0.86	19	4	18	3	0.97
BestTestAccuracyScore	0.84	0.68	0.84	0.83	0.86	19	4	18	3	0.97
BestValidationF1Score	0.81	0.58	0.77	0.71	0.96	22	9	12	1	0.97
BestValidationMCC	0.81	0.58	0.77	0.71	0.96	22	9	12	1	0.97
BestValidationAccuracy	0.81	0.58	0.77	0.71	0.96	22	9	12	1	0.97
TestPredictions (Threshold:0.97)
CHEMBL240503,TN,INACT,0.49000000953674316	CHEMBL373792,TN,INACT,0.7400000095367432	CHEMBL395391,TN,INACT,0.949999988079071	CHEMBL589158,TP,ACT,0.9800000190734863	CHEMBL241716,TN,INACT,0.3400000035762787	CHEMBL206331,TP,ACT,0.9900000095367432	CHEMBL396611,TN,INACT,0.949999988079071	CHEMBL1934491,TP,ACT,0.9900000095367432	CHEMBL2018130,FN,ACT,0.9599999785423279	CHEMBL1813731,FN,ACT,0.9599999785423279	CHEMBL1813912,FN,ACT,0.9599999785423279	CHEMBL202841,TP,ACT,1.0	CHEMBL202967,TP,ACT,1.0	CHEMBL3765140,TN,INACT,0.9200000166893005	CHEMBL204051,TP,ACT,1.0	CHEMBL240980,TN,INACT,0.9599999785423279	CHEMBL1915959,TN,INACT,0.6800000071525574	CHEMBL325841,TP,ACT,0.9900000095367432	CHEMBL437522,TN,INACT,0.07999999821186066	CHEMBL589883,TP,ACT,1.0	CHEMBL3138065,TP,ACT,0.9900000095367432	CHEMBL376293,TN,INACT,0.9200000166893005	CHEMBL203237,TP,ACT,1.0	CHEMBL3138398,TP,ACT,0.9900000095367432	CHEMBL392913,TN,INACT,0.9300000071525574	CHEMBL413980,TN,INACT,0.9399999976158142	CHEMBL1163877,TN,INACT,0.9599999785423279	CHEMBL593427,TP,ACT,0.9800000190734863	CHEMBL3765321,TN,INACT,0.6600000262260437	CHEMBL589155,TP,ACT,0.9900000095367432	CHEMBL2018251,TP,ACT,0.9900000095367432	CHEMBL1165640,FP,INACT,0.9700000286102295	CHEMBL397792,FP,INACT,0.9800000190734863	CHEMBL472508,FP,INACT,0.9800000190734863	CHEMBL3138113,TP,ACT,0.9800000190734863	CHEMBL592524,TP,ACT,0.9700000286102295	CHEMBL1934492,TP,ACT,0.9900000095367432	CHEMBL220102,TN,INACT,0.6399999856948853	CHEMBL2018240,TP,ACT,1.0	CHEMBL246442,TN,INACT,0.7300000190734863	CHEMBL376226,TN,INACT,0.699999988079071	CHEMBL248168,FP,INACT,1.0	CHEMBL374728,TN,INACT,0.75	CHEMBL214141,TP,ACT,0.9900000095367432	

