CNNModel CHEMBL1937 adam 0.0005 30 256 0 0.6 False True
Number of active compounds :	379
Number of inactive compounds :	253
---------------------------------
Run id: CNNModel_CHEMBL1937_adam_0.0005_30_256_0_0.6_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL1937_adam_0.0005_30_256_0.6_True/
---------------------------------
Training samples: 332
Validation samples: 105
--
Training Step: 1  | time: 1.619s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/332
[A[ATraining Step: 2  | total loss: [1m[32m0.62400[0m[0m | time: 5.262s
[2K
| Adam | epoch: 001 | loss: 0.62400 - acc: 0.3656 -- iter: 064/332
[A[ATraining Step: 3  | total loss: [1m[32m0.67957[0m[0m | time: 11.113s
[2K
| Adam | epoch: 001 | loss: 0.67957 - acc: 0.5523 -- iter: 096/332
[A[ATraining Step: 4  | total loss: [1m[32m0.68691[0m[0m | time: 12.075s
[2K
| Adam | epoch: 001 | loss: 0.68691 - acc: 0.5834 -- iter: 128/332
[A[ATraining Step: 5  | total loss: [1m[32m0.68370[0m[0m | time: 13.117s
[2K
| Adam | epoch: 001 | loss: 0.68370 - acc: 0.6122 -- iter: 160/332
[A[ATraining Step: 6  | total loss: [1m[32m0.69787[0m[0m | time: 14.106s
[2K
| Adam | epoch: 001 | loss: 0.69787 - acc: 0.5200 -- iter: 192/332
[A[ATraining Step: 7  | total loss: [1m[32m0.68065[0m[0m | time: 15.135s
[2K
| Adam | epoch: 001 | loss: 0.68065 - acc: 0.6017 -- iter: 224/332
[A[ATraining Step: 8  | total loss: [1m[32m0.68162[0m[0m | time: 16.200s
[2K
| Adam | epoch: 001 | loss: 0.68162 - acc: 0.5797 -- iter: 256/332
[A[ATraining Step: 9  | total loss: [1m[32m0.67322[0m[0m | time: 17.055s
[2K
| Adam | epoch: 001 | loss: 0.67322 - acc: 0.6037 -- iter: 288/332
[A[ATraining Step: 10  | total loss: [1m[32m0.63405[0m[0m | time: 18.202s
[2K
| Adam | epoch: 001 | loss: 0.63405 - acc: 0.6925 -- iter: 320/332
[A[ATraining Step: 11  | total loss: [1m[32m0.65671[0m[0m | time: 19.780s
[2K
| Adam | epoch: 001 | loss: 0.65671 - acc: 0.6457 | val_loss: 0.70787 - val_acc: 0.6095 -- iter: 332/332
--
2018-08-02 03:13:32.593821: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at save_restore_v2_ops.cc:137 : Unknown: /hps/nobackup/uniprot/tdogan/Ahmet/DEEPScreen/tflearnModels/CNNModel_CHEMBL1937_adam_0.0005_30_256_0.6_True-11.index.tempstate15518154836209980909; Input/output error
Traceback (most recent call last):
  File "/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1322, in _do_call
    return fn(*args)
  File "/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1307, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.UnknownError: /hps/nobackup/uniprot/tdogan/Ahmet/DEEPScreen/tflearnModels/CNNModel_CHEMBL1937_adam_0.0005_30_256_0.6_True-11.index.tempstate15518154836209980909; Input/output error
	 [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_BOOL, DT_FLOAT, DT_FLOAT], _device="/job:localhost/replica:0/task:0/device:CPU:0"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, Accuracy/Mean/moving_avg, Adam/beta1_power, Adam/beta2_power, Conv2D/W, Conv2D/W/Adam, Conv2D/W/Adam_1, Conv2D/b, Conv2D/b/Adam, Conv2D/b/Adam_1, Conv2D_1/W, Conv2D_1/W/Adam, Conv2D_1/W/Adam_1, Conv2D_1/b, Conv2D_1/b/Adam, Conv2D_1/b/Adam_1, Conv2D_2/W, Conv2D_2/W/Adam, Conv2D_2/W/Adam_1, Conv2D_2/b, Conv2D_2/b/Adam, Conv2D_2/b/Adam_1, Conv2D_3/W, Conv2D_3/W/Adam, Conv2D_3/W/Adam_1, Conv2D_3/b, Conv2D_3/b/Adam, Conv2D_3/b/Adam_1, Conv2D_4/W, Conv2D_4/W/Adam, Conv2D_4/W/Adam_1, Conv2D_4/b, Conv2D_4/b/Adam, Conv2D_4/b/Adam_1, Crossentropy/Mean/moving_avg, FullyConnected/W, FullyConnected/W/Adam, FullyConnected/W/Adam_1, FullyConnected/b, FullyConnected/b/Adam, FullyConnected/b/Adam_1, FullyConnected_1/W, FullyConnected_1/W/Adam, FullyConnected_1/W/Adam_1, FullyConnected_1/b, FullyConnected_1/b/Adam, FullyConnected_1/b/Adam_1, Global_Step, Training_step, is_training, val_acc, val_loss)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "trainConvNet.py", line 277, in <module>
    trainModelTarget(model_name, trgt, optim, learning_rate, n_epoch, n_of_h1, n_of_h2, dropout_keep_rate, rotate, save_model)
  File "trainConvNet.py", line 109, in trainModelTarget
    snapshot_epoch=True, run_id="{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_id".format(model_name, target, optimizer, learning_rate, epch,  n_of_h1, n_of_h2, dropout_keep_rate, rotate, save_model))
  File "/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/tflearn/models/dnn.py", line 216, in fit
    callbacks=callbacks)
  File "/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/tflearn/helpers/trainer.py", line 349, in fit
    caller.on_batch_end(self.training_state, snapshot)
  File "/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/tflearn/callbacks.py", line 76, in on_batch_end
    callback.on_batch_end(training_state, snapshot)
  File "/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/tflearn/callbacks.py", line 283, in on_batch_end
    self.save(training_state.step)
  File "/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/tflearn/callbacks.py", line 304, in save
    self.save_func(self.snapshot_path, training_step)
  File "/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/tflearn/helpers/trainer.py", line 414, in save
    self.saver.save(self.session, model_file, global_step=global_step)
  File "/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/training/saver.py", line 1703, in save
    {self.saver_def.filename_tensor_name: checkpoint_file})
  File "/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 900, in run
    run_metadata_ptr)
  File "/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File "/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1316, in _do_run
    run_metadata)
  File "/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnknownError: /hps/nobackup/uniprot/tdogan/Ahmet/DEEPScreen/tflearnModels/CNNModel_CHEMBL1937_adam_0.0005_30_256_0.6_True-11.index.tempstate15518154836209980909; Input/output error
	 [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_BOOL, DT_FLOAT, DT_FLOAT], _device="/job:localhost/replica:0/task:0/device:CPU:0"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, Accuracy/Mean/moving_avg, Adam/beta1_power, Adam/beta2_power, Conv2D/W, Conv2D/W/Adam, Conv2D/W/Adam_1, Conv2D/b, Conv2D/b/Adam, Conv2D/b/Adam_1, Conv2D_1/W, Conv2D_1/W/Adam, Conv2D_1/W/Adam_1, Conv2D_1/b, Conv2D_1/b/Adam, Conv2D_1/b/Adam_1, Conv2D_2/W, Conv2D_2/W/Adam, Conv2D_2/W/Adam_1, Conv2D_2/b, Conv2D_2/b/Adam, Conv2D_2/b/Adam_1, Conv2D_3/W, Conv2D_3/W/Adam, Conv2D_3/W/Adam_1, Conv2D_3/b, Conv2D_3/b/Adam, Conv2D_3/b/Adam_1, Conv2D_4/W, Conv2D_4/W/Adam, Conv2D_4/W/Adam_1, Conv2D_4/b, Conv2D_4/b/Adam, Conv2D_4/b/Adam_1, Crossentropy/Mean/moving_avg, FullyConnected/W, FullyConnected/W/Adam, FullyConnected/W/Adam_1, FullyConnected/b, FullyConnected/b/Adam, FullyConnected/b/Adam_1, FullyConnected_1/W, FullyConnected_1/W/Adam, FullyConnected_1/W/Adam_1, FullyConnected_1/b, FullyConnected_1/b/Adam, FullyConnected_1/b/Adam_1, Global_Step, Training_step, is_training, val_acc, val_loss)]]

Caused by op 'save/SaveV2', defined at:
  File "trainConvNet.py", line 277, in <module>
    trainModelTarget(model_name, trgt, optim, learning_rate, n_epoch, n_of_h1, n_of_h2, dropout_keep_rate, rotate, save_model)
  File "trainConvNet.py", line 41, in trainModelTarget
    model = CNNModel(2, model_name,  target, optimizer, learning_rate, epch, n_of_h1, dropout_keep_rate, save_model)
  File "/hps/nobackup/uniprot/tdogan/Ahmet/DEEPScreen/bin/models.py", line 296, in CNNModel
    max_checkpoints=2, tensorboard_verbose=0, tensorboard_dir="../tflearnLogs/{}/".format(str_model_name))
  File "/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/tflearn/models/dnn.py", line 65, in __init__
    best_val_accuracy=best_val_accuracy)
  File "/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/tflearn/helpers/trainer.py", line 137, in __init__
    allow_empty=True)
  File "/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/training/saver.py", line 1338, in __init__
    self.build()
  File "/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/training/saver.py", line 1347, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File "/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/training/saver.py", line 1384, in _build
    build_save=build_save, build_restore=build_restore)
  File "/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/training/saver.py", line 832, in _build_internal
    save_tensor = self._AddSaveOps(filename_tensor, saveables)
  File "/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/training/saver.py", line 350, in _AddSaveOps
    save = self.save_op(filename_tensor, saveables)
  File "/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/training/saver.py", line 266, in save_op
    tensors)
  File "/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py", line 1687, in save_v2
    shape_and_slices=shape_and_slices, tensors=tensors, name=name)
  File "/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3392, in create_op
    op_def=op_def)
  File "/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1718, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

UnknownError (see above for traceback): /hps/nobackup/uniprot/tdogan/Ahmet/DEEPScreen/tflearnModels/CNNModel_CHEMBL1937_adam_0.0005_30_256_0.6_True-11.index.tempstate15518154836209980909; Input/output error
	 [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_BOOL, DT_FLOAT, DT_FLOAT], _device="/job:localhost/replica:0/task:0/device:CPU:0"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, Accuracy/Mean/moving_avg, Adam/beta1_power, Adam/beta2_power, Conv2D/W, Conv2D/W/Adam, Conv2D/W/Adam_1, Conv2D/b, Conv2D/b/Adam, Conv2D/b/Adam_1, Conv2D_1/W, Conv2D_1/W/Adam, Conv2D_1/W/Adam_1, Conv2D_1/b, Conv2D_1/b/Adam, Conv2D_1/b/Adam_1, Conv2D_2/W, Conv2D_2/W/Adam, Conv2D_2/W/Adam_1, Conv2D_2/b, Conv2D_2/b/Adam, Conv2D_2/b/Adam_1, Conv2D_3/W, Conv2D_3/W/Adam, Conv2D_3/W/Adam_1, Conv2D_3/b, Conv2D_3/b/Adam, Conv2D_3/b/Adam_1, Conv2D_4/W, Conv2D_4/W/Adam, Conv2D_4/W/Adam_1, Conv2D_4/b, Conv2D_4/b/Adam, Conv2D_4/b/Adam_1, Crossentropy/Mean/moving_avg, FullyConnected/W, FullyConnected/W/Adam, FullyConnected/W/Adam_1, FullyConnected/b, FullyConnected/b/Adam, FullyConnected/b/Adam_1, FullyConnected_1/W, FullyConnected_1/W/Adam, FullyConnected_1/W/Adam_1, FullyConnected_1/b, FullyConnected_1/b/Adam, FullyConnected_1/b/Adam_1, Global_Step, Training_step, is_training, val_acc, val_loss)]]

Sender: LSF System <lsf@hh-yoda-11-06.ebi.ac.uk>
Subject: Job 8006276: <python trainConvNet.py  CNNModel CHEMBL1937 adam 0.0005 30 256 0 0.6 0 1> in cluster <YODA> Done

Job <python trainConvNet.py  CNNModel CHEMBL1937 adam 0.0005 30 256 0 0.6 0 1> was submitted from host <hh-yoda-06-14.ebi.ac.uk> by user <tdogan> in cluster <YODA>.
Job was executed on host(s) <hh-yoda-11-06.ebi.ac.uk>, in queue <research>, as user <tdogan> in cluster <YODA>.
</homes/tdogan> was used as the home directory.
</hps/nobackup/uniprot/tdogan/Ahmet/DEEPScreen/bin> was used as the working directory.
Started at Thu Sep  6 16:03:38 2018
Results reported on Thu Sep  6 16:23:41 2018

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python trainConvNet.py  CNNModel CHEMBL1937 adam 0.0005 30 256 0 0.6 0 1
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   3312.09 sec.
    Max Memory :                                 2429 MB
    Average Memory :                             1162.00 MB
    Total Requested Memory :                     15360.00 MB
    Delta Memory :                               12931.00 MB
    Max Swap :                                   36476 MB
    Max Processes :                              4
    Max Threads :                                88
    Run time :                                   1203 sec.
    Turnaround time :                            1203 sec.

The output (if any) follows:

WARNING:tensorflow:From /homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/tflearn/initializations.py:119: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.
WARNING:tensorflow:From /homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/tflearn/objectives.py:66: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
2018-09-06 16:03:44.702636: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
hdf5 is not supported on this machine (please install/reinstall h5py for optimal experience)
CNNModel CHEMBL1937 adam 0.0005 30 256 0 0.6 False True
Number of active compounds :	379
Number of inactive compounds :	253
---------------------------------
Run id: CNNModel_CHEMBL1937_adam_0.0005_30_256_0_0.6_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL1937_adam_0.0005_30_256_0.6_True/
---------------------------------
Training samples: 404
Validation samples: 127
--
Training Step: 1  | time: 0.794s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/404
[A[ATraining Step: 2  | total loss: [1m[32m0.62303[0m[0m | time: 1.389s
[2K
| Adam | epoch: 001 | loss: 0.62303 - acc: 0.5906 -- iter: 064/404
[A[ATraining Step: 3  | total loss: [1m[32m0.67522[0m[0m | time: 2.051s
[2K
| Adam | epoch: 001 | loss: 0.67522 - acc: 0.6187 -- iter: 096/404
[A[ATraining Step: 4  | total loss: [1m[32m0.67254[0m[0m | time: 2.829s
[2K
| Adam | epoch: 001 | loss: 0.67254 - acc: 0.6469 -- iter: 128/404
[A[ATraining Step: 5  | total loss: [1m[32m0.69444[0m[0m | time: 3.595s
[2K
| Adam | epoch: 001 | loss: 0.69444 - acc: 0.5452 -- iter: 160/404
[A[ATraining Step: 6  | total loss: [1m[32m0.64703[0m[0m | time: 4.356s
[2K
| Adam | epoch: 001 | loss: 0.64703 - acc: 0.6769 -- iter: 192/404
[A[ATraining Step: 7  | total loss: [1m[32m0.62574[0m[0m | time: 5.102s
[2K
| Adam | epoch: 001 | loss: 0.62574 - acc: 0.7020 -- iter: 224/404
[A[ATraining Step: 8  | total loss: [1m[32m0.69341[0m[0m | time: 5.879s
[2K
| Adam | epoch: 001 | loss: 0.69341 - acc: 0.6059 -- iter: 256/404
[A[ATraining Step: 9  | total loss: [1m[32m0.66823[0m[0m | time: 6.625s
[2K
| Adam | epoch: 001 | loss: 0.66823 - acc: 0.6326 -- iter: 288/404
[A[ATraining Step: 10  | total loss: [1m[32m0.65529[0m[0m | time: 7.342s
[2K
| Adam | epoch: 001 | loss: 0.65529 - acc: 0.6444 -- iter: 320/404
[A[ATraining Step: 11  | total loss: [1m[32m0.66890[0m[0m | time: 8.104s
[2K
| Adam | epoch: 001 | loss: 0.66890 - acc: 0.6204 -- iter: 352/404
[A[ATraining Step: 12  | total loss: [1m[32m0.67398[0m[0m | time: 8.887s
[2K
| Adam | epoch: 001 | loss: 0.67398 - acc: 0.6084 -- iter: 384/404
[A[ATraining Step: 13  | total loss: [1m[32m0.66865[0m[0m | time: 10.373s
[2K
| Adam | epoch: 001 | loss: 0.66865 - acc: 0.6155 | val_loss: 0.67686 - val_acc: 0.5906 -- iter: 404/404
--
Training Step: 14  | total loss: [1m[32m0.67820[0m[0m | time: 0.494s
[2K
| Adam | epoch: 002 | loss: 0.67820 - acc: 0.5887 -- iter: 032/404
[A[ATraining Step: 15  | total loss: [1m[32m0.68261[0m[0m | time: 1.254s
[2K
| Adam | epoch: 002 | loss: 0.68261 - acc: 0.5736 -- iter: 064/404
[A[ATraining Step: 16  | total loss: [1m[32m0.68310[0m[0m | time: 2.038s
[2K
| Adam | epoch: 002 | loss: 0.68310 - acc: 0.5694 -- iter: 096/404
[A[ATraining Step: 17  | total loss: [1m[32m0.68174[0m[0m | time: 2.844s
[2K
| Adam | epoch: 002 | loss: 0.68174 - acc: 0.5782 -- iter: 128/404
[A[ATraining Step: 18  | total loss: [1m[32m0.67905[0m[0m | time: 3.592s
[2K
| Adam | epoch: 002 | loss: 0.67905 - acc: 0.5944 -- iter: 160/404
[A[ATraining Step: 19  | total loss: [1m[32m0.67762[0m[0m | time: 4.346s
[2K
| Adam | epoch: 002 | loss: 0.67762 - acc: 0.6046 -- iter: 192/404
[A[ATraining Step: 20  | total loss: [1m[32m0.67693[0m[0m | time: 5.101s
[2K
| Adam | epoch: 002 | loss: 0.67693 - acc: 0.6112 -- iter: 224/404
[A[ATraining Step: 21  | total loss: [1m[32m0.67630[0m[0m | time: 5.888s
[2K
| Adam | epoch: 002 | loss: 0.67630 - acc: 0.6154 -- iter: 256/404
[A[ATraining Step: 22  | total loss: [1m[32m0.68101[0m[0m | time: 6.614s
[2K
| Adam | epoch: 002 | loss: 0.68101 - acc: 0.5902 -- iter: 288/404
[A[ATraining Step: 23  | total loss: [1m[32m0.67276[0m[0m | time: 7.388s
[2K
| Adam | epoch: 002 | loss: 0.67276 - acc: 0.6275 -- iter: 320/404
[A[ATraining Step: 24  | total loss: [1m[32m0.68228[0m[0m | time: 8.060s
[2K
| Adam | epoch: 002 | loss: 0.68228 - acc: 0.5829 -- iter: 352/404
[A[ATraining Step: 25  | total loss: [1m[32m0.67888[0m[0m | time: 8.909s
[2K
| Adam | epoch: 002 | loss: 0.67888 - acc: 0.5944 -- iter: 384/404
[A[ATraining Step: 26  | total loss: [1m[32m0.67800[0m[0m | time: 10.526s
[2K
| Adam | epoch: 002 | loss: 0.67800 - acc: 0.5942 | val_loss: 0.67630 - val_acc: 0.5906 -- iter: 404/404
--
Training Step: 27  | total loss: [1m[32m0.67141[0m[0m | time: 0.494s
[2K
| Adam | epoch: 003 | loss: 0.67141 - acc: 0.6102 -- iter: 032/404
[A[ATraining Step: 28  | total loss: [1m[32m0.68451[0m[0m | time: 1.015s
[2K
| Adam | epoch: 003 | loss: 0.68451 - acc: 0.5701 -- iter: 064/404
[A[ATraining Step: 29  | total loss: [1m[32m0.69508[0m[0m | time: 1.765s
[2K
| Adam | epoch: 003 | loss: 0.69508 - acc: 0.5409 -- iter: 096/404
[A[ATraining Step: 30  | total loss: [1m[32m0.67743[0m[0m | time: 2.482s
[2K
| Adam | epoch: 003 | loss: 0.67743 - acc: 0.5904 -- iter: 128/404
[A[ATraining Step: 31  | total loss: [1m[32m0.67508[0m[0m | time: 3.198s
[2K
| Adam | epoch: 003 | loss: 0.67508 - acc: 0.5984 -- iter: 160/404
[A[ATraining Step: 32  | total loss: [1m[32m0.66152[0m[0m | time: 3.927s
[2K
| Adam | epoch: 003 | loss: 0.66152 - acc: 0.6325 -- iter: 192/404
[A[ATraining Step: 33  | total loss: [1m[32m0.66925[0m[0m | time: 4.635s
[2K
| Adam | epoch: 003 | loss: 0.66925 - acc: 0.6171 -- iter: 224/404
[A[ATraining Step: 34  | total loss: [1m[32m0.66689[0m[0m | time: 5.398s
[2K
| Adam | epoch: 003 | loss: 0.66689 - acc: 0.6188 -- iter: 256/404
[A[ATraining Step: 35  | total loss: [1m[32m0.68015[0m[0m | time: 6.120s
[2K
| Adam | epoch: 003 | loss: 0.68015 - acc: 0.5940 -- iter: 288/404
[A[ATraining Step: 36  | total loss: [1m[32m0.67038[0m[0m | time: 6.905s
[2K
| Adam | epoch: 003 | loss: 0.67038 - acc: 0.6131 -- iter: 320/404
[A[ATraining Step: 37  | total loss: [1m[32m0.67204[0m[0m | time: 7.516s
[2K
| Adam | epoch: 003 | loss: 0.67204 - acc: 0.6092 -- iter: 352/404
[A[ATraining Step: 38  | total loss: [1m[32m0.67047[0m[0m | time: 8.126s
[2K
| Adam | epoch: 003 | loss: 0.67047 - acc: 0.6123 -- iter: 384/404
[A[ATraining Step: 39  | total loss: [1m[32m0.67163[0m[0m | time: 9.742s
[2K
| Adam | epoch: 003 | loss: 0.67163 - acc: 0.6088 | val_loss: 0.67762 - val_acc: 0.5906 -- iter: 404/404
--
Training Step: 40  | total loss: [1m[32m0.66670[0m[0m | time: 0.725s
[2K
| Adam | epoch: 004 | loss: 0.66670 - acc: 0.6177 -- iter: 032/404
[A[ATraining Step: 41  | total loss: [1m[32m0.66002[0m[0m | time: 1.224s
[2K
| Adam | epoch: 004 | loss: 0.66002 - acc: 0.6305 -- iter: 064/404
[A[ATraining Step: 42  | total loss: [1m[32m0.65857[0m[0m | time: 1.667s
[2K
| Adam | epoch: 004 | loss: 0.65857 - acc: 0.6340 -- iter: 096/404
[A[ATraining Step: 43  | total loss: [1m[32m0.65588[0m[0m | time: 2.435s
[2K
| Adam | epoch: 004 | loss: 0.65588 - acc: 0.6368 -- iter: 128/404
[A[ATraining Step: 44  | total loss: [1m[32m0.68246[0m[0m | time: 3.184s
[2K
| Adam | epoch: 004 | loss: 0.68246 - acc: 0.5915 -- iter: 160/404
[A[ATraining Step: 45  | total loss: [1m[32m0.68400[0m[0m | time: 3.925s
[2K
| Adam | epoch: 004 | loss: 0.68400 - acc: 0.5866 -- iter: 192/404
[A[ATraining Step: 46  | total loss: [1m[32m0.68684[0m[0m | time: 4.671s
[2K
| Adam | epoch: 004 | loss: 0.68684 - acc: 0.5774 -- iter: 224/404
[A[ATraining Step: 47  | total loss: [1m[32m0.68839[0m[0m | time: 5.430s
[2K
| Adam | epoch: 004 | loss: 0.68839 - acc: 0.5698 -- iter: 256/404
[A[ATraining Step: 48  | total loss: [1m[32m0.68792[0m[0m | time: 6.241s
[2K
| Adam | epoch: 004 | loss: 0.68792 - acc: 0.5686 -- iter: 288/404
[A[ATraining Step: 49  | total loss: [1m[32m0.68227[0m[0m | time: 6.856s
[2K
| Adam | epoch: 004 | loss: 0.68227 - acc: 0.5923 -- iter: 320/404
[A[ATraining Step: 50  | total loss: [1m[32m0.67814[0m[0m | time: 7.479s
[2K
| Adam | epoch: 004 | loss: 0.67814 - acc: 0.6120 -- iter: 352/404
[A[ATraining Step: 51  | total loss: [1m[32m0.67375[0m[0m | time: 8.082s
[2K
| Adam | epoch: 004 | loss: 0.67375 - acc: 0.6330 -- iter: 384/404
[A[ATraining Step: 52  | total loss: [1m[32m0.67891[0m[0m | time: 9.756s
[2K
| Adam | epoch: 004 | loss: 0.67891 - acc: 0.6037 | val_loss: 0.68020 - val_acc: 0.5906 -- iter: 404/404
--
Training Step: 53  | total loss: [1m[32m0.67738[0m[0m | time: 0.744s
[2K
| Adam | epoch: 005 | loss: 0.67738 - acc: 0.6114 -- iter: 032/404
[A[ATraining Step: 54  | total loss: [1m[32m0.67949[0m[0m | time: 1.456s
[2K
| Adam | epoch: 005 | loss: 0.67949 - acc: 0.5998 -- iter: 064/404
[A[ATraining Step: 55  | total loss: [1m[32m0.68021[0m[0m | time: 1.885s
[2K
| Adam | epoch: 005 | loss: 0.68021 - acc: 0.5945 -- iter: 096/404
[A[ATraining Step: 56  | total loss: [1m[32m0.67408[0m[0m | time: 2.384s
[2K
| Adam | epoch: 005 | loss: 0.67408 - acc: 0.6234 -- iter: 128/404
[A[ATraining Step: 57  | total loss: [1m[32m0.66861[0m[0m | time: 3.148s
[2K
| Adam | epoch: 005 | loss: 0.66861 - acc: 0.6478 -- iter: 160/404
[A[ATraining Step: 58  | total loss: [1m[32m0.67075[0m[0m | time: 3.890s
[2K
| Adam | epoch: 005 | loss: 0.67075 - acc: 0.6362 -- iter: 192/404
[A[ATraining Step: 59  | total loss: [1m[32m0.67025[0m[0m | time: 4.677s
[2K
| Adam | epoch: 005 | loss: 0.67025 - acc: 0.6347 -- iter: 224/404
[A[ATraining Step: 60  | total loss: [1m[32m0.66695[0m[0m | time: 5.473s
[2K
| Adam | epoch: 005 | loss: 0.66695 - acc: 0.6417 -- iter: 256/404
[A[ATraining Step: 61  | total loss: [1m[32m0.66656[0m[0m | time: 6.083s
[2K
| Adam | epoch: 005 | loss: 0.66656 - acc: 0.6395 -- iter: 288/404
[A[ATraining Step: 62  | total loss: [1m[32m0.66376[0m[0m | time: 6.698s
[2K
| Adam | epoch: 005 | loss: 0.66376 - acc: 0.6417 -- iter: 320/404
[A[ATraining Step: 63  | total loss: [1m[32m0.66124[0m[0m | time: 7.333s
[2K
| Adam | epoch: 005 | loss: 0.66124 - acc: 0.6435 -- iter: 352/404
[A[ATraining Step: 64  | total loss: [1m[32m0.65951[0m[0m | time: 8.390s
[2K
| Adam | epoch: 005 | loss: 0.65951 - acc: 0.6451 -- iter: 384/404
[A[ATraining Step: 65  | total loss: [1m[32m0.66689[0m[0m | time: 10.003s
[2K
| Adam | epoch: 005 | loss: 0.66689 - acc: 0.6349 | val_loss: 0.69865 - val_acc: 0.5906 -- iter: 404/404
--
Training Step: 66  | total loss: [1m[32m0.65827[0m[0m | time: 0.625s
[2K
| Adam | epoch: 006 | loss: 0.65827 - acc: 0.6451 -- iter: 032/404
[A[ATraining Step: 67  | total loss: [1m[32m0.66886[0m[0m | time: 1.269s
[2K
| Adam | epoch: 006 | loss: 0.66886 - acc: 0.6314 -- iter: 064/404
[A[ATraining Step: 68  | total loss: [1m[32m0.67456[0m[0m | time: 1.887s
[2K
| Adam | epoch: 006 | loss: 0.67456 - acc: 0.6233 -- iter: 096/404
[A[ATraining Step: 69  | total loss: [1m[32m0.68187[0m[0m | time: 2.272s
[2K
| Adam | epoch: 006 | loss: 0.68187 - acc: 0.6089 -- iter: 128/404
[A[ATraining Step: 70  | total loss: [1m[32m0.68103[0m[0m | time: 2.687s
[2K
| Adam | epoch: 006 | loss: 0.68103 - acc: 0.6079 -- iter: 160/404
[A[ATraining Step: 71  | total loss: [1m[32m0.68045[0m[0m | time: 3.348s
[2K
| Adam | epoch: 006 | loss: 0.68045 - acc: 0.6070 -- iter: 192/404
[A[ATraining Step: 72  | total loss: [1m[32m0.67836[0m[0m | time: 3.951s
[2K
| Adam | epoch: 006 | loss: 0.67836 - acc: 0.6125 -- iter: 224/404
[A[ATraining Step: 73  | total loss: [1m[32m0.68136[0m[0m | time: 4.564s
[2K
| Adam | epoch: 006 | loss: 0.68136 - acc: 0.5965 -- iter: 256/404
[A[ATraining Step: 74  | total loss: [1m[32m0.67842[0m[0m | time: 5.185s
[2K
| Adam | epoch: 006 | loss: 0.67842 - acc: 0.6099 -- iter: 288/404
[A[ATraining Step: 75  | total loss: [1m[32m0.67621[0m[0m | time: 5.819s
[2K
| Adam | epoch: 006 | loss: 0.67621 - acc: 0.6217 -- iter: 320/404
[A[ATraining Step: 76  | total loss: [1m[32m0.67736[0m[0m | time: 6.454s
[2K
| Adam | epoch: 006 | loss: 0.67736 - acc: 0.6154 -- iter: 352/404
[A[ATraining Step: 77  | total loss: [1m[32m0.67840[0m[0m | time: 7.061s
[2K
| Adam | epoch: 006 | loss: 0.67840 - acc: 0.6098 -- iter: 384/404
[A[ATraining Step: 78  | total loss: [1m[32m0.67731[0m[0m | time: 8.679s
[2K
| Adam | epoch: 006 | loss: 0.67731 - acc: 0.6179 | val_loss: 0.68183 - val_acc: 0.5906 -- iter: 404/404
--
Training Step: 79  | total loss: [1m[32m0.67675[0m[0m | time: 0.619s
[2K
| Adam | epoch: 007 | loss: 0.67675 - acc: 0.6219 -- iter: 032/404
[A[ATraining Step: 80  | total loss: [1m[32m0.67777[0m[0m | time: 1.223s
[2K
| Adam | epoch: 007 | loss: 0.67777 - acc: 0.6158 -- iter: 064/404
[A[ATraining Step: 81  | total loss: [1m[32m0.67957[0m[0m | time: 1.833s
[2K
| Adam | epoch: 007 | loss: 0.67957 - acc: 0.6041 -- iter: 096/404
[A[ATraining Step: 82  | total loss: [1m[32m0.67767[0m[0m | time: 2.479s
[2K
| Adam | epoch: 007 | loss: 0.67767 - acc: 0.6156 -- iter: 128/404
[A[ATraining Step: 83  | total loss: [1m[32m0.67944[0m[0m | time: 2.872s
[2K
| Adam | epoch: 007 | loss: 0.67944 - acc: 0.6040 -- iter: 160/404
[A[ATraining Step: 84  | total loss: [1m[32m0.67685[0m[0m | time: 3.258s
[2K
| Adam | epoch: 007 | loss: 0.67685 - acc: 0.6186 -- iter: 192/404
[A[ATraining Step: 85  | total loss: [1m[32m0.67425[0m[0m | time: 3.897s
[2K
| Adam | epoch: 007 | loss: 0.67425 - acc: 0.6318 -- iter: 224/404
[A[ATraining Step: 86  | total loss: [1m[32m0.67654[0m[0m | time: 4.499s
[2K
| Adam | epoch: 007 | loss: 0.67654 - acc: 0.6186 -- iter: 256/404
[A[ATraining Step: 87  | total loss: [1m[32m0.67420[0m[0m | time: 5.122s
[2K
| Adam | epoch: 007 | loss: 0.67420 - acc: 0.6286 -- iter: 288/404
[A[ATraining Step: 88  | total loss: [1m[32m0.67252[0m[0m | time: 5.731s
[2K
| Adam | epoch: 007 | loss: 0.67252 - acc: 0.6345 -- iter: 320/404
[A[ATraining Step: 89  | total loss: [1m[32m0.67138[0m[0m | time: 6.335s
[2K
| Adam | epoch: 007 | loss: 0.67138 - acc: 0.6367 -- iter: 352/404
[A[ATraining Step: 90  | total loss: [1m[32m0.66667[0m[0m | time: 6.967s
[2K
| Adam | epoch: 007 | loss: 0.66667 - acc: 0.6511 -- iter: 384/404
[A[ATraining Step: 91  | total loss: [1m[32m0.66570[0m[0m | time: 8.599s
[2K
| Adam | epoch: 007 | loss: 0.66570 - acc: 0.6516 | val_loss: 0.67636 - val_acc: 0.5906 -- iter: 404/404
--
Training Step: 92  | total loss: [1m[32m0.67157[0m[0m | time: 0.612s
[2K
| Adam | epoch: 008 | loss: 0.67157 - acc: 0.6333 -- iter: 032/404
[A[ATraining Step: 93  | total loss: [1m[32m0.67588[0m[0m | time: 1.220s
[2K
| Adam | epoch: 008 | loss: 0.67588 - acc: 0.6200 -- iter: 064/404
[A[ATraining Step: 94  | total loss: [1m[32m0.66764[0m[0m | time: 1.838s
[2K
| Adam | epoch: 008 | loss: 0.66764 - acc: 0.6361 -- iter: 096/404
[A[ATraining Step: 95  | total loss: [1m[32m0.66700[0m[0m | time: 2.455s
[2K
| Adam | epoch: 008 | loss: 0.66700 - acc: 0.6350 -- iter: 128/404
[A[ATraining Step: 96  | total loss: [1m[32m0.67110[0m[0m | time: 3.079s
[2K
| Adam | epoch: 008 | loss: 0.67110 - acc: 0.6246 -- iter: 160/404
[A[ATraining Step: 97  | total loss: [1m[32m0.67718[0m[0m | time: 3.493s
[2K
| Adam | epoch: 008 | loss: 0.67718 - acc: 0.6122 -- iter: 192/404
[A[ATraining Step: 98  | total loss: [1m[32m0.67676[0m[0m | time: 3.900s
[2K
| Adam | epoch: 008 | loss: 0.67676 - acc: 0.6110 -- iter: 224/404
[A[ATraining Step: 99  | total loss: [1m[32m0.67629[0m[0m | time: 4.517s
[2K
| Adam | epoch: 008 | loss: 0.67629 - acc: 0.6099 -- iter: 256/404
[A[ATraining Step: 100  | total loss: [1m[32m0.66845[0m[0m | time: 5.126s
[2K
| Adam | epoch: 008 | loss: 0.66845 - acc: 0.6239 -- iter: 288/404
[A[ATraining Step: 101  | total loss: [1m[32m0.67084[0m[0m | time: 5.757s
[2K
| Adam | epoch: 008 | loss: 0.67084 - acc: 0.6177 -- iter: 320/404
[A[ATraining Step: 102  | total loss: [1m[32m0.66837[0m[0m | time: 6.388s
[2K
| Adam | epoch: 008 | loss: 0.66837 - acc: 0.6216 -- iter: 352/404
[A[ATraining Step: 103  | total loss: [1m[32m0.67222[0m[0m | time: 7.005s
[2K
| Adam | epoch: 008 | loss: 0.67222 - acc: 0.6126 -- iter: 384/404
[A[ATraining Step: 104  | total loss: [1m[32m0.67398[0m[0m | time: 8.631s
[2K
| Adam | epoch: 008 | loss: 0.67398 - acc: 0.6076 | val_loss: 0.67685 - val_acc: 0.5906 -- iter: 404/404
--
Training Step: 105  | total loss: [1m[32m0.66733[0m[0m | time: 0.621s
[2K
| Adam | epoch: 009 | loss: 0.66733 - acc: 0.6218 -- iter: 032/404
[A[ATraining Step: 106  | total loss: [1m[32m0.66846[0m[0m | time: 1.225s
[2K
| Adam | epoch: 009 | loss: 0.66846 - acc: 0.6190 -- iter: 064/404
[A[ATraining Step: 107  | total loss: [1m[32m0.67354[0m[0m | time: 1.867s
[2K
| Adam | epoch: 009 | loss: 0.67354 - acc: 0.6071 -- iter: 096/404
[A[ATraining Step: 108  | total loss: [1m[32m0.67350[0m[0m | time: 2.503s
[2K
| Adam | epoch: 009 | loss: 0.67350 - acc: 0.6058 -- iter: 128/404
[A[ATraining Step: 109  | total loss: [1m[32m0.67115[0m[0m | time: 3.112s
[2K
| Adam | epoch: 009 | loss: 0.67115 - acc: 0.6108 -- iter: 160/404
[A[ATraining Step: 110  | total loss: [1m[32m0.67665[0m[0m | time: 3.733s
[2K
| Adam | epoch: 009 | loss: 0.67665 - acc: 0.5966 -- iter: 192/404
[A[ATraining Step: 111  | total loss: [1m[32m0.67161[0m[0m | time: 4.125s
[2K
| Adam | epoch: 009 | loss: 0.67161 - acc: 0.6088 -- iter: 224/404
[A[ATraining Step: 112  | total loss: [1m[32m0.66781[0m[0m | time: 4.519s
[2K
| Adam | epoch: 009 | loss: 0.66781 - acc: 0.6179 -- iter: 256/404
[A[ATraining Step: 113  | total loss: [1m[32m0.66449[0m[0m | time: 5.130s
[2K
| Adam | epoch: 009 | loss: 0.66449 - acc: 0.6261 -- iter: 288/404
[A[ATraining Step: 114  | total loss: [1m[32m0.66958[0m[0m | time: 5.758s
[2K
| Adam | epoch: 009 | loss: 0.66958 - acc: 0.6135 -- iter: 320/404
[A[ATraining Step: 115  | total loss: [1m[32m0.66745[0m[0m | time: 6.384s
[2K
| Adam | epoch: 009 | loss: 0.66745 - acc: 0.6178 -- iter: 352/404
[A[ATraining Step: 116  | total loss: [1m[32m0.66153[0m[0m | time: 6.999s
[2K
| Adam | epoch: 009 | loss: 0.66153 - acc: 0.6310 -- iter: 384/404
[A[ATraining Step: 117  | total loss: [1m[32m0.66263[0m[0m | time: 8.632s
[2K
| Adam | epoch: 009 | loss: 0.66263 - acc: 0.6273 | val_loss: 0.67807 - val_acc: 0.5906 -- iter: 404/404
--
Training Step: 118  | total loss: [1m[32m0.66240[0m[0m | time: 0.635s
[2K
| Adam | epoch: 010 | loss: 0.66240 - acc: 0.6271 -- iter: 032/404
[A[ATraining Step: 119  | total loss: [1m[32m0.66373[0m[0m | time: 1.287s
[2K
| Adam | epoch: 010 | loss: 0.66373 - acc: 0.6237 -- iter: 064/404
[A[ATraining Step: 120  | total loss: [1m[32m0.67050[0m[0m | time: 1.905s
[2K
| Adam | epoch: 010 | loss: 0.67050 - acc: 0.6114 -- iter: 096/404
[A[ATraining Step: 121  | total loss: [1m[32m0.66891[0m[0m | time: 2.537s
[2K
| Adam | epoch: 010 | loss: 0.66891 - acc: 0.6127 -- iter: 128/404
[A[ATraining Step: 122  | total loss: [1m[32m0.66170[0m[0m | time: 3.166s
[2K
| Adam | epoch: 010 | loss: 0.66170 - acc: 0.6265 -- iter: 160/404
[A[ATraining Step: 123  | total loss: [1m[32m0.65314[0m[0m | time: 3.787s
[2K
| Adam | epoch: 010 | loss: 0.65314 - acc: 0.6419 -- iter: 192/404
[A[ATraining Step: 124  | total loss: [1m[32m0.65926[0m[0m | time: 4.413s
[2K
| Adam | epoch: 010 | loss: 0.65926 - acc: 0.6309 -- iter: 224/404
[A[ATraining Step: 125  | total loss: [1m[32m0.66505[0m[0m | time: 4.814s
[2K
| Adam | epoch: 010 | loss: 0.66505 - acc: 0.6209 -- iter: 256/404
[A[ATraining Step: 126  | total loss: [1m[32m0.67175[0m[0m | time: 5.227s
[2K
| Adam | epoch: 010 | loss: 0.67175 - acc: 0.6088 -- iter: 288/404
[A[ATraining Step: 127  | total loss: [1m[32m0.67720[0m[0m | time: 5.853s
[2K
| Adam | epoch: 010 | loss: 0.67720 - acc: 0.5979 -- iter: 320/404
[A[ATraining Step: 128  | total loss: [1m[32m0.67382[0m[0m | time: 6.497s
[2K
| Adam | epoch: 010 | loss: 0.67382 - acc: 0.6038 -- iter: 352/404
[A[ATraining Step: 129  | total loss: [1m[32m0.67124[0m[0m | time: 7.117s
[2K
| Adam | epoch: 010 | loss: 0.67124 - acc: 0.6090 -- iter: 384/404
[A[ATraining Step: 130  | total loss: [1m[32m0.67153[0m[0m | time: 8.749s
[2K
| Adam | epoch: 010 | loss: 0.67153 - acc: 0.6075 | val_loss: 0.67520 - val_acc: 0.5906 -- iter: 404/404
--
Training Step: 131  | total loss: [1m[32m0.67073[0m[0m | time: 0.652s
[2K
| Adam | epoch: 011 | loss: 0.67073 - acc: 0.6092 -- iter: 032/404
[A[ATraining Step: 132  | total loss: [1m[32m0.67333[0m[0m | time: 1.253s
[2K
| Adam | epoch: 011 | loss: 0.67333 - acc: 0.6014 -- iter: 064/404
[A[ATraining Step: 133  | total loss: [1m[32m0.67344[0m[0m | time: 1.880s
[2K
| Adam | epoch: 011 | loss: 0.67344 - acc: 0.6007 -- iter: 096/404
[A[ATraining Step: 134  | total loss: [1m[32m0.67921[0m[0m | time: 2.504s
[2K
| Adam | epoch: 011 | loss: 0.67921 - acc: 0.5812 -- iter: 128/404
[A[ATraining Step: 135  | total loss: [1m[32m0.67430[0m[0m | time: 3.120s
[2K
| Adam | epoch: 011 | loss: 0.67430 - acc: 0.5981 -- iter: 160/404
[A[ATraining Step: 136  | total loss: [1m[32m0.67430[0m[0m | time: 3.742s
[2K
| Adam | epoch: 011 | loss: 0.67430 - acc: 0.5977 -- iter: 192/404
[A[ATraining Step: 137  | total loss: [1m[32m0.67605[0m[0m | time: 4.346s
[2K
| Adam | epoch: 011 | loss: 0.67605 - acc: 0.5910 -- iter: 224/404
[A[ATraining Step: 138  | total loss: [1m[32m0.67499[0m[0m | time: 4.966s
[2K
| Adam | epoch: 011 | loss: 0.67499 - acc: 0.5944 -- iter: 256/404
[A[ATraining Step: 139  | total loss: [1m[32m0.67343[0m[0m | time: 5.362s
[2K
| Adam | epoch: 011 | loss: 0.67343 - acc: 0.6006 -- iter: 288/404
[A[ATraining Step: 140  | total loss: [1m[32m0.67462[0m[0m | time: 5.765s
[2K
| Adam | epoch: 011 | loss: 0.67462 - acc: 0.5955 -- iter: 320/404
[A[ATraining Step: 141  | total loss: [1m[32m0.67564[0m[0m | time: 6.488s
[2K
| Adam | epoch: 011 | loss: 0.67564 - acc: 0.5910 -- iter: 352/404
[A[ATraining Step: 142  | total loss: [1m[32m0.67810[0m[0m | time: 7.097s
[2K
| Adam | epoch: 011 | loss: 0.67810 - acc: 0.5819 -- iter: 384/404
[A[ATraining Step: 143  | total loss: [1m[32m0.67550[0m[0m | time: 8.731s
[2K
| Adam | epoch: 011 | loss: 0.67550 - acc: 0.5925 | val_loss: 0.67535 - val_acc: 0.5906 -- iter: 404/404
--
Training Step: 144  | total loss: [1m[32m0.67526[0m[0m | time: 0.627s
[2K
| Adam | epoch: 012 | loss: 0.67526 - acc: 0.5926 -- iter: 032/404
[A[ATraining Step: 145  | total loss: [1m[32m0.67594[0m[0m | time: 1.241s
[2K
| Adam | epoch: 012 | loss: 0.67594 - acc: 0.5896 -- iter: 064/404
[A[ATraining Step: 146  | total loss: [1m[32m0.67396[0m[0m | time: 1.861s
[2K
| Adam | epoch: 012 | loss: 0.67396 - acc: 0.5962 -- iter: 096/404
[A[ATraining Step: 147  | total loss: [1m[32m0.67177[0m[0m | time: 2.488s
[2K
| Adam | epoch: 012 | loss: 0.67177 - acc: 0.6022 -- iter: 128/404
[A[ATraining Step: 148  | total loss: [1m[32m0.66883[0m[0m | time: 3.158s
[2K
| Adam | epoch: 012 | loss: 0.66883 - acc: 0.6108 -- iter: 160/404
[A[ATraining Step: 149  | total loss: [1m[32m0.66762[0m[0m | time: 3.776s
[2K
| Adam | epoch: 012 | loss: 0.66762 - acc: 0.6122 -- iter: 192/404
[A[ATraining Step: 150  | total loss: [1m[32m0.66167[0m[0m | time: 4.393s
[2K
| Adam | epoch: 012 | loss: 0.66167 - acc: 0.6260 -- iter: 224/404
[A[ATraining Step: 151  | total loss: [1m[32m0.66295[0m[0m | time: 5.010s
[2K
| Adam | epoch: 012 | loss: 0.66295 - acc: 0.6228 -- iter: 256/404
[A[ATraining Step: 152  | total loss: [1m[32m0.66461[0m[0m | time: 5.615s
[2K
| Adam | epoch: 012 | loss: 0.66461 - acc: 0.6199 -- iter: 288/404
[A[ATraining Step: 153  | total loss: [1m[32m0.66509[0m[0m | time: 6.012s
[2K
| Adam | epoch: 012 | loss: 0.66509 - acc: 0.6172 -- iter: 320/404
[A[ATraining Step: 154  | total loss: [1m[32m0.66586[0m[0m | time: 6.406s
[2K
| Adam | epoch: 012 | loss: 0.66586 - acc: 0.6155 -- iter: 352/404
[A[ATraining Step: 155  | total loss: [1m[32m0.66670[0m[0m | time: 7.027s
[2K
| Adam | epoch: 012 | loss: 0.66670 - acc: 0.6140 -- iter: 384/404
[A[ATraining Step: 156  | total loss: [1m[32m0.67545[0m[0m | time: 8.655s
[2K
| Adam | epoch: 012 | loss: 0.67545 - acc: 0.5994 | val_loss: 0.67319 - val_acc: 0.5906 -- iter: 404/404
--
Training Step: 157  | total loss: [1m[32m0.67820[0m[0m | time: 0.616s
[2K
| Adam | epoch: 013 | loss: 0.67820 - acc: 0.5926 -- iter: 032/404
[A[ATraining Step: 158  | total loss: [1m[32m0.67631[0m[0m | time: 1.227s
[2K
| Adam | epoch: 013 | loss: 0.67631 - acc: 0.5959 -- iter: 064/404
[A[ATraining Step: 159  | total loss: [1m[32m0.67535[0m[0m | time: 1.852s
[2K
| Adam | epoch: 013 | loss: 0.67535 - acc: 0.5957 -- iter: 096/404
[A[ATraining Step: 160  | total loss: [1m[32m0.67596[0m[0m | time: 2.493s
[2K
| Adam | epoch: 013 | loss: 0.67596 - acc: 0.5923 -- iter: 128/404
[A[ATraining Step: 161  | total loss: [1m[32m0.67107[0m[0m | time: 3.095s
[2K
| Adam | epoch: 013 | loss: 0.67107 - acc: 0.6050 -- iter: 160/404
[A[ATraining Step: 162  | total loss: [1m[32m0.67136[0m[0m | time: 3.699s
[2K
| Adam | epoch: 013 | loss: 0.67136 - acc: 0.6039 -- iter: 192/404
[A[ATraining Step: 163  | total loss: [1m[32m0.66790[0m[0m | time: 4.308s
[2K
| Adam | epoch: 013 | loss: 0.66790 - acc: 0.6153 -- iter: 224/404
[A[ATraining Step: 164  | total loss: [1m[32m0.66715[0m[0m | time: 4.918s
[2K
| Adam | epoch: 013 | loss: 0.66715 - acc: 0.6163 -- iter: 256/404
[A[ATraining Step: 165  | total loss: [1m[32m0.66534[0m[0m | time: 5.543s
[2K
| Adam | epoch: 013 | loss: 0.66534 - acc: 0.6203 -- iter: 288/404
[A[ATraining Step: 166  | total loss: [1m[32m0.66377[0m[0m | time: 6.152s
[2K
| Adam | epoch: 013 | loss: 0.66377 - acc: 0.6239 -- iter: 320/404
[A[ATraining Step: 167  | total loss: [1m[32m0.66603[0m[0m | time: 6.558s
[2K
| Adam | epoch: 013 | loss: 0.66603 - acc: 0.6178 -- iter: 352/404
[A[ATraining Step: 168  | total loss: [1m[32m0.66802[0m[0m | time: 6.960s
[2K
| Adam | epoch: 013 | loss: 0.66802 - acc: 0.6110 -- iter: 384/404
[A[ATraining Step: 169  | total loss: [1m[32m0.67017[0m[0m | time: 8.624s
[2K
| Adam | epoch: 013 | loss: 0.67017 - acc: 0.6049 | val_loss: 0.67181 - val_acc: 0.5906 -- iter: 404/404
--
Training Step: 170  | total loss: [1m[32m0.66627[0m[0m | time: 0.627s
[2K
| Adam | epoch: 014 | loss: 0.66627 - acc: 0.6131 -- iter: 032/404
[A[ATraining Step: 171  | total loss: [1m[32m0.66895[0m[0m | time: 1.245s
[2K
| Adam | epoch: 014 | loss: 0.66895 - acc: 0.6050 -- iter: 064/404
[A[ATraining Step: 172  | total loss: [1m[32m0.66714[0m[0m | time: 1.865s
[2K
| Adam | epoch: 014 | loss: 0.66714 - acc: 0.6070 -- iter: 096/404
[A[ATraining Step: 173  | total loss: [1m[32m0.66281[0m[0m | time: 2.476s
[2K
| Adam | epoch: 014 | loss: 0.66281 - acc: 0.6150 -- iter: 128/404
[A[ATraining Step: 174  | total loss: [1m[32m0.66501[0m[0m | time: 3.142s
[2K
| Adam | epoch: 014 | loss: 0.66501 - acc: 0.6098 -- iter: 160/404
[A[ATraining Step: 175  | total loss: [1m[32m0.66784[0m[0m | time: 3.760s
[2K
| Adam | epoch: 014 | loss: 0.66784 - acc: 0.6019 -- iter: 192/404
[A[ATraining Step: 176  | total loss: [1m[32m0.66230[0m[0m | time: 4.419s
[2K
| Adam | epoch: 014 | loss: 0.66230 - acc: 0.6105 -- iter: 224/404
[A[ATraining Step: 177  | total loss: [1m[32m0.66461[0m[0m | time: 5.039s
[2K
| Adam | epoch: 014 | loss: 0.66461 - acc: 0.6057 -- iter: 256/404
[A[ATraining Step: 178  | total loss: [1m[32m0.66440[0m[0m | time: 5.674s
[2K
| Adam | epoch: 014 | loss: 0.66440 - acc: 0.6076 -- iter: 288/404
[A[ATraining Step: 179  | total loss: [1m[32m0.65919[0m[0m | time: 6.302s
[2K
| Adam | epoch: 014 | loss: 0.65919 - acc: 0.6125 -- iter: 320/404
[A[ATraining Step: 180  | total loss: [1m[32m0.66540[0m[0m | time: 6.923s
[2K
| Adam | epoch: 014 | loss: 0.66540 - acc: 0.6012 -- iter: 352/404
[A[ATraining Step: 181  | total loss: [1m[32m0.66953[0m[0m | time: 7.323s
[2K
| Adam | epoch: 014 | loss: 0.66953 - acc: 0.5911 -- iter: 384/404
[A[ATraining Step: 182  | total loss: [1m[32m0.67032[0m[0m | time: 8.720s
[2K
| Adam | epoch: 014 | loss: 0.67032 - acc: 0.5870 | val_loss: 0.66655 - val_acc: 0.5906 -- iter: 404/404
--
Training Step: 183  | total loss: [1m[32m0.67028[0m[0m | time: 0.619s
[2K
| Adam | epoch: 015 | loss: 0.67028 - acc: 0.5833 -- iter: 032/404
[A[ATraining Step: 184  | total loss: [1m[32m0.66540[0m[0m | time: 1.255s
[2K
| Adam | epoch: 015 | loss: 0.66540 - acc: 0.6000 -- iter: 064/404
[A[ATraining Step: 185  | total loss: [1m[32m0.66277[0m[0m | time: 1.877s
[2K
| Adam | epoch: 015 | loss: 0.66277 - acc: 0.6056 -- iter: 096/404
[A[ATraining Step: 186  | total loss: [1m[32m0.66181[0m[0m | time: 2.490s
[2K
| Adam | epoch: 015 | loss: 0.66181 - acc: 0.6013 -- iter: 128/404
[A[ATraining Step: 187  | total loss: [1m[32m0.66078[0m[0m | time: 3.096s
[2K
| Adam | epoch: 015 | loss: 0.66078 - acc: 0.6068 -- iter: 160/404
[A[ATraining Step: 188  | total loss: [1m[32m0.66281[0m[0m | time: 3.727s
[2K
| Adam | epoch: 015 | loss: 0.66281 - acc: 0.5992 -- iter: 192/404
[A[ATraining Step: 189  | total loss: [1m[32m0.66333[0m[0m | time: 4.378s
[2K
| Adam | epoch: 015 | loss: 0.66333 - acc: 0.6018 -- iter: 224/404
[A[ATraining Step: 190  | total loss: [1m[32m0.66091[0m[0m | time: 5.013s
[2K
| Adam | epoch: 015 | loss: 0.66091 - acc: 0.6072 -- iter: 256/404
[A[ATraining Step: 191  | total loss: [1m[32m0.65433[0m[0m | time: 5.628s
[2K
| Adam | epoch: 015 | loss: 0.65433 - acc: 0.6121 -- iter: 288/404
[A[ATraining Step: 192  | total loss: [1m[32m0.65311[0m[0m | time: 6.247s
[2K
| Adam | epoch: 015 | loss: 0.65311 - acc: 0.6134 -- iter: 320/404
[A[ATraining Step: 193  | total loss: [1m[32m0.65386[0m[0m | time: 6.853s
[2K
| Adam | epoch: 015 | loss: 0.65386 - acc: 0.6115 -- iter: 352/404
[A[ATraining Step: 194  | total loss: [1m[32m0.65019[0m[0m | time: 7.470s
[2K
| Adam | epoch: 015 | loss: 0.65019 - acc: 0.6128 -- iter: 384/404
[A[ATraining Step: 195  | total loss: [1m[32m0.64511[0m[0m | time: 8.871s
[2K
| Adam | epoch: 015 | loss: 0.64511 - acc: 0.6172 | val_loss: 0.65295 - val_acc: 0.5906 -- iter: 404/404
--
Training Step: 196  | total loss: [1m[32m0.64742[0m[0m | time: 0.401s
[2K
| Adam | epoch: 016 | loss: 0.64742 - acc: 0.6104 -- iter: 032/404
[A[ATraining Step: 197  | total loss: [1m[32m0.64752[0m[0m | time: 1.043s
[2K
| Adam | epoch: 016 | loss: 0.64752 - acc: 0.6044 -- iter: 064/404
[A[ATraining Step: 198  | total loss: [1m[32m0.64575[0m[0m | time: 1.667s
[2K
| Adam | epoch: 016 | loss: 0.64575 - acc: 0.6033 -- iter: 096/404
[A[ATraining Step: 199  | total loss: [1m[32m0.64926[0m[0m | time: 2.294s
[2K
| Adam | epoch: 016 | loss: 0.64926 - acc: 0.5930 -- iter: 128/404
[A[ATraining Step: 200  | total loss: [1m[32m0.64667[0m[0m | time: 3.911s
[2K
| Adam | epoch: 016 | loss: 0.64667 - acc: 0.5962 | val_loss: 0.64517 - val_acc: 0.5906 -- iter: 160/404
--
Training Step: 201  | total loss: [1m[32m0.65210[0m[0m | time: 4.521s
[2K
| Adam | epoch: 016 | loss: 0.65210 - acc: 0.5928 -- iter: 192/404
[A[ATraining Step: 202  | total loss: [1m[32m0.65048[0m[0m | time: 5.136s
[2K
| Adam | epoch: 016 | loss: 0.65048 - acc: 0.5898 -- iter: 224/404
[A[ATraining Step: 203  | total loss: [1m[32m0.64181[0m[0m | time: 5.735s
[2K
| Adam | epoch: 016 | loss: 0.64181 - acc: 0.5996 -- iter: 256/404
[A[ATraining Step: 204  | total loss: [1m[32m0.63178[0m[0m | time: 6.366s
[2K
| Adam | epoch: 016 | loss: 0.63178 - acc: 0.6115 -- iter: 288/404
[A[ATraining Step: 205  | total loss: [1m[32m0.63149[0m[0m | time: 6.989s
[2K
| Adam | epoch: 016 | loss: 0.63149 - acc: 0.6160 -- iter: 320/404
[A[ATraining Step: 206  | total loss: [1m[32m0.63989[0m[0m | time: 7.602s
[2K
| Adam | epoch: 016 | loss: 0.63989 - acc: 0.6137 -- iter: 352/404
[A[ATraining Step: 207  | total loss: [1m[32m0.64184[0m[0m | time: 8.214s
[2K
| Adam | epoch: 016 | loss: 0.64184 - acc: 0.6024 -- iter: 384/404
[A[ATraining Step: 208  | total loss: [1m[32m0.63335[0m[0m | time: 9.839s
[2K
| Adam | epoch: 016 | loss: 0.63335 - acc: 0.6234 | val_loss: 0.63819 - val_acc: 0.6457 -- iter: 404/404
--
Training Step: 209  | total loss: [1m[32m0.63070[0m[0m | time: 0.417s
[2K
| Adam | epoch: 017 | loss: 0.63070 - acc: 0.6329 -- iter: 032/404
[A[ATraining Step: 210  | total loss: [1m[32m0.63486[0m[0m | time: 0.810s
[2K
| Adam | epoch: 017 | loss: 0.63486 - acc: 0.6246 -- iter: 064/404
[A[ATraining Step: 211  | total loss: [1m[32m0.63921[0m[0m | time: 1.440s
[2K
| Adam | epoch: 017 | loss: 0.63921 - acc: 0.6172 -- iter: 096/404
[A[ATraining Step: 212  | total loss: [1m[32m0.63207[0m[0m | time: 2.086s
[2K
| Adam | epoch: 017 | loss: 0.63207 - acc: 0.6211 -- iter: 128/404
[A[ATraining Step: 213  | total loss: [1m[32m0.63402[0m[0m | time: 2.691s
[2K
| Adam | epoch: 017 | loss: 0.63402 - acc: 0.6215 -- iter: 160/404
[A[ATraining Step: 214  | total loss: [1m[32m0.63322[0m[0m | time: 3.290s
[2K
| Adam | epoch: 017 | loss: 0.63322 - acc: 0.6187 -- iter: 192/404
[A[ATraining Step: 215  | total loss: [1m[32m0.63213[0m[0m | time: 3.915s
[2K
| Adam | epoch: 017 | loss: 0.63213 - acc: 0.6224 -- iter: 224/404
[A[ATraining Step: 216  | total loss: [1m[32m0.62445[0m[0m | time: 4.528s
[2K
| Adam | epoch: 017 | loss: 0.62445 - acc: 0.6352 -- iter: 256/404
[A[ATraining Step: 217  | total loss: [1m[32m0.62776[0m[0m | time: 5.181s
[2K
| Adam | epoch: 017 | loss: 0.62776 - acc: 0.6154 -- iter: 288/404
[A[ATraining Step: 218  | total loss: [1m[32m0.61916[0m[0m | time: 5.807s
[2K
| Adam | epoch: 017 | loss: 0.61916 - acc: 0.6383 -- iter: 320/404
[A[ATraining Step: 219  | total loss: [1m[32m0.62112[0m[0m | time: 6.417s
[2K
| Adam | epoch: 017 | loss: 0.62112 - acc: 0.6401 -- iter: 352/404
[A[ATraining Step: 220  | total loss: [1m[32m0.62063[0m[0m | time: 7.046s
[2K
| Adam | epoch: 017 | loss: 0.62063 - acc: 0.6448 -- iter: 384/404
[A[ATraining Step: 221  | total loss: [1m[32m0.62194[0m[0m | time: 8.661s
[2K
| Adam | epoch: 017 | loss: 0.62194 - acc: 0.6491 | val_loss: 0.62589 - val_acc: 0.6772 -- iter: 404/404
--
Training Step: 222  | total loss: [1m[32m0.61359[0m[0m | time: 0.656s
[2K
| Adam | epoch: 018 | loss: 0.61359 - acc: 0.6654 -- iter: 032/404
[A[ATraining Step: 223  | total loss: [1m[32m0.60337[0m[0m | time: 1.052s
[2K
| Adam | epoch: 018 | loss: 0.60337 - acc: 0.6676 -- iter: 064/404
[A[ATraining Step: 224  | total loss: [1m[32m0.60953[0m[0m | time: 1.441s
[2K
| Adam | epoch: 018 | loss: 0.60953 - acc: 0.6759 -- iter: 096/404
[A[ATraining Step: 225  | total loss: [1m[32m0.61362[0m[0m | time: 2.063s
[2K
| Adam | epoch: 018 | loss: 0.61362 - acc: 0.6833 -- iter: 128/404
[A[ATraining Step: 226  | total loss: [1m[32m0.60799[0m[0m | time: 2.682s
[2K
| Adam | epoch: 018 | loss: 0.60799 - acc: 0.6900 -- iter: 160/404
[A[ATraining Step: 227  | total loss: [1m[32m0.60033[0m[0m | time: 3.285s
[2K
| Adam | epoch: 018 | loss: 0.60033 - acc: 0.6960 -- iter: 192/404
[A[ATraining Step: 228  | total loss: [1m[32m0.60329[0m[0m | time: 3.900s
[2K
| Adam | epoch: 018 | loss: 0.60329 - acc: 0.6889 -- iter: 224/404
[A[ATraining Step: 229  | total loss: [1m[32m0.58627[0m[0m | time: 4.533s
[2K
| Adam | epoch: 018 | loss: 0.58627 - acc: 0.6950 -- iter: 256/404
[A[ATraining Step: 230  | total loss: [1m[32m0.59779[0m[0m | time: 5.161s
[2K
| Adam | epoch: 018 | loss: 0.59779 - acc: 0.6880 -- iter: 288/404
[A[ATraining Step: 231  | total loss: [1m[32m0.58785[0m[0m | time: 5.792s
[2K
| Adam | epoch: 018 | loss: 0.58785 - acc: 0.6911 -- iter: 320/404
[A[ATraining Step: 232  | total loss: [1m[32m0.59881[0m[0m | time: 6.430s
[2K
| Adam | epoch: 018 | loss: 0.59881 - acc: 0.6782 -- iter: 352/404
[A[ATraining Step: 233  | total loss: [1m[32m0.59833[0m[0m | time: 7.074s
[2K
| Adam | epoch: 018 | loss: 0.59833 - acc: 0.6729 -- iter: 384/404
[A[ATraining Step: 234  | total loss: [1m[32m0.60016[0m[0m | time: 8.730s
[2K
| Adam | epoch: 018 | loss: 0.60016 - acc: 0.6650 | val_loss: 0.63131 - val_acc: 0.6614 -- iter: 404/404
--
Training Step: 235  | total loss: [1m[32m0.59995[0m[0m | time: 0.624s
[2K
| Adam | epoch: 019 | loss: 0.59995 - acc: 0.6672 -- iter: 032/404
[A[ATraining Step: 236  | total loss: [1m[32m0.59859[0m[0m | time: 1.241s
[2K
| Adam | epoch: 019 | loss: 0.59859 - acc: 0.6849 -- iter: 064/404
[A[ATraining Step: 237  | total loss: [1m[32m0.59714[0m[0m | time: 1.664s
[2K
| Adam | epoch: 019 | loss: 0.59714 - acc: 0.6914 -- iter: 096/404
[A[ATraining Step: 238  | total loss: [1m[32m0.59018[0m[0m | time: 2.058s
[2K
| Adam | epoch: 019 | loss: 0.59018 - acc: 0.7022 -- iter: 128/404
[A[ATraining Step: 239  | total loss: [1m[32m0.58130[0m[0m | time: 2.686s
[2K
| Adam | epoch: 019 | loss: 0.58130 - acc: 0.7020 -- iter: 160/404
[A[ATraining Step: 240  | total loss: [1m[32m0.57560[0m[0m | time: 3.327s
[2K
| Adam | epoch: 019 | loss: 0.57560 - acc: 0.7037 -- iter: 192/404
[A[ATraining Step: 241  | total loss: [1m[32m0.55620[0m[0m | time: 3.949s
[2K
| Adam | epoch: 019 | loss: 0.55620 - acc: 0.7302 -- iter: 224/404
[A[ATraining Step: 242  | total loss: [1m[32m0.54887[0m[0m | time: 4.556s
[2K
| Adam | epoch: 019 | loss: 0.54887 - acc: 0.7384 -- iter: 256/404
[A[ATraining Step: 243  | total loss: [1m[32m0.54035[0m[0m | time: 5.199s
[2K
| Adam | epoch: 019 | loss: 0.54035 - acc: 0.7427 -- iter: 288/404
[A[ATraining Step: 244  | total loss: [1m[32m0.53090[0m[0m | time: 5.816s
[2K
| Adam | epoch: 019 | loss: 0.53090 - acc: 0.7497 -- iter: 320/404
[A[ATraining Step: 245  | total loss: [1m[32m0.54442[0m[0m | time: 6.440s
[2K
| Adam | epoch: 019 | loss: 0.54442 - acc: 0.7403 -- iter: 352/404
[A[ATraining Step: 246  | total loss: [1m[32m0.57625[0m[0m | time: 7.060s
[2K
| Adam | epoch: 019 | loss: 0.57625 - acc: 0.7194 -- iter: 384/404
[A[ATraining Step: 247  | total loss: [1m[32m0.57854[0m[0m | time: 8.680s
[2K
| Adam | epoch: 019 | loss: 0.57854 - acc: 0.7162 | val_loss: 0.64937 - val_acc: 0.6299 -- iter: 404/404
--
Training Step: 248  | total loss: [1m[32m0.56499[0m[0m | time: 0.619s
[2K
| Adam | epoch: 020 | loss: 0.56499 - acc: 0.7227 -- iter: 032/404
[A[ATraining Step: 249  | total loss: [1m[32m0.57708[0m[0m | time: 1.282s
[2K
| Adam | epoch: 020 | loss: 0.57708 - acc: 0.7223 -- iter: 064/404
[A[ATraining Step: 250  | total loss: [1m[32m0.56699[0m[0m | time: 1.918s
[2K
| Adam | epoch: 020 | loss: 0.56699 - acc: 0.7345 -- iter: 096/404
[A[ATraining Step: 251  | total loss: [1m[32m0.54669[0m[0m | time: 2.313s
[2K
| Adam | epoch: 020 | loss: 0.54669 - acc: 0.7485 -- iter: 128/404
[A[ATraining Step: 252  | total loss: [1m[32m0.54469[0m[0m | time: 2.719s
[2K
| Adam | epoch: 020 | loss: 0.54469 - acc: 0.7537 -- iter: 160/404
[A[ATraining Step: 253  | total loss: [1m[32m0.54402[0m[0m | time: 3.334s
[2K
| Adam | epoch: 020 | loss: 0.54402 - acc: 0.7583 -- iter: 192/404
[A[ATraining Step: 254  | total loss: [1m[32m0.53409[0m[0m | time: 3.960s
[2K
| Adam | epoch: 020 | loss: 0.53409 - acc: 0.7575 -- iter: 224/404
[A[ATraining Step: 255  | total loss: [1m[32m0.52194[0m[0m | time: 4.567s
[2K
| Adam | epoch: 020 | loss: 0.52194 - acc: 0.7630 -- iter: 256/404
[A[ATraining Step: 256  | total loss: [1m[32m0.51076[0m[0m | time: 5.191s
[2K
| Adam | epoch: 020 | loss: 0.51076 - acc: 0.7742 -- iter: 288/404
[A[ATraining Step: 257  | total loss: [1m[32m0.49979[0m[0m | time: 5.823s
[2K
| Adam | epoch: 020 | loss: 0.49979 - acc: 0.7811 -- iter: 320/404
[A[ATraining Step: 258  | total loss: [1m[32m0.50106[0m[0m | time: 6.443s
[2K
| Adam | epoch: 020 | loss: 0.50106 - acc: 0.7780 -- iter: 352/404
[A[ATraining Step: 259  | total loss: [1m[32m0.48880[0m[0m | time: 7.084s
[2K
| Adam | epoch: 020 | loss: 0.48880 - acc: 0.7909 -- iter: 384/404
[A[ATraining Step: 260  | total loss: [1m[32m0.47709[0m[0m | time: 8.713s
[2K
| Adam | epoch: 020 | loss: 0.47709 - acc: 0.8024 | val_loss: 0.60098 - val_acc: 0.7165 -- iter: 404/404
--
Training Step: 261  | total loss: [1m[32m0.47500[0m[0m | time: 0.621s
[2K
| Adam | epoch: 021 | loss: 0.47500 - acc: 0.8034 -- iter: 032/404
[A[ATraining Step: 262  | total loss: [1m[32m0.45390[0m[0m | time: 1.251s
[2K
| Adam | epoch: 021 | loss: 0.45390 - acc: 0.8199 -- iter: 064/404
[A[ATraining Step: 263  | total loss: [1m[32m0.46940[0m[0m | time: 1.872s
[2K
| Adam | epoch: 021 | loss: 0.46940 - acc: 0.8036 -- iter: 096/404
[A[ATraining Step: 264  | total loss: [1m[32m0.45457[0m[0m | time: 2.498s
[2K
| Adam | epoch: 021 | loss: 0.45457 - acc: 0.8138 -- iter: 128/404
[A[ATraining Step: 265  | total loss: [1m[32m0.43479[0m[0m | time: 2.899s
[2K
| Adam | epoch: 021 | loss: 0.43479 - acc: 0.8293 -- iter: 160/404
[A[ATraining Step: 266  | total loss: [1m[32m0.42863[0m[0m | time: 3.290s
[2K
| Adam | epoch: 021 | loss: 0.42863 - acc: 0.8264 -- iter: 192/404
[A[ATraining Step: 267  | total loss: [1m[32m0.41759[0m[0m | time: 3.916s
[2K
| Adam | epoch: 021 | loss: 0.41759 - acc: 0.8288 -- iter: 224/404
[A[ATraining Step: 268  | total loss: [1m[32m0.40412[0m[0m | time: 4.546s
[2K
| Adam | epoch: 021 | loss: 0.40412 - acc: 0.8334 -- iter: 256/404
[A[ATraining Step: 269  | total loss: [1m[32m0.39130[0m[0m | time: 5.155s
[2K
| Adam | epoch: 021 | loss: 0.39130 - acc: 0.8438 -- iter: 288/404
[A[ATraining Step: 270  | total loss: [1m[32m0.41039[0m[0m | time: 5.767s
[2K
| Adam | epoch: 021 | loss: 0.41039 - acc: 0.8313 -- iter: 320/404
[A[ATraining Step: 271  | total loss: [1m[32m0.39075[0m[0m | time: 6.400s
[2K
| Adam | epoch: 021 | loss: 0.39075 - acc: 0.8419 -- iter: 352/404
[A[ATraining Step: 272  | total loss: [1m[32m0.39232[0m[0m | time: 7.057s
[2K
| Adam | epoch: 021 | loss: 0.39232 - acc: 0.8390 -- iter: 384/404
[A[ATraining Step: 273  | total loss: [1m[32m0.39110[0m[0m | time: 8.714s
[2K
| Adam | epoch: 021 | loss: 0.39110 - acc: 0.8363 | val_loss: 0.58682 - val_acc: 0.7559 -- iter: 404/404
--
Training Step: 274  | total loss: [1m[32m0.39224[0m[0m | time: 0.626s
[2K
| Adam | epoch: 022 | loss: 0.39224 - acc: 0.8339 -- iter: 032/404
[A[ATraining Step: 275  | total loss: [1m[32m0.38003[0m[0m | time: 1.254s
[2K
| Adam | epoch: 022 | loss: 0.38003 - acc: 0.8380 -- iter: 064/404
[A[ATraining Step: 276  | total loss: [1m[32m0.37774[0m[0m | time: 1.897s
[2K
| Adam | epoch: 022 | loss: 0.37774 - acc: 0.8355 -- iter: 096/404
[A[ATraining Step: 277  | total loss: [1m[32m0.35233[0m[0m | time: 2.561s
[2K
| Adam | epoch: 022 | loss: 0.35233 - acc: 0.8488 -- iter: 128/404
[A[ATraining Step: 278  | total loss: [1m[32m0.36148[0m[0m | time: 3.196s
[2K
| Adam | epoch: 022 | loss: 0.36148 - acc: 0.8452 -- iter: 160/404
[A[ATraining Step: 279  | total loss: [1m[32m0.34389[0m[0m | time: 3.622s
[2K
| Adam | epoch: 022 | loss: 0.34389 - acc: 0.8513 -- iter: 192/404
[A[ATraining Step: 280  | total loss: [1m[32m0.33709[0m[0m | time: 4.019s
[2K
| Adam | epoch: 022 | loss: 0.33709 - acc: 0.8612 -- iter: 224/404
[A[ATraining Step: 281  | total loss: [1m[32m0.32643[0m[0m | time: 4.642s
[2K
| Adam | epoch: 022 | loss: 0.32643 - acc: 0.8700 -- iter: 256/404
[A[ATraining Step: 282  | total loss: [1m[32m0.31012[0m[0m | time: 5.266s
[2K
| Adam | epoch: 022 | loss: 0.31012 - acc: 0.8799 -- iter: 288/404
[A[ATraining Step: 283  | total loss: [1m[32m0.31980[0m[0m | time: 5.916s
[2K
| Adam | epoch: 022 | loss: 0.31980 - acc: 0.8700 -- iter: 320/404
[A[ATraining Step: 284  | total loss: [1m[32m0.31060[0m[0m | time: 6.543s
[2K
| Adam | epoch: 022 | loss: 0.31060 - acc: 0.8737 -- iter: 352/404
[A[ATraining Step: 285  | total loss: [1m[32m0.34187[0m[0m | time: 7.185s
[2K
| Adam | epoch: 022 | loss: 0.34187 - acc: 0.8488 -- iter: 384/404
[A[ATraining Step: 286  | total loss: [1m[32m0.37149[0m[0m | time: 8.820s
[2K
| Adam | epoch: 022 | loss: 0.37149 - acc: 0.8327 | val_loss: 0.68734 - val_acc: 0.7480 -- iter: 404/404
--
Training Step: 287  | total loss: [1m[32m0.37440[0m[0m | time: 0.626s
[2K
| Adam | epoch: 023 | loss: 0.37440 - acc: 0.8307 -- iter: 032/404
[A[ATraining Step: 288  | total loss: [1m[32m0.35168[0m[0m | time: 1.253s
[2K
| Adam | epoch: 023 | loss: 0.35168 - acc: 0.8445 -- iter: 064/404
[A[ATraining Step: 289  | total loss: [1m[32m0.33961[0m[0m | time: 1.880s
[2K
| Adam | epoch: 023 | loss: 0.33961 - acc: 0.8444 -- iter: 096/404
[A[ATraining Step: 290  | total loss: [1m[32m0.33199[0m[0m | time: 2.538s
[2K
| Adam | epoch: 023 | loss: 0.33199 - acc: 0.8506 -- iter: 128/404
[A[ATraining Step: 291  | total loss: [1m[32m0.32310[0m[0m | time: 3.186s
[2K
| Adam | epoch: 023 | loss: 0.32310 - acc: 0.8561 -- iter: 160/404
[A[ATraining Step: 292  | total loss: [1m[32m0.30586[0m[0m | time: 3.810s
[2K
| Adam | epoch: 023 | loss: 0.30586 - acc: 0.8674 -- iter: 192/404
[A[ATraining Step: 293  | total loss: [1m[32m0.29348[0m[0m | time: 4.216s
[2K
| Adam | epoch: 023 | loss: 0.29348 - acc: 0.8744 -- iter: 224/404
[A[ATraining Step: 294  | total loss: [1m[32m0.28998[0m[0m | time: 4.622s
[2K
| Adam | epoch: 023 | loss: 0.28998 - acc: 0.8720 -- iter: 256/404
[A[ATraining Step: 295  | total loss: [1m[32m0.27653[0m[0m | time: 5.275s
[2K
| Adam | epoch: 023 | loss: 0.27653 - acc: 0.8798 -- iter: 288/404
[A[ATraining Step: 296  | total loss: [1m[32m0.26217[0m[0m | time: 5.911s
[2K
| Adam | epoch: 023 | loss: 0.26217 - acc: 0.8887 -- iter: 320/404
[A[ATraining Step: 297  | total loss: [1m[32m0.26536[0m[0m | time: 6.538s
[2K
| Adam | epoch: 023 | loss: 0.26536 - acc: 0.8842 -- iter: 352/404
[A[ATraining Step: 298  | total loss: [1m[32m0.24963[0m[0m | time: 7.194s
[2K
| Adam | epoch: 023 | loss: 0.24963 - acc: 0.8926 -- iter: 384/404
[A[ATraining Step: 299  | total loss: [1m[32m0.23737[0m[0m | time: 8.830s
[2K
| Adam | epoch: 023 | loss: 0.23737 - acc: 0.9002 | val_loss: 0.74884 - val_acc: 0.7402 -- iter: 404/404
--
Training Step: 300  | total loss: [1m[32m0.22985[0m[0m | time: 0.644s
[2K
| Adam | epoch: 024 | loss: 0.22985 - acc: 0.9008 -- iter: 032/404
[A[ATraining Step: 301  | total loss: [1m[32m0.23754[0m[0m | time: 1.285s
[2K
| Adam | epoch: 024 | loss: 0.23754 - acc: 0.8951 -- iter: 064/404
[A[ATraining Step: 302  | total loss: [1m[32m0.22736[0m[0m | time: 1.911s
[2K
| Adam | epoch: 024 | loss: 0.22736 - acc: 0.8994 -- iter: 096/404
[A[ATraining Step: 303  | total loss: [1m[32m0.22938[0m[0m | time: 2.572s
[2K
| Adam | epoch: 024 | loss: 0.22938 - acc: 0.9032 -- iter: 128/404
[A[ATraining Step: 304  | total loss: [1m[32m0.21625[0m[0m | time: 3.212s
[2K
| Adam | epoch: 024 | loss: 0.21625 - acc: 0.9097 -- iter: 160/404
[A[ATraining Step: 305  | total loss: [1m[32m0.20623[0m[0m | time: 3.889s
[2K
| Adam | epoch: 024 | loss: 0.20623 - acc: 0.9156 -- iter: 192/404
[A[ATraining Step: 306  | total loss: [1m[32m0.19259[0m[0m | time: 4.518s
[2K
| Adam | epoch: 024 | loss: 0.19259 - acc: 0.9241 -- iter: 224/404
[A[ATraining Step: 307  | total loss: [1m[32m0.19399[0m[0m | time: 4.911s
[2K
| Adam | epoch: 024 | loss: 0.19399 - acc: 0.9254 -- iter: 256/404
[A[ATraining Step: 308  | total loss: [1m[32m0.18162[0m[0m | time: 5.337s
[2K
| Adam | epoch: 024 | loss: 0.18162 - acc: 0.9279 -- iter: 288/404
[A[ATraining Step: 309  | total loss: [1m[32m0.16953[0m[0m | time: 5.962s
[2K
| Adam | epoch: 024 | loss: 0.16953 - acc: 0.9351 -- iter: 320/404
[A[ATraining Step: 310  | total loss: [1m[32m0.17143[0m[0m | time: 6.582s
[2K
| Adam | epoch: 024 | loss: 0.17143 - acc: 0.9353 -- iter: 352/404
[A[ATraining Step: 311  | total loss: [1m[32m0.16642[0m[0m | time: 7.207s
[2K
| Adam | epoch: 024 | loss: 0.16642 - acc: 0.9355 -- iter: 384/404
[A[ATraining Step: 312  | total loss: [1m[32m0.19484[0m[0m | time: 8.828s
[2K
| Adam | epoch: 024 | loss: 0.19484 - acc: 0.9232 | val_loss: 0.83471 - val_acc: 0.7165 -- iter: 404/404
--
Training Step: 313  | total loss: [1m[32m0.17979[0m[0m | time: 0.617s
[2K
| Adam | epoch: 025 | loss: 0.17979 - acc: 0.9309 -- iter: 032/404
[A[ATraining Step: 314  | total loss: [1m[32m0.17573[0m[0m | time: 1.231s
[2K
| Adam | epoch: 025 | loss: 0.17573 - acc: 0.9347 -- iter: 064/404
[A[ATraining Step: 315  | total loss: [1m[32m0.16286[0m[0m | time: 1.855s
[2K
| Adam | epoch: 025 | loss: 0.16286 - acc: 0.9412 -- iter: 096/404
[A[ATraining Step: 316  | total loss: [1m[32m0.17132[0m[0m | time: 2.472s
[2K
| Adam | epoch: 025 | loss: 0.17132 - acc: 0.9346 -- iter: 128/404
[A[ATraining Step: 317  | total loss: [1m[32m0.20590[0m[0m | time: 3.093s
[2K
| Adam | epoch: 025 | loss: 0.20590 - acc: 0.9224 -- iter: 160/404
[A[ATraining Step: 318  | total loss: [1m[32m0.21082[0m[0m | time: 3.710s
[2K
| Adam | epoch: 025 | loss: 0.21082 - acc: 0.9239 -- iter: 192/404
[A[ATraining Step: 319  | total loss: [1m[32m0.19752[0m[0m | time: 4.320s
[2K
| Adam | epoch: 025 | loss: 0.19752 - acc: 0.9284 -- iter: 224/404
[A[ATraining Step: 320  | total loss: [1m[32m0.19935[0m[0m | time: 4.939s
[2K
| Adam | epoch: 025 | loss: 0.19935 - acc: 0.9293 -- iter: 256/404
[A[ATraining Step: 321  | total loss: [1m[32m0.18296[0m[0m | time: 5.338s
[2K
| Adam | epoch: 025 | loss: 0.18296 - acc: 0.9364 -- iter: 288/404
[A[ATraining Step: 322  | total loss: [1m[32m0.16998[0m[0m | time: 5.728s
[2K
| Adam | epoch: 025 | loss: 0.16998 - acc: 0.9427 -- iter: 320/404
[A[ATraining Step: 323  | total loss: [1m[32m0.15714[0m[0m | time: 6.379s
[2K
| Adam | epoch: 025 | loss: 0.15714 - acc: 0.9485 -- iter: 352/404
[A[ATraining Step: 324  | total loss: [1m[32m0.14382[0m[0m | time: 7.003s
[2K
| Adam | epoch: 025 | loss: 0.14382 - acc: 0.9536 -- iter: 384/404
[A[ATraining Step: 325  | total loss: [1m[32m0.14427[0m[0m | time: 8.640s
[2K
| Adam | epoch: 025 | loss: 0.14427 - acc: 0.9520 | val_loss: 0.90621 - val_acc: 0.7165 -- iter: 404/404
--
Training Step: 326  | total loss: [1m[32m0.13391[0m[0m | time: 0.639s
[2K
| Adam | epoch: 026 | loss: 0.13391 - acc: 0.9568 -- iter: 032/404
[A[ATraining Step: 327  | total loss: [1m[32m0.13922[0m[0m | time: 1.262s
[2K
| Adam | epoch: 026 | loss: 0.13922 - acc: 0.9580 -- iter: 064/404
[A[ATraining Step: 328  | total loss: [1m[32m0.12940[0m[0m | time: 1.882s
[2K
| Adam | epoch: 026 | loss: 0.12940 - acc: 0.9622 -- iter: 096/404
[A[ATraining Step: 329  | total loss: [1m[32m0.12042[0m[0m | time: 2.519s
[2K
| Adam | epoch: 026 | loss: 0.12042 - acc: 0.9660 -- iter: 128/404
[A[ATraining Step: 330  | total loss: [1m[32m0.12564[0m[0m | time: 3.160s
[2K
| Adam | epoch: 026 | loss: 0.12564 - acc: 0.9600 -- iter: 160/404
[A[ATraining Step: 331  | total loss: [1m[32m0.11831[0m[0m | time: 3.800s
[2K
| Adam | epoch: 026 | loss: 0.11831 - acc: 0.9640 -- iter: 192/404
[A[ATraining Step: 332  | total loss: [1m[32m0.13097[0m[0m | time: 4.402s
[2K
| Adam | epoch: 026 | loss: 0.13097 - acc: 0.9645 -- iter: 224/404
[A[ATraining Step: 333  | total loss: [1m[32m0.12301[0m[0m | time: 5.027s
[2K
| Adam | epoch: 026 | loss: 0.12301 - acc: 0.9680 -- iter: 256/404
[A[ATraining Step: 334  | total loss: [1m[32m0.11266[0m[0m | time: 5.650s
[2K
| Adam | epoch: 026 | loss: 0.11266 - acc: 0.9712 -- iter: 288/404
[A[ATraining Step: 335  | total loss: [1m[32m0.10518[0m[0m | time: 6.038s
[2K
| Adam | epoch: 026 | loss: 0.10518 - acc: 0.9741 -- iter: 320/404
[A[ATraining Step: 336  | total loss: [1m[32m0.09660[0m[0m | time: 6.430s
[2K
| Adam | epoch: 026 | loss: 0.09660 - acc: 0.9767 -- iter: 352/404
[A[ATraining Step: 337  | total loss: [1m[32m0.08876[0m[0m | time: 7.036s
[2K
| Adam | epoch: 026 | loss: 0.08876 - acc: 0.9790 -- iter: 384/404
[A[ATraining Step: 338  | total loss: [1m[32m0.09617[0m[0m | time: 8.685s
[2K
| Adam | epoch: 026 | loss: 0.09617 - acc: 0.9780 | val_loss: 0.86988 - val_acc: 0.7638 -- iter: 404/404
--
Training Step: 339  | total loss: [1m[32m0.08817[0m[0m | time: 0.625s
[2K
| Adam | epoch: 027 | loss: 0.08817 - acc: 0.9802 -- iter: 032/404
[A[ATraining Step: 340  | total loss: [1m[32m0.08153[0m[0m | time: 1.269s
[2K
| Adam | epoch: 027 | loss: 0.08153 - acc: 0.9822 -- iter: 064/404
[A[ATraining Step: 341  | total loss: [1m[32m0.07417[0m[0m | time: 1.919s
[2K
| Adam | epoch: 027 | loss: 0.07417 - acc: 0.9840 -- iter: 096/404
[A[ATraining Step: 342  | total loss: [1m[32m0.08246[0m[0m | time: 2.531s
[2K
| Adam | epoch: 027 | loss: 0.08246 - acc: 0.9824 -- iter: 128/404
[A[ATraining Step: 343  | total loss: [1m[32m0.07595[0m[0m | time: 3.169s
[2K
| Adam | epoch: 027 | loss: 0.07595 - acc: 0.9842 -- iter: 160/404
[A[ATraining Step: 344  | total loss: [1m[32m0.07112[0m[0m | time: 3.786s
[2K
| Adam | epoch: 027 | loss: 0.07112 - acc: 0.9858 -- iter: 192/404
[A[ATraining Step: 345  | total loss: [1m[32m0.07713[0m[0m | time: 4.408s
[2K
| Adam | epoch: 027 | loss: 0.07713 - acc: 0.9841 -- iter: 224/404
[A[ATraining Step: 346  | total loss: [1m[32m0.07143[0m[0m | time: 5.037s
[2K
| Adam | epoch: 027 | loss: 0.07143 - acc: 0.9857 -- iter: 256/404
[A[ATraining Step: 347  | total loss: [1m[32m0.06627[0m[0m | time: 5.654s
[2K
| Adam | epoch: 027 | loss: 0.06627 - acc: 0.9871 -- iter: 288/404
[A[ATraining Step: 348  | total loss: [1m[32m0.06329[0m[0m | time: 6.283s
[2K
| Adam | epoch: 027 | loss: 0.06329 - acc: 0.9884 -- iter: 320/404
[A[ATraining Step: 349  | total loss: [1m[32m0.07236[0m[0m | time: 6.676s
[2K
| Adam | epoch: 027 | loss: 0.07236 - acc: 0.9833 -- iter: 352/404
[A[ATraining Step: 350  | total loss: [1m[32m0.06869[0m[0m | time: 7.089s
[2K
| Adam | epoch: 027 | loss: 0.06869 - acc: 0.9850 -- iter: 384/404
[A[ATraining Step: 351  | total loss: [1m[32m0.06831[0m[0m | time: 8.711s
[2K
| Adam | epoch: 027 | loss: 0.06831 - acc: 0.9865 | val_loss: 0.90620 - val_acc: 0.7559 -- iter: 404/404
--
Training Step: 352  | total loss: [1m[32m0.06267[0m[0m | time: 0.632s
[2K
| Adam | epoch: 028 | loss: 0.06267 - acc: 0.9878 -- iter: 032/404
[A[ATraining Step: 353  | total loss: [1m[32m0.05714[0m[0m | time: 1.274s
[2K
| Adam | epoch: 028 | loss: 0.05714 - acc: 0.9890 -- iter: 064/404
[A[ATraining Step: 354  | total loss: [1m[32m0.05275[0m[0m | time: 1.892s
[2K
| Adam | epoch: 028 | loss: 0.05275 - acc: 0.9901 -- iter: 096/404
[A[ATraining Step: 355  | total loss: [1m[32m0.05171[0m[0m | time: 2.517s
[2K
| Adam | epoch: 028 | loss: 0.05171 - acc: 0.9911 -- iter: 128/404
[A[ATraining Step: 356  | total loss: [1m[32m0.04869[0m[0m | time: 3.140s
[2K
| Adam | epoch: 028 | loss: 0.04869 - acc: 0.9920 -- iter: 160/404
[A[ATraining Step: 357  | total loss: [1m[32m0.04680[0m[0m | time: 3.776s
[2K
| Adam | epoch: 028 | loss: 0.04680 - acc: 0.9928 -- iter: 192/404
[A[ATraining Step: 358  | total loss: [1m[32m0.05815[0m[0m | time: 4.396s
[2K
| Adam | epoch: 028 | loss: 0.05815 - acc: 0.9904 -- iter: 224/404
[A[ATraining Step: 359  | total loss: [1m[32m0.05310[0m[0m | time: 5.013s
[2K
| Adam | epoch: 028 | loss: 0.05310 - acc: 0.9914 -- iter: 256/404
[A[ATraining Step: 360  | total loss: [1m[32m0.07332[0m[0m | time: 5.636s
[2K
| Adam | epoch: 028 | loss: 0.07332 - acc: 0.9891 -- iter: 288/404
[A[ATraining Step: 361  | total loss: [1m[32m0.06697[0m[0m | time: 6.264s
[2K
| Adam | epoch: 028 | loss: 0.06697 - acc: 0.9902 -- iter: 320/404
[A[ATraining Step: 362  | total loss: [1m[32m0.07622[0m[0m | time: 6.890s
[2K
| Adam | epoch: 028 | loss: 0.07622 - acc: 0.9849 -- iter: 352/404
[A[ATraining Step: 363  | total loss: [1m[32m0.06940[0m[0m | time: 7.294s
[2K
| Adam | epoch: 028 | loss: 0.06940 - acc: 0.9864 -- iter: 384/404
[A[ATraining Step: 364  | total loss: [1m[32m0.06479[0m[0m | time: 8.721s
[2K
| Adam | epoch: 028 | loss: 0.06479 - acc: 0.9878 | val_loss: 0.92357 - val_acc: 0.7402 -- iter: 404/404
--
Training Step: 365  | total loss: [1m[32m0.06043[0m[0m | time: 0.605s
[2K
| Adam | epoch: 029 | loss: 0.06043 - acc: 0.9890 -- iter: 032/404
[A[ATraining Step: 366  | total loss: [1m[32m0.06071[0m[0m | time: 1.226s
[2K
| Adam | epoch: 029 | loss: 0.06071 - acc: 0.9870 -- iter: 064/404
[A[ATraining Step: 367  | total loss: [1m[32m0.06357[0m[0m | time: 1.844s
[2K
| Adam | epoch: 029 | loss: 0.06357 - acc: 0.9820 -- iter: 096/404
[A[ATraining Step: 368  | total loss: [1m[32m0.06250[0m[0m | time: 2.456s
[2K
| Adam | epoch: 029 | loss: 0.06250 - acc: 0.9807 -- iter: 128/404
[A[ATraining Step: 369  | total loss: [1m[32m0.06222[0m[0m | time: 3.053s
[2K
| Adam | epoch: 029 | loss: 0.06222 - acc: 0.9795 -- iter: 160/404
[A[ATraining Step: 370  | total loss: [1m[32m0.05714[0m[0m | time: 3.663s
[2K
| Adam | epoch: 029 | loss: 0.05714 - acc: 0.9816 -- iter: 192/404
[A[ATraining Step: 371  | total loss: [1m[32m0.05423[0m[0m | time: 4.256s
[2K
| Adam | epoch: 029 | loss: 0.05423 - acc: 0.9834 -- iter: 224/404
[A[ATraining Step: 372  | total loss: [1m[32m0.05777[0m[0m | time: 4.892s
[2K
| Adam | epoch: 029 | loss: 0.05777 - acc: 0.9819 -- iter: 256/404
[A[ATraining Step: 373  | total loss: [1m[32m0.06125[0m[0m | time: 5.506s
[2K
| Adam | epoch: 029 | loss: 0.06125 - acc: 0.9806 -- iter: 288/404
[A[ATraining Step: 374  | total loss: [1m[32m0.06493[0m[0m | time: 6.126s
[2K
| Adam | epoch: 029 | loss: 0.06493 - acc: 0.9794 -- iter: 320/404
[A[ATraining Step: 375  | total loss: [1m[32m0.05897[0m[0m | time: 6.761s
[2K
| Adam | epoch: 029 | loss: 0.05897 - acc: 0.9815 -- iter: 352/404
[A[ATraining Step: 376  | total loss: [1m[32m0.05389[0m[0m | time: 7.379s
[2K
| Adam | epoch: 029 | loss: 0.05389 - acc: 0.9833 -- iter: 384/404
[A[ATraining Step: 377  | total loss: [1m[32m0.04947[0m[0m | time: 8.775s
[2K
| Adam | epoch: 029 | loss: 0.04947 - acc: 0.9850 | val_loss: 1.02300 - val_acc: 0.7244 -- iter: 404/404
--
Training Step: 378  | total loss: [1m[32m0.04485[0m[0m | time: 0.397s
[2K
| Adam | epoch: 030 | loss: 0.04485 - acc: 0.9865 -- iter: 032/404
[A[ATraining Step: 379  | total loss: [1m[32m0.04070[0m[0m | time: 1.012s
[2K
| Adam | epoch: 030 | loss: 0.04070 - acc: 0.9879 -- iter: 064/404
[A[ATraining Step: 380  | total loss: [1m[32m0.03952[0m[0m | time: 1.634s
[2K
| Adam | epoch: 030 | loss: 0.03952 - acc: 0.9859 -- iter: 096/404
[A[ATraining Step: 381  | total loss: [1m[32m0.03592[0m[0m | time: 2.256s
[2K
| Adam | epoch: 030 | loss: 0.03592 - acc: 0.9874 -- iter: 128/404
[A[ATraining Step: 382  | total loss: [1m[32m0.03315[0m[0m | time: 2.861s
[2K
| Adam | epoch: 030 | loss: 0.03315 - acc: 0.9886 -- iter: 160/404
[A[ATraining Step: 383  | total loss: [1m[32m0.03162[0m[0m | time: 3.470s
[2K
| Adam | epoch: 030 | loss: 0.03162 - acc: 0.9898 -- iter: 192/404
[A[ATraining Step: 384  | total loss: [1m[32m0.02958[0m[0m | time: 4.122s
[2K
| Adam | epoch: 030 | loss: 0.02958 - acc: 0.9908 -- iter: 224/404
[A[ATraining Step: 385  | total loss: [1m[32m0.03760[0m[0m | time: 4.742s
[2K
| Adam | epoch: 030 | loss: 0.03760 - acc: 0.9886 -- iter: 256/404
[A[ATraining Step: 386  | total loss: [1m[32m0.04779[0m[0m | time: 5.369s
[2K
| Adam | epoch: 030 | loss: 0.04779 - acc: 0.9866 -- iter: 288/404
[A[ATraining Step: 387  | total loss: [1m[32m0.04529[0m[0m | time: 6.001s
[2K
| Adam | epoch: 030 | loss: 0.04529 - acc: 0.9879 -- iter: 320/404
[A[ATraining Step: 388  | total loss: [1m[32m0.06882[0m[0m | time: 6.629s
[2K
| Adam | epoch: 030 | loss: 0.06882 - acc: 0.9860 -- iter: 352/404
[A[ATraining Step: 389  | total loss: [1m[32m0.06720[0m[0m | time: 7.272s
[2K
| Adam | epoch: 030 | loss: 0.06720 - acc: 0.9843 -- iter: 384/404
[A[ATraining Step: 390  | total loss: [1m[32m0.06119[0m[0m | time: 8.884s
[2K
| Adam | epoch: 030 | loss: 0.06119 - acc: 0.9859 | val_loss: 0.99455 - val_acc: 0.7244 -- iter: 404/404
--
Validation AUC:0.7864102564102565
Validation AUPRC:0.8284846187864359
Test AUC:0.8179500254971952
Test AUPRC:0.8409970132742411
BestTestF1Score	0.83	0.55	0.78	0.75	0.93	69	23	30	5	0.02
BestTestMCCScore	0.76	0.51	0.75	0.84	0.7	52	10	43	22	0.79
BestTestAccuracyScore	0.76	0.51	0.75	0.84	0.7	52	10	43	22	0.79
BestValidationF1Score	0.8	0.44	0.73	0.72	0.89	67	26	26	8	0.02
BestValidationMCC	0.78	0.52	0.76	0.84	0.72	54	10	42	21	0.79
BestValidationAccuracy	0.78	0.52	0.76	0.84	0.72	54	10	42	21	0.79
TestPredictions (Threshold:0.79)
CHEMBL3356938,TN,INACT,0.019999999552965164	CHEMBL3633769,FN,ACT,0.3700000047683716	CHEMBL3770180,TN,INACT,0.0	CHEMBL3235463,FN,ACT,0.05000000074505806	CHEMBL3593414,TP,ACT,1.0	CHEMBL2407723,TP,ACT,1.0	CHEMBL555906,FN,ACT,0.12999999523162842	CHEMBL3605494,TP,ACT,0.9900000095367432	CHEMBL3655913,TP,ACT,0.9900000095367432	CHEMBL3655949,TP,ACT,0.9900000095367432	CHEMBL2408778,FN,ACT,0.029999999329447746	CHEMBL3655991,TP,ACT,0.9900000095367432	CHEMBL1767043,TN,INACT,0.0	CHEMBL3670664,TP,ACT,1.0	CHEMBL3758281,FN,ACT,0.09000000357627869	CHEMBL213071,FP,INACT,1.0	CHEMBL1171228,TN,INACT,0.009999999776482582	CHEMBL2011675,TN,INACT,0.28999999165534973	CHEMBL2177582,TP,ACT,1.0	CHEMBL2381520,TN,INACT,0.0	CHEMBL1767036,TN,INACT,0.0	CHEMBL251011,TP,ACT,0.9399999976158142	CHEMBL3317818,TP,ACT,0.7900000214576721	CHEMBL2147944,TN,INACT,0.0	CHEMBL512179,TN,INACT,0.009999999776482582	CHEMBL1668018,TN,INACT,0.0	CHEMBL1934895,TN,INACT,0.009999999776482582	CHEMBL2323287,TN,INACT,0.07999999821186066	CHEMBL3827894,TP,ACT,1.0	CHEMBL2364628,FN,ACT,0.7400000095367432	CHEMBL491926,FP,INACT,0.9900000095367432	CHEMBL3698506,TN,INACT,0.4300000071525574	CHEMBL3585970,FN,ACT,0.4399999976158142	CHEMBL3655981,TP,ACT,0.9700000286102295	CHEMBL2413289,FP,INACT,1.0	CHEMBL1771485,TP,ACT,0.9599999785423279	CHEMBL2414098,FN,ACT,0.3199999928474426	CHEMBL468527,FN,ACT,0.05999999865889549	CHEMBL2408243,FN,ACT,0.75	CHEMBL3126833,TN,INACT,0.0	CHEMBL3770566,TN,INACT,0.05999999865889549	CHEMBL2170168,TN,INACT,0.0	CHEMBL1934891,TN,INACT,0.0	CHEMBL3673101,TP,ACT,1.0	CHEMBL3675758,FN,ACT,0.0	CHEMBL2417785,TN,INACT,0.009999999776482582	CHEMBL3655962,FN,ACT,0.7099999785423279	CHEMBL468842,TP,ACT,0.8999999761581421	CHEMBL1938433,TP,ACT,1.0	CHEMBL2407733,TP,ACT,1.0	CHEMBL1213457,FN,ACT,0.07000000029802322	CHEMBL1630110,TN,INACT,0.7799999713897705	CHEMBL483693,TP,ACT,0.9900000095367432	CHEMBL1096072,FP,INACT,0.949999988079071	CHEMBL2170173,TN,INACT,0.0	CHEMBL3656010,TP,ACT,1.0	CHEMBL2057826,TN,INACT,0.25999999046325684	CHEMBL1767039,TN,INACT,0.0	CHEMBL442661,TN,INACT,0.09000000357627869	CHEMBL256440,FN,ACT,0.7200000286102295	CHEMBL427510,TN,INACT,0.009999999776482582	CHEMBL1934902,TN,INACT,0.0	CHEMBL2170167,TN,INACT,0.0	CHEMBL3656018,TP,ACT,1.0	CHEMBL1934892,TN,INACT,0.009999999776482582	CHEMBL3655926,TP,ACT,0.8999999761581421	CHEMBL3605504,FN,ACT,0.5199999809265137	CHEMBL3770119,TN,INACT,0.12999999523162842	CHEMBL3670670,TP,ACT,1.0	CHEMBL3593408,TP,ACT,1.0	CHEMBL3810397,TN,INACT,0.4699999988079071	CHEMBL3347516,FN,ACT,0.03999999910593033	CHEMBL140525,TP,ACT,1.0	CHEMBL2047608,TP,ACT,1.0	CHEMBL2046609,FP,INACT,1.0	CHEMBL487253,FN,ACT,0.05000000074505806	CHEMBL3260858,FP,INACT,0.9200000166893005	CHEMBL3421743,TP,ACT,1.0	CHEMBL515432,FP,INACT,1.0	CHEMBL186311,TN,INACT,0.05000000074505806	CHEMBL408513,TP,ACT,0.8199999928474426	CHEMBL472345,TN,INACT,0.019999999552965164	CHEMBL3621296,TP,ACT,1.0	CHEMBL3394285,TP,ACT,1.0	CHEMBL2022827,TP,ACT,1.0	CHEMBL3235787,TN,INACT,0.3199999928474426	CHEMBL3656006,TP,ACT,0.9900000095367432	CHEMBL3655912,TP,ACT,0.9900000095367432	CHEMBL469134,FN,ACT,0.009999999776482582	CHEMBL389688,TP,ACT,1.0	CHEMBL1934906,FP,INACT,1.0	CHEMBL563287,TN,INACT,0.009999999776482582	CHEMBL1800241,TN,INACT,0.0	CHEMBL3769491,FP,INACT,0.8500000238418579	CHEMBL3652239,TP,ACT,0.9700000286102295	CHEMBL3655932,TP,ACT,1.0	CHEMBL3655984,TP,ACT,0.9800000190734863	CHEMBL2022826,TP,ACT,0.949999988079071	CHEMBL1099078,FN,ACT,0.0	CHEMBL3670665,TP,ACT,0.9800000190734863	CHEMBL2046611,TN,INACT,0.009999999776482582	CHEMBL3110023,TN,INACT,0.019999999552965164	CHEMBL253868,FN,ACT,0.0	CHEMBL3421744,TP,ACT,0.9900000095367432	CHEMBL1165293,TN,INACT,0.0	CHEMBL2425965,TP,ACT,1.0	CHEMBL1097744,TN,INACT,0.0	CHEMBL557066,TP,ACT,0.9900000095367432	CHEMBL260080,TN,INACT,0.46000000834465027	CHEMBL272401,TP,ACT,1.0	CHEMBL2425959,TP,ACT,0.9800000190734863	CHEMBL2407731,TP,ACT,1.0	CHEMBL2170175,FP,INACT,1.0	CHEMBL3670674,TP,ACT,1.0	CHEMBL3098697,TP,ACT,0.9900000095367432	CHEMBL3527525,TN,INACT,0.0	CHEMBL3354920,TP,ACT,0.9900000095367432	CHEMBL3098695,FN,ACT,0.029999999329447746	CHEMBL248798,TP,ACT,0.9900000095367432	CHEMBL1808672,TN,INACT,0.09000000357627869	CHEMBL471042,FN,ACT,0.0	CHEMBL487943,TP,ACT,1.0	CHEMBL2333343,TP,ACT,0.9599999785423279	CHEMBL1164243,TN,INACT,0.0	CHEMBL3426804,TP,ACT,0.7900000214576721	CHEMBL2407721,TP,ACT,1.0	CHEMBL3735770,TN,INACT,0.009999999776482582	

