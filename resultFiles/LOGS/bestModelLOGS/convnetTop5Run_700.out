CNNModel CHEMBL4123 adam 0.001 30 256 0 0.8 False True
Number of active compounds :	171
Number of inactive compounds :	171
---------------------------------
Run id: CNNModel_CHEMBL4123_adam_0.001_30_256_0_0.8_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL4123_adam_0.001_30_256_0.8_True/
---------------------------------
Training samples: 216
Validation samples: 68
--
Training Step: 1  | time: 0.784s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/216
[A[ATraining Step: 2  | total loss: [1m[32m0.62390[0m[0m | time: 1.403s
[2K
| Adam | epoch: 001 | loss: 0.62390 - acc: 0.4781 -- iter: 064/216
[A[ATraining Step: 3  | total loss: [1m[32m0.68353[0m[0m | time: 1.999s
[2K
| Adam | epoch: 001 | loss: 0.68353 - acc: 0.3682 -- iter: 096/216
[A[ATraining Step: 4  | total loss: [1m[32m0.69072[0m[0m | time: 2.591s
[2K
| Adam | epoch: 001 | loss: 0.69072 - acc: 0.4905 -- iter: 128/216
[A[ATraining Step: 5  | total loss: [1m[32m0.69211[0m[0m | time: 3.211s
[2K
| Adam | epoch: 001 | loss: 0.69211 - acc: 0.5620 -- iter: 160/216
[A[ATraining Step: 6  | total loss: [1m[32m0.69199[0m[0m | time: 3.820s
[2K
| Adam | epoch: 001 | loss: 0.69199 - acc: 0.5623 -- iter: 192/216
[A[ATraining Step: 7  | total loss: [1m[32m0.69312[0m[0m | time: 5.310s
[2K
| Adam | epoch: 001 | loss: 0.69312 - acc: 0.5062 | val_loss: 0.70322 - val_acc: 0.4265 -- iter: 216/216
--
Training Step: 8  | total loss: [1m[32m0.68896[0m[0m | time: 0.478s
[2K
| Adam | epoch: 002 | loss: 0.68896 - acc: 0.5730 -- iter: 032/216
[A[ATraining Step: 9  | total loss: [1m[32m0.68443[0m[0m | time: 1.089s
[2K
| Adam | epoch: 002 | loss: 0.68443 - acc: 0.6005 -- iter: 064/216
[A[ATraining Step: 10  | total loss: [1m[32m0.69278[0m[0m | time: 1.686s
[2K
| Adam | epoch: 002 | loss: 0.69278 - acc: 0.5503 -- iter: 096/216
[A[ATraining Step: 11  | total loss: [1m[32m0.69306[0m[0m | time: 2.290s
[2K
| Adam | epoch: 002 | loss: 0.69306 - acc: 0.5413 -- iter: 128/216
[A[ATraining Step: 12  | total loss: [1m[32m0.70004[0m[0m | time: 2.896s
[2K
| Adam | epoch: 002 | loss: 0.70004 - acc: 0.5086 -- iter: 160/216
[A[ATraining Step: 13  | total loss: [1m[32m0.70273[0m[0m | time: 3.518s
[2K
| Adam | epoch: 002 | loss: 0.70273 - acc: 0.4781 -- iter: 192/216
[A[ATraining Step: 14  | total loss: [1m[32m0.70001[0m[0m | time: 5.121s
[2K
| Adam | epoch: 002 | loss: 0.70001 - acc: 0.4743 | val_loss: 0.69512 - val_acc: 0.4265 -- iter: 216/216
--
Training Step: 15  | total loss: [1m[32m0.69682[0m[0m | time: 0.475s
[2K
| Adam | epoch: 003 | loss: 0.69682 - acc: 0.4966 -- iter: 032/216
[A[ATraining Step: 16  | total loss: [1m[32m0.69446[0m[0m | time: 0.932s
[2K
| Adam | epoch: 003 | loss: 0.69446 - acc: 0.5291 -- iter: 064/216
[A[ATraining Step: 17  | total loss: [1m[32m0.69328[0m[0m | time: 1.542s
[2K
| Adam | epoch: 003 | loss: 0.69328 - acc: 0.5486 -- iter: 096/216
[A[ATraining Step: 18  | total loss: [1m[32m0.69306[0m[0m | time: 2.141s
[2K
| Adam | epoch: 003 | loss: 0.69306 - acc: 0.5426 -- iter: 128/216
[A[ATraining Step: 19  | total loss: [1m[32m0.69229[0m[0m | time: 2.743s
[2K
| Adam | epoch: 003 | loss: 0.69229 - acc: 0.5701 -- iter: 160/216
[A[ATraining Step: 20  | total loss: [1m[32m0.69390[0m[0m | time: 3.340s
[2K
| Adam | epoch: 003 | loss: 0.69390 - acc: 0.4873 -- iter: 192/216
[A[ATraining Step: 21  | total loss: [1m[32m0.69346[0m[0m | time: 4.938s
[2K
| Adam | epoch: 003 | loss: 0.69346 - acc: 0.5009 | val_loss: 0.69435 - val_acc: 0.4265 -- iter: 216/216
--
Training Step: 22  | total loss: [1m[32m0.69373[0m[0m | time: 0.618s
[2K
| Adam | epoch: 004 | loss: 0.69373 - acc: 0.4819 -- iter: 032/216
[A[ATraining Step: 23  | total loss: [1m[32m0.69341[0m[0m | time: 1.087s
[2K
| Adam | epoch: 004 | loss: 0.69341 - acc: 0.4962 -- iter: 064/216
[A[ATraining Step: 24  | total loss: [1m[32m0.69356[0m[0m | time: 1.547s
[2K
| Adam | epoch: 004 | loss: 0.69356 - acc: 0.4856 -- iter: 096/216
[A[ATraining Step: 25  | total loss: [1m[32m0.69359[0m[0m | time: 2.156s
[2K
| Adam | epoch: 004 | loss: 0.69359 - acc: 0.4781 -- iter: 128/216
[A[ATraining Step: 26  | total loss: [1m[32m0.69321[0m[0m | time: 2.759s
[2K
| Adam | epoch: 004 | loss: 0.69321 - acc: 0.5087 -- iter: 160/216
[A[ATraining Step: 27  | total loss: [1m[32m0.69327[0m[0m | time: 3.372s
[2K
| Adam | epoch: 004 | loss: 0.69327 - acc: 0.4985 -- iter: 192/216
[A[ATraining Step: 28  | total loss: [1m[32m0.69304[0m[0m | time: 5.003s
[2K
| Adam | epoch: 004 | loss: 0.69304 - acc: 0.5223 | val_loss: 0.69384 - val_acc: 0.4265 -- iter: 216/216
--
Training Step: 29  | total loss: [1m[32m0.69332[0m[0m | time: 0.610s
[2K
| Adam | epoch: 005 | loss: 0.69332 - acc: 0.4941 -- iter: 032/216
[A[ATraining Step: 30  | total loss: [1m[32m0.69321[0m[0m | time: 1.204s
[2K
| Adam | epoch: 005 | loss: 0.69321 - acc: 0.5029 -- iter: 064/216
[A[ATraining Step: 31  | total loss: [1m[32m0.69322[0m[0m | time: 1.667s
[2K
| Adam | epoch: 005 | loss: 0.69322 - acc: 0.4950 -- iter: 096/216
[A[ATraining Step: 32  | total loss: [1m[32m0.69319[0m[0m | time: 2.154s
[2K
| Adam | epoch: 005 | loss: 0.69319 - acc: 0.4961 -- iter: 128/216
[A[ATraining Step: 33  | total loss: [1m[32m0.69317[0m[0m | time: 2.783s
[2K
| Adam | epoch: 005 | loss: 0.69317 - acc: 0.4970 -- iter: 160/216
[A[ATraining Step: 34  | total loss: [1m[32m0.69292[0m[0m | time: 3.402s
[2K
| Adam | epoch: 005 | loss: 0.69292 - acc: 0.5177 -- iter: 192/216
[A[ATraining Step: 35  | total loss: [1m[32m0.69272[0m[0m | time: 5.008s
[2K
| Adam | epoch: 005 | loss: 0.69272 - acc: 0.5336 | val_loss: 0.69420 - val_acc: 0.4265 -- iter: 216/216
--
Training Step: 36  | total loss: [1m[32m0.69275[0m[0m | time: 0.617s
[2K
| Adam | epoch: 006 | loss: 0.69275 - acc: 0.5267 -- iter: 032/216
[A[ATraining Step: 37  | total loss: [1m[32m0.69268[0m[0m | time: 1.214s
[2K
| Adam | epoch: 006 | loss: 0.69268 - acc: 0.5276 -- iter: 064/216
[A[ATraining Step: 38  | total loss: [1m[32m0.69282[0m[0m | time: 1.813s
[2K
| Adam | epoch: 006 | loss: 0.69282 - acc: 0.5161 -- iter: 096/216
[A[ATraining Step: 39  | total loss: [1m[32m0.69350[0m[0m | time: 2.262s
[2K
| Adam | epoch: 006 | loss: 0.69350 - acc: 0.4831 -- iter: 128/216
[A[ATraining Step: 40  | total loss: [1m[32m0.69336[0m[0m | time: 2.716s
[2K
| Adam | epoch: 006 | loss: 0.69336 - acc: 0.4863 -- iter: 160/216
[A[ATraining Step: 41  | total loss: [1m[32m0.69327[0m[0m | time: 3.309s
[2K
| Adam | epoch: 006 | loss: 0.69327 - acc: 0.4888 -- iter: 192/216
[A[ATraining Step: 42  | total loss: [1m[32m0.69299[0m[0m | time: 4.914s
[2K
| Adam | epoch: 006 | loss: 0.69299 - acc: 0.5021 | val_loss: 0.69319 - val_acc: 0.4265 -- iter: 216/216
--
Training Step: 43  | total loss: [1m[32m0.69351[0m[0m | time: 0.623s
[2K
| Adam | epoch: 007 | loss: 0.69351 - acc: 0.4631 -- iter: 032/216
[A[ATraining Step: 44  | total loss: [1m[32m0.69341[0m[0m | time: 1.230s
[2K
| Adam | epoch: 007 | loss: 0.69341 - acc: 0.4587 -- iter: 064/216
[A[ATraining Step: 45  | total loss: [1m[32m0.69314[0m[0m | time: 1.846s
[2K
| Adam | epoch: 007 | loss: 0.69314 - acc: 0.4816 -- iter: 096/216
[A[ATraining Step: 46  | total loss: [1m[32m0.69290[0m[0m | time: 2.473s
[2K
| Adam | epoch: 007 | loss: 0.69290 - acc: 0.5107 -- iter: 128/216
[A[ATraining Step: 47  | total loss: [1m[32m0.69266[0m[0m | time: 2.930s
[2K
| Adam | epoch: 007 | loss: 0.69266 - acc: 0.5499 -- iter: 160/216
[A[ATraining Step: 48  | total loss: [1m[32m0.69276[0m[0m | time: 3.411s
[2K
| Adam | epoch: 007 | loss: 0.69276 - acc: 0.5285 -- iter: 192/216
[A[ATraining Step: 49  | total loss: [1m[32m0.69276[0m[0m | time: 5.018s
[2K
| Adam | epoch: 007 | loss: 0.69276 - acc: 0.4977 | val_loss: 0.69066 - val_acc: 0.7941 -- iter: 216/216
--
Training Step: 50  | total loss: [1m[32m0.69234[0m[0m | time: 0.599s
[2K
| Adam | epoch: 008 | loss: 0.69234 - acc: 0.5465 -- iter: 032/216
[A[ATraining Step: 51  | total loss: [1m[32m0.69195[0m[0m | time: 1.201s
[2K
| Adam | epoch: 008 | loss: 0.69195 - acc: 0.5966 -- iter: 064/216
[A[ATraining Step: 52  | total loss: [1m[32m0.69174[0m[0m | time: 1.798s
[2K
| Adam | epoch: 008 | loss: 0.69174 - acc: 0.6243 -- iter: 096/216
[A[ATraining Step: 53  | total loss: [1m[32m0.69143[0m[0m | time: 2.407s
[2K
| Adam | epoch: 008 | loss: 0.69143 - acc: 0.6336 -- iter: 128/216
[A[ATraining Step: 54  | total loss: [1m[32m0.69115[0m[0m | time: 3.014s
[2K
| Adam | epoch: 008 | loss: 0.69115 - acc: 0.6415 -- iter: 160/216
[A[ATraining Step: 55  | total loss: [1m[32m0.69071[0m[0m | time: 3.497s
[2K
| Adam | epoch: 008 | loss: 0.69071 - acc: 0.6480 -- iter: 192/216
[A[ATraining Step: 56  | total loss: [1m[32m0.69133[0m[0m | time: 4.950s
[2K
| Adam | epoch: 008 | loss: 0.69133 - acc: 0.6096 | val_loss: 0.68542 - val_acc: 0.8088 -- iter: 216/216
--
Training Step: 57  | total loss: [1m[32m0.69158[0m[0m | time: 0.639s
[2K
| Adam | epoch: 009 | loss: 0.69158 - acc: 0.5829 -- iter: 032/216
[A[ATraining Step: 58  | total loss: [1m[32m0.69080[0m[0m | time: 1.249s
[2K
| Adam | epoch: 009 | loss: 0.69080 - acc: 0.6057 -- iter: 064/216
[A[ATraining Step: 59  | total loss: [1m[32m0.68978[0m[0m | time: 1.861s
[2K
| Adam | epoch: 009 | loss: 0.68978 - acc: 0.6293 -- iter: 096/216
[A[ATraining Step: 60  | total loss: [1m[32m0.68834[0m[0m | time: 2.483s
[2K
| Adam | epoch: 009 | loss: 0.68834 - acc: 0.6659 -- iter: 128/216
[A[ATraining Step: 61  | total loss: [1m[32m0.68728[0m[0m | time: 3.088s
[2K
| Adam | epoch: 009 | loss: 0.68728 - acc: 0.6769 -- iter: 160/216
[A[ATraining Step: 62  | total loss: [1m[32m0.68580[0m[0m | time: 3.688s
[2K
| Adam | epoch: 009 | loss: 0.68580 - acc: 0.6903 -- iter: 192/216
[A[ATraining Step: 63  | total loss: [1m[32m0.68325[0m[0m | time: 5.146s
[2K
| Adam | epoch: 009 | loss: 0.68325 - acc: 0.7058 | val_loss: 0.69263 - val_acc: 0.4265 -- iter: 216/216
--
Training Step: 64  | total loss: [1m[32m0.68187[0m[0m | time: 0.468s
[2K
| Adam | epoch: 010 | loss: 0.68187 - acc: 0.6853 -- iter: 032/216
[A[ATraining Step: 65  | total loss: [1m[32m0.67944[0m[0m | time: 1.082s
[2K
| Adam | epoch: 010 | loss: 0.67944 - acc: 0.6676 -- iter: 064/216
[A[ATraining Step: 66  | total loss: [1m[32m0.68570[0m[0m | time: 1.683s
[2K
| Adam | epoch: 010 | loss: 0.68570 - acc: 0.6282 -- iter: 096/216
[A[ATraining Step: 67  | total loss: [1m[32m0.68051[0m[0m | time: 2.307s
[2K
| Adam | epoch: 010 | loss: 0.68051 - acc: 0.6166 -- iter: 128/216
[A[ATraining Step: 68  | total loss: [1m[32m0.67522[0m[0m | time: 2.908s
[2K
| Adam | epoch: 010 | loss: 0.67522 - acc: 0.6361 -- iter: 160/216
[A[ATraining Step: 69  | total loss: [1m[32m0.67190[0m[0m | time: 3.524s
[2K
| Adam | epoch: 010 | loss: 0.67190 - acc: 0.6457 -- iter: 192/216
[A[ATraining Step: 70  | total loss: [1m[32m0.66456[0m[0m | time: 5.128s
[2K
| Adam | epoch: 010 | loss: 0.66456 - acc: 0.6614 | val_loss: 0.62667 - val_acc: 0.5294 -- iter: 216/216
--
Training Step: 71  | total loss: [1m[32m0.65417[0m[0m | time: 0.464s
[2K
| Adam | epoch: 011 | loss: 0.65417 - acc: 0.6857 -- iter: 032/216
[A[ATraining Step: 72  | total loss: [1m[32m0.64688[0m[0m | time: 0.916s
[2K
| Adam | epoch: 011 | loss: 0.64688 - acc: 0.6789 -- iter: 064/216
[A[ATraining Step: 73  | total loss: [1m[32m0.63418[0m[0m | time: 1.528s
[2K
| Adam | epoch: 011 | loss: 0.63418 - acc: 0.7053 -- iter: 096/216
[A[ATraining Step: 74  | total loss: [1m[32m0.61959[0m[0m | time: 2.128s
[2K
| Adam | epoch: 011 | loss: 0.61959 - acc: 0.7308 -- iter: 128/216
[A[ATraining Step: 75  | total loss: [1m[32m0.60166[0m[0m | time: 2.732s
[2K
| Adam | epoch: 011 | loss: 0.60166 - acc: 0.7363 -- iter: 160/216
[A[ATraining Step: 76  | total loss: [1m[32m0.58878[0m[0m | time: 3.341s
[2K
| Adam | epoch: 011 | loss: 0.58878 - acc: 0.7277 -- iter: 192/216
[A[ATraining Step: 77  | total loss: [1m[32m0.60361[0m[0m | time: 4.969s
[2K
| Adam | epoch: 011 | loss: 0.60361 - acc: 0.7334 | val_loss: 0.65553 - val_acc: 0.6471 -- iter: 216/216
--
Training Step: 78  | total loss: [1m[32m0.60241[0m[0m | time: 0.609s
[2K
| Adam | epoch: 012 | loss: 0.60241 - acc: 0.7318 -- iter: 032/216
[A[ATraining Step: 79  | total loss: [1m[32m0.61142[0m[0m | time: 1.087s
[2K
| Adam | epoch: 012 | loss: 0.61142 - acc: 0.7175 -- iter: 064/216
[A[ATraining Step: 80  | total loss: [1m[32m0.61784[0m[0m | time: 1.545s
[2K
| Adam | epoch: 012 | loss: 0.61784 - acc: 0.7038 -- iter: 096/216
[A[ATraining Step: 81  | total loss: [1m[32m0.60200[0m[0m | time: 2.148s
[2K
| Adam | epoch: 012 | loss: 0.60200 - acc: 0.7085 -- iter: 128/216
[A[ATraining Step: 82  | total loss: [1m[32m0.64652[0m[0m | time: 2.764s
[2K
| Adam | epoch: 012 | loss: 0.64652 - acc: 0.6814 -- iter: 160/216
[A[ATraining Step: 83  | total loss: [1m[32m0.63433[0m[0m | time: 3.359s
[2K
| Adam | epoch: 012 | loss: 0.63433 - acc: 0.6789 -- iter: 192/216
[A[ATraining Step: 84  | total loss: [1m[32m0.61875[0m[0m | time: 4.974s
[2K
| Adam | epoch: 012 | loss: 0.61875 - acc: 0.6891 | val_loss: 0.55518 - val_acc: 0.8088 -- iter: 216/216
--
Training Step: 85  | total loss: [1m[32m0.60893[0m[0m | time: 0.615s
[2K
| Adam | epoch: 013 | loss: 0.60893 - acc: 0.7015 -- iter: 032/216
[A[ATraining Step: 86  | total loss: [1m[32m0.60321[0m[0m | time: 1.215s
[2K
| Adam | epoch: 013 | loss: 0.60321 - acc: 0.7126 -- iter: 064/216
[A[ATraining Step: 87  | total loss: [1m[32m0.60703[0m[0m | time: 1.676s
[2K
| Adam | epoch: 013 | loss: 0.60703 - acc: 0.7038 -- iter: 096/216
[A[ATraining Step: 88  | total loss: [1m[32m0.60666[0m[0m | time: 2.131s
[2K
| Adam | epoch: 013 | loss: 0.60666 - acc: 0.7043 -- iter: 128/216
[A[ATraining Step: 89  | total loss: [1m[32m0.60505[0m[0m | time: 2.739s
[2K
| Adam | epoch: 013 | loss: 0.60505 - acc: 0.7088 -- iter: 160/216
[A[ATraining Step: 90  | total loss: [1m[32m0.59745[0m[0m | time: 3.344s
[2K
| Adam | epoch: 013 | loss: 0.59745 - acc: 0.7286 -- iter: 192/216
[A[ATraining Step: 91  | total loss: [1m[32m0.58445[0m[0m | time: 4.959s
[2K
| Adam | epoch: 013 | loss: 0.58445 - acc: 0.7370 | val_loss: 0.59435 - val_acc: 0.5294 -- iter: 216/216
--
Training Step: 92  | total loss: [1m[32m0.57555[0m[0m | time: 0.632s
[2K
| Adam | epoch: 014 | loss: 0.57555 - acc: 0.7351 -- iter: 032/216
[A[ATraining Step: 93  | total loss: [1m[32m0.58429[0m[0m | time: 1.223s
[2K
| Adam | epoch: 014 | loss: 0.58429 - acc: 0.7085 -- iter: 064/216
[A[ATraining Step: 94  | total loss: [1m[32m0.57751[0m[0m | time: 1.822s
[2K
| Adam | epoch: 014 | loss: 0.57751 - acc: 0.7127 -- iter: 096/216
[A[ATraining Step: 95  | total loss: [1m[32m0.55566[0m[0m | time: 2.296s
[2K
| Adam | epoch: 014 | loss: 0.55566 - acc: 0.7320 -- iter: 128/216
[A[ATraining Step: 96  | total loss: [1m[32m0.53426[0m[0m | time: 2.766s
[2K
| Adam | epoch: 014 | loss: 0.53426 - acc: 0.7546 -- iter: 160/216
[A[ATraining Step: 97  | total loss: [1m[32m0.51017[0m[0m | time: 3.391s
[2K
| Adam | epoch: 014 | loss: 0.51017 - acc: 0.7750 -- iter: 192/216
[A[ATraining Step: 98  | total loss: [1m[32m0.51413[0m[0m | time: 4.998s
[2K
| Adam | epoch: 014 | loss: 0.51413 - acc: 0.7694 | val_loss: 0.40516 - val_acc: 0.8382 -- iter: 216/216
--
Training Step: 99  | total loss: [1m[32m0.52414[0m[0m | time: 0.623s
[2K
| Adam | epoch: 015 | loss: 0.52414 - acc: 0.7674 -- iter: 032/216
[A[ATraining Step: 100  | total loss: [1m[32m0.50313[0m[0m | time: 1.233s
[2K
| Adam | epoch: 015 | loss: 0.50313 - acc: 0.7782 -- iter: 064/216
[A[ATraining Step: 101  | total loss: [1m[32m0.48358[0m[0m | time: 1.830s
[2K
| Adam | epoch: 015 | loss: 0.48358 - acc: 0.7910 -- iter: 096/216
[A[ATraining Step: 102  | total loss: [1m[32m0.46937[0m[0m | time: 2.434s
[2K
| Adam | epoch: 015 | loss: 0.46937 - acc: 0.8057 -- iter: 128/216
[A[ATraining Step: 103  | total loss: [1m[32m0.44790[0m[0m | time: 2.899s
[2K
| Adam | epoch: 015 | loss: 0.44790 - acc: 0.8095 -- iter: 160/216
[A[ATraining Step: 104  | total loss: [1m[32m0.46385[0m[0m | time: 3.351s
[2K
| Adam | epoch: 015 | loss: 0.46385 - acc: 0.8035 -- iter: 192/216
[A[ATraining Step: 105  | total loss: [1m[32m0.45333[0m[0m | time: 4.949s
[2K
| Adam | epoch: 015 | loss: 0.45333 - acc: 0.8107 | val_loss: 0.56916 - val_acc: 0.7059 -- iter: 216/216
--
Training Step: 106  | total loss: [1m[32m0.45538[0m[0m | time: 0.674s
[2K
| Adam | epoch: 016 | loss: 0.45538 - acc: 0.8015 -- iter: 032/216
[A[ATraining Step: 107  | total loss: [1m[32m0.48078[0m[0m | time: 1.276s
[2K
| Adam | epoch: 016 | loss: 0.48078 - acc: 0.7901 -- iter: 064/216
[A[ATraining Step: 108  | total loss: [1m[32m0.44914[0m[0m | time: 1.907s
[2K
| Adam | epoch: 016 | loss: 0.44914 - acc: 0.8079 -- iter: 096/216
[A[ATraining Step: 109  | total loss: [1m[32m0.42253[0m[0m | time: 2.510s
[2K
| Adam | epoch: 016 | loss: 0.42253 - acc: 0.8178 -- iter: 128/216
[A[ATraining Step: 110  | total loss: [1m[32m0.43373[0m[0m | time: 3.137s
[2K
| Adam | epoch: 016 | loss: 0.43373 - acc: 0.8172 -- iter: 160/216
[A[ATraining Step: 111  | total loss: [1m[32m0.41539[0m[0m | time: 3.594s
[2K
| Adam | epoch: 016 | loss: 0.41539 - acc: 0.8293 -- iter: 192/216
[A[ATraining Step: 112  | total loss: [1m[32m0.38802[0m[0m | time: 5.081s
[2K
| Adam | epoch: 016 | loss: 0.38802 - acc: 0.8463 | val_loss: 0.35155 - val_acc: 0.8529 -- iter: 216/216
--
Training Step: 113  | total loss: [1m[32m0.36515[0m[0m | time: 0.603s
[2K
| Adam | epoch: 017 | loss: 0.36515 - acc: 0.8617 -- iter: 032/216
[A[ATraining Step: 114  | total loss: [1m[32m0.36244[0m[0m | time: 1.222s
[2K
| Adam | epoch: 017 | loss: 0.36244 - acc: 0.8630 -- iter: 064/216
[A[ATraining Step: 115  | total loss: [1m[32m0.34696[0m[0m | time: 1.837s
[2K
| Adam | epoch: 017 | loss: 0.34696 - acc: 0.8705 -- iter: 096/216
[A[ATraining Step: 116  | total loss: [1m[32m0.33519[0m[0m | time: 2.431s
[2K
| Adam | epoch: 017 | loss: 0.33519 - acc: 0.8772 -- iter: 128/216
[A[ATraining Step: 117  | total loss: [1m[32m0.32881[0m[0m | time: 3.035s
[2K
| Adam | epoch: 017 | loss: 0.32881 - acc: 0.8801 -- iter: 160/216
[A[ATraining Step: 118  | total loss: [1m[32m0.34122[0m[0m | time: 3.636s
[2K
| Adam | epoch: 017 | loss: 0.34122 - acc: 0.8765 -- iter: 192/216
[A[ATraining Step: 119  | total loss: [1m[32m0.32449[0m[0m | time: 5.124s
[2K
| Adam | epoch: 017 | loss: 0.32449 - acc: 0.8826 | val_loss: 0.39390 - val_acc: 0.8235 -- iter: 216/216
--
Training Step: 120  | total loss: [1m[32m0.32006[0m[0m | time: 0.471s
[2K
| Adam | epoch: 018 | loss: 0.32006 - acc: 0.8818 -- iter: 032/216
[A[ATraining Step: 121  | total loss: [1m[32m0.30578[0m[0m | time: 1.077s
[2K
| Adam | epoch: 018 | loss: 0.30578 - acc: 0.8936 -- iter: 064/216
[A[ATraining Step: 122  | total loss: [1m[32m0.29374[0m[0m | time: 1.697s
[2K
| Adam | epoch: 018 | loss: 0.29374 - acc: 0.9011 -- iter: 096/216
[A[ATraining Step: 123  | total loss: [1m[32m0.27570[0m[0m | time: 2.298s
[2K
| Adam | epoch: 018 | loss: 0.27570 - acc: 0.9079 -- iter: 128/216
[A[ATraining Step: 124  | total loss: [1m[32m0.28756[0m[0m | time: 2.902s
[2K
| Adam | epoch: 018 | loss: 0.28756 - acc: 0.8984 -- iter: 160/216
[A[ATraining Step: 125  | total loss: [1m[32m0.29165[0m[0m | time: 3.503s
[2K
| Adam | epoch: 018 | loss: 0.29165 - acc: 0.8960 -- iter: 192/216
[A[ATraining Step: 126  | total loss: [1m[32m0.29108[0m[0m | time: 5.106s
[2K
| Adam | epoch: 018 | loss: 0.29108 - acc: 0.8939 | val_loss: 0.78408 - val_acc: 0.6765 -- iter: 216/216
--
Training Step: 127  | total loss: [1m[32m0.28423[0m[0m | time: 0.457s
[2K
| Adam | epoch: 019 | loss: 0.28423 - acc: 0.8952 -- iter: 032/216
[A[ATraining Step: 128  | total loss: [1m[32m0.30459[0m[0m | time: 0.925s
[2K
| Adam | epoch: 019 | loss: 0.30459 - acc: 0.8848 -- iter: 064/216
[A[ATraining Step: 129  | total loss: [1m[32m0.29581[0m[0m | time: 1.534s
[2K
| Adam | epoch: 019 | loss: 0.29581 - acc: 0.8838 -- iter: 096/216
[A[ATraining Step: 130  | total loss: [1m[32m0.27735[0m[0m | time: 2.137s
[2K
| Adam | epoch: 019 | loss: 0.27735 - acc: 0.8923 -- iter: 128/216
[A[ATraining Step: 131  | total loss: [1m[32m0.27351[0m[0m | time: 2.738s
[2K
| Adam | epoch: 019 | loss: 0.27351 - acc: 0.8937 -- iter: 160/216
[A[ATraining Step: 132  | total loss: [1m[32m0.31091[0m[0m | time: 3.358s
[2K
| Adam | epoch: 019 | loss: 0.31091 - acc: 0.8762 -- iter: 192/216
[A[ATraining Step: 133  | total loss: [1m[32m0.29199[0m[0m | time: 4.967s
[2K
| Adam | epoch: 019 | loss: 0.29199 - acc: 0.8823 | val_loss: 0.35445 - val_acc: 0.8382 -- iter: 216/216
--
Training Step: 134  | total loss: [1m[32m0.28236[0m[0m | time: 0.617s
[2K
| Adam | epoch: 020 | loss: 0.28236 - acc: 0.8879 -- iter: 032/216
[A[ATraining Step: 135  | total loss: [1m[32m0.28888[0m[0m | time: 1.067s
[2K
| Adam | epoch: 020 | loss: 0.28888 - acc: 0.8897 -- iter: 064/216
[A[ATraining Step: 136  | total loss: [1m[32m0.29923[0m[0m | time: 1.523s
[2K
| Adam | epoch: 020 | loss: 0.29923 - acc: 0.8882 -- iter: 096/216
[A[ATraining Step: 137  | total loss: [1m[32m0.29094[0m[0m | time: 2.177s
[2K
| Adam | epoch: 020 | loss: 0.29094 - acc: 0.8911 -- iter: 128/216
[A[ATraining Step: 138  | total loss: [1m[32m0.28577[0m[0m | time: 2.780s
[2K
| Adam | epoch: 020 | loss: 0.28577 - acc: 0.8957 -- iter: 160/216
[A[ATraining Step: 139  | total loss: [1m[32m0.27051[0m[0m | time: 3.392s
[2K
| Adam | epoch: 020 | loss: 0.27051 - acc: 0.9030 -- iter: 192/216
[A[ATraining Step: 140  | total loss: [1m[32m0.27253[0m[0m | time: 4.998s
[2K
| Adam | epoch: 020 | loss: 0.27253 - acc: 0.9033 | val_loss: 0.26802 - val_acc: 0.8971 -- iter: 216/216
--
Training Step: 141  | total loss: [1m[32m0.26356[0m[0m | time: 0.621s
[2K
| Adam | epoch: 021 | loss: 0.26356 - acc: 0.9068 -- iter: 032/216
[A[ATraining Step: 142  | total loss: [1m[32m0.24938[0m[0m | time: 1.268s
[2K
| Adam | epoch: 021 | loss: 0.24938 - acc: 0.9130 -- iter: 064/216
[A[ATraining Step: 143  | total loss: [1m[32m0.23292[0m[0m | time: 1.728s
[2K
| Adam | epoch: 021 | loss: 0.23292 - acc: 0.9217 -- iter: 096/216
[A[ATraining Step: 144  | total loss: [1m[32m0.24288[0m[0m | time: 2.200s
[2K
| Adam | epoch: 021 | loss: 0.24288 - acc: 0.9212 -- iter: 128/216
[A[ATraining Step: 145  | total loss: [1m[32m0.23730[0m[0m | time: 2.796s
[2K
| Adam | epoch: 021 | loss: 0.23730 - acc: 0.9249 -- iter: 160/216
[A[ATraining Step: 146  | total loss: [1m[32m0.23146[0m[0m | time: 3.403s
[2K
| Adam | epoch: 021 | loss: 0.23146 - acc: 0.9230 -- iter: 192/216
[A[ATraining Step: 147  | total loss: [1m[32m0.21307[0m[0m | time: 5.006s
[2K
| Adam | epoch: 021 | loss: 0.21307 - acc: 0.9307 | val_loss: 0.33844 - val_acc: 0.8529 -- iter: 216/216
--
Training Step: 148  | total loss: [1m[32m0.20094[0m[0m | time: 0.598s
[2K
| Adam | epoch: 022 | loss: 0.20094 - acc: 0.9345 -- iter: 032/216
[A[ATraining Step: 149  | total loss: [1m[32m0.18812[0m[0m | time: 1.203s
[2K
| Adam | epoch: 022 | loss: 0.18812 - acc: 0.9379 -- iter: 064/216
[A[ATraining Step: 150  | total loss: [1m[32m0.18027[0m[0m | time: 1.808s
[2K
| Adam | epoch: 022 | loss: 0.18027 - acc: 0.9379 -- iter: 096/216
[A[ATraining Step: 151  | total loss: [1m[32m0.17559[0m[0m | time: 2.265s
[2K
| Adam | epoch: 022 | loss: 0.17559 - acc: 0.9410 -- iter: 128/216
[A[ATraining Step: 152  | total loss: [1m[32m0.17571[0m[0m | time: 2.748s
[2K
| Adam | epoch: 022 | loss: 0.17571 - acc: 0.9427 -- iter: 160/216
[A[ATraining Step: 153  | total loss: [1m[32m0.16449[0m[0m | time: 3.347s
[2K
| Adam | epoch: 022 | loss: 0.16449 - acc: 0.9484 -- iter: 192/216
[A[ATraining Step: 154  | total loss: [1m[32m0.15507[0m[0m | time: 4.947s
[2K
| Adam | epoch: 022 | loss: 0.15507 - acc: 0.9505 | val_loss: 0.30045 - val_acc: 0.8971 -- iter: 216/216
--
Training Step: 155  | total loss: [1m[32m0.15412[0m[0m | time: 0.614s
[2K
| Adam | epoch: 023 | loss: 0.15412 - acc: 0.9492 -- iter: 032/216
[A[ATraining Step: 156  | total loss: [1m[32m0.13975[0m[0m | time: 1.214s
[2K
| Adam | epoch: 023 | loss: 0.13975 - acc: 0.9543 -- iter: 064/216
[A[ATraining Step: 157  | total loss: [1m[32m0.13051[0m[0m | time: 1.821s
[2K
| Adam | epoch: 023 | loss: 0.13051 - acc: 0.9557 -- iter: 096/216
[A[ATraining Step: 158  | total loss: [1m[32m0.13446[0m[0m | time: 2.453s
[2K
| Adam | epoch: 023 | loss: 0.13446 - acc: 0.9508 -- iter: 128/216
[A[ATraining Step: 159  | total loss: [1m[32m0.13819[0m[0m | time: 2.916s
[2K
| Adam | epoch: 023 | loss: 0.13819 - acc: 0.9463 -- iter: 160/216
[A[ATraining Step: 160  | total loss: [1m[32m0.15356[0m[0m | time: 3.373s
[2K
| Adam | epoch: 023 | loss: 0.15356 - acc: 0.9475 -- iter: 192/216
[A[ATraining Step: 161  | total loss: [1m[32m0.14343[0m[0m | time: 4.979s
[2K
| Adam | epoch: 023 | loss: 0.14343 - acc: 0.9486 | val_loss: 0.46884 - val_acc: 0.8382 -- iter: 216/216
--
Training Step: 162  | total loss: [1m[32m0.13310[0m[0m | time: 0.649s
[2K
| Adam | epoch: 024 | loss: 0.13310 - acc: 0.9506 -- iter: 032/216
[A[ATraining Step: 163  | total loss: [1m[32m0.13692[0m[0m | time: 1.253s
[2K
| Adam | epoch: 024 | loss: 0.13692 - acc: 0.9524 -- iter: 064/216
[A[ATraining Step: 164  | total loss: [1m[32m0.12875[0m[0m | time: 1.869s
[2K
| Adam | epoch: 024 | loss: 0.12875 - acc: 0.9572 -- iter: 096/216
[A[ATraining Step: 165  | total loss: [1m[32m0.11847[0m[0m | time: 2.468s
[2K
| Adam | epoch: 024 | loss: 0.11847 - acc: 0.9615 -- iter: 128/216
[A[ATraining Step: 166  | total loss: [1m[32m0.11027[0m[0m | time: 3.068s
[2K
| Adam | epoch: 024 | loss: 0.11027 - acc: 0.9653 -- iter: 160/216
[A[ATraining Step: 167  | total loss: [1m[32m0.10190[0m[0m | time: 3.549s
[2K
| Adam | epoch: 024 | loss: 0.10190 - acc: 0.9688 -- iter: 192/216
[A[ATraining Step: 168  | total loss: [1m[32m0.11674[0m[0m | time: 5.032s
[2K
| Adam | epoch: 024 | loss: 0.11674 - acc: 0.9594 | val_loss: 0.33912 - val_acc: 0.8529 -- iter: 216/216
--
Training Step: 169  | total loss: [1m[32m0.10909[0m[0m | time: 0.617s
[2K
| Adam | epoch: 025 | loss: 0.10909 - acc: 0.9635 -- iter: 032/216
[A[ATraining Step: 170  | total loss: [1m[32m0.10047[0m[0m | time: 1.225s
[2K
| Adam | epoch: 025 | loss: 0.10047 - acc: 0.9671 -- iter: 064/216
[A[ATraining Step: 171  | total loss: [1m[32m0.09503[0m[0m | time: 1.849s
[2K
| Adam | epoch: 025 | loss: 0.09503 - acc: 0.9704 -- iter: 096/216
[A[ATraining Step: 172  | total loss: [1m[32m0.08751[0m[0m | time: 2.463s
[2K
| Adam | epoch: 025 | loss: 0.08751 - acc: 0.9734 -- iter: 128/216
[A[ATraining Step: 173  | total loss: [1m[32m0.08670[0m[0m | time: 3.100s
[2K
| Adam | epoch: 025 | loss: 0.08670 - acc: 0.9729 -- iter: 160/216
[A[ATraining Step: 174  | total loss: [1m[32m0.08976[0m[0m | time: 3.713s
[2K
| Adam | epoch: 025 | loss: 0.08976 - acc: 0.9725 -- iter: 192/216
[A[ATraining Step: 175  | total loss: [1m[32m0.08238[0m[0m | time: 5.180s
[2K
| Adam | epoch: 025 | loss: 0.08238 - acc: 0.9752 | val_loss: 0.45379 - val_acc: 0.8235 -- iter: 216/216
--
Training Step: 176  | total loss: [1m[32m0.10143[0m[0m | time: 0.462s
[2K
| Adam | epoch: 026 | loss: 0.10143 - acc: 0.9694 -- iter: 032/216
[A[ATraining Step: 177  | total loss: [1m[32m0.09335[0m[0m | time: 1.062s
[2K
| Adam | epoch: 026 | loss: 0.09335 - acc: 0.9724 -- iter: 064/216
[A[ATraining Step: 178  | total loss: [1m[32m0.09271[0m[0m | time: 1.662s
[2K
| Adam | epoch: 026 | loss: 0.09271 - acc: 0.9752 -- iter: 096/216
[A[ATraining Step: 179  | total loss: [1m[32m0.09023[0m[0m | time: 2.258s
[2K
| Adam | epoch: 026 | loss: 0.09023 - acc: 0.9746 -- iter: 128/216
[A[ATraining Step: 180  | total loss: [1m[32m0.08182[0m[0m | time: 2.862s
[2K
| Adam | epoch: 026 | loss: 0.08182 - acc: 0.9771 -- iter: 160/216
[A[ATraining Step: 181  | total loss: [1m[32m0.07504[0m[0m | time: 3.465s
[2K
| Adam | epoch: 026 | loss: 0.07504 - acc: 0.9794 -- iter: 192/216
[A[ATraining Step: 182  | total loss: [1m[32m0.06953[0m[0m | time: 5.093s
[2K
| Adam | epoch: 026 | loss: 0.06953 - acc: 0.9815 | val_loss: 0.48122 - val_acc: 0.8088 -- iter: 216/216
--
Training Step: 183  | total loss: [1m[32m0.07067[0m[0m | time: 0.475s
[2K
| Adam | epoch: 027 | loss: 0.07067 - acc: 0.9833 -- iter: 032/216
[A[ATraining Step: 184  | total loss: [1m[32m0.09060[0m[0m | time: 0.956s
[2K
| Adam | epoch: 027 | loss: 0.09060 - acc: 0.9808 -- iter: 064/216
[A[ATraining Step: 185  | total loss: [1m[32m0.08230[0m[0m | time: 1.572s
[2K
| Adam | epoch: 027 | loss: 0.08230 - acc: 0.9827 -- iter: 096/216
[A[ATraining Step: 186  | total loss: [1m[32m0.07517[0m[0m | time: 2.187s
[2K
| Adam | epoch: 027 | loss: 0.07517 - acc: 0.9845 -- iter: 128/216
[A[ATraining Step: 187  | total loss: [1m[32m0.07076[0m[0m | time: 2.801s
[2K
| Adam | epoch: 027 | loss: 0.07076 - acc: 0.9860 -- iter: 160/216
[A[ATraining Step: 188  | total loss: [1m[32m0.06815[0m[0m | time: 3.385s
[2K
| Adam | epoch: 027 | loss: 0.06815 - acc: 0.9843 -- iter: 192/216
[A[ATraining Step: 189  | total loss: [1m[32m0.07160[0m[0m | time: 4.991s
[2K
| Adam | epoch: 027 | loss: 0.07160 - acc: 0.9827 | val_loss: 0.45667 - val_acc: 0.8235 -- iter: 216/216
--
Training Step: 190  | total loss: [1m[32m0.06491[0m[0m | time: 0.617s
[2K
| Adam | epoch: 028 | loss: 0.06491 - acc: 0.9845 -- iter: 032/216
[A[ATraining Step: 191  | total loss: [1m[32m0.05932[0m[0m | time: 1.091s
[2K
| Adam | epoch: 028 | loss: 0.05932 - acc: 0.9860 -- iter: 064/216
[A[ATraining Step: 192  | total loss: [1m[32m0.10200[0m[0m | time: 1.548s
[2K
| Adam | epoch: 028 | loss: 0.10200 - acc: 0.9832 -- iter: 096/216
[A[ATraining Step: 193  | total loss: [1m[32m0.09415[0m[0m | time: 2.152s
[2K
| Adam | epoch: 028 | loss: 0.09415 - acc: 0.9849 -- iter: 128/216
[A[ATraining Step: 194  | total loss: [1m[32m0.08646[0m[0m | time: 2.762s
[2K
| Adam | epoch: 028 | loss: 0.08646 - acc: 0.9864 -- iter: 160/216
[A[ATraining Step: 195  | total loss: [1m[32m0.07875[0m[0m | time: 3.363s
[2K
| Adam | epoch: 028 | loss: 0.07875 - acc: 0.9878 -- iter: 192/216
[A[ATraining Step: 196  | total loss: [1m[32m0.07192[0m[0m | time: 4.965s
[2K
| Adam | epoch: 028 | loss: 0.07192 - acc: 0.9890 | val_loss: 0.37700 - val_acc: 0.8529 -- iter: 216/216
--
Training Step: 197  | total loss: [1m[32m0.06598[0m[0m | time: 0.630s
[2K
| Adam | epoch: 029 | loss: 0.06598 - acc: 0.9901 -- iter: 032/216
[A[ATraining Step: 198  | total loss: [1m[32m0.06061[0m[0m | time: 1.229s
[2K
| Adam | epoch: 029 | loss: 0.06061 - acc: 0.9911 -- iter: 064/216
[A[ATraining Step: 199  | total loss: [1m[32m0.05597[0m[0m | time: 1.683s
[2K
| Adam | epoch: 029 | loss: 0.05597 - acc: 0.9920 -- iter: 096/216
[A[ATraining Step: 200  | total loss: [1m[32m0.08414[0m[0m | time: 3.142s
[2K
| Adam | epoch: 029 | loss: 0.08414 - acc: 0.9886 | val_loss: 0.48570 - val_acc: 0.8529 -- iter: 128/216
--
Training Step: 201  | total loss: [1m[32m0.07851[0m[0m | time: 3.755s
[2K
| Adam | epoch: 029 | loss: 0.07851 - acc: 0.9898 -- iter: 160/216
[A[ATraining Step: 202  | total loss: [1m[32m0.07213[0m[0m | time: 4.352s
[2K
| Adam | epoch: 029 | loss: 0.07213 - acc: 0.9908 -- iter: 192/216
[A[ATraining Step: 203  | total loss: [1m[32m0.06795[0m[0m | time: 5.953s
[2K
| Adam | epoch: 029 | loss: 0.06795 - acc: 0.9917 | val_loss: 0.46188 - val_acc: 0.8529 -- iter: 216/216
--
Training Step: 204  | total loss: [1m[32m0.06246[0m[0m | time: 0.710s
[2K
| Adam | epoch: 030 | loss: 0.06246 - acc: 0.9925 -- iter: 032/216
[A[ATraining Step: 205  | total loss: [1m[32m0.05807[0m[0m | time: 1.343s
[2K
| Adam | epoch: 030 | loss: 0.05807 - acc: 0.9933 -- iter: 064/216
[A[ATraining Step: 206  | total loss: [1m[32m0.05454[0m[0m | time: 1.968s
[2K
| Adam | epoch: 030 | loss: 0.05454 - acc: 0.9940 -- iter: 096/216
[A[ATraining Step: 207  | total loss: [1m[32m0.05008[0m[0m | time: 2.424s
[2K
| Adam | epoch: 030 | loss: 0.05008 - acc: 0.9946 -- iter: 128/216
[A[ATraining Step: 208  | total loss: [1m[32m0.07268[0m[0m | time: 2.876s
[2K
| Adam | epoch: 030 | loss: 0.07268 - acc: 0.9909 -- iter: 160/216
[A[ATraining Step: 209  | total loss: [1m[32m0.06616[0m[0m | time: 3.477s
[2K
| Adam | epoch: 030 | loss: 0.06616 - acc: 0.9918 -- iter: 192/216
[A[ATraining Step: 210  | total loss: [1m[32m0.06014[0m[0m | time: 5.082s
[2K
| Adam | epoch: 030 | loss: 0.06014 - acc: 0.9927 | val_loss: 0.55312 - val_acc: 0.8382 -- iter: 216/216
--
Validation AUC:0.9354553492484527
Validation AUPRC:0.9269040874815943
Test AUC:0.9631578947368422
Test AUPRC:0.9735010198648444
BestTestF1Score	0.91	0.79	0.9	0.92	0.89	34	3	27	4	0.94
BestTestMCCScore	0.91	0.79	0.9	0.92	0.89	34	3	27	4	0.94
BestTestAccuracyScore	0.92	0.82	0.91	0.94	0.89	34	2	28	4	0.95
BestValidationF1Score	0.88	0.79	0.9	0.87	0.9	26	4	35	3	0.94
BestValidationMCC	0.88	0.79	0.9	0.87	0.9	26	4	35	3	0.94
BestValidationAccuracy	0.88	0.79	0.9	0.89	0.86	25	3	36	4	0.95
TestPredictions (Threshold:0.94)
CHEMBL43661,FP,INACT,0.949999988079071	CHEMBL3099777,FN,ACT,0.30000001192092896	CHEMBL3315200,TP,ACT,0.9900000095367432	CHEMBL420359,TN,INACT,0.07999999821186066	CHEMBL2312376,FP,INACT,0.9599999785423279	CHEMBL3622806,TP,ACT,1.0	CHEMBL2322893,TN,INACT,0.03999999910593033	CHEMBL141872,TP,ACT,1.0	CHEMBL3315199,TP,ACT,0.9900000095367432	CHEMBL141910,TP,ACT,1.0	CHEMBL593861,TN,INACT,0.019999999552965164	CHEMBL506981,FN,ACT,0.07999999821186066	CHEMBL515170,TN,INACT,0.029999999329447746	CHEMBL172788,TN,INACT,0.029999999329447746	CHEMBL508507,TP,ACT,1.0	CHEMBL133378,TP,ACT,0.9900000095367432	CHEMBL461502,TN,INACT,0.0	CHEMBL503059,TP,ACT,1.0	CHEMBL165175,TN,INACT,0.009999999776482582	CHEMBL297335,TN,INACT,0.009999999776482582	CHEMBL375002,TP,ACT,0.9900000095367432	CHEMBL501956,TP,ACT,1.0	CHEMBL441305,TN,INACT,0.27000001072883606	CHEMBL44262,TN,INACT,0.0	CHEMBL3040381,TP,ACT,0.9900000095367432	CHEMBL132794,TP,ACT,0.9800000190734863	CHEMBL42360,TN,INACT,0.0	CHEMBL602269,TN,INACT,0.07000000029802322	CHEMBL2431121,FN,ACT,0.8899999856948853	CHEMBL138970,TP,ACT,1.0	CHEMBL344020,TP,ACT,1.0	CHEMBL118553,TN,INACT,0.03999999910593033	CHEMBL3099769,FN,ACT,0.2800000011920929	CHEMBL129953,TP,ACT,0.9900000095367432	CHEMBL3315223,TP,ACT,0.9900000095367432	CHEMBL405804,TP,ACT,1.0	CHEMBL501575,TP,ACT,1.0	CHEMBL508130,TP,ACT,1.0	CHEMBL2369133,TP,ACT,1.0	CHEMBL1766923,TP,ACT,1.0	CHEMBL2369722,TP,ACT,0.9800000190734863	CHEMBL2369509,TP,ACT,0.9599999785423279	CHEMBL1076,TN,INACT,0.6200000047683716	CHEMBL227429,TN,INACT,0.009999999776482582	CHEMBL3350741,TN,INACT,0.8899999856948853	CHEMBL413010,TP,ACT,1.0	CHEMBL133849,TP,ACT,0.9900000095367432	CHEMBL1086462,TP,ACT,1.0	CHEMBL3786852,TP,ACT,1.0	CHEMBL404942,TP,ACT,0.9900000095367432	CHEMBL132507,TP,ACT,0.9900000095367432	CHEMBL107680,TN,INACT,0.4699999988079071	CHEMBL1983100,TN,INACT,0.009999999776482582	CHEMBL461709,TN,INACT,0.009999999776482582	CHEMBL128360,TN,INACT,0.46000000834465027	CHEMBL9666,TN,INACT,0.009999999776482582	CHEMBL59347,TN,INACT,0.3799999952316284	CHEMBL270332,TP,ACT,0.9800000190734863	CHEMBL217002,TN,INACT,0.49000000953674316	CHEMBL445035,TP,ACT,1.0	CHEMBL499673,TP,ACT,1.0	CHEMBL2431108,TN,INACT,0.05000000074505806	CHEMBL281232,TN,INACT,0.0	CHEMBL3099775,FP,INACT,0.9900000095367432	CHEMBL2431111,TN,INACT,0.0	CHEMBL1766935,TP,ACT,1.0	CHEMBL15936,TN,INACT,0.10999999940395355	CHEMBL386387,TP,ACT,1.0	

