ImageNetInceptionV2 CHEMBL3474 adam 0.001 15 0 0 0.8 False True
Number of active compounds :	193
Number of inactive compounds :	129
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL3474_adam_0.001_15_0_0_0.8_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL3474_adam_0.001_15_0.8/
---------------------------------
Training samples: 205
Validation samples: 65
--
Training Step: 1  | time: 36.879s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/205
[A[ATraining Step: 2  | total loss: [1m[32m0.63442[0m[0m | time: 45.860s
[2K
| Adam | epoch: 001 | loss: 0.63442 - acc: 0.5062 -- iter: 064/205
[A[ATraining Step: 3  | total loss: [1m[32m0.81559[0m[0m | time: 55.121s
[2K
| Adam | epoch: 001 | loss: 0.81559 - acc: 0.5267 -- iter: 096/205
[A[ATraining Step: 4  | total loss: [1m[32m0.71775[0m[0m | time: 64.139s
[2K
| Adam | epoch: 001 | loss: 0.71775 - acc: 0.6004 -- iter: 128/205
[A[ATraining Step: 5  | total loss: [1m[32m0.95345[0m[0m | time: 73.126s
[2K
| Adam | epoch: 001 | loss: 0.95345 - acc: 0.4876 -- iter: 160/205
[A[ATraining Step: 6  | total loss: [1m[32m0.68288[0m[0m | time: 82.088s
[2K
| Adam | epoch: 001 | loss: 0.68288 - acc: 0.6563 -- iter: 192/205
[A[ATraining Step: 7  | total loss: [1m[32m0.64765[0m[0m | time: 95.678s
[2K
| Adam | epoch: 001 | loss: 0.64765 - acc: 0.6750 | val_loss: 0.66102 - val_acc: 0.6308 -- iter: 205/205
--
Training Step: 8  | total loss: [1m[32m0.59357[0m[0m | time: 4.360s
[2K
| Adam | epoch: 002 | loss: 0.59357 - acc: 0.7713 -- iter: 032/205
[A[ATraining Step: 9  | total loss: [1m[32m0.43025[0m[0m | time: 13.318s
[2K
| Adam | epoch: 002 | loss: 0.43025 - acc: 0.8516 -- iter: 064/205
[A[ATraining Step: 10  | total loss: [1m[32m0.54710[0m[0m | time: 22.612s
[2K
| Adam | epoch: 002 | loss: 0.54710 - acc: 0.7852 -- iter: 096/205
[A[ATraining Step: 11  | total loss: [1m[32m0.61801[0m[0m | time: 31.673s
[2K
| Adam | epoch: 002 | loss: 0.61801 - acc: 0.6649 -- iter: 128/205
[A[ATraining Step: 12  | total loss: [1m[32m0.60122[0m[0m | time: 40.692s
[2K
| Adam | epoch: 002 | loss: 0.60122 - acc: 0.6891 -- iter: 160/205
[A[ATraining Step: 13  | total loss: [1m[32m0.54013[0m[0m | time: 49.721s
[2K
| Adam | epoch: 002 | loss: 0.54013 - acc: 0.7554 -- iter: 192/205
[A[ATraining Step: 14  | total loss: [1m[32m0.53339[0m[0m | time: 61.826s
[2K
| Adam | epoch: 002 | loss: 0.53339 - acc: 0.7148 | val_loss: 0.90443 - val_acc: 0.6308 -- iter: 205/205
--
Training Step: 15  | total loss: [1m[32m0.44206[0m[0m | time: 4.323s
[2K
| Adam | epoch: 003 | loss: 0.44206 - acc: 0.7897 -- iter: 032/205
[A[ATraining Step: 16  | total loss: [1m[32m0.53083[0m[0m | time: 8.624s
[2K
| Adam | epoch: 003 | loss: 0.53083 - acc: 0.7532 -- iter: 064/205
[A[ATraining Step: 17  | total loss: [1m[32m0.44646[0m[0m | time: 17.575s
[2K
| Adam | epoch: 003 | loss: 0.44646 - acc: 0.8144 -- iter: 096/205
[A[ATraining Step: 18  | total loss: [1m[32m0.59749[0m[0m | time: 26.566s
[2K
| Adam | epoch: 003 | loss: 0.59749 - acc: 0.7380 -- iter: 128/205
[A[ATraining Step: 19  | total loss: [1m[32m0.54551[0m[0m | time: 35.662s
[2K
| Adam | epoch: 003 | loss: 0.54551 - acc: 0.7628 -- iter: 160/205
[A[ATraining Step: 20  | total loss: [1m[32m0.48646[0m[0m | time: 44.879s
[2K
| Adam | epoch: 003 | loss: 0.48646 - acc: 0.8089 -- iter: 192/205
[A[ATraining Step: 21  | total loss: [1m[32m0.46052[0m[0m | time: 56.946s
[2K
| Adam | epoch: 003 | loss: 0.46052 - acc: 0.8003 | val_loss: 2.32318 - val_acc: 0.6308 -- iter: 205/205
--
Training Step: 22  | total loss: [1m[32m0.45412[0m[0m | time: 8.928s
[2K
| Adam | epoch: 004 | loss: 0.45412 - acc: 0.7852 -- iter: 032/205
[A[ATraining Step: 23  | total loss: [1m[32m0.46290[0m[0m | time: 13.376s
[2K
| Adam | epoch: 004 | loss: 0.46290 - acc: 0.7841 -- iter: 064/205
[A[ATraining Step: 24  | total loss: [1m[32m0.47288[0m[0m | time: 17.598s
[2K
| Adam | epoch: 004 | loss: 0.47288 - acc: 0.8232 -- iter: 096/205
[A[ATraining Step: 25  | total loss: [1m[32m0.38353[0m[0m | time: 26.617s
[2K
| Adam | epoch: 004 | loss: 0.38353 - acc: 0.8714 -- iter: 128/205
[A[ATraining Step: 26  | total loss: [1m[32m0.37767[0m[0m | time: 35.597s
[2K
| Adam | epoch: 004 | loss: 0.37767 - acc: 0.8806 -- iter: 160/205
[A[ATraining Step: 27  | total loss: [1m[32m0.31710[0m[0m | time: 44.704s
[2K
| Adam | epoch: 004 | loss: 0.31710 - acc: 0.9033 -- iter: 192/205
[A[ATraining Step: 28  | total loss: [1m[32m0.35163[0m[0m | time: 56.750s
[2K
| Adam | epoch: 004 | loss: 0.35163 - acc: 0.8806 | val_loss: 3.66516 - val_acc: 0.6308 -- iter: 205/205
--
Training Step: 29  | total loss: [1m[32m0.38112[0m[0m | time: 9.117s
[2K
| Adam | epoch: 005 | loss: 0.38112 - acc: 0.8640 -- iter: 032/205
[A[ATraining Step: 30  | total loss: [1m[32m0.38020[0m[0m | time: 18.153s
[2K
| Adam | epoch: 005 | loss: 0.38020 - acc: 0.8370 -- iter: 064/205
[A[ATraining Step: 31  | total loss: [1m[32m0.36505[0m[0m | time: 22.492s
[2K
| Adam | epoch: 005 | loss: 0.36505 - acc: 0.8458 -- iter: 096/205
[A[ATraining Step: 32  | total loss: [1m[32m0.34502[0m[0m | time: 26.778s
[2K
| Adam | epoch: 005 | loss: 0.34502 - acc: 0.8459 -- iter: 128/205
[A[ATraining Step: 33  | total loss: [1m[32m0.29912[0m[0m | time: 35.894s
[2K
| Adam | epoch: 005 | loss: 0.29912 - acc: 0.8797 -- iter: 160/205
[A[ATraining Step: 34  | total loss: [1m[32m0.29488[0m[0m | time: 44.949s
[2K
| Adam | epoch: 005 | loss: 0.29488 - acc: 0.8921 -- iter: 192/205
[A[ATraining Step: 35  | total loss: [1m[32m0.29655[0m[0m | time: 56.938s
[2K
| Adam | epoch: 005 | loss: 0.29655 - acc: 0.8820 | val_loss: 1.52545 - val_acc: 0.6308 -- iter: 205/205
--
Training Step: 36  | total loss: [1m[32m0.30172[0m[0m | time: 8.995s
[2K
| Adam | epoch: 006 | loss: 0.30172 - acc: 0.8678 -- iter: 032/205
[A[ATraining Step: 37  | total loss: [1m[32m0.31080[0m[0m | time: 17.933s
[2K
| Adam | epoch: 006 | loss: 0.31080 - acc: 0.8692 -- iter: 064/205
[A[ATraining Step: 38  | total loss: [1m[32m0.32596[0m[0m | time: 27.024s
[2K
| Adam | epoch: 006 | loss: 0.32596 - acc: 0.8703 -- iter: 096/205
[A[ATraining Step: 39  | total loss: [1m[32m0.29836[0m[0m | time: 31.466s
[2K
| Adam | epoch: 006 | loss: 0.29836 - acc: 0.8832 -- iter: 128/205
[A[ATraining Step: 40  | total loss: [1m[32m0.27829[0m[0m | time: 35.747s
[2K
| Adam | epoch: 006 | loss: 0.27829 - acc: 0.8907 -- iter: 160/205
[A[ATraining Step: 41  | total loss: [1m[32m0.24567[0m[0m | time: 44.659s
[2K
| Adam | epoch: 006 | loss: 0.24567 - acc: 0.9108 -- iter: 192/205
[A[ATraining Step: 42  | total loss: [1m[32m0.26597[0m[0m | time: 56.834s
[2K
| Adam | epoch: 006 | loss: 0.26597 - acc: 0.8987 | val_loss: 0.71248 - val_acc: 0.5692 -- iter: 205/205
--
Training Step: 43  | total loss: [1m[32m0.22909[0m[0m | time: 9.103s
[2K
| Adam | epoch: 007 | loss: 0.22909 - acc: 0.9111 -- iter: 032/205
[A[ATraining Step: 44  | total loss: [1m[32m0.22427[0m[0m | time: 18.109s
[2K
| Adam | epoch: 007 | loss: 0.22427 - acc: 0.9156 -- iter: 064/205
[A[ATraining Step: 45  | total loss: [1m[32m0.23147[0m[0m | time: 27.186s
[2K
| Adam | epoch: 007 | loss: 0.23147 - acc: 0.9140 -- iter: 096/205
[A[ATraining Step: 46  | total loss: [1m[32m0.27314[0m[0m | time: 36.125s
[2K
| Adam | epoch: 007 | loss: 0.27314 - acc: 0.9127 -- iter: 128/205
[A[ATraining Step: 47  | total loss: [1m[32m0.27206[0m[0m | time: 40.425s
[2K
| Adam | epoch: 007 | loss: 0.27206 - acc: 0.9066 -- iter: 160/205
[A[ATraining Step: 48  | total loss: [1m[32m0.25292[0m[0m | time: 44.661s
[2K
| Adam | epoch: 007 | loss: 0.25292 - acc: 0.9216 -- iter: 192/205
[A[ATraining Step: 49  | total loss: [1m[32m0.22285[0m[0m | time: 56.610s
[2K
| Adam | epoch: 007 | loss: 0.22285 - acc: 0.9340 | val_loss: 0.90043 - val_acc: 0.5231 -- iter: 205/205
--
Training Step: 50  | total loss: [1m[32m0.28356[0m[0m | time: 9.043s
[2K
| Adam | epoch: 008 | loss: 0.28356 - acc: 0.9151 -- iter: 032/205
[A[ATraining Step: 51  | total loss: [1m[32m0.25576[0m[0m | time: 17.983s
[2K
| Adam | epoch: 008 | loss: 0.25576 - acc: 0.9233 -- iter: 064/205
[A[ATraining Step: 52  | total loss: [1m[32m0.25416[0m[0m | time: 26.915s
[2K
| Adam | epoch: 008 | loss: 0.25416 - acc: 0.9207 -- iter: 096/205
[A[ATraining Step: 53  | total loss: [1m[32m0.26407[0m[0m | time: 35.751s
[2K
| Adam | epoch: 008 | loss: 0.26407 - acc: 0.9140 -- iter: 128/205
[A[ATraining Step: 54  | total loss: [1m[32m0.28449[0m[0m | time: 44.963s
[2K
| Adam | epoch: 008 | loss: 0.28449 - acc: 0.9129 -- iter: 160/205
[A[ATraining Step: 55  | total loss: [1m[32m0.30824[0m[0m | time: 49.282s
[2K
| Adam | epoch: 008 | loss: 0.30824 - acc: 0.8985 -- iter: 192/205
[A[ATraining Step: 56  | total loss: [1m[32m0.34292[0m[0m | time: 56.584s
[2K
| Adam | epoch: 008 | loss: 0.34292 - acc: 0.8803 | val_loss: 0.61830 - val_acc: 0.6923 -- iter: 205/205
--
Training Step: 57  | total loss: [1m[32m0.30738[0m[0m | time: 9.085s
[2K
| Adam | epoch: 009 | loss: 0.30738 - acc: 0.8969 -- iter: 032/205
[A[ATraining Step: 58  | total loss: [1m[32m0.30239[0m[0m | time: 18.227s
[2K
| Adam | epoch: 009 | loss: 0.30239 - acc: 0.8811 -- iter: 064/205
[A[ATraining Step: 59  | total loss: [1m[32m0.28856[0m[0m | time: 27.250s
[2K
| Adam | epoch: 009 | loss: 0.28856 - acc: 0.8845 -- iter: 096/205
[A[ATraining Step: 60  | total loss: [1m[32m0.26031[0m[0m | time: 36.109s
[2K
| Adam | epoch: 009 | loss: 0.26031 - acc: 0.8998 -- iter: 128/205
[A[ATraining Step: 61  | total loss: [1m[32m0.25732[0m[0m | time: 45.130s
[2K
| Adam | epoch: 009 | loss: 0.25732 - acc: 0.8966 -- iter: 160/205
[A[ATraining Step: 62  | total loss: [1m[32m0.24675[0m[0m | time: 54.045s
[2K
| Adam | epoch: 009 | loss: 0.24675 - acc: 0.8978 -- iter: 192/205
[A[ATraining Step: 63  | total loss: [1m[32m0.23993[0m[0m | time: 61.282s
[2K
| Adam | epoch: 009 | loss: 0.23993 - acc: 0.9068 | val_loss: 0.67455 - val_acc: 0.7385 -- iter: 205/205
--
Training Step: 64  | total loss: [1m[32m0.23782[0m[0m | time: 4.158s
[2K
| Adam | epoch: 010 | loss: 0.23782 - acc: 0.9088 -- iter: 032/205
[A[ATraining Step: 65  | total loss: [1m[32m0.22040[0m[0m | time: 13.006s
[2K
| Adam | epoch: 010 | loss: 0.22040 - acc: 0.9201 -- iter: 064/205
[A[ATraining Step: 66  | total loss: [1m[32m0.21175[0m[0m | time: 21.980s
[2K
| Adam | epoch: 010 | loss: 0.21175 - acc: 0.9222 -- iter: 096/205
[A[ATraining Step: 67  | total loss: [1m[32m0.21876[0m[0m | time: 31.011s
[2K
| Adam | epoch: 010 | loss: 0.21876 - acc: 0.9203 -- iter: 128/205
[A[ATraining Step: 68  | total loss: [1m[32m0.20508[0m[0m | time: 39.978s
[2K
| Adam | epoch: 010 | loss: 0.20508 - acc: 0.9223 -- iter: 160/205
[A[ATraining Step: 69  | total loss: [1m[32m0.19038[0m[0m | time: 49.121s
[2K
| Adam | epoch: 010 | loss: 0.19038 - acc: 0.9277 -- iter: 192/205
[A[ATraining Step: 70  | total loss: [1m[32m0.20290[0m[0m | time: 61.078s
[2K
| Adam | epoch: 010 | loss: 0.20290 - acc: 0.9217 | val_loss: 0.95969 - val_acc: 0.6769 -- iter: 205/205
--
Training Step: 71  | total loss: [1m[32m0.21033[0m[0m | time: 4.325s
[2K
| Adam | epoch: 011 | loss: 0.21033 - acc: 0.9235 -- iter: 032/205
[A[ATraining Step: 72  | total loss: [1m[32m0.20713[0m[0m | time: 8.667s
[2K
| Adam | epoch: 011 | loss: 0.20713 - acc: 0.9234 -- iter: 064/205
[A[ATraining Step: 73  | total loss: [1m[32m0.19215[0m[0m | time: 17.646s
[2K
| Adam | epoch: 011 | loss: 0.19215 - acc: 0.9319 -- iter: 096/205
[A[ATraining Step: 74  | total loss: [1m[32m0.17692[0m[0m | time: 26.689s
[2K
| Adam | epoch: 011 | loss: 0.17692 - acc: 0.9394 -- iter: 128/205
[A[ATraining Step: 75  | total loss: [1m[32m0.16567[0m[0m | time: 35.723s
[2K
| Adam | epoch: 011 | loss: 0.16567 - acc: 0.9426 -- iter: 160/205
[A[ATraining Step: 76  | total loss: [1m[32m0.16527[0m[0m | time: 44.618s
[2K
| Adam | epoch: 011 | loss: 0.16527 - acc: 0.9420 -- iter: 192/205
[A[ATraining Step: 77  | total loss: [1m[32m0.16655[0m[0m | time: 56.609s
[2K
| Adam | epoch: 011 | loss: 0.16655 - acc: 0.9349 | val_loss: 1.15687 - val_acc: 0.7077 -- iter: 205/205
--
Training Step: 78  | total loss: [1m[32m0.15257[0m[0m | time: 8.808s
[2K
| Adam | epoch: 012 | loss: 0.15257 - acc: 0.9417 -- iter: 032/205
[A[ATraining Step: 79  | total loss: [1m[32m0.14842[0m[0m | time: 15.915s
[2K
| Adam | epoch: 012 | loss: 0.14842 - acc: 0.9445 -- iter: 064/205
[A[ATraining Step: 80  | total loss: [1m[32m0.26869[0m[0m | time: 20.127s
[2K
| Adam | epoch: 012 | loss: 0.26869 - acc: 0.9345 -- iter: 096/205
[A[ATraining Step: 81  | total loss: [1m[32m0.24947[0m[0m | time: 29.061s
[2K
| Adam | epoch: 012 | loss: 0.24947 - acc: 0.9411 -- iter: 128/205
[A[ATraining Step: 82  | total loss: [1m[32m0.23422[0m[0m | time: 37.964s
[2K
| Adam | epoch: 012 | loss: 0.23422 - acc: 0.9439 -- iter: 160/205
[A[ATraining Step: 83  | total loss: [1m[32m0.21576[0m[0m | time: 46.784s
[2K
| Adam | epoch: 012 | loss: 0.21576 - acc: 0.9495 -- iter: 192/205
[A[ATraining Step: 84  | total loss: [1m[32m0.19839[0m[0m | time: 58.463s
[2K
| Adam | epoch: 012 | loss: 0.19839 - acc: 0.9545 | val_loss: 1.37834 - val_acc: 0.6308 -- iter: 205/205
--
Training Step: 85  | total loss: [1m[32m0.19164[0m[0m | time: 8.950s
[2K
| Adam | epoch: 013 | loss: 0.19164 - acc: 0.9528 -- iter: 032/205
[A[ATraining Step: 86  | total loss: [1m[32m0.19071[0m[0m | time: 17.913s
[2K
| Adam | epoch: 013 | loss: 0.19071 - acc: 0.9450 -- iter: 064/205
[A[ATraining Step: 87  | total loss: [1m[32m0.17382[0m[0m | time: 22.163s
[2K
| Adam | epoch: 013 | loss: 0.17382 - acc: 0.9505 -- iter: 096/205
[A[ATraining Step: 88  | total loss: [1m[32m0.19066[0m[0m | time: 26.409s
[2K
| Adam | epoch: 013 | loss: 0.19066 - acc: 0.9324 -- iter: 128/205
[A[ATraining Step: 89  | total loss: [1m[32m0.19757[0m[0m | time: 35.231s
[2K
| Adam | epoch: 013 | loss: 0.19757 - acc: 0.9315 -- iter: 160/205
[A[ATraining Step: 90  | total loss: [1m[32m0.18344[0m[0m | time: 43.957s
[2K
| Adam | epoch: 013 | loss: 0.18344 - acc: 0.9383 -- iter: 192/205
[A[ATraining Step: 91  | total loss: [1m[32m0.16954[0m[0m | time: 55.953s
[2K
| Adam | epoch: 013 | loss: 0.16954 - acc: 0.9445 | val_loss: 1.42821 - val_acc: 0.6769 -- iter: 205/205
--
Training Step: 92  | total loss: [1m[32m0.16044[0m[0m | time: 8.807s
[2K
| Adam | epoch: 014 | loss: 0.16044 - acc: 0.9500 -- iter: 032/205
[A[ATraining Step: 93  | total loss: [1m[32m0.15995[0m[0m | time: 17.566s
[2K
| Adam | epoch: 014 | loss: 0.15995 - acc: 0.9425 -- iter: 064/205
[A[ATraining Step: 94  | total loss: [1m[32m0.14759[0m[0m | time: 26.442s
[2K
| Adam | epoch: 014 | loss: 0.14759 - acc: 0.9483 -- iter: 096/205
[A[ATraining Step: 95  | total loss: [1m[32m0.13711[0m[0m | time: 30.630s
[2K
| Adam | epoch: 014 | loss: 0.13711 - acc: 0.9535 -- iter: 128/205
[A[ATraining Step: 96  | total loss: [1m[32m0.25454[0m[0m | time: 34.933s
[2K
| Adam | epoch: 014 | loss: 0.25454 - acc: 0.9504 -- iter: 160/205
[A[ATraining Step: 97  | total loss: [1m[32m0.24139[0m[0m | time: 43.934s
[2K
| Adam | epoch: 014 | loss: 0.24139 - acc: 0.9477 -- iter: 192/205
[A[ATraining Step: 98  | total loss: [1m[32m0.22772[0m[0m | time: 56.106s
[2K
| Adam | epoch: 014 | loss: 0.22772 - acc: 0.9467 | val_loss: 0.94299 - val_acc: 0.6769 -- iter: 205/205
--
Training Step: 99  | total loss: [1m[32m0.21268[0m[0m | time: 8.834s
[2K
| Adam | epoch: 015 | loss: 0.21268 - acc: 0.9458 -- iter: 032/205
[A[ATraining Step: 100  | total loss: [1m[32m0.20541[0m[0m | time: 17.650s
[2K
| Adam | epoch: 015 | loss: 0.20541 - acc: 0.9449 -- iter: 064/205
[A[ATraining Step: 101  | total loss: [1m[32m0.19993[0m[0m | time: 26.526s
[2K
| Adam | epoch: 015 | loss: 0.19993 - acc: 0.9411 -- iter: 096/205
[A[ATraining Step: 102  | total loss: [1m[32m0.19025[0m[0m | time: 35.563s
[2K
| Adam | epoch: 015 | loss: 0.19025 - acc: 0.9438 -- iter: 128/205
[A[ATraining Step: 103  | total loss: [1m[32m0.17676[0m[0m | time: 39.882s
[2K
| Adam | epoch: 015 | loss: 0.17676 - acc: 0.9494 -- iter: 160/205
[A[ATraining Step: 104  | total loss: [1m[32m0.30961[0m[0m | time: 44.127s
[2K
| Adam | epoch: 015 | loss: 0.30961 - acc: 0.9391 -- iter: 192/205
[A[ATraining Step: 105  | total loss: [1m[32m0.28545[0m[0m | time: 56.185s
[2K
| Adam | epoch: 015 | loss: 0.28545 - acc: 0.9452 | val_loss: 1.40607 - val_acc: 0.5231 -- iter: 205/205
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.6839430894308942
Validation AUPRC:0.7068654595287478
Test AUC:0.8303747534516764
Test AUPRC:0.8677190502666808
BestTestF1Score	0.81	0.48	0.75	0.74	0.9	35	12	14	4	0.01
BestTestMCCScore	0.81	0.48	0.75	0.74	0.9	35	12	14	4	0.01
BestTestAccuracyScore	0.82	0.51	0.77	0.77	0.87	34	10	16	5	0.02
BestValidationF1Score	0.82	0.42	0.74	0.72	0.95	39	15	9	2	0.01
BestValidationMCC	0.82	0.42	0.74	0.72	0.95	39	15	9	2	0.01
BestValidationAccuracy	0.81	0.41	0.74	0.74	0.9	37	13	11	4	0.02
TestPredictions (Threshold:0.01)
CHEMBL149453,TP,ACT,0.47999998927116394	CHEMBL357394,TP,ACT,0.05999999865889549	CHEMBL356814,TP,ACT,0.23000000417232513	CHEMBL1928704,TN,INACT,0.0	CHEMBL1910608,TN,INACT,0.0	CHEMBL356607,FN,ACT,0.009999999776482582	CHEMBL356752,FP,INACT,0.1899999976158142	CHEMBL361552,TP,ACT,0.46000000834465027	CHEMBL347957,TP,ACT,0.029999999329447746	CHEMBL356133,FN,ACT,0.0	CHEMBL358872,TP,ACT,0.03999999910593033	CHEMBL121702,FP,INACT,0.019999999552965164	CHEMBL2316056,TN,INACT,0.009999999776482582	CHEMBL149075,TP,ACT,0.7400000095367432	CHEMBL423947,TP,ACT,0.17000000178813934	CHEMBL146568,TP,ACT,0.05000000074505806	CHEMBL343906,TP,ACT,0.23999999463558197	CHEMBL453984,TP,ACT,0.4099999964237213	CHEMBL346855,TP,ACT,0.3400000035762787	CHEMBL356606,TP,ACT,0.09000000357627869	CHEMBL484289,TP,ACT,0.8500000238418579	CHEMBL487005,FN,ACT,0.0	CHEMBL353623,FP,INACT,0.8299999833106995	CHEMBL3337974,TN,INACT,0.009999999776482582	CHEMBL1910375,TN,INACT,0.0	CHEMBL223574,TP,ACT,0.5400000214576721	CHEMBL148567,TP,ACT,0.23999999463558197	CHEMBL439687,TN,INACT,0.0	CHEMBL146962,FP,INACT,0.15000000596046448	CHEMBL411915,TP,ACT,0.8100000023841858	CHEMBL222723,TP,ACT,0.17000000178813934	CHEMBL148773,TP,ACT,0.1899999976158142	CHEMBL1910759,TN,INACT,0.0	CHEMBL1814414,TN,INACT,0.0	CHEMBL593720,FP,INACT,0.05000000074505806	CHEMBL1097810,TP,ACT,0.8700000047683716	CHEMBL269819,TP,ACT,0.4099999964237213	CHEMBL1910629,FP,INACT,0.029999999329447746	CHEMBL152402,TP,ACT,0.14000000059604645	CHEMBL205219,FN,ACT,0.0	CHEMBL383316,TN,INACT,0.0	CHEMBL444450,TP,ACT,0.5099999904632568	CHEMBL150373,TP,ACT,0.4699999988079071	CHEMBL1095155,TP,ACT,0.550000011920929	CHEMBL2316063,FP,INACT,0.05999999865889549	CHEMBL1814410,TP,ACT,0.019999999552965164	CHEMBL1910602,TN,INACT,0.009999999776482582	CHEMBL602232,TN,INACT,0.0	CHEMBL1910762,FP,INACT,0.029999999329447746	CHEMBL284600,FP,INACT,0.07999999821186066	CHEMBL423219,TP,ACT,0.10000000149011612	CHEMBL281041,FP,INACT,0.11999999731779099	CHEMBL3337973,TN,INACT,0.0	CHEMBL146186,TP,ACT,0.800000011920929	CHEMBL1644546,TP,ACT,0.6700000166893005	CHEMBL1644549,TP,ACT,0.7599999904632568	CHEMBL313583,FP,INACT,0.07000000029802322	CHEMBL148601,TP,ACT,0.1899999976158142	CHEMBL1910758,TN,INACT,0.009999999776482582	CHEMBL109301,TP,ACT,0.23999999463558197	CHEMBL178260,TP,ACT,0.5899999737739563	CHEMBL480138,FP,INACT,0.019999999552965164	CHEMBL251628,TN,INACT,0.0	CHEMBL152921,TP,ACT,0.029999999329447746	CHEMBL262428,TP,ACT,0.4000000059604645	

