CNNModel CHEMBL311 adam 0.001 15 32 0 0.8 False True
Number of active compounds :	214
Number of inactive compounds :	214
---------------------------------
Run id: CNNModel_CHEMBL311_adam_0.001_15_32_0_0.8_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL311_adam_0.001_15_32_0.8_True/
---------------------------------
Training samples: 272
Validation samples: 86
--
Training Step: 1  | time: 1.682s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/272
[A[ATraining Step: 2  | total loss: [1m[32m0.62327[0m[0m | time: 3.162s
[2K
| Adam | epoch: 001 | loss: 0.62327 - acc: 0.6187 -- iter: 064/272
[A[ATraining Step: 3  | total loss: [1m[32m0.67654[0m[0m | time: 4.629s
[2K
| Adam | epoch: 001 | loss: 0.67654 - acc: 0.5727 -- iter: 096/272
[A[ATraining Step: 4  | total loss: [1m[32m0.74591[0m[0m | time: 5.907s
[2K
| Adam | epoch: 001 | loss: 0.74591 - acc: 0.3776 -- iter: 128/272
[A[ATraining Step: 5  | total loss: [1m[32m0.70691[0m[0m | time: 6.935s
[2K
| Adam | epoch: 001 | loss: 0.70691 - acc: 0.4840 -- iter: 160/272
[A[ATraining Step: 6  | total loss: [1m[32m0.69709[0m[0m | time: 8.053s
[2K
| Adam | epoch: 001 | loss: 0.69709 - acc: 0.5144 -- iter: 192/272
[A[ATraining Step: 7  | total loss: [1m[32m0.69372[0m[0m | time: 9.314s
[2K
| Adam | epoch: 001 | loss: 0.69372 - acc: 0.5432 -- iter: 224/272
[A[ATraining Step: 8  | total loss: [1m[32m0.69222[0m[0m | time: 10.587s
[2K
| Adam | epoch: 001 | loss: 0.69222 - acc: 0.5892 -- iter: 256/272
[A[ATraining Step: 9  | total loss: [1m[32m0.69252[0m[0m | time: 12.294s
[2K
| Adam | epoch: 001 | loss: 0.69252 - acc: 0.5420 | val_loss: 0.69315 - val_acc: 0.5000 -- iter: 272/272
--
Training Step: 10  | total loss: [1m[32m0.69408[0m[0m | time: 0.980s
[2K
| Adam | epoch: 002 | loss: 0.69408 - acc: 0.4585 -- iter: 032/272
[A[ATraining Step: 11  | total loss: [1m[32m0.69457[0m[0m | time: 2.462s
[2K
| Adam | epoch: 002 | loss: 0.69457 - acc: 0.4189 -- iter: 064/272
[A[ATraining Step: 12  | total loss: [1m[32m0.69445[0m[0m | time: 3.868s
[2K
| Adam | epoch: 002 | loss: 0.69445 - acc: 0.4132 -- iter: 096/272
[A[ATraining Step: 13  | total loss: [1m[32m0.69406[0m[0m | time: 5.200s
[2K
| Adam | epoch: 002 | loss: 0.69406 - acc: 0.4370 -- iter: 128/272
[A[ATraining Step: 14  | total loss: [1m[32m0.69365[0m[0m | time: 6.581s
[2K
| Adam | epoch: 002 | loss: 0.69365 - acc: 0.4756 -- iter: 160/272
[A[ATraining Step: 15  | total loss: [1m[32m0.69347[0m[0m | time: 7.921s
[2K
| Adam | epoch: 002 | loss: 0.69347 - acc: 0.5096 -- iter: 192/272
[A[ATraining Step: 16  | total loss: [1m[32m0.69370[0m[0m | time: 9.201s
[2K
| Adam | epoch: 002 | loss: 0.69370 - acc: 0.4122 -- iter: 224/272
[A[ATraining Step: 17  | total loss: [1m[32m0.69344[0m[0m | time: 10.333s
[2K
| Adam | epoch: 002 | loss: 0.69344 - acc: 0.4776 -- iter: 256/272
[A[ATraining Step: 18  | total loss: [1m[32m0.69349[0m[0m | time: 12.551s
[2K
| Adam | epoch: 002 | loss: 0.69349 - acc: 0.4096 | val_loss: 0.69315 - val_acc: 0.5000 -- iter: 272/272
--
Training Step: 19  | total loss: [1m[32m0.69337[0m[0m | time: 0.716s
[2K
| Adam | epoch: 003 | loss: 0.69337 - acc: 0.4606 -- iter: 032/272
[A[ATraining Step: 20  | total loss: [1m[32m0.69347[0m[0m | time: 1.307s
[2K
| Adam | epoch: 003 | loss: 0.69347 - acc: 0.4130 -- iter: 064/272
[A[ATraining Step: 21  | total loss: [1m[32m0.69348[0m[0m | time: 2.492s
[2K
| Adam | epoch: 003 | loss: 0.69348 - acc: 0.3818 -- iter: 096/272
[A[ATraining Step: 22  | total loss: [1m[32m0.69339[0m[0m | time: 3.846s
[2K
| Adam | epoch: 003 | loss: 0.69339 - acc: 0.4173 -- iter: 128/272
[A[ATraining Step: 23  | total loss: [1m[32m0.69332[0m[0m | time: 5.144s
[2K
| Adam | epoch: 003 | loss: 0.69332 - acc: 0.4594 -- iter: 160/272
[A[ATraining Step: 24  | total loss: [1m[32m0.69315[0m[0m | time: 6.299s
[2K
| Adam | epoch: 003 | loss: 0.69315 - acc: 0.5148 -- iter: 192/272
[A[ATraining Step: 25  | total loss: [1m[32m0.69320[0m[0m | time: 7.402s
[2K
| Adam | epoch: 003 | loss: 0.69320 - acc: 0.5108 -- iter: 224/272
[A[ATraining Step: 26  | total loss: [1m[32m0.69353[0m[0m | time: 8.619s
[2K
| Adam | epoch: 003 | loss: 0.69353 - acc: 0.4665 -- iter: 256/272
[A[ATraining Step: 27  | total loss: [1m[32m0.69342[0m[0m | time: 10.924s
[2K
| Adam | epoch: 003 | loss: 0.69342 - acc: 0.4751 | val_loss: 0.69314 - val_acc: 0.5000 -- iter: 272/272
--
Training Step: 28  | total loss: [1m[32m0.69333[0m[0m | time: 0.956s
[2K
| Adam | epoch: 004 | loss: 0.69333 - acc: 0.4814 -- iter: 032/272
[A[ATraining Step: 29  | total loss: [1m[32m0.69339[0m[0m | time: 1.471s
[2K
| Adam | epoch: 004 | loss: 0.69339 - acc: 0.4783 -- iter: 064/272
[A[ATraining Step: 30  | total loss: [1m[32m0.69302[0m[0m | time: 1.953s
[2K
| Adam | epoch: 004 | loss: 0.69302 - acc: 0.5130 -- iter: 096/272
[A[ATraining Step: 31  | total loss: [1m[32m0.69266[0m[0m | time: 2.891s
[2K
| Adam | epoch: 004 | loss: 0.69266 - acc: 0.5389 -- iter: 128/272
[A[ATraining Step: 32  | total loss: [1m[32m0.69258[0m[0m | time: 3.992s
[2K
| Adam | epoch: 004 | loss: 0.69258 - acc: 0.5442 -- iter: 160/272
[A[ATraining Step: 33  | total loss: [1m[32m0.69285[0m[0m | time: 4.963s
[2K
| Adam | epoch: 004 | loss: 0.69285 - acc: 0.5276 -- iter: 192/272
[A[ATraining Step: 34  | total loss: [1m[32m0.69271[0m[0m | time: 5.793s
[2K
| Adam | epoch: 004 | loss: 0.69271 - acc: 0.5351 -- iter: 224/272
[A[ATraining Step: 35  | total loss: [1m[32m0.69319[0m[0m | time: 6.986s
[2K
| Adam | epoch: 004 | loss: 0.69319 - acc: 0.5081 -- iter: 256/272
[A[ATraining Step: 36  | total loss: [1m[32m0.69329[0m[0m | time: 9.206s
[2K
| Adam | epoch: 004 | loss: 0.69329 - acc: 0.5001 | val_loss: 0.69314 - val_acc: 0.5000 -- iter: 272/272
--
Training Step: 37  | total loss: [1m[32m0.69366[0m[0m | time: 0.920s
[2K
| Adam | epoch: 005 | loss: 0.69366 - acc: 0.4813 -- iter: 032/272
[A[ATraining Step: 38  | total loss: [1m[32m0.69385[0m[0m | time: 1.838s
[2K
| Adam | epoch: 005 | loss: 0.69385 - acc: 0.4727 -- iter: 064/272
[A[ATraining Step: 39  | total loss: [1m[32m0.69359[0m[0m | time: 2.307s
[2K
| Adam | epoch: 005 | loss: 0.69359 - acc: 0.4839 -- iter: 096/272
[A[ATraining Step: 40  | total loss: [1m[32m0.69368[0m[0m | time: 2.768s
[2K
| Adam | epoch: 005 | loss: 0.69368 - acc: 0.4752 -- iter: 128/272
[A[ATraining Step: 41  | total loss: [1m[32m0.69380[0m[0m | time: 3.711s
[2K
| Adam | epoch: 005 | loss: 0.69380 - acc: 0.4683 -- iter: 160/272
[A[ATraining Step: 42  | total loss: [1m[32m0.69359[0m[0m | time: 4.744s
[2K
| Adam | epoch: 005 | loss: 0.69359 - acc: 0.4796 -- iter: 192/272
[A[ATraining Step: 43  | total loss: [1m[32m0.69384[0m[0m | time: 5.672s
[2K
| Adam | epoch: 005 | loss: 0.69384 - acc: 0.4612 -- iter: 224/272
[A[ATraining Step: 44  | total loss: [1m[32m0.69363[0m[0m | time: 6.531s
[2K
| Adam | epoch: 005 | loss: 0.69363 - acc: 0.4733 -- iter: 256/272
[A[ATraining Step: 45  | total loss: [1m[32m0.69347[0m[0m | time: 9.146s
[2K
| Adam | epoch: 005 | loss: 0.69347 - acc: 0.4831 | val_loss: 0.69308 - val_acc: 0.5000 -- iter: 272/272
--
Training Step: 46  | total loss: [1m[32m0.69327[0m[0m | time: 1.385s
[2K
| Adam | epoch: 006 | loss: 0.69327 - acc: 0.5016 -- iter: 032/272
[A[ATraining Step: 47  | total loss: [1m[32m0.69339[0m[0m | time: 6.301s
[2K
| Adam | epoch: 006 | loss: 0.69339 - acc: 0.4860 -- iter: 064/272
[A[ATraining Step: 48  | total loss: [1m[32m0.69323[0m[0m | time: 7.298s
[2K
| Adam | epoch: 006 | loss: 0.69323 - acc: 0.5033 -- iter: 096/272
[A[ATraining Step: 49  | total loss: [1m[32m0.69306[0m[0m | time: 7.903s
[2K
| Adam | epoch: 006 | loss: 0.69306 - acc: 0.5176 -- iter: 128/272
[A[ATraining Step: 50  | total loss: [1m[32m0.69305[0m[0m | time: 8.556s
[2K
| Adam | epoch: 006 | loss: 0.69305 - acc: 0.5149 -- iter: 160/272
[A[ATraining Step: 51  | total loss: [1m[32m0.69305[0m[0m | time: 9.716s
[2K
| Adam | epoch: 006 | loss: 0.69305 - acc: 0.5126 -- iter: 192/272
[A[ATraining Step: 52  | total loss: [1m[32m0.69332[0m[0m | time: 10.827s
[2K
| Adam | epoch: 006 | loss: 0.69332 - acc: 0.4919 -- iter: 224/272
[A[ATraining Step: 53  | total loss: [1m[32m0.69349[0m[0m | time: 12.006s
[2K
| Adam | epoch: 006 | loss: 0.69349 - acc: 0.4747 -- iter: 256/272
[A[ATraining Step: 54  | total loss: [1m[32m0.69337[0m[0m | time: 14.458s
[2K
| Adam | epoch: 006 | loss: 0.69337 - acc: 0.4829 | val_loss: 0.69287 - val_acc: 0.5000 -- iter: 272/272
--
Training Step: 55  | total loss: [1m[32m0.69329[0m[0m | time: 1.082s
[2K
| Adam | epoch: 007 | loss: 0.69329 - acc: 0.4898 -- iter: 032/272
[A[ATraining Step: 56  | total loss: [1m[32m0.69336[0m[0m | time: 2.461s
[2K
| Adam | epoch: 007 | loss: 0.69336 - acc: 0.4737 -- iter: 064/272
[A[ATraining Step: 57  | total loss: [1m[32m0.69327[0m[0m | time: 4.054s
[2K
| Adam | epoch: 007 | loss: 0.69327 - acc: 0.4816 -- iter: 096/272
[A[ATraining Step: 58  | total loss: [1m[32m0.69319[0m[0m | time: 5.527s
[2K
| Adam | epoch: 007 | loss: 0.69319 - acc: 0.4927 -- iter: 128/272
[A[ATraining Step: 59  | total loss: [1m[32m0.69308[0m[0m | time: 6.273s
[2K
| Adam | epoch: 007 | loss: 0.69308 - acc: 0.5062 -- iter: 160/272
[A[ATraining Step: 60  | total loss: [1m[32m0.69312[0m[0m | time: 7.308s
[2K
| Adam | epoch: 007 | loss: 0.69312 - acc: 0.4889 -- iter: 192/272
[A[ATraining Step: 61  | total loss: [1m[32m0.69316[0m[0m | time: 10.714s
[2K
| Adam | epoch: 007 | loss: 0.69316 - acc: 0.4740 -- iter: 224/272
[A[ATraining Step: 62  | total loss: [1m[32m0.69301[0m[0m | time: 17.628s
[2K
| Adam | epoch: 007 | loss: 0.69301 - acc: 0.4894 -- iter: 256/272
[A[ATraining Step: 63  | total loss: [1m[32m0.69290[0m[0m | time: 33.333s
[2K
| Adam | epoch: 007 | loss: 0.69290 - acc: 0.5224 | val_loss: 0.69088 - val_acc: 0.7558 -- iter: 272/272
--
Training Step: 64  | total loss: [1m[32m0.69270[0m[0m | time: 1.129s
[2K
| Adam | epoch: 008 | loss: 0.69270 - acc: 0.5587 -- iter: 032/272
[A[ATraining Step: 65  | total loss: [1m[32m0.69259[0m[0m | time: 2.268s
[2K
| Adam | epoch: 008 | loss: 0.69259 - acc: 0.5746 -- iter: 064/272
[A[ATraining Step: 66  | total loss: [1m[32m0.69236[0m[0m | time: 3.546s
[2K
| Adam | epoch: 008 | loss: 0.69236 - acc: 0.5845 -- iter: 096/272
[A[ATraining Step: 67  | total loss: [1m[32m0.69222[0m[0m | time: 4.707s
[2K
| Adam | epoch: 008 | loss: 0.69222 - acc: 0.5781 -- iter: 128/272
[A[ATraining Step: 68  | total loss: [1m[32m0.69189[0m[0m | time: 5.919s
[2K
| Adam | epoch: 008 | loss: 0.69189 - acc: 0.5763 -- iter: 160/272
[A[ATraining Step: 69  | total loss: [1m[32m0.69154[0m[0m | time: 6.583s
[2K
| Adam | epoch: 008 | loss: 0.69154 - acc: 0.5710 -- iter: 192/272
[A[ATraining Step: 70  | total loss: [1m[32m0.69089[0m[0m | time: 7.244s
[2K
| Adam | epoch: 008 | loss: 0.69089 - acc: 0.5917 -- iter: 224/272
[A[ATraining Step: 71  | total loss: [1m[32m0.68994[0m[0m | time: 8.580s
[2K
| Adam | epoch: 008 | loss: 0.68994 - acc: 0.6026 -- iter: 256/272
[A[ATraining Step: 72  | total loss: [1m[32m0.68822[0m[0m | time: 10.734s
[2K
| Adam | epoch: 008 | loss: 0.68822 - acc: 0.6016 | val_loss: 0.67245 - val_acc: 0.7791 -- iter: 272/272
--
Training Step: 73  | total loss: [1m[32m0.68938[0m[0m | time: 1.500s
[2K
| Adam | epoch: 009 | loss: 0.68938 - acc: 0.5764 -- iter: 032/272
[A[ATraining Step: 74  | total loss: [1m[32m0.68788[0m[0m | time: 2.669s
[2K
| Adam | epoch: 009 | loss: 0.68788 - acc: 0.6023 -- iter: 064/272
[A[ATraining Step: 75  | total loss: [1m[32m0.68705[0m[0m | time: 7.336s
[2K
| Adam | epoch: 009 | loss: 0.68705 - acc: 0.6014 -- iter: 096/272
[A[ATraining Step: 76  | total loss: [1m[32m0.68811[0m[0m | time: 12.079s
[2K
| Adam | epoch: 009 | loss: 0.68811 - acc: 0.5805 -- iter: 128/272
[A[ATraining Step: 77  | total loss: [1m[32m0.68815[0m[0m | time: 13.140s
[2K
| Adam | epoch: 009 | loss: 0.68815 - acc: 0.5687 -- iter: 160/272
[A[ATraining Step: 78  | total loss: [1m[32m0.68519[0m[0m | time: 14.442s
[2K
| Adam | epoch: 009 | loss: 0.68519 - acc: 0.5844 -- iter: 192/272
[A[ATraining Step: 79  | total loss: [1m[32m0.68016[0m[0m | time: 15.069s
[2K
| Adam | epoch: 009 | loss: 0.68016 - acc: 0.6144 -- iter: 224/272
[A[ATraining Step: 80  | total loss: [1m[32m0.67933[0m[0m | time: 15.757s
[2K
| Adam | epoch: 009 | loss: 0.67933 - acc: 0.6027 -- iter: 256/272
[A[ATraining Step: 81  | total loss: [1m[32m0.67775[0m[0m | time: 18.138s
[2K
| Adam | epoch: 009 | loss: 0.67775 - acc: 0.5987 | val_loss: 0.58818 - val_acc: 0.7442 -- iter: 272/272
--
Training Step: 82  | total loss: [1m[32m0.67434[0m[0m | time: 1.516s
[2K
| Adam | epoch: 010 | loss: 0.67434 - acc: 0.5888 -- iter: 032/272
[A[ATraining Step: 83  | total loss: [1m[32m0.66718[0m[0m | time: 2.741s
[2K
| Adam | epoch: 010 | loss: 0.66718 - acc: 0.5955 -- iter: 064/272
[A[ATraining Step: 84  | total loss: [1m[32m0.65976[0m[0m | time: 3.896s
[2K
| Adam | epoch: 010 | loss: 0.65976 - acc: 0.6016 -- iter: 096/272
[A[ATraining Step: 85  | total loss: [1m[32m0.64254[0m[0m | time: 5.337s
[2K
| Adam | epoch: 010 | loss: 0.64254 - acc: 0.6321 -- iter: 128/272
[A[ATraining Step: 86  | total loss: [1m[32m0.62838[0m[0m | time: 6.745s
[2K
| Adam | epoch: 010 | loss: 0.62838 - acc: 0.6439 -- iter: 160/272
[A[ATraining Step: 87  | total loss: [1m[32m0.62449[0m[0m | time: 8.031s
[2K
| Adam | epoch: 010 | loss: 0.62449 - acc: 0.6420 -- iter: 192/272
[A[ATraining Step: 88  | total loss: [1m[32m0.61913[0m[0m | time: 9.251s
[2K
| Adam | epoch: 010 | loss: 0.61913 - acc: 0.6528 -- iter: 224/272
[A[ATraining Step: 89  | total loss: [1m[32m0.59325[0m[0m | time: 9.793s
[2K
| Adam | epoch: 010 | loss: 0.59325 - acc: 0.6719 -- iter: 256/272
[A[ATraining Step: 90  | total loss: [1m[32m0.59086[0m[0m | time: 11.337s
[2K
| Adam | epoch: 010 | loss: 0.59086 - acc: 0.6609 | val_loss: 0.79773 - val_acc: 0.5116 -- iter: 272/272
--
Training Step: 91  | total loss: [1m[32m0.56020[0m[0m | time: 1.156s
[2K
| Adam | epoch: 011 | loss: 0.56020 - acc: 0.6823 -- iter: 032/272
[A[ATraining Step: 92  | total loss: [1m[32m0.57323[0m[0m | time: 2.561s
[2K
| Adam | epoch: 011 | loss: 0.57323 - acc: 0.6735 -- iter: 064/272
[A[ATraining Step: 93  | total loss: [1m[32m0.57425[0m[0m | time: 4.018s
[2K
| Adam | epoch: 011 | loss: 0.57425 - acc: 0.6718 -- iter: 096/272
[A[ATraining Step: 94  | total loss: [1m[32m0.56301[0m[0m | time: 5.307s
[2K
| Adam | epoch: 011 | loss: 0.56301 - acc: 0.6796 -- iter: 128/272
[A[ATraining Step: 95  | total loss: [1m[32m0.56904[0m[0m | time: 6.395s
[2K
| Adam | epoch: 011 | loss: 0.56904 - acc: 0.6773 -- iter: 160/272
[A[ATraining Step: 96  | total loss: [1m[32m0.56435[0m[0m | time: 7.444s
[2K
| Adam | epoch: 011 | loss: 0.56435 - acc: 0.6814 -- iter: 192/272
[A[ATraining Step: 97  | total loss: [1m[32m0.55113[0m[0m | time: 8.934s
[2K
| Adam | epoch: 011 | loss: 0.55113 - acc: 0.6976 -- iter: 224/272
[A[ATraining Step: 98  | total loss: [1m[32m0.53196[0m[0m | time: 10.289s
[2K
| Adam | epoch: 011 | loss: 0.53196 - acc: 0.7185 -- iter: 256/272
[A[ATraining Step: 99  | total loss: [1m[32m0.51797[0m[0m | time: 12.021s
[2K
| Adam | epoch: 011 | loss: 0.51797 - acc: 0.7341 | val_loss: 0.42142 - val_acc: 0.8488 -- iter: 272/272
--
Training Step: 100  | total loss: [1m[32m0.51689[0m[0m | time: 0.720s
[2K
| Adam | epoch: 012 | loss: 0.51689 - acc: 0.7357 -- iter: 032/272
[A[ATraining Step: 101  | total loss: [1m[32m0.50318[0m[0m | time: 1.912s
[2K
| Adam | epoch: 012 | loss: 0.50318 - acc: 0.7559 -- iter: 064/272
[A[ATraining Step: 102  | total loss: [1m[32m0.50041[0m[0m | time: 3.039s
[2K
| Adam | epoch: 012 | loss: 0.50041 - acc: 0.7584 -- iter: 096/272
[A[ATraining Step: 103  | total loss: [1m[32m0.49834[0m[0m | time: 3.984s
[2K
| Adam | epoch: 012 | loss: 0.49834 - acc: 0.7639 -- iter: 128/272
[A[ATraining Step: 104  | total loss: [1m[32m0.48074[0m[0m | time: 5.019s
[2K
| Adam | epoch: 012 | loss: 0.48074 - acc: 0.7718 -- iter: 160/272
[A[ATraining Step: 105  | total loss: [1m[32m0.46833[0m[0m | time: 6.078s
[2K
| Adam | epoch: 012 | loss: 0.46833 - acc: 0.7790 -- iter: 192/272
[A[ATraining Step: 106  | total loss: [1m[32m0.45781[0m[0m | time: 6.974s
[2K
| Adam | epoch: 012 | loss: 0.45781 - acc: 0.7824 -- iter: 224/272
[A[ATraining Step: 107  | total loss: [1m[32m0.45153[0m[0m | time: 7.911s
[2K
| Adam | epoch: 012 | loss: 0.45153 - acc: 0.7885 -- iter: 256/272
[A[ATraining Step: 108  | total loss: [1m[32m0.43605[0m[0m | time: 10.181s
[2K
| Adam | epoch: 012 | loss: 0.43605 - acc: 0.8003 | val_loss: 0.39852 - val_acc: 0.8721 -- iter: 272/272
--
Training Step: 109  | total loss: [1m[32m0.41173[0m[0m | time: 0.456s
[2K
| Adam | epoch: 013 | loss: 0.41173 - acc: 0.8171 -- iter: 032/272
[A[ATraining Step: 110  | total loss: [1m[32m0.38305[0m[0m | time: 0.958s
[2K
| Adam | epoch: 013 | loss: 0.38305 - acc: 0.8354 -- iter: 064/272
[A[ATraining Step: 111  | total loss: [1m[32m0.35221[0m[0m | time: 1.856s
[2K
| Adam | epoch: 013 | loss: 0.35221 - acc: 0.8519 -- iter: 096/272
[A[ATraining Step: 112  | total loss: [1m[32m0.39599[0m[0m | time: 2.777s
[2K
| Adam | epoch: 013 | loss: 0.39599 - acc: 0.8354 -- iter: 128/272
[A[ATraining Step: 113  | total loss: [1m[32m0.40420[0m[0m | time: 3.666s
[2K
| Adam | epoch: 013 | loss: 0.40420 - acc: 0.8331 -- iter: 160/272
[A[ATraining Step: 114  | total loss: [1m[32m0.39821[0m[0m | time: 4.630s
[2K
| Adam | epoch: 013 | loss: 0.39821 - acc: 0.8405 -- iter: 192/272
[A[ATraining Step: 115  | total loss: [1m[32m0.36671[0m[0m | time: 5.629s
[2K
| Adam | epoch: 013 | loss: 0.36671 - acc: 0.8564 -- iter: 224/272
[A[ATraining Step: 116  | total loss: [1m[32m0.36965[0m[0m | time: 6.569s
[2K
| Adam | epoch: 013 | loss: 0.36965 - acc: 0.8583 -- iter: 256/272
[A[ATraining Step: 117  | total loss: [1m[32m0.36133[0m[0m | time: 8.412s
[2K
| Adam | epoch: 013 | loss: 0.36133 - acc: 0.8599 | val_loss: 0.41000 - val_acc: 0.8605 -- iter: 272/272
--
Training Step: 118  | total loss: [1m[32m0.36616[0m[0m | time: 1.290s
[2K
| Adam | epoch: 014 | loss: 0.36616 - acc: 0.8552 -- iter: 032/272
[A[ATraining Step: 119  | total loss: [1m[32m0.34568[0m[0m | time: 1.849s
[2K
| Adam | epoch: 014 | loss: 0.34568 - acc: 0.8634 -- iter: 064/272
[A[ATraining Step: 120  | total loss: [1m[32m0.32406[0m[0m | time: 2.334s
[2K
| Adam | epoch: 014 | loss: 0.32406 - acc: 0.8708 -- iter: 096/272
[A[ATraining Step: 121  | total loss: [1m[32m0.30381[0m[0m | time: 3.219s
[2K
| Adam | epoch: 014 | loss: 0.30381 - acc: 0.8838 -- iter: 128/272
[A[ATraining Step: 122  | total loss: [1m[32m0.28788[0m[0m | time: 4.184s
[2K
| Adam | epoch: 014 | loss: 0.28788 - acc: 0.8923 -- iter: 160/272
[A[ATraining Step: 123  | total loss: [1m[32m0.28466[0m[0m | time: 5.070s
[2K
| Adam | epoch: 014 | loss: 0.28466 - acc: 0.8968 -- iter: 192/272
[A[ATraining Step: 124  | total loss: [1m[32m0.28794[0m[0m | time: 6.122s
[2K
| Adam | epoch: 014 | loss: 0.28794 - acc: 0.8977 -- iter: 224/272
[A[ATraining Step: 125  | total loss: [1m[32m0.27383[0m[0m | time: 7.772s
[2K
| Adam | epoch: 014 | loss: 0.27383 - acc: 0.9017 -- iter: 256/272
[A[ATraining Step: 126  | total loss: [1m[32m0.28268[0m[0m | time: 10.281s
[2K
| Adam | epoch: 014 | loss: 0.28268 - acc: 0.9053 | val_loss: 0.41249 - val_acc: 0.8488 -- iter: 272/272
--
Training Step: 127  | total loss: [1m[32m0.25966[0m[0m | time: 1.266s
[2K
| Adam | epoch: 015 | loss: 0.25966 - acc: 0.9148 -- iter: 032/272
[A[ATraining Step: 128  | total loss: [1m[32m0.30961[0m[0m | time: 2.663s
[2K
| Adam | epoch: 015 | loss: 0.30961 - acc: 0.8983 -- iter: 064/272
[A[ATraining Step: 129  | total loss: [1m[32m0.29116[0m[0m | time: 3.400s
[2K
| Adam | epoch: 015 | loss: 0.29116 - acc: 0.9022 -- iter: 096/272
[A[ATraining Step: 130  | total loss: [1m[32m0.26767[0m[0m | time: 4.170s
[2K
| Adam | epoch: 015 | loss: 0.26767 - acc: 0.9120 -- iter: 128/272
[A[ATraining Step: 131  | total loss: [1m[32m0.24682[0m[0m | time: 5.527s
[2K
| Adam | epoch: 015 | loss: 0.24682 - acc: 0.9208 -- iter: 160/272
[A[ATraining Step: 132  | total loss: [1m[32m0.24181[0m[0m | time: 6.677s
[2K
| Adam | epoch: 015 | loss: 0.24181 - acc: 0.9225 -- iter: 192/272
[A[ATraining Step: 133  | total loss: [1m[32m0.22585[0m[0m | time: 7.986s
[2K
| Adam | epoch: 015 | loss: 0.22585 - acc: 0.9271 -- iter: 224/272
[A[ATraining Step: 134  | total loss: [1m[32m0.23058[0m[0m | time: 9.397s
[2K
| Adam | epoch: 015 | loss: 0.23058 - acc: 0.9281 -- iter: 256/272
[A[ATraining Step: 135  | total loss: [1m[32m0.22276[0m[0m | time: 11.688s
[2K
| Adam | epoch: 015 | loss: 0.22276 - acc: 0.9259 | val_loss: 0.43016 - val_acc: 0.8140 -- iter: 272/272
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.9259058950784208
Validation AUPRC:0.9318053031885901
Test AUC:0.9476268412438626
Test AUPRC:0.9606763937989513
BestTestF1Score	0.93	0.84	0.92	0.92	0.94	44	4	35	3	0.12
BestTestMCCScore	0.93	0.84	0.92	0.92	0.94	44	4	35	3	0.12
BestTestAccuracyScore	0.87	0.76	0.87	0.95	0.81	38	2	37	9	0.25
BestValidationF1Score	0.88	0.75	0.87	0.85	0.91	39	7	36	4	0.12
BestValidationMCC	0.88	0.75	0.87	0.85	0.91	39	7	36	4	0.12
BestValidationAccuracy	0.87	0.74	0.87	0.88	0.86	37	5	38	6	0.25
TestPredictions (Threshold:0.12)
CHEMBL611036,TN,INACT,0.009999999776482582	CHEMBL246815,TP,ACT,0.9300000071525574	CHEMBL2181206,TP,ACT,0.8700000047683716	CHEMBL29830,TN,INACT,0.019999999552965164	CHEMBL1779005,TP,ACT,0.8999999761581421	CHEMBL1778870,TP,ACT,0.14000000059604645	CHEMBL299291,TN,INACT,0.009999999776482582	CHEMBL156667,TN,INACT,0.009999999776482582	CHEMBL212990,TP,ACT,0.9399999976158142	CHEMBL246818,TP,ACT,0.3799999952316284	CHEMBL402979,TP,ACT,0.9100000262260437	CHEMBL222457,TP,ACT,0.9399999976158142	CHEMBL2425978,TN,INACT,0.0	CHEMBL326678,TN,INACT,0.009999999776482582	CHEMBL237533,TP,ACT,0.6299999952316284	CHEMBL17703,TP,ACT,0.5600000023841858	CHEMBL380179,TP,ACT,0.9700000286102295	CHEMBL3633656,TN,INACT,0.009999999776482582	CHEMBL154700,TN,INACT,0.009999999776482582	CHEMBL2426086,TN,INACT,0.009999999776482582	CHEMBL2312686,TN,INACT,0.009999999776482582	CHEMBL9253,TN,INACT,0.009999999776482582	CHEMBL398724,FP,INACT,0.20000000298023224	CHEMBL310435,TN,INACT,0.05000000074505806	CHEMBL400917,TP,ACT,0.23999999463558197	CHEMBL244584,TN,INACT,0.11999999731779099	CHEMBL2312398,TN,INACT,0.03999999910593033	CHEMBL1940480,TP,ACT,0.25	CHEMBL1779004,TP,ACT,0.9100000262260437	CHEMBL127510,TP,ACT,0.949999988079071	CHEMBL247837,TP,ACT,0.8700000047683716	CHEMBL247840,TP,ACT,0.9300000071525574	CHEMBL246817,TP,ACT,0.20999999344348907	CHEMBL156519,TN,INACT,0.019999999552965164	CHEMBL2426076,TN,INACT,0.009999999776482582	CHEMBL2115569,TN,INACT,0.019999999552965164	CHEMBL2312397,FP,INACT,0.14000000059604645	CHEMBL2180988,TP,ACT,0.9300000071525574	CHEMBL291278,TN,INACT,0.009999999776482582	CHEMBL1779006,TP,ACT,0.18000000715255737	CHEMBL237101,TP,ACT,0.33000001311302185	CHEMBL2426105,TN,INACT,0.0	CHEMBL610746,TN,INACT,0.029999999329447746	CHEMBL3786901,TN,INACT,0.009999999776482582	CHEMBL236895,TP,ACT,0.4300000071525574	CHEMBL222197,TP,ACT,0.9399999976158142	CHEMBL2312402,TN,INACT,0.09000000357627869	CHEMBL327361,TN,INACT,0.019999999552965164	CHEMBL255697,FN,ACT,0.019999999552965164	CHEMBL108043,FN,ACT,0.019999999552965164	CHEMBL2426078,TN,INACT,0.0	CHEMBL1778994,TP,ACT,0.9200000166893005	CHEMBL3763487,TP,ACT,0.9599999785423279	CHEMBL429297,TP,ACT,0.949999988079071	CHEMBL401882,TP,ACT,0.8500000238418579	CHEMBL3103134,TP,ACT,0.8999999761581421	CHEMBL1779008,TP,ACT,0.8700000047683716	CHEMBL209461,TN,INACT,0.019999999552965164	CHEMBL2426066,TN,INACT,0.0	CHEMBL246816,TP,ACT,0.8700000047683716	CHEMBL2036513,TP,ACT,0.9300000071525574	CHEMBL538307,TN,INACT,0.019999999552965164	CHEMBL1779001,TP,ACT,0.8999999761581421	CHEMBL2426089,TN,INACT,0.009999999776482582	CHEMBL2372184,FN,ACT,0.009999999776482582	CHEMBL2111523,TN,INACT,0.05000000074505806	CHEMBL398357,TP,ACT,0.8500000238418579	CHEMBL1270333,TP,ACT,0.9700000286102295	CHEMBL2425973,TN,INACT,0.009999999776482582	CHEMBL3114911,TP,ACT,0.12999999523162842	CHEMBL2426093,TN,INACT,0.0	CHEMBL2178783,FP,INACT,0.8799999952316284	CHEMBL238184,TP,ACT,0.75	CHEMBL13260,TN,INACT,0.009999999776482582	CHEMBL1778993,TP,ACT,0.9599999785423279	CHEMBL124516,TP,ACT,0.9599999785423279	CHEMBL493892,TN,INACT,0.009999999776482582	CHEMBL1778995,TP,ACT,0.9399999976158142	CHEMBL404851,TP,ACT,0.9300000071525574	CHEMBL1940597,TP,ACT,0.18000000715255737	CHEMBL126228,TP,ACT,0.9800000190734863	CHEMBL1940599,TP,ACT,0.8399999737739563	CHEMBL2145458,TN,INACT,0.009999999776482582	CHEMBL446749,TN,INACT,0.009999999776482582	CHEMBL94717,FP,INACT,0.6399999856948853	CHEMBL249093,TP,ACT,0.7300000190734863	

