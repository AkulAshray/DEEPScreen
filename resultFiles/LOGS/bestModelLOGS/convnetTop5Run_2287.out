CNNModel CHEMBL3072 adam 0.0005 15 32 0 0.6 False True
Number of active compounds :	320
Number of inactive compounds :	320
---------------------------------
Run id: CNNModel_CHEMBL3072_adam_0.0005_15_32_0_0.6_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL3072_adam_0.0005_15_32_0.6_True/
---------------------------------
Training samples: 375
Validation samples: 118
--
Training Step: 1  | time: 1.040s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/375
[A[ATraining Step: 2  | total loss: [1m[32m0.62355[0m[0m | time: 1.804s
[2K
| Adam | epoch: 001 | loss: 0.62355 - acc: 0.5625 -- iter: 064/375
[A[ATraining Step: 3  | total loss: [1m[32m0.68049[0m[0m | time: 2.525s
[2K
| Adam | epoch: 001 | loss: 0.68049 - acc: 0.5114 -- iter: 096/375
[A[ATraining Step: 4  | total loss: [1m[32m0.68908[0m[0m | time: 3.365s
[2K
| Adam | epoch: 001 | loss: 0.68908 - acc: 0.5263 -- iter: 128/375
[A[ATraining Step: 5  | total loss: [1m[32m0.68403[0m[0m | time: 3.977s
[2K
| Adam | epoch: 001 | loss: 0.68403 - acc: 0.6163 -- iter: 160/375
[A[ATraining Step: 6  | total loss: [1m[32m0.69754[0m[0m | time: 4.622s
[2K
| Adam | epoch: 001 | loss: 0.69754 - acc: 0.5013 -- iter: 192/375
[A[ATraining Step: 7  | total loss: [1m[32m0.68495[0m[0m | time: 5.267s
[2K
| Adam | epoch: 001 | loss: 0.68495 - acc: 0.5755 -- iter: 224/375
[A[ATraining Step: 8  | total loss: [1m[32m0.68310[0m[0m | time: 5.987s
[2K
| Adam | epoch: 001 | loss: 0.68310 - acc: 0.5858 -- iter: 256/375
[A[ATraining Step: 9  | total loss: [1m[32m0.68061[0m[0m | time: 6.827s
[2K
| Adam | epoch: 001 | loss: 0.68061 - acc: 0.5900 -- iter: 288/375
[A[ATraining Step: 10  | total loss: [1m[32m0.69554[0m[0m | time: 7.612s
[2K
| Adam | epoch: 001 | loss: 0.69554 - acc: 0.5450 -- iter: 320/375
[A[ATraining Step: 11  | total loss: [1m[32m0.70552[0m[0m | time: 8.382s
[2K
| Adam | epoch: 001 | loss: 0.70552 - acc: 0.5089 -- iter: 352/375
[A[ATraining Step: 12  | total loss: [1m[32m0.69012[0m[0m | time: 9.995s
[2K
| Adam | epoch: 001 | loss: 0.69012 - acc: 0.5611 | val_loss: 0.68258 - val_acc: 0.5847 -- iter: 375/375
--
Training Step: 13  | total loss: [1m[32m0.69530[0m[0m | time: 0.607s
[2K
| Adam | epoch: 002 | loss: 0.69530 - acc: 0.5256 -- iter: 032/375
[A[ATraining Step: 14  | total loss: [1m[32m0.69813[0m[0m | time: 1.359s
[2K
| Adam | epoch: 002 | loss: 0.69813 - acc: 0.5062 -- iter: 064/375
[A[ATraining Step: 15  | total loss: [1m[32m0.69644[0m[0m | time: 2.124s
[2K
| Adam | epoch: 002 | loss: 0.69644 - acc: 0.5038 -- iter: 096/375
[A[ATraining Step: 16  | total loss: [1m[32m0.68890[0m[0m | time: 2.872s
[2K
| Adam | epoch: 002 | loss: 0.68890 - acc: 0.5727 -- iter: 128/375
[A[ATraining Step: 17  | total loss: [1m[32m0.69279[0m[0m | time: 3.596s
[2K
| Adam | epoch: 002 | loss: 0.69279 - acc: 0.5240 -- iter: 160/375
[A[ATraining Step: 18  | total loss: [1m[32m0.69725[0m[0m | time: 4.415s
[2K
| Adam | epoch: 002 | loss: 0.69725 - acc: 0.4616 -- iter: 192/375
[A[ATraining Step: 19  | total loss: [1m[32m0.69668[0m[0m | time: 5.104s
[2K
| Adam | epoch: 002 | loss: 0.69668 - acc: 0.4640 -- iter: 224/375
[A[ATraining Step: 20  | total loss: [1m[32m0.69610[0m[0m | time: 5.759s
[2K
| Adam | epoch: 002 | loss: 0.69610 - acc: 0.4655 -- iter: 256/375
[A[ATraining Step: 21  | total loss: [1m[32m0.69480[0m[0m | time: 6.426s
[2K
| Adam | epoch: 002 | loss: 0.69480 - acc: 0.4859 -- iter: 288/375
[A[ATraining Step: 22  | total loss: [1m[32m0.69280[0m[0m | time: 7.253s
[2K
| Adam | epoch: 002 | loss: 0.69280 - acc: 0.5464 -- iter: 320/375
[A[ATraining Step: 23  | total loss: [1m[32m0.69274[0m[0m | time: 7.996s
[2K
| Adam | epoch: 002 | loss: 0.69274 - acc: 0.5420 -- iter: 352/375
[A[ATraining Step: 24  | total loss: [1m[32m0.69334[0m[0m | time: 9.768s
[2K
| Adam | epoch: 002 | loss: 0.69334 - acc: 0.5126 | val_loss: 0.69115 - val_acc: 0.5847 -- iter: 375/375
--
Training Step: 25  | total loss: [1m[32m0.69246[0m[0m | time: 0.565s
[2K
| Adam | epoch: 003 | loss: 0.69246 - acc: 0.5433 -- iter: 032/375
[A[ATraining Step: 26  | total loss: [1m[32m0.69247[0m[0m | time: 1.155s
[2K
| Adam | epoch: 003 | loss: 0.69247 - acc: 0.5376 -- iter: 064/375
[A[ATraining Step: 27  | total loss: [1m[32m0.69252[0m[0m | time: 1.964s
[2K
| Adam | epoch: 003 | loss: 0.69252 - acc: 0.5335 -- iter: 096/375
[A[ATraining Step: 28  | total loss: [1m[32m0.69246[0m[0m | time: 2.689s
[2K
| Adam | epoch: 003 | loss: 0.69246 - acc: 0.5329 -- iter: 128/375
[A[ATraining Step: 29  | total loss: [1m[32m0.69263[0m[0m | time: 3.443s
[2K
| Adam | epoch: 003 | loss: 0.69263 - acc: 0.5249 -- iter: 160/375
[A[ATraining Step: 30  | total loss: [1m[32m0.69302[0m[0m | time: 4.174s
[2K
| Adam | epoch: 003 | loss: 0.69302 - acc: 0.5116 -- iter: 192/375
[A[ATraining Step: 31  | total loss: [1m[32m0.69204[0m[0m | time: 5.024s
[2K
| Adam | epoch: 003 | loss: 0.69204 - acc: 0.5450 -- iter: 224/375
[A[ATraining Step: 32  | total loss: [1m[32m0.69316[0m[0m | time: 5.647s
[2K
| Adam | epoch: 003 | loss: 0.69316 - acc: 0.5067 -- iter: 256/375
[A[ATraining Step: 33  | total loss: [1m[32m0.69337[0m[0m | time: 6.289s
[2K
| Adam | epoch: 003 | loss: 0.69337 - acc: 0.4984 -- iter: 288/375
[A[ATraining Step: 34  | total loss: [1m[32m0.69227[0m[0m | time: 7.017s
[2K
| Adam | epoch: 003 | loss: 0.69227 - acc: 0.5322 -- iter: 320/375
[A[ATraining Step: 35  | total loss: [1m[32m0.69204[0m[0m | time: 7.708s
[2K
| Adam | epoch: 003 | loss: 0.69204 - acc: 0.5386 -- iter: 352/375
[A[ATraining Step: 36  | total loss: [1m[32m0.69311[0m[0m | time: 9.489s
[2K
| Adam | epoch: 003 | loss: 0.69311 - acc: 0.5051 | val_loss: 0.69033 - val_acc: 0.5847 -- iter: 375/375
--
Training Step: 37  | total loss: [1m[32m0.69293[0m[0m | time: 0.735s
[2K
| Adam | epoch: 004 | loss: 0.69293 - acc: 0.5103 -- iter: 032/375
[A[ATraining Step: 38  | total loss: [1m[32m0.69195[0m[0m | time: 1.255s
[2K
| Adam | epoch: 004 | loss: 0.69195 - acc: 0.5389 -- iter: 064/375
[A[ATraining Step: 39  | total loss: [1m[32m0.69299[0m[0m | time: 1.827s
[2K
| Adam | epoch: 004 | loss: 0.69299 - acc: 0.5106 -- iter: 096/375
[A[ATraining Step: 40  | total loss: [1m[32m0.69383[0m[0m | time: 2.594s
[2K
| Adam | epoch: 004 | loss: 0.69383 - acc: 0.4883 -- iter: 128/375
[A[ATraining Step: 41  | total loss: [1m[32m0.69371[0m[0m | time: 3.350s
[2K
| Adam | epoch: 004 | loss: 0.69371 - acc: 0.4904 -- iter: 160/375
[A[ATraining Step: 42  | total loss: [1m[32m0.69265[0m[0m | time: 4.133s
[2K
| Adam | epoch: 004 | loss: 0.69265 - acc: 0.5203 -- iter: 192/375
[A[ATraining Step: 43  | total loss: [1m[32m0.69254[0m[0m | time: 4.841s
[2K
| Adam | epoch: 004 | loss: 0.69254 - acc: 0.5222 -- iter: 224/375
[A[ATraining Step: 44  | total loss: [1m[32m0.69248[0m[0m | time: 5.609s
[2K
| Adam | epoch: 004 | loss: 0.69248 - acc: 0.5238 -- iter: 256/375
[A[ATraining Step: 45  | total loss: [1m[32m0.69299[0m[0m | time: 6.215s
[2K
| Adam | epoch: 004 | loss: 0.69299 - acc: 0.5091 -- iter: 288/375
[A[ATraining Step: 46  | total loss: [1m[32m0.69323[0m[0m | time: 6.878s
[2K
| Adam | epoch: 004 | loss: 0.69323 - acc: 0.5024 -- iter: 320/375
[A[ATraining Step: 47  | total loss: [1m[32m0.69249[0m[0m | time: 7.603s
[2K
| Adam | epoch: 004 | loss: 0.69249 - acc: 0.5225 -- iter: 352/375
[A[ATraining Step: 48  | total loss: [1m[32m0.69340[0m[0m | time: 9.296s
[2K
| Adam | epoch: 004 | loss: 0.69340 - acc: 0.4988 | val_loss: 0.69033 - val_acc: 0.5847 -- iter: 375/375
--
Training Step: 49  | total loss: [1m[32m0.69427[0m[0m | time: 0.739s
[2K
| Adam | epoch: 005 | loss: 0.69427 - acc: 0.4743 -- iter: 032/375
[A[ATraining Step: 50  | total loss: [1m[32m0.69296[0m[0m | time: 1.484s
[2K
| Adam | epoch: 005 | loss: 0.69296 - acc: 0.5122 -- iter: 064/375
[A[ATraining Step: 51  | total loss: [1m[32m0.69343[0m[0m | time: 2.029s
[2K
| Adam | epoch: 005 | loss: 0.69343 - acc: 0.4961 -- iter: 096/375
[A[ATraining Step: 52  | total loss: [1m[32m0.69307[0m[0m | time: 2.584s
[2K
| Adam | epoch: 005 | loss: 0.69307 - acc: 0.5064 -- iter: 128/375
[A[ATraining Step: 53  | total loss: [1m[32m0.69275[0m[0m | time: 3.313s
[2K
| Adam | epoch: 005 | loss: 0.69275 - acc: 0.5151 -- iter: 160/375
[A[ATraining Step: 54  | total loss: [1m[32m0.69236[0m[0m | time: 4.141s
[2K
| Adam | epoch: 005 | loss: 0.69236 - acc: 0.5265 -- iter: 192/375
[A[ATraining Step: 55  | total loss: [1m[32m0.69215[0m[0m | time: 4.877s
[2K
| Adam | epoch: 005 | loss: 0.69215 - acc: 0.5317 -- iter: 224/375
[A[ATraining Step: 56  | total loss: [1m[32m0.69166[0m[0m | time: 5.691s
[2K
| Adam | epoch: 005 | loss: 0.69166 - acc: 0.5448 -- iter: 256/375
[A[ATraining Step: 57  | total loss: [1m[32m0.69169[0m[0m | time: 6.389s
[2K
| Adam | epoch: 005 | loss: 0.69169 - acc: 0.5429 -- iter: 288/375
[A[ATraining Step: 58  | total loss: [1m[32m0.69170[0m[0m | time: 7.002s
[2K
| Adam | epoch: 005 | loss: 0.69170 - acc: 0.5413 -- iter: 320/375
[A[ATraining Step: 59  | total loss: [1m[32m0.69147[0m[0m | time: 7.625s
[2K
| Adam | epoch: 005 | loss: 0.69147 - acc: 0.5442 -- iter: 352/375
[A[ATraining Step: 60  | total loss: [1m[32m0.69100[0m[0m | time: 9.405s
[2K
| Adam | epoch: 005 | loss: 0.69100 - acc: 0.5507 | val_loss: 0.68768 - val_acc: 0.5847 -- iter: 375/375
--
Training Step: 61  | total loss: [1m[32m0.69231[0m[0m | time: 0.792s
[2K
| Adam | epoch: 006 | loss: 0.69231 - acc: 0.5278 -- iter: 032/375
[A[ATraining Step: 62  | total loss: [1m[32m0.69211[0m[0m | time: 1.637s
[2K
| Adam | epoch: 006 | loss: 0.69211 - acc: 0.5282 -- iter: 064/375
[A[ATraining Step: 63  | total loss: [1m[32m0.69225[0m[0m | time: 2.535s
[2K
| Adam | epoch: 006 | loss: 0.69225 - acc: 0.5247 -- iter: 096/375
[A[ATraining Step: 64  | total loss: [1m[32m0.69355[0m[0m | time: 3.065s
[2K
| Adam | epoch: 006 | loss: 0.69355 - acc: 0.5060 -- iter: 128/375
[A[ATraining Step: 65  | total loss: [1m[32m0.69181[0m[0m | time: 3.635s
[2K
| Adam | epoch: 006 | loss: 0.69181 - acc: 0.5293 -- iter: 160/375
[A[ATraining Step: 66  | total loss: [1m[32m0.69031[0m[0m | time: 4.424s
[2K
| Adam | epoch: 006 | loss: 0.69031 - acc: 0.5496 -- iter: 192/375
[A[ATraining Step: 67  | total loss: [1m[32m0.69073[0m[0m | time: 5.137s
[2K
| Adam | epoch: 006 | loss: 0.69073 - acc: 0.5436 -- iter: 224/375
[A[ATraining Step: 68  | total loss: [1m[32m0.69035[0m[0m | time: 5.869s
[2K
| Adam | epoch: 006 | loss: 0.69035 - acc: 0.5459 -- iter: 256/375
[A[ATraining Step: 69  | total loss: [1m[32m0.68956[0m[0m | time: 6.681s
[2K
| Adam | epoch: 006 | loss: 0.68956 - acc: 0.5515 -- iter: 288/375
[A[ATraining Step: 70  | total loss: [1m[32m0.68859[0m[0m | time: 7.357s
[2K
| Adam | epoch: 006 | loss: 0.68859 - acc: 0.5563 -- iter: 320/375
[A[ATraining Step: 71  | total loss: [1m[32m0.68936[0m[0m | time: 7.976s
[2K
| Adam | epoch: 006 | loss: 0.68936 - acc: 0.5499 -- iter: 352/375
[A[ATraining Step: 72  | total loss: [1m[32m0.68883[0m[0m | time: 9.599s
[2K
| Adam | epoch: 006 | loss: 0.68883 - acc: 0.5513 | val_loss: 0.67866 - val_acc: 0.5847 -- iter: 375/375
--
Training Step: 73  | total loss: [1m[32m0.68997[0m[0m | time: 0.848s
[2K
| Adam | epoch: 007 | loss: 0.68997 - acc: 0.5456 -- iter: 032/375
[A[ATraining Step: 74  | total loss: [1m[32m0.68596[0m[0m | time: 1.591s
[2K
| Adam | epoch: 007 | loss: 0.68596 - acc: 0.5612 -- iter: 064/375
[A[ATraining Step: 75  | total loss: [1m[32m0.68566[0m[0m | time: 2.398s
[2K
| Adam | epoch: 007 | loss: 0.68566 - acc: 0.5613 -- iter: 096/375
[A[ATraining Step: 76  | total loss: [1m[32m0.68662[0m[0m | time: 3.184s
[2K
| Adam | epoch: 007 | loss: 0.68662 - acc: 0.5581 -- iter: 128/375
[A[ATraining Step: 77  | total loss: [1m[32m0.68746[0m[0m | time: 3.727s
[2K
| Adam | epoch: 007 | loss: 0.68746 - acc: 0.5553 -- iter: 160/375
[A[ATraining Step: 78  | total loss: [1m[32m0.68719[0m[0m | time: 4.279s
[2K
| Adam | epoch: 007 | loss: 0.68719 - acc: 0.5563 -- iter: 192/375
[A[ATraining Step: 79  | total loss: [1m[32m0.68671[0m[0m | time: 5.003s
[2K
| Adam | epoch: 007 | loss: 0.68671 - acc: 0.5572 -- iter: 224/375
[A[ATraining Step: 80  | total loss: [1m[32m0.68922[0m[0m | time: 5.743s
[2K
| Adam | epoch: 007 | loss: 0.68922 - acc: 0.5450 -- iter: 256/375
[A[ATraining Step: 81  | total loss: [1m[32m0.69079[0m[0m | time: 6.473s
[2K
| Adam | epoch: 007 | loss: 0.69079 - acc: 0.5341 -- iter: 288/375
[A[ATraining Step: 82  | total loss: [1m[32m0.69104[0m[0m | time: 7.218s
[2K
| Adam | epoch: 007 | loss: 0.69104 - acc: 0.5307 -- iter: 320/375
[A[ATraining Step: 83  | total loss: [1m[32m0.69136[0m[0m | time: 7.969s
[2K
| Adam | epoch: 007 | loss: 0.69136 - acc: 0.5245 -- iter: 352/375
[A[ATraining Step: 84  | total loss: [1m[32m0.69113[0m[0m | time: 9.812s
[2K
| Adam | epoch: 007 | loss: 0.69113 - acc: 0.5283 | val_loss: 0.68877 - val_acc: 0.5847 -- iter: 375/375
--
Training Step: 85  | total loss: [1m[32m0.69130[0m[0m | time: 0.745s
[2K
| Adam | epoch: 008 | loss: 0.69130 - acc: 0.5255 -- iter: 032/375
[A[ATraining Step: 86  | total loss: [1m[32m0.69052[0m[0m | time: 1.527s
[2K
| Adam | epoch: 008 | loss: 0.69052 - acc: 0.5417 -- iter: 064/375
[A[ATraining Step: 87  | total loss: [1m[32m0.69027[0m[0m | time: 2.248s
[2K
| Adam | epoch: 008 | loss: 0.69027 - acc: 0.5469 -- iter: 096/375
[A[ATraining Step: 88  | total loss: [1m[32m0.69101[0m[0m | time: 2.964s
[2K
| Adam | epoch: 008 | loss: 0.69101 - acc: 0.5328 -- iter: 128/375
[A[ATraining Step: 89  | total loss: [1m[32m0.69071[0m[0m | time: 3.658s
[2K
| Adam | epoch: 008 | loss: 0.69071 - acc: 0.5358 -- iter: 160/375
[A[ATraining Step: 90  | total loss: [1m[32m0.69091[0m[0m | time: 4.278s
[2K
| Adam | epoch: 008 | loss: 0.69091 - acc: 0.5322 -- iter: 192/375
[A[ATraining Step: 91  | total loss: [1m[32m0.69029[0m[0m | time: 4.848s
[2K
| Adam | epoch: 008 | loss: 0.69029 - acc: 0.5399 -- iter: 224/375
[A[ATraining Step: 92  | total loss: [1m[32m0.68965[0m[0m | time: 5.574s
[2K
| Adam | epoch: 008 | loss: 0.68965 - acc: 0.5467 -- iter: 256/375
[A[ATraining Step: 93  | total loss: [1m[32m0.68966[0m[0m | time: 6.326s
[2K
| Adam | epoch: 008 | loss: 0.68966 - acc: 0.5452 -- iter: 288/375
[A[ATraining Step: 94  | total loss: [1m[32m0.69046[0m[0m | time: 7.066s
[2K
| Adam | epoch: 008 | loss: 0.69046 - acc: 0.5376 -- iter: 320/375
[A[ATraining Step: 95  | total loss: [1m[32m0.68903[0m[0m | time: 7.837s
[2K
| Adam | epoch: 008 | loss: 0.68903 - acc: 0.5494 -- iter: 352/375
[A[ATraining Step: 96  | total loss: [1m[32m0.68985[0m[0m | time: 9.588s
[2K
| Adam | epoch: 008 | loss: 0.68985 - acc: 0.5414 | val_loss: 0.68146 - val_acc: 0.5847 -- iter: 375/375
--
Training Step: 97  | total loss: [1m[32m0.69230[0m[0m | time: 0.633s
[2K
| Adam | epoch: 009 | loss: 0.69230 - acc: 0.5247 -- iter: 032/375
[A[ATraining Step: 98  | total loss: [1m[32m0.69223[0m[0m | time: 1.246s
[2K
| Adam | epoch: 009 | loss: 0.69223 - acc: 0.5222 -- iter: 064/375
[A[ATraining Step: 99  | total loss: [1m[32m0.69139[0m[0m | time: 2.000s
[2K
| Adam | epoch: 009 | loss: 0.69139 - acc: 0.5263 -- iter: 096/375
[A[ATraining Step: 100  | total loss: [1m[32m0.69085[0m[0m | time: 2.724s
[2K
| Adam | epoch: 009 | loss: 0.69085 - acc: 0.5236 -- iter: 128/375
[A[ATraining Step: 101  | total loss: [1m[32m0.69096[0m[0m | time: 3.489s
[2K
| Adam | epoch: 009 | loss: 0.69096 - acc: 0.5213 -- iter: 160/375
[A[ATraining Step: 102  | total loss: [1m[32m0.69105[0m[0m | time: 4.211s
[2K
| Adam | epoch: 009 | loss: 0.69105 - acc: 0.5129 -- iter: 192/375
[A[ATraining Step: 103  | total loss: [1m[32m0.68977[0m[0m | time: 4.744s
[2K
| Adam | epoch: 009 | loss: 0.68977 - acc: 0.5366 -- iter: 224/375
[A[ATraining Step: 104  | total loss: [1m[32m0.69107[0m[0m | time: 5.313s
[2K
| Adam | epoch: 009 | loss: 0.69107 - acc: 0.5134 -- iter: 256/375
[A[ATraining Step: 105  | total loss: [1m[32m0.69228[0m[0m | time: 6.076s
[2K
| Adam | epoch: 009 | loss: 0.69228 - acc: 0.4838 -- iter: 288/375
[A[ATraining Step: 106  | total loss: [1m[32m0.69098[0m[0m | time: 6.839s
[2K
| Adam | epoch: 009 | loss: 0.69098 - acc: 0.5198 -- iter: 320/375
[A[ATraining Step: 107  | total loss: [1m[32m0.69021[0m[0m | time: 7.603s
[2K
| Adam | epoch: 009 | loss: 0.69021 - acc: 0.5366 -- iter: 352/375
[A[ATraining Step: 108  | total loss: [1m[32m0.68981[0m[0m | time: 9.372s
[2K
| Adam | epoch: 009 | loss: 0.68981 - acc: 0.5454 | val_loss: 0.67638 - val_acc: 0.5847 -- iter: 375/375
--
Training Step: 109  | total loss: [1m[32m0.68954[0m[0m | time: 0.908s
[2K
| Adam | epoch: 010 | loss: 0.68954 - acc: 0.5471 -- iter: 032/375
[A[ATraining Step: 110  | total loss: [1m[32m0.68846[0m[0m | time: 1.574s
[2K
| Adam | epoch: 010 | loss: 0.68846 - acc: 0.5518 -- iter: 064/375
[A[ATraining Step: 111  | total loss: [1m[32m0.68735[0m[0m | time: 2.226s
[2K
| Adam | epoch: 010 | loss: 0.68735 - acc: 0.5560 -- iter: 096/375
[A[ATraining Step: 112  | total loss: [1m[32m0.68692[0m[0m | time: 3.070s
[2K
| Adam | epoch: 010 | loss: 0.68692 - acc: 0.5535 -- iter: 128/375
[A[ATraining Step: 113  | total loss: [1m[32m0.68433[0m[0m | time: 3.798s
[2K
| Adam | epoch: 010 | loss: 0.68433 - acc: 0.5575 -- iter: 160/375
[A[ATraining Step: 114  | total loss: [1m[32m0.68329[0m[0m | time: 4.613s
[2K
| Adam | epoch: 010 | loss: 0.68329 - acc: 0.5518 -- iter: 192/375
[A[ATraining Step: 115  | total loss: [1m[32m0.68453[0m[0m | time: 5.420s
[2K
| Adam | epoch: 010 | loss: 0.68453 - acc: 0.5466 -- iter: 224/375
[A[ATraining Step: 116  | total loss: [1m[32m0.68082[0m[0m | time: 6.061s
[2K
| Adam | epoch: 010 | loss: 0.68082 - acc: 0.5544 -- iter: 256/375
[A[ATraining Step: 117  | total loss: [1m[32m0.68008[0m[0m | time: 6.616s
[2K
| Adam | epoch: 010 | loss: 0.68008 - acc: 0.5555 -- iter: 288/375
[A[ATraining Step: 118  | total loss: [1m[32m0.67775[0m[0m | time: 7.401s
[2K
| Adam | epoch: 010 | loss: 0.67775 - acc: 0.5739 -- iter: 320/375
[A[ATraining Step: 119  | total loss: [1m[32m0.67715[0m[0m | time: 8.127s
[2K
| Adam | epoch: 010 | loss: 0.67715 - acc: 0.5696 -- iter: 352/375
[A[ATraining Step: 120  | total loss: [1m[32m0.67585[0m[0m | time: 9.940s
[2K
| Adam | epoch: 010 | loss: 0.67585 - acc: 0.5783 | val_loss: 0.63601 - val_acc: 0.6356 -- iter: 375/375
--
Training Step: 121  | total loss: [1m[32m0.67426[0m[0m | time: 0.857s
[2K
| Adam | epoch: 011 | loss: 0.67426 - acc: 0.5829 -- iter: 032/375
[A[ATraining Step: 122  | total loss: [1m[32m0.67630[0m[0m | time: 1.477s
[2K
| Adam | epoch: 011 | loss: 0.67630 - acc: 0.5715 -- iter: 064/375
[A[ATraining Step: 123  | total loss: [1m[32m0.67042[0m[0m | time: 2.117s
[2K
| Adam | epoch: 011 | loss: 0.67042 - acc: 0.5738 -- iter: 096/375
[A[ATraining Step: 124  | total loss: [1m[32m0.66472[0m[0m | time: 2.880s
[2K
| Adam | epoch: 011 | loss: 0.66472 - acc: 0.5789 -- iter: 128/375
[A[ATraining Step: 125  | total loss: [1m[32m0.65682[0m[0m | time: 3.639s
[2K
| Adam | epoch: 011 | loss: 0.65682 - acc: 0.5929 -- iter: 160/375
[A[ATraining Step: 126  | total loss: [1m[32m0.64906[0m[0m | time: 4.399s
[2K
| Adam | epoch: 011 | loss: 0.64906 - acc: 0.5961 -- iter: 192/375
[A[ATraining Step: 127  | total loss: [1m[32m0.64537[0m[0m | time: 5.189s
[2K
| Adam | epoch: 011 | loss: 0.64537 - acc: 0.5990 -- iter: 224/375
[A[ATraining Step: 128  | total loss: [1m[32m0.64323[0m[0m | time: 5.939s
[2K
| Adam | epoch: 011 | loss: 0.64323 - acc: 0.6016 -- iter: 256/375
[A[ATraining Step: 129  | total loss: [1m[32m0.63972[0m[0m | time: 6.497s
[2K
| Adam | epoch: 011 | loss: 0.63972 - acc: 0.6070 -- iter: 288/375
[A[ATraining Step: 130  | total loss: [1m[32m0.63017[0m[0m | time: 7.073s
[2K
| Adam | epoch: 011 | loss: 0.63017 - acc: 0.6202 -- iter: 320/375
[A[ATraining Step: 131  | total loss: [1m[32m0.61996[0m[0m | time: 7.849s
[2K
| Adam | epoch: 011 | loss: 0.61996 - acc: 0.6365 -- iter: 352/375
[A[ATraining Step: 132  | total loss: [1m[32m0.60993[0m[0m | time: 9.575s
[2K
| Adam | epoch: 011 | loss: 0.60993 - acc: 0.6478 | val_loss: 0.58568 - val_acc: 0.7542 -- iter: 375/375
--
Training Step: 133  | total loss: [1m[32m0.60055[0m[0m | time: 0.652s
[2K
| Adam | epoch: 012 | loss: 0.60055 - acc: 0.6549 -- iter: 032/375
[A[ATraining Step: 134  | total loss: [1m[32m0.59380[0m[0m | time: 1.261s
[2K
| Adam | epoch: 012 | loss: 0.59380 - acc: 0.6582 -- iter: 064/375
[A[ATraining Step: 135  | total loss: [1m[32m0.63868[0m[0m | time: 1.856s
[2K
| Adam | epoch: 012 | loss: 0.63868 - acc: 0.6424 -- iter: 096/375
[A[ATraining Step: 136  | total loss: [1m[32m0.64471[0m[0m | time: 2.552s
[2K
| Adam | epoch: 012 | loss: 0.64471 - acc: 0.6406 -- iter: 128/375
[A[ATraining Step: 137  | total loss: [1m[32m0.64199[0m[0m | time: 3.271s
[2K
| Adam | epoch: 012 | loss: 0.64199 - acc: 0.6391 -- iter: 160/375
[A[ATraining Step: 138  | total loss: [1m[32m0.62470[0m[0m | time: 4.011s
[2K
| Adam | epoch: 012 | loss: 0.62470 - acc: 0.6564 -- iter: 192/375
[A[ATraining Step: 139  | total loss: [1m[32m0.62608[0m[0m | time: 4.770s
[2K
| Adam | epoch: 012 | loss: 0.62608 - acc: 0.6595 -- iter: 224/375
[A[ATraining Step: 140  | total loss: [1m[32m0.62533[0m[0m | time: 5.477s
[2K
| Adam | epoch: 012 | loss: 0.62533 - acc: 0.6592 -- iter: 256/375
[A[ATraining Step: 141  | total loss: [1m[32m0.61857[0m[0m | time: 6.209s
[2K
| Adam | epoch: 012 | loss: 0.61857 - acc: 0.6620 -- iter: 288/375
[A[ATraining Step: 142  | total loss: [1m[32m0.60834[0m[0m | time: 6.737s
[2K
| Adam | epoch: 012 | loss: 0.60834 - acc: 0.6802 -- iter: 320/375
[A[ATraining Step: 143  | total loss: [1m[32m0.60209[0m[0m | time: 7.233s
[2K
| Adam | epoch: 012 | loss: 0.60209 - acc: 0.6817 -- iter: 352/375
[A[ATraining Step: 144  | total loss: [1m[32m0.59502[0m[0m | time: 8.971s
[2K
| Adam | epoch: 012 | loss: 0.59502 - acc: 0.6788 | val_loss: 0.54741 - val_acc: 0.7712 -- iter: 375/375
--
Training Step: 145  | total loss: [1m[32m0.57985[0m[0m | time: 0.703s
[2K
| Adam | epoch: 013 | loss: 0.57985 - acc: 0.6984 -- iter: 032/375
[A[ATraining Step: 146  | total loss: [1m[32m0.57270[0m[0m | time: 1.330s
[2K
| Adam | epoch: 013 | loss: 0.57270 - acc: 0.7067 -- iter: 064/375
[A[ATraining Step: 147  | total loss: [1m[32m0.56602[0m[0m | time: 2.029s
[2K
| Adam | epoch: 013 | loss: 0.56602 - acc: 0.7110 -- iter: 096/375
[A[ATraining Step: 148  | total loss: [1m[32m0.55553[0m[0m | time: 2.760s
[2K
| Adam | epoch: 013 | loss: 0.55553 - acc: 0.7243 -- iter: 128/375
[A[ATraining Step: 149  | total loss: [1m[32m0.54411[0m[0m | time: 3.481s
[2K
| Adam | epoch: 013 | loss: 0.54411 - acc: 0.7331 -- iter: 160/375
[A[ATraining Step: 150  | total loss: [1m[32m0.53992[0m[0m | time: 4.256s
[2K
| Adam | epoch: 013 | loss: 0.53992 - acc: 0.7442 -- iter: 192/375
[A[ATraining Step: 151  | total loss: [1m[32m0.53933[0m[0m | time: 4.983s
[2K
| Adam | epoch: 013 | loss: 0.53933 - acc: 0.7291 -- iter: 224/375
[A[ATraining Step: 152  | total loss: [1m[32m0.51747[0m[0m | time: 5.745s
[2K
| Adam | epoch: 013 | loss: 0.51747 - acc: 0.7500 -- iter: 256/375
[A[ATraining Step: 153  | total loss: [1m[32m0.50354[0m[0m | time: 6.518s
[2K
| Adam | epoch: 013 | loss: 0.50354 - acc: 0.7562 -- iter: 288/375
[A[ATraining Step: 154  | total loss: [1m[32m0.49282[0m[0m | time: 7.296s
[2K
| Adam | epoch: 013 | loss: 0.49282 - acc: 0.7587 -- iter: 320/375
[A[ATraining Step: 155  | total loss: [1m[32m0.48701[0m[0m | time: 7.857s
[2K
| Adam | epoch: 013 | loss: 0.48701 - acc: 0.7610 -- iter: 352/375
[A[ATraining Step: 156  | total loss: [1m[32m0.47092[0m[0m | time: 9.410s
[2K
| Adam | epoch: 013 | loss: 0.47092 - acc: 0.7718 | val_loss: 0.50662 - val_acc: 0.7797 -- iter: 375/375
--
Training Step: 157  | total loss: [1m[32m0.45317[0m[0m | time: 0.822s
[2K
| Adam | epoch: 014 | loss: 0.45317 - acc: 0.7860 -- iter: 032/375
[A[ATraining Step: 158  | total loss: [1m[32m0.44449[0m[0m | time: 1.438s
[2K
| Adam | epoch: 014 | loss: 0.44449 - acc: 0.7949 -- iter: 064/375
[A[ATraining Step: 159  | total loss: [1m[32m0.43006[0m[0m | time: 2.052s
[2K
| Adam | epoch: 014 | loss: 0.43006 - acc: 0.7998 -- iter: 096/375
[A[ATraining Step: 160  | total loss: [1m[32m0.42767[0m[0m | time: 2.813s
[2K
| Adam | epoch: 014 | loss: 0.42767 - acc: 0.7948 -- iter: 128/375
[A[ATraining Step: 161  | total loss: [1m[32m0.46195[0m[0m | time: 3.543s
[2K
| Adam | epoch: 014 | loss: 0.46195 - acc: 0.7809 -- iter: 160/375
[A[ATraining Step: 162  | total loss: [1m[32m0.47135[0m[0m | time: 4.275s
[2K
| Adam | epoch: 014 | loss: 0.47135 - acc: 0.7778 -- iter: 192/375
[A[ATraining Step: 163  | total loss: [1m[32m0.46271[0m[0m | time: 5.037s
[2K
| Adam | epoch: 014 | loss: 0.46271 - acc: 0.7813 -- iter: 224/375
[A[ATraining Step: 164  | total loss: [1m[32m0.45366[0m[0m | time: 5.795s
[2K
| Adam | epoch: 014 | loss: 0.45366 - acc: 0.7907 -- iter: 256/375
[A[ATraining Step: 165  | total loss: [1m[32m0.44807[0m[0m | time: 6.521s
[2K
| Adam | epoch: 014 | loss: 0.44807 - acc: 0.7960 -- iter: 288/375
[A[ATraining Step: 166  | total loss: [1m[32m0.42705[0m[0m | time: 7.247s
[2K
| Adam | epoch: 014 | loss: 0.42705 - acc: 0.8070 -- iter: 320/375
[A[ATraining Step: 167  | total loss: [1m[32m0.41790[0m[0m | time: 8.003s
[2K
| Adam | epoch: 014 | loss: 0.41790 - acc: 0.8076 -- iter: 352/375
[A[ATraining Step: 168  | total loss: [1m[32m0.42742[0m[0m | time: 9.570s
[2K
| Adam | epoch: 014 | loss: 0.42742 - acc: 0.8112 | val_loss: 0.47757 - val_acc: 0.7881 -- iter: 375/375
--
Training Step: 169  | total loss: [1m[32m0.40979[0m[0m | time: 0.562s
[2K
| Adam | epoch: 015 | loss: 0.40979 - acc: 0.8170 -- iter: 032/375
[A[ATraining Step: 170  | total loss: [1m[32m0.39016[0m[0m | time: 1.196s
[2K
| Adam | epoch: 015 | loss: 0.39016 - acc: 0.8310 -- iter: 064/375
[A[ATraining Step: 171  | total loss: [1m[32m0.38866[0m[0m | time: 1.809s
[2K
| Adam | epoch: 015 | loss: 0.38866 - acc: 0.8322 -- iter: 096/375
[A[ATraining Step: 172  | total loss: [1m[32m0.40068[0m[0m | time: 2.416s
[2K
| Adam | epoch: 015 | loss: 0.40068 - acc: 0.8303 -- iter: 128/375
[A[ATraining Step: 173  | total loss: [1m[32m0.40522[0m[0m | time: 3.043s
[2K
| Adam | epoch: 015 | loss: 0.40522 - acc: 0.8222 -- iter: 160/375
[A[ATraining Step: 174  | total loss: [1m[32m0.39640[0m[0m | time: 3.785s
[2K
| Adam | epoch: 015 | loss: 0.39640 - acc: 0.8244 -- iter: 192/375
[A[ATraining Step: 175  | total loss: [1m[32m0.38797[0m[0m | time: 4.543s
[2K
| Adam | epoch: 015 | loss: 0.38797 - acc: 0.8263 -- iter: 224/375
[A[ATraining Step: 176  | total loss: [1m[32m0.38425[0m[0m | time: 5.333s
[2K
| Adam | epoch: 015 | loss: 0.38425 - acc: 0.8187 -- iter: 256/375
[A[ATraining Step: 177  | total loss: [1m[32m0.37712[0m[0m | time: 6.065s
[2K
| Adam | epoch: 015 | loss: 0.37712 - acc: 0.8181 -- iter: 288/375
[A[ATraining Step: 178  | total loss: [1m[32m0.37837[0m[0m | time: 6.886s
[2K
| Adam | epoch: 015 | loss: 0.37837 - acc: 0.8238 -- iter: 320/375
[A[ATraining Step: 179  | total loss: [1m[32m0.37732[0m[0m | time: 7.502s
[2K
| Adam | epoch: 015 | loss: 0.37732 - acc: 0.8258 -- iter: 352/375
[A[ATraining Step: 180  | total loss: [1m[32m0.35880[0m[0m | time: 9.180s
[2K
| Adam | epoch: 015 | loss: 0.35880 - acc: 0.8338 | val_loss: 0.44924 - val_acc: 0.8051 -- iter: 375/375
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.8745933155871044
Validation AUPRC:0.8179132508771008
Test AUC:0.9026470588235294
Test AUPRC:0.8653996785800526
BestTestF1Score	0.8	0.65	0.83	0.8	0.8	40	10	58	10	0.69
BestTestMCCScore	0.8	0.65	0.83	0.8	0.8	40	10	58	10	0.69
BestTestAccuracyScore	0.8	0.65	0.83	0.8	0.8	40	10	58	10	0.69
BestValidationF1Score	0.79	0.67	0.84	0.86	0.73	36	6	63	13	0.69
BestValidationMCC	0.79	0.67	0.84	0.86	0.73	36	6	63	13	0.69
BestValidationAccuracy	0.79	0.67	0.84	0.86	0.73	36	6	63	13	0.69
TestPredictions (Threshold:0.69)
CHEMBL193377,TN,INACT,0.3100000023841858	CHEMBL3640903,TP,ACT,0.9700000286102295	CHEMBL1257895,TN,INACT,0.009999999776482582	CHEMBL3745944,TN,INACT,0.25	CHEMBL1387,TP,ACT,0.8999999761581421	CHEMBL1085,TP,ACT,0.8700000047683716	CHEMBL261406,FP,INACT,0.9100000262260437	CHEMBL218181,TP,ACT,0.8600000143051147	CHEMBL83953,TN,INACT,0.36000001430511475	CHEMBL1200624,FN,ACT,0.07000000029802322	CHEMBL325172,FP,INACT,0.8799999952316284	CHEMBL3360848,FP,INACT,0.9200000166893005	CHEMBL378733,TP,ACT,0.9399999976158142	CHEMBL208088,TP,ACT,0.9399999976158142	CHEMBL3640888,TP,ACT,0.800000011920929	CHEMBL633,TN,INACT,0.4699999988079071	CHEMBL300629,TN,INACT,0.3199999928474426	CHEMBL394913,TN,INACT,0.1599999964237213	CHEMBL2204698,TN,INACT,0.019999999552965164	CHEMBL108861,TN,INACT,0.23999999463558197	CHEMBL3640824,TP,ACT,0.9599999785423279	CHEMBL370039,TN,INACT,0.05999999865889549	CHEMBL371356,TP,ACT,0.9200000166893005	CHEMBL425941,FP,INACT,0.75	CHEMBL121992,TP,ACT,0.9200000166893005	CHEMBL1532659,TN,INACT,0.2199999988079071	CHEMBL3640902,FN,ACT,0.5199999809265137	CHEMBL3397549,TN,INACT,0.029999999329447746	CHEMBL46403,TN,INACT,0.009999999776482582	CHEMBL3640836,TP,ACT,0.8100000023841858	CHEMBL3640889,TP,ACT,0.9399999976158142	CHEMBL1956359,TN,INACT,0.23000000417232513	CHEMBL3753560,TN,INACT,0.3100000023841858	CHEMBL482593,TN,INACT,0.009999999776482582	CHEMBL1393548,TN,INACT,0.2800000011920929	CHEMBL110739,TN,INACT,0.6399999856948853	CHEMBL2348903,TN,INACT,0.5299999713897705	CHEMBL2337126,TN,INACT,0.3700000047683716	CHEMBL139837,FN,ACT,0.47999998927116394	CHEMBL202733,TN,INACT,0.10999999940395355	CHEMBL237241,TN,INACT,0.009999999776482582	CHEMBL371320,TN,INACT,0.05999999865889549	CHEMBL3640883,TP,ACT,0.8700000047683716	CHEMBL260315,FP,INACT,0.7699999809265137	CHEMBL1200585,FN,ACT,0.5899999737739563	CHEMBL503,FP,INACT,0.8199999928474426	CHEMBL143533,TN,INACT,0.1599999964237213	CHEMBL3640860,TP,ACT,0.7400000095367432	CHEMBL3109602,FP,INACT,0.9300000071525574	CHEMBL124737,TP,ACT,0.8700000047683716	CHEMBL2070860,TN,INACT,0.07999999821186066	CHEMBL1910408,TP,ACT,0.8999999761581421	CHEMBL366162,TP,ACT,0.9100000262260437	CHEMBL1505049,TN,INACT,0.07000000029802322	CHEMBL419767,TP,ACT,0.8999999761581421	CHEMBL178634,FN,ACT,0.5299999713897705	CHEMBL142221,TN,INACT,0.30000001192092896	CHEMBL3640825,TP,ACT,0.7900000214576721	CHEMBL207518,TP,ACT,0.9300000071525574	CHEMBL464360,FP,INACT,0.8899999856948853	CHEMBL1201151,FN,ACT,0.6499999761581421	CHEMBL1080699,TN,INACT,0.10000000149011612	CHEMBL2335811,FP,INACT,0.7099999785423279	CHEMBL483032,TN,INACT,0.30000001192092896	CHEMBL1627757,TN,INACT,0.5600000023841858	CHEMBL757,TP,ACT,0.9300000071525574	CHEMBL375111,TP,ACT,0.8700000047683716	CHEMBL2070862,FP,INACT,0.7300000190734863	CHEMBL3263233,TN,INACT,0.10000000149011612	CHEMBL275638,TN,INACT,0.25	CHEMBL3640843,TP,ACT,0.9599999785423279	CHEMBL3640821,FN,ACT,0.41999998688697815	CHEMBL1173474,TN,INACT,0.009999999776482582	CHEMBL1629810,TP,ACT,0.800000011920929	CHEMBL1087253,TN,INACT,0.05999999865889549	CHEMBL341272,TP,ACT,0.9599999785423279	CHEMBL1628012,TN,INACT,0.4399999976158142	CHEMBL2348917,TN,INACT,0.15000000596046448	CHEMBL1539325,TN,INACT,0.5899999737739563	CHEMBL218417,TP,ACT,0.8899999856948853	CHEMBL3640887,TP,ACT,0.9100000262260437	CHEMBL3640820,TP,ACT,0.9599999785423279	CHEMBL3640845,FN,ACT,0.4000000059604645	CHEMBL236339,TN,INACT,0.009999999776482582	CHEMBL3640908,FN,ACT,0.07000000029802322	CHEMBL220503,TP,ACT,0.7900000214576721	CHEMBL477940,TN,INACT,0.17000000178813934	CHEMBL2348879,TN,INACT,0.3100000023841858	CHEMBL2011545,TN,INACT,0.019999999552965164	CHEMBL2203301,TN,INACT,0.10999999940395355	CHEMBL378087,TP,ACT,0.949999988079071	CHEMBL3640910,TP,ACT,0.949999988079071	CHEMBL111077,TN,INACT,0.009999999776482582	CHEMBL179525,TP,ACT,0.7400000095367432	CHEMBL3640882,TP,ACT,0.8600000143051147	CHEMBL2070870,TN,INACT,0.18000000715255737	CHEMBL484264,TN,INACT,0.05999999865889549	CHEMBL1201146,TP,ACT,0.8299999833106995	CHEMBL2070851,TN,INACT,0.03999999910593033	CHEMBL3640859,TP,ACT,0.9599999785423279	CHEMBL108766,TN,INACT,0.009999999776482582	CHEMBL3189499,TN,INACT,0.029999999329447746	CHEMBL371077,TN,INACT,0.6600000262260437	CHEMBL483035,TN,INACT,0.3799999952316284	CHEMBL197595,FN,ACT,0.5400000214576721	CHEMBL491820,TN,INACT,0.3100000023841858	CHEMBL2437030,TN,INACT,0.029999999329447746	CHEMBL3640855,TP,ACT,0.9599999785423279	CHEMBL196731,TN,INACT,0.14000000059604645	CHEMBL551185,TN,INACT,0.029999999329447746	CHEMBL2204699,TN,INACT,0.09000000357627869	CHEMBL195836,TP,ACT,0.9200000166893005	CHEMBL219167,TP,ACT,0.8600000143051147	CHEMBL331820,TP,ACT,0.8799999952316284	CHEMBL119869,TN,INACT,0.05999999865889549	CHEMBL421,TN,INACT,0.18000000715255737	CHEMBL428309,TP,ACT,0.8199999928474426	CHEMBL2204687,TN,INACT,0.03999999910593033	

