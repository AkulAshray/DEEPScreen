CNNModel CHEMBL5409 adam 0.0005 15 32 0 0.8 False True
Number of active compounds :	221
Number of inactive compounds :	221
---------------------------------
Run id: CNNModel_CHEMBL5409_adam_0.0005_15_32_0_0.8_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL5409_adam_0.0005_15_32_0.8_True/
---------------------------------
Training samples: 263
Validation samples: 83
--
Training Step: 1  | time: 0.819s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/263
[A[ATraining Step: 2  | total loss: [1m[32m0.62383[0m[0m | time: 1.435s
[2K
| Adam | epoch: 001 | loss: 0.62383 - acc: 0.4781 -- iter: 064/263
[A[ATraining Step: 3  | total loss: [1m[32m0.67941[0m[0m | time: 2.225s
[2K
| Adam | epoch: 001 | loss: 0.67941 - acc: 0.5472 -- iter: 096/263
[A[ATraining Step: 4  | total loss: [1m[32m0.68853[0m[0m | time: 2.831s
[2K
| Adam | epoch: 001 | loss: 0.68853 - acc: 0.5352 -- iter: 128/263
[A[ATraining Step: 5  | total loss: [1m[32m0.69503[0m[0m | time: 3.434s
[2K
| Adam | epoch: 001 | loss: 0.69503 - acc: 0.4892 -- iter: 160/263
[A[ATraining Step: 6  | total loss: [1m[32m0.68839[0m[0m | time: 4.041s
[2K
| Adam | epoch: 001 | loss: 0.68839 - acc: 0.5564 -- iter: 192/263
[A[ATraining Step: 7  | total loss: [1m[32m0.68945[0m[0m | time: 4.666s
[2K
| Adam | epoch: 001 | loss: 0.68945 - acc: 0.5413 -- iter: 224/263
[A[ATraining Step: 8  | total loss: [1m[32m0.69834[0m[0m | time: 5.283s
[2K
| Adam | epoch: 001 | loss: 0.69834 - acc: 0.4829 -- iter: 256/263
[A[ATraining Step: 9  | total loss: [1m[32m0.69365[0m[0m | time: 6.490s
[2K
| Adam | epoch: 001 | loss: 0.69365 - acc: 0.5085 | val_loss: 0.69693 - val_acc: 0.4699 -- iter: 263/263
--
Training Step: 10  | total loss: [1m[32m0.68472[0m[0m | time: 0.184s
[2K
| Adam | epoch: 002 | loss: 0.68472 - acc: 0.6114 -- iter: 032/263
[A[ATraining Step: 11  | total loss: [1m[32m0.67751[0m[0m | time: 0.811s
[2K
| Adam | epoch: 002 | loss: 0.67751 - acc: 0.6601 -- iter: 064/263
[A[ATraining Step: 12  | total loss: [1m[32m0.67930[0m[0m | time: 1.421s
[2K
| Adam | epoch: 002 | loss: 0.67930 - acc: 0.6162 -- iter: 096/263
[A[ATraining Step: 13  | total loss: [1m[32m0.68211[0m[0m | time: 2.028s
[2K
| Adam | epoch: 002 | loss: 0.68211 - acc: 0.5798 -- iter: 128/263
[A[ATraining Step: 14  | total loss: [1m[32m0.68598[0m[0m | time: 2.662s
[2K
| Adam | epoch: 002 | loss: 0.68598 - acc: 0.5599 -- iter: 160/263
[A[ATraining Step: 15  | total loss: [1m[32m0.67920[0m[0m | time: 3.277s
[2K
| Adam | epoch: 002 | loss: 0.67920 - acc: 0.5732 -- iter: 192/263
[A[ATraining Step: 16  | total loss: [1m[32m0.68169[0m[0m | time: 3.869s
[2K
| Adam | epoch: 002 | loss: 0.68169 - acc: 0.5692 -- iter: 224/263
[A[ATraining Step: 17  | total loss: [1m[32m0.69211[0m[0m | time: 4.484s
[2K
| Adam | epoch: 002 | loss: 0.69211 - acc: 0.5443 -- iter: 256/263
[A[ATraining Step: 18  | total loss: [1m[32m0.70146[0m[0m | time: 6.085s
[2K
| Adam | epoch: 002 | loss: 0.70146 - acc: 0.5073 | val_loss: 0.69704 - val_acc: 0.4699 -- iter: 263/263
--
Training Step: 19  | total loss: [1m[32m0.70402[0m[0m | time: 0.170s
[2K
| Adam | epoch: 003 | loss: 0.70402 - acc: 0.4840 -- iter: 032/263
[A[ATraining Step: 20  | total loss: [1m[32m0.70288[0m[0m | time: 0.347s
[2K
| Adam | epoch: 003 | loss: 0.70288 - acc: 0.4662 -- iter: 064/263
[A[ATraining Step: 21  | total loss: [1m[32m0.70062[0m[0m | time: 0.955s
[2K
| Adam | epoch: 003 | loss: 0.70062 - acc: 0.4545 -- iter: 096/263
[A[ATraining Step: 22  | total loss: [1m[32m0.69847[0m[0m | time: 1.715s
[2K
| Adam | epoch: 003 | loss: 0.69847 - acc: 0.4588 -- iter: 128/263
[A[ATraining Step: 23  | total loss: [1m[32m0.69556[0m[0m | time: 2.321s
[2K
| Adam | epoch: 003 | loss: 0.69556 - acc: 0.5070 -- iter: 160/263
[A[ATraining Step: 24  | total loss: [1m[32m0.69382[0m[0m | time: 2.935s
[2K
| Adam | epoch: 003 | loss: 0.69382 - acc: 0.5402 -- iter: 192/263
[A[ATraining Step: 25  | total loss: [1m[32m0.69286[0m[0m | time: 3.536s
[2K
| Adam | epoch: 003 | loss: 0.69286 - acc: 0.5633 -- iter: 224/263
[A[ATraining Step: 26  | total loss: [1m[32m0.69266[0m[0m | time: 4.137s
[2K
| Adam | epoch: 003 | loss: 0.69266 - acc: 0.5548 -- iter: 256/263
[A[ATraining Step: 27  | total loss: [1m[32m0.69242[0m[0m | time: 5.746s
[2K
| Adam | epoch: 003 | loss: 0.69242 - acc: 0.5568 | val_loss: 0.69341 - val_acc: 0.4699 -- iter: 263/263
--
Training Step: 28  | total loss: [1m[32m0.69235[0m[0m | time: 0.618s
[2K
| Adam | epoch: 004 | loss: 0.69235 - acc: 0.5504 -- iter: 032/263
[A[ATraining Step: 29  | total loss: [1m[32m0.69217[0m[0m | time: 0.801s
[2K
| Adam | epoch: 004 | loss: 0.69217 - acc: 0.5534 -- iter: 064/263
[A[ATraining Step: 30  | total loss: [1m[32m0.69256[0m[0m | time: 0.979s
[2K
| Adam | epoch: 004 | loss: 0.69256 - acc: 0.5238 -- iter: 096/263
[A[ATraining Step: 31  | total loss: [1m[32m0.69293[0m[0m | time: 1.592s
[2K
| Adam | epoch: 004 | loss: 0.69293 - acc: 0.5018 -- iter: 128/263
[A[ATraining Step: 32  | total loss: [1m[32m0.69242[0m[0m | time: 2.183s
[2K
| Adam | epoch: 004 | loss: 0.69242 - acc: 0.5225 -- iter: 160/263
[A[ATraining Step: 33  | total loss: [1m[32m0.69189[0m[0m | time: 2.808s
[2K
| Adam | epoch: 004 | loss: 0.69189 - acc: 0.5450 -- iter: 192/263
[A[ATraining Step: 34  | total loss: [1m[32m0.69252[0m[0m | time: 3.433s
[2K
| Adam | epoch: 004 | loss: 0.69252 - acc: 0.5086 -- iter: 224/263
[A[ATraining Step: 35  | total loss: [1m[32m0.69226[0m[0m | time: 4.052s
[2K
| Adam | epoch: 004 | loss: 0.69226 - acc: 0.5199 -- iter: 256/263
[A[ATraining Step: 36  | total loss: [1m[32m0.69234[0m[0m | time: 5.687s
[2K
| Adam | epoch: 004 | loss: 0.69234 - acc: 0.5094 | val_loss: 0.69244 - val_acc: 0.4699 -- iter: 263/263
--
Training Step: 37  | total loss: [1m[32m0.69183[0m[0m | time: 0.624s
[2K
| Adam | epoch: 005 | loss: 0.69183 - acc: 0.5263 -- iter: 032/263
[A[ATraining Step: 38  | total loss: [1m[32m0.69138[0m[0m | time: 1.259s
[2K
| Adam | epoch: 005 | loss: 0.69138 - acc: 0.5334 -- iter: 064/263
[A[ATraining Step: 39  | total loss: [1m[32m0.69102[0m[0m | time: 1.438s
[2K
| Adam | epoch: 005 | loss: 0.69102 - acc: 0.5449 -- iter: 096/263
[A[ATraining Step: 40  | total loss: [1m[32m0.69065[0m[0m | time: 1.598s
[2K
| Adam | epoch: 005 | loss: 0.69065 - acc: 0.5499 -- iter: 128/263
[A[ATraining Step: 41  | total loss: [1m[32m0.69033[0m[0m | time: 2.209s
[2K
| Adam | epoch: 005 | loss: 0.69033 - acc: 0.5539 -- iter: 160/263
[A[ATraining Step: 42  | total loss: [1m[32m0.69012[0m[0m | time: 2.831s
[2K
| Adam | epoch: 005 | loss: 0.69012 - acc: 0.5498 -- iter: 192/263
[A[ATraining Step: 43  | total loss: [1m[32m0.68956[0m[0m | time: 3.431s
[2K
| Adam | epoch: 005 | loss: 0.68956 - acc: 0.5410 -- iter: 224/263
[A[ATraining Step: 44  | total loss: [1m[32m0.68912[0m[0m | time: 4.058s
[2K
| Adam | epoch: 005 | loss: 0.68912 - acc: 0.5285 -- iter: 256/263
[A[ATraining Step: 45  | total loss: [1m[32m0.68905[0m[0m | time: 5.675s
[2K
| Adam | epoch: 005 | loss: 0.68905 - acc: 0.5183 | val_loss: 0.68338 - val_acc: 0.5663 -- iter: 263/263
--
Training Step: 46  | total loss: [1m[32m0.68777[0m[0m | time: 0.613s
[2K
| Adam | epoch: 006 | loss: 0.68777 - acc: 0.5413 -- iter: 032/263
[A[ATraining Step: 47  | total loss: [1m[32m0.68484[0m[0m | time: 1.239s
[2K
| Adam | epoch: 006 | loss: 0.68484 - acc: 0.5653 -- iter: 064/263
[A[ATraining Step: 48  | total loss: [1m[32m0.68304[0m[0m | time: 1.842s
[2K
| Adam | epoch: 006 | loss: 0.68304 - acc: 0.5548 -- iter: 096/263
[A[ATraining Step: 49  | total loss: [1m[32m0.67842[0m[0m | time: 2.023s
[2K
| Adam | epoch: 006 | loss: 0.67842 - acc: 0.5708 -- iter: 128/263
[A[ATraining Step: 50  | total loss: [1m[32m0.67836[0m[0m | time: 2.201s
[2K
| Adam | epoch: 006 | loss: 0.67836 - acc: 0.5709 -- iter: 160/263
[A[ATraining Step: 51  | total loss: [1m[32m0.67654[0m[0m | time: 2.803s
[2K
| Adam | epoch: 006 | loss: 0.67654 - acc: 0.5710 -- iter: 192/263
[A[ATraining Step: 52  | total loss: [1m[32m0.67066[0m[0m | time: 3.439s
[2K
| Adam | epoch: 006 | loss: 0.67066 - acc: 0.5978 -- iter: 224/263
[A[ATraining Step: 53  | total loss: [1m[32m0.66425[0m[0m | time: 4.055s
[2K
| Adam | epoch: 006 | loss: 0.66425 - acc: 0.6203 -- iter: 256/263
[A[ATraining Step: 54  | total loss: [1m[32m0.66061[0m[0m | time: 5.661s
[2K
| Adam | epoch: 006 | loss: 0.66061 - acc: 0.6164 | val_loss: 0.62617 - val_acc: 0.6867 -- iter: 263/263
--
Training Step: 55  | total loss: [1m[32m0.66109[0m[0m | time: 0.608s
[2K
| Adam | epoch: 007 | loss: 0.66109 - acc: 0.6087 -- iter: 032/263
[A[ATraining Step: 56  | total loss: [1m[32m0.65353[0m[0m | time: 1.257s
[2K
| Adam | epoch: 007 | loss: 0.65353 - acc: 0.6154 -- iter: 064/263
[A[ATraining Step: 57  | total loss: [1m[32m0.64591[0m[0m | time: 1.871s
[2K
| Adam | epoch: 007 | loss: 0.64591 - acc: 0.6297 -- iter: 096/263
[A[ATraining Step: 58  | total loss: [1m[32m0.63459[0m[0m | time: 2.476s
[2K
| Adam | epoch: 007 | loss: 0.63459 - acc: 0.6504 -- iter: 128/263
[A[ATraining Step: 59  | total loss: [1m[32m0.62235[0m[0m | time: 2.642s
[2K
| Adam | epoch: 007 | loss: 0.62235 - acc: 0.6554 -- iter: 160/263
[A[ATraining Step: 60  | total loss: [1m[32m0.61145[0m[0m | time: 2.821s
[2K
| Adam | epoch: 007 | loss: 0.61145 - acc: 0.6821 -- iter: 192/263
[A[ATraining Step: 61  | total loss: [1m[32m0.58588[0m[0m | time: 3.427s
[2K
| Adam | epoch: 007 | loss: 0.58588 - acc: 0.7049 -- iter: 224/263
[A[ATraining Step: 62  | total loss: [1m[32m0.62081[0m[0m | time: 4.028s
[2K
| Adam | epoch: 007 | loss: 0.62081 - acc: 0.6786 -- iter: 256/263
[A[ATraining Step: 63  | total loss: [1m[32m0.67488[0m[0m | time: 5.655s
[2K
| Adam | epoch: 007 | loss: 0.67488 - acc: 0.6559 | val_loss: 0.58804 - val_acc: 0.6867 -- iter: 263/263
--
Training Step: 64  | total loss: [1m[32m0.67431[0m[0m | time: 0.605s
[2K
| Adam | epoch: 008 | loss: 0.67431 - acc: 0.6482 -- iter: 032/263
[A[ATraining Step: 65  | total loss: [1m[32m0.63831[0m[0m | time: 1.207s
[2K
| Adam | epoch: 008 | loss: 0.63831 - acc: 0.6761 -- iter: 064/263
[A[ATraining Step: 66  | total loss: [1m[32m0.62212[0m[0m | time: 1.824s
[2K
| Adam | epoch: 008 | loss: 0.62212 - acc: 0.6889 -- iter: 096/263
[A[ATraining Step: 67  | total loss: [1m[32m0.61231[0m[0m | time: 2.506s
[2K
| Adam | epoch: 008 | loss: 0.61231 - acc: 0.6850 -- iter: 128/263
[A[ATraining Step: 68  | total loss: [1m[32m0.60045[0m[0m | time: 3.108s
[2K
| Adam | epoch: 008 | loss: 0.60045 - acc: 0.7001 -- iter: 160/263
[A[ATraining Step: 69  | total loss: [1m[32m0.59652[0m[0m | time: 3.284s
[2K
| Adam | epoch: 008 | loss: 0.59652 - acc: 0.7059 -- iter: 192/263
[A[ATraining Step: 70  | total loss: [1m[32m0.61153[0m[0m | time: 3.463s
[2K
| Adam | epoch: 008 | loss: 0.61153 - acc: 0.6739 -- iter: 224/263
[A[ATraining Step: 71  | total loss: [1m[32m0.62054[0m[0m | time: 4.076s
[2K
| Adam | epoch: 008 | loss: 0.62054 - acc: 0.6622 -- iter: 256/263
[A[ATraining Step: 72  | total loss: [1m[32m0.61548[0m[0m | time: 5.731s
[2K
| Adam | epoch: 008 | loss: 0.61548 - acc: 0.6686 | val_loss: 0.61899 - val_acc: 0.6506 -- iter: 263/263
--
Training Step: 73  | total loss: [1m[32m0.60490[0m[0m | time: 0.619s
[2K
| Adam | epoch: 009 | loss: 0.60490 - acc: 0.6881 -- iter: 032/263
[A[ATraining Step: 74  | total loss: [1m[32m0.60317[0m[0m | time: 1.235s
[2K
| Adam | epoch: 009 | loss: 0.60317 - acc: 0.6846 -- iter: 064/263
[A[ATraining Step: 75  | total loss: [1m[32m0.58588[0m[0m | time: 1.841s
[2K
| Adam | epoch: 009 | loss: 0.58588 - acc: 0.7052 -- iter: 096/263
[A[ATraining Step: 76  | total loss: [1m[32m0.57519[0m[0m | time: 2.442s
[2K
| Adam | epoch: 009 | loss: 0.57519 - acc: 0.7134 -- iter: 128/263
[A[ATraining Step: 77  | total loss: [1m[32m0.56719[0m[0m | time: 3.051s
[2K
| Adam | epoch: 009 | loss: 0.56719 - acc: 0.7206 -- iter: 160/263
[A[ATraining Step: 78  | total loss: [1m[32m0.56678[0m[0m | time: 3.828s
[2K
| Adam | epoch: 009 | loss: 0.56678 - acc: 0.7138 -- iter: 192/263
[A[ATraining Step: 79  | total loss: [1m[32m0.55163[0m[0m | time: 4.100s
[2K
| Adam | epoch: 009 | loss: 0.55163 - acc: 0.7273 -- iter: 224/263
[A[ATraining Step: 80  | total loss: [1m[32m0.54350[0m[0m | time: 4.386s
[2K
| Adam | epoch: 009 | loss: 0.54350 - acc: 0.7405 -- iter: 256/263
[A[ATraining Step: 81  | total loss: [1m[32m0.52901[0m[0m | time: 6.439s
[2K
| Adam | epoch: 009 | loss: 0.52901 - acc: 0.7523 | val_loss: 0.62506 - val_acc: 0.7108 -- iter: 263/263
--
Training Step: 82  | total loss: [1m[32m0.51436[0m[0m | time: 1.037s
[2K
| Adam | epoch: 010 | loss: 0.51436 - acc: 0.7615 -- iter: 032/263
[A[ATraining Step: 83  | total loss: [1m[32m0.50830[0m[0m | time: 1.788s
[2K
| Adam | epoch: 010 | loss: 0.50830 - acc: 0.7635 -- iter: 064/263
[A[ATraining Step: 84  | total loss: [1m[32m0.51051[0m[0m | time: 2.406s
[2K
| Adam | epoch: 010 | loss: 0.51051 - acc: 0.7559 -- iter: 096/263
[A[ATraining Step: 85  | total loss: [1m[32m0.50166[0m[0m | time: 3.253s
[2K
| Adam | epoch: 010 | loss: 0.50166 - acc: 0.7584 -- iter: 128/263
[A[ATraining Step: 86  | total loss: [1m[32m0.48627[0m[0m | time: 4.273s
[2K
| Adam | epoch: 010 | loss: 0.48627 - acc: 0.7669 -- iter: 160/263
[A[ATraining Step: 87  | total loss: [1m[32m0.47522[0m[0m | time: 5.319s
[2K
| Adam | epoch: 010 | loss: 0.47522 - acc: 0.7652 -- iter: 192/263
[A[ATraining Step: 88  | total loss: [1m[32m0.47973[0m[0m | time: 6.292s
[2K
| Adam | epoch: 010 | loss: 0.47973 - acc: 0.7606 -- iter: 224/263
[A[ATraining Step: 89  | total loss: [1m[32m0.45349[0m[0m | time: 6.518s
[2K
| Adam | epoch: 010 | loss: 0.45349 - acc: 0.7752 -- iter: 256/263
[A[ATraining Step: 90  | total loss: [1m[32m0.47594[0m[0m | time: 7.745s
[2K
| Adam | epoch: 010 | loss: 0.47594 - acc: 0.7548 | val_loss: 0.57124 - val_acc: 0.7349 -- iter: 263/263
--
Training Step: 91  | total loss: [1m[32m0.49026[0m[0m | time: 1.092s
[2K
| Adam | epoch: 011 | loss: 0.49026 - acc: 0.7507 -- iter: 032/263
[A[ATraining Step: 92  | total loss: [1m[32m0.46733[0m[0m | time: 2.155s
[2K
| Adam | epoch: 011 | loss: 0.46733 - acc: 0.7632 -- iter: 064/263
[A[ATraining Step: 93  | total loss: [1m[32m0.46018[0m[0m | time: 3.102s
[2K
| Adam | epoch: 011 | loss: 0.46018 - acc: 0.7650 -- iter: 096/263
[A[ATraining Step: 94  | total loss: [1m[32m0.44564[0m[0m | time: 4.048s
[2K
| Adam | epoch: 011 | loss: 0.44564 - acc: 0.7728 -- iter: 128/263
[A[ATraining Step: 95  | total loss: [1m[32m0.42999[0m[0m | time: 5.071s
[2K
| Adam | epoch: 011 | loss: 0.42999 - acc: 0.7831 -- iter: 160/263
[A[ATraining Step: 96  | total loss: [1m[32m0.41777[0m[0m | time: 6.039s
[2K
| Adam | epoch: 011 | loss: 0.41777 - acc: 0.7923 -- iter: 192/263
[A[ATraining Step: 97  | total loss: [1m[32m0.40939[0m[0m | time: 7.129s
[2K
| Adam | epoch: 011 | loss: 0.40939 - acc: 0.7974 -- iter: 224/263
[A[ATraining Step: 98  | total loss: [1m[32m0.40169[0m[0m | time: 8.230s
[2K
| Adam | epoch: 011 | loss: 0.40169 - acc: 0.8052 -- iter: 256/263
[A[ATraining Step: 99  | total loss: [1m[32m0.39698[0m[0m | time: 9.447s
[2K
| Adam | epoch: 011 | loss: 0.39698 - acc: 0.8028 | val_loss: 0.45564 - val_acc: 0.7711 -- iter: 263/263
--
Training Step: 100  | total loss: [1m[32m0.40012[0m[0m | time: 0.285s
[2K
| Adam | epoch: 012 | loss: 0.40012 - acc: 0.8082 -- iter: 032/263
[A[ATraining Step: 101  | total loss: [1m[32m0.39920[0m[0m | time: 1.275s
[2K
| Adam | epoch: 012 | loss: 0.39920 - acc: 0.7988 -- iter: 064/263
[A[ATraining Step: 102  | total loss: [1m[32m0.40288[0m[0m | time: 2.263s
[2K
| Adam | epoch: 012 | loss: 0.40288 - acc: 0.7971 -- iter: 096/263
[A[ATraining Step: 103  | total loss: [1m[32m0.37879[0m[0m | time: 3.422s
[2K
| Adam | epoch: 012 | loss: 0.37879 - acc: 0.8142 -- iter: 128/263
[A[ATraining Step: 104  | total loss: [1m[32m0.37676[0m[0m | time: 4.347s
[2K
| Adam | epoch: 012 | loss: 0.37676 - acc: 0.8172 -- iter: 160/263
[A[ATraining Step: 105  | total loss: [1m[32m0.36286[0m[0m | time: 5.141s
[2K
| Adam | epoch: 012 | loss: 0.36286 - acc: 0.8230 -- iter: 192/263
[A[ATraining Step: 106  | total loss: [1m[32m0.34718[0m[0m | time: 5.841s
[2K
| Adam | epoch: 012 | loss: 0.34718 - acc: 0.8344 -- iter: 224/263
[A[ATraining Step: 107  | total loss: [1m[32m0.33364[0m[0m | time: 6.541s
[2K
| Adam | epoch: 012 | loss: 0.33364 - acc: 0.8479 -- iter: 256/263
[A[ATraining Step: 108  | total loss: [1m[32m0.31826[0m[0m | time: 8.151s
[2K
| Adam | epoch: 012 | loss: 0.31826 - acc: 0.8537 | val_loss: 0.39132 - val_acc: 0.8193 -- iter: 263/263
--
Training Step: 109  | total loss: [1m[32m0.31327[0m[0m | time: 0.175s
[2K
| Adam | epoch: 013 | loss: 0.31327 - acc: 0.8589 -- iter: 032/263
[A[ATraining Step: 110  | total loss: [1m[32m0.29831[0m[0m | time: 0.340s
[2K
| Adam | epoch: 013 | loss: 0.29831 - acc: 0.8731 -- iter: 064/263
[A[ATraining Step: 111  | total loss: [1m[32m0.28385[0m[0m | time: 0.957s
[2K
| Adam | epoch: 013 | loss: 0.28385 - acc: 0.8857 -- iter: 096/263
[A[ATraining Step: 112  | total loss: [1m[32m0.27944[0m[0m | time: 1.718s
[2K
| Adam | epoch: 013 | loss: 0.27944 - acc: 0.8878 -- iter: 128/263
[A[ATraining Step: 113  | total loss: [1m[32m0.26620[0m[0m | time: 2.708s
[2K
| Adam | epoch: 013 | loss: 0.26620 - acc: 0.8928 -- iter: 160/263
[A[ATraining Step: 114  | total loss: [1m[32m0.26777[0m[0m | time: 3.694s
[2K
| Adam | epoch: 013 | loss: 0.26777 - acc: 0.8910 -- iter: 192/263
[A[ATraining Step: 115  | total loss: [1m[32m0.24883[0m[0m | time: 4.324s
[2K
| Adam | epoch: 013 | loss: 0.24883 - acc: 0.8988 -- iter: 224/263
[A[ATraining Step: 116  | total loss: [1m[32m0.24915[0m[0m | time: 4.934s
[2K
| Adam | epoch: 013 | loss: 0.24915 - acc: 0.8964 -- iter: 256/263
[A[ATraining Step: 117  | total loss: [1m[32m0.23155[0m[0m | time: 6.542s
[2K
| Adam | epoch: 013 | loss: 0.23155 - acc: 0.9036 | val_loss: 0.66409 - val_acc: 0.7831 -- iter: 263/263
--
Training Step: 118  | total loss: [1m[32m0.23697[0m[0m | time: 0.618s
[2K
| Adam | epoch: 014 | loss: 0.23697 - acc: 0.9008 -- iter: 032/263
[A[ATraining Step: 119  | total loss: [1m[32m0.26272[0m[0m | time: 0.777s
[2K
| Adam | epoch: 014 | loss: 0.26272 - acc: 0.8857 -- iter: 064/263
[A[ATraining Step: 120  | total loss: [1m[32m0.24792[0m[0m | time: 0.970s
[2K
| Adam | epoch: 014 | loss: 0.24792 - acc: 0.8971 -- iter: 096/263
[A[ATraining Step: 121  | total loss: [1m[32m0.22441[0m[0m | time: 1.584s
[2K
| Adam | epoch: 014 | loss: 0.22441 - acc: 0.9074 -- iter: 128/263
[A[ATraining Step: 122  | total loss: [1m[32m0.22935[0m[0m | time: 2.204s
[2K
| Adam | epoch: 014 | loss: 0.22935 - acc: 0.9073 -- iter: 160/263
[A[ATraining Step: 123  | total loss: [1m[32m0.24376[0m[0m | time: 2.837s
[2K
| Adam | epoch: 014 | loss: 0.24376 - acc: 0.9009 -- iter: 192/263
[A[ATraining Step: 124  | total loss: [1m[32m0.24732[0m[0m | time: 3.464s
[2K
| Adam | epoch: 014 | loss: 0.24732 - acc: 0.8952 -- iter: 224/263
[A[ATraining Step: 125  | total loss: [1m[32m0.22923[0m[0m | time: 4.101s
[2K
| Adam | epoch: 014 | loss: 0.22923 - acc: 0.9057 -- iter: 256/263
[A[ATraining Step: 126  | total loss: [1m[32m0.21053[0m[0m | time: 5.741s
[2K
| Adam | epoch: 014 | loss: 0.21053 - acc: 0.9151 | val_loss: 0.57152 - val_acc: 0.8193 -- iter: 263/263
--
Training Step: 127  | total loss: [1m[32m0.22352[0m[0m | time: 0.621s
[2K
| Adam | epoch: 015 | loss: 0.22352 - acc: 0.9111 -- iter: 032/263
[A[ATraining Step: 128  | total loss: [1m[32m0.26934[0m[0m | time: 1.229s
[2K
| Adam | epoch: 015 | loss: 0.26934 - acc: 0.8981 -- iter: 064/263
[A[ATraining Step: 129  | total loss: [1m[32m0.25697[0m[0m | time: 1.405s
[2K
| Adam | epoch: 015 | loss: 0.25697 - acc: 0.8989 -- iter: 096/263
[A[ATraining Step: 130  | total loss: [1m[32m0.23913[0m[0m | time: 1.577s
[2K
| Adam | epoch: 015 | loss: 0.23913 - acc: 0.9090 -- iter: 128/263
[A[ATraining Step: 131  | total loss: [1m[32m0.22710[0m[0m | time: 2.173s
[2K
| Adam | epoch: 015 | loss: 0.22710 - acc: 0.9181 -- iter: 160/263
[A[ATraining Step: 132  | total loss: [1m[32m0.25297[0m[0m | time: 2.838s
[2K
| Adam | epoch: 015 | loss: 0.25297 - acc: 0.8982 -- iter: 192/263
[A[ATraining Step: 133  | total loss: [1m[32m0.23571[0m[0m | time: 3.601s
[2K
| Adam | epoch: 015 | loss: 0.23571 - acc: 0.9053 -- iter: 224/263
[A[ATraining Step: 134  | total loss: [1m[32m0.21607[0m[0m | time: 4.220s
[2K
| Adam | epoch: 015 | loss: 0.21607 - acc: 0.9147 -- iter: 256/263
[A[ATraining Step: 135  | total loss: [1m[32m0.20240[0m[0m | time: 5.846s
[2K
| Adam | epoch: 015 | loss: 0.20240 - acc: 0.9201 | val_loss: 0.46496 - val_acc: 0.8072 -- iter: 263/263
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.9149184149184149
Validation AUPRC:0.8855529673995874
Test AUC:0.953939393939394
Test AUPRC:0.9701463516297145
BestTestF1Score	0.92	0.78	0.89	0.86	0.98	49	8	25	1	0.07
BestTestMCCScore	0.86	0.7	0.84	0.95	0.78	39	2	31	11	0.77
BestTestAccuracyScore	0.86	0.7	0.84	0.95	0.78	39	2	31	11	0.77
BestValidationF1Score	0.84	0.69	0.83	0.76	0.95	37	12	32	2	0.07
BestValidationMCC	0.84	0.71	0.86	0.89	0.79	31	4	40	8	0.77
BestValidationAccuracy	0.84	0.71	0.86	0.89	0.79	31	4	40	8	0.77
TestPredictions (Threshold:0.77)
CHEMBL8287,TN,INACT,0.3499999940395355	CHEMBL237314,TN,INACT,0.0	CHEMBL2407934,TP,ACT,1.0	CHEMBL2435939,TP,ACT,0.9900000095367432	CHEMBL113,TN,INACT,0.12999999523162842	CHEMBL3577991,FP,INACT,0.8899999856948853	CHEMBL3401352,TP,ACT,0.8700000047683716	CHEMBL273549,TN,INACT,0.009999999776482582	CHEMBL307326,TN,INACT,0.009999999776482582	CHEMBL62066,TN,INACT,0.009999999776482582	CHEMBL3290734,FN,ACT,0.7200000286102295	CHEMBL3321758,TP,ACT,0.9900000095367432	CHEMBL3290728,TP,ACT,0.9300000071525574	CHEMBL2435918,TP,ACT,0.9100000262260437	CHEMBL3290708,TP,ACT,0.9399999976158142	CHEMBL67109,TN,INACT,0.019999999552965164	CHEMBL302196,TN,INACT,0.009999999776482582	CHEMBL3234565,TP,ACT,0.9599999785423279	CHEMBL2331647,TP,ACT,0.9399999976158142	CHEMBL2336220,TP,ACT,0.9599999785423279	CHEMBL2435930,TP,ACT,0.9599999785423279	CHEMBL3234549,TP,ACT,0.9700000286102295	CHEMBL104994,TN,INACT,0.019999999552965164	CHEMBL3290723,TP,ACT,0.949999988079071	CHEMBL2386558,TN,INACT,0.7599999904632568	CHEMBL3290714,FN,ACT,0.4399999976158142	CHEMBL91765,TN,INACT,0.0	CHEMBL3234583,TP,ACT,0.7900000214576721	CHEMBL102390,TN,INACT,0.009999999776482582	CHEMBL3290712,TP,ACT,0.9700000286102295	CHEMBL3356914,TP,ACT,0.9700000286102295	CHEMBL2435914,TP,ACT,0.9700000286102295	CHEMBL1214503,TN,INACT,0.009999999776482582	CHEMBL102452,TN,INACT,0.009999999776482582	CHEMBL3234566,TP,ACT,0.9100000262260437	CHEMBL2336235,FN,ACT,0.09000000357627869	CHEMBL15153,TN,INACT,0.0	CHEMBL2435924,TP,ACT,0.9599999785423279	CHEMBL2407955,TP,ACT,0.9900000095367432	CHEMBL2336224,TP,ACT,0.9900000095367432	CHEMBL95063,TN,INACT,0.009999999776482582	CHEMBL3401353,TP,ACT,0.9900000095367432	CHEMBL3793436,TP,ACT,0.9399999976158142	CHEMBL2435942,TP,ACT,0.9800000190734863	CHEMBL3793012,TP,ACT,0.9800000190734863	CHEMBL277149,TN,INACT,0.05999999865889549	CHEMBL260563,TN,INACT,0.019999999552965164	CHEMBL8208,TN,INACT,0.3400000035762787	CHEMBL381108,TN,INACT,0.5199999809265137	CHEMBL2181251,FN,ACT,0.10999999940395355	CHEMBL3234570,FN,ACT,0.029999999329447746	CHEMBL2435919,TP,ACT,0.949999988079071	CHEMBL3794293,TP,ACT,0.8399999737739563	CHEMBL319387,TN,INACT,0.009999999776482582	CHEMBL3234573,TP,ACT,0.9300000071525574	CHEMBL3421900,TP,ACT,0.7799999713897705	CHEMBL3234553,TP,ACT,0.9100000262260437	CHEMBL302468,TN,INACT,0.0	CHEMBL2181550,TN,INACT,0.2199999988079071	CHEMBL64321,TN,INACT,0.0	CHEMBL3234577,FN,ACT,0.75	CHEMBL3290715,FN,ACT,0.5899999737739563	CHEMBL2336239,TP,ACT,0.9900000095367432	CHEMBL62527,TN,INACT,0.0	CHEMBL2407947,TP,ACT,0.9900000095367432	CHEMBL3234580,TP,ACT,0.949999988079071	CHEMBL2205616,FP,INACT,0.8399999737739563	CHEMBL318221,TN,INACT,0.0	CHEMBL419912,TN,INACT,0.0	CHEMBL240597,TP,ACT,0.8199999928474426	CHEMBL3421904,TN,INACT,0.0	CHEMBL2407931,TP,ACT,0.9399999976158142	CHEMBL106602,TN,INACT,0.009999999776482582	CHEMBL566315,TP,ACT,0.9900000095367432	CHEMBL3234550,FN,ACT,0.07999999821186066	CHEMBL3401356,TP,ACT,0.9800000190734863	CHEMBL341733,TN,INACT,0.009999999776482582	CHEMBL235592,TN,INACT,0.05000000074505806	CHEMBL3234870,FN,ACT,0.15000000596046448	CHEMBL2407952,TP,ACT,0.9800000190734863	CHEMBL2181246,FN,ACT,0.11999999731779099	CHEMBL2336238,TP,ACT,0.9200000166893005	CHEMBL2336215,FN,ACT,0.3199999928474426	

