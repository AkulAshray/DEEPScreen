ImageNetInceptionV2 CHEMBL5747 adam 0.001 15 0 0 0.6 False True
Number of active compounds :	168
Number of inactive compounds :	168
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL5747_adam_0.001_15_0_0_0.6_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL5747_adam_0.001_15_0.6/
---------------------------------
Training samples: 211
Validation samples: 67
--
Training Step: 1  | time: 78.686s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/211
[A[ATraining Step: 2  | total loss: [1m[32m0.61107[0m[0m | time: 93.941s
[2K
| Adam | epoch: 001 | loss: 0.61107 - acc: 0.5344 -- iter: 064/211
[A[ATraining Step: 3  | total loss: [1m[32m0.55555[0m[0m | time: 109.436s
[2K
| Adam | epoch: 001 | loss: 0.55555 - acc: 0.6852 -- iter: 096/211
[A[ATraining Step: 4  | total loss: [1m[32m0.90509[0m[0m | time: 124.907s
[2K
| Adam | epoch: 001 | loss: 0.90509 - acc: 0.6635 -- iter: 128/211
[A[ATraining Step: 5  | total loss: [1m[32m0.61848[0m[0m | time: 140.495s
[2K
| Adam | epoch: 001 | loss: 0.61848 - acc: 0.7450 -- iter: 160/211
[A[ATraining Step: 6  | total loss: [1m[32m0.66450[0m[0m | time: 155.992s
[2K
| Adam | epoch: 001 | loss: 0.66450 - acc: 0.6880 -- iter: 192/211
[A[ATraining Step: 7  | total loss: [1m[32m0.65340[0m[0m | time: 184.744s
[2K
| Adam | epoch: 001 | loss: 0.65340 - acc: 0.7064 | val_loss: 2.35813 - val_acc: 0.4478 -- iter: 211/211
--
Training Step: 8  | total loss: [1m[32m0.53879[0m[0m | time: 56.430s
[2K
| Adam | epoch: 002 | loss: 0.53879 - acc: 0.7235 -- iter: 032/211
[A[ATraining Step: 9  | total loss: [1m[32m0.39007[0m[0m | time: 254.057s
[2K
| Adam | epoch: 002 | loss: 0.39007 - acc: 0.8420 -- iter: 064/211
[A[ATraining Step: 10  | total loss: [1m[32m0.52716[0m[0m | time: 458.151s
[2K
| Adam | epoch: 002 | loss: 0.52716 - acc: 0.7648 -- iter: 096/211
[A[ATraining Step: 11  | total loss: [1m[32m0.50174[0m[0m | time: 649.487s
[2K
| Adam | epoch: 002 | loss: 0.50174 - acc: 0.7874 -- iter: 128/211
[A[ATraining Step: 12  | total loss: [1m[32m0.43837[0m[0m | time: 728.089s
[2K
| Adam | epoch: 002 | loss: 0.43837 - acc: 0.7987 -- iter: 160/211
[A[ATraining Step: 13  | total loss: [1m[32m0.42804[0m[0m | time: 745.976s
[2K
| Adam | epoch: 002 | loss: 0.42804 - acc: 0.8046 -- iter: 192/211
[A[ATraining Step: 14  | total loss: [1m[32m0.50725[0m[0m | time: 775.423s
[2K
| Adam | epoch: 002 | loss: 0.50725 - acc: 0.7695 | val_loss: 4.88430 - val_acc: 0.4478 -- iter: 211/211
--
Training Step: 15  | total loss: [1m[32m0.50900[0m[0m | time: 33.035s
[2K
| Adam | epoch: 003 | loss: 0.50900 - acc: 0.7741 -- iter: 032/211
[A[ATraining Step: 16  | total loss: [1m[32m0.44456[0m[0m | time: 45.468s
[2K
| Adam | epoch: 003 | loss: 0.44456 - acc: 0.7996 -- iter: 064/211
[A[ATraining Step: 17  | total loss: [1m[32m0.35585[0m[0m | time: 62.978s
[2K
| Adam | epoch: 003 | loss: 0.35585 - acc: 0.8338 -- iter: 096/211
[A[ATraining Step: 18  | total loss: [1m[32m0.33256[0m[0m | time: 97.089s
[2K
| Adam | epoch: 003 | loss: 0.33256 - acc: 0.8373 -- iter: 128/211
[A[ATraining Step: 19  | total loss: [1m[32m0.32012[0m[0m | time: 114.664s
[2K
| Adam | epoch: 003 | loss: 0.32012 - acc: 0.8603 -- iter: 160/211
[A[ATraining Step: 20  | total loss: [1m[32m0.35360[0m[0m | time: 134.074s
[2K
| Adam | epoch: 003 | loss: 0.35360 - acc: 0.8550 -- iter: 192/211
[A[ATraining Step: 21  | total loss: [1m[32m0.36743[0m[0m | time: 159.199s
[2K
| Adam | epoch: 003 | loss: 0.36743 - acc: 0.8612 | val_loss: 3.83490 - val_acc: 0.4478 -- iter: 211/211
--
Training Step: 22  | total loss: [1m[32m0.36376[0m[0m | time: 97.093s
[2K
| Adam | epoch: 004 | loss: 0.36376 - acc: 0.8653 -- iter: 032/211
[A[ATraining Step: 23  | total loss: [1m[32m0.33684[0m[0m | time: 108.617s
[2K
| Adam | epoch: 004 | loss: 0.33684 - acc: 0.8681 -- iter: 064/211
[A[ATraining Step: 24  | total loss: [1m[32m0.38754[0m[0m | time: 120.689s
[2K
| Adam | epoch: 004 | loss: 0.38754 - acc: 0.8460 -- iter: 096/211
[A[ATraining Step: 25  | total loss: [1m[32m0.34985[0m[0m | time: 148.838s
[2K
| Adam | epoch: 004 | loss: 0.34985 - acc: 0.8737 -- iter: 128/211
[A[ATraining Step: 26  | total loss: [1m[32m0.33100[0m[0m | time: 166.161s
[2K
| Adam | epoch: 004 | loss: 0.33100 - acc: 0.8575 -- iter: 160/211
[A[ATraining Step: 27  | total loss: [1m[32m0.33428[0m[0m | time: 185.388s
[2K
| Adam | epoch: 004 | loss: 0.33428 - acc: 0.8620 -- iter: 192/211
[A[ATraining Step: 28  | total loss: [1m[32m0.31638[0m[0m | time: 213.787s
[2K
| Adam | epoch: 004 | loss: 0.31638 - acc: 0.8730 | val_loss: 2.66815 - val_acc: 0.4478 -- iter: 211/211
--
Training Step: 29  | total loss: [1m[32m0.33127[0m[0m | time: 37.919s
[2K
| Adam | epoch: 005 | loss: 0.33127 - acc: 0.8583 -- iter: 032/211
[A[ATraining Step: 30  | total loss: [1m[32m0.29616[0m[0m | time: 55.368s
[2K
| Adam | epoch: 005 | loss: 0.29616 - acc: 0.8697 -- iter: 064/211
[A[ATraining Step: 31  | total loss: [1m[32m0.36204[0m[0m | time: 67.285s
[2K
| Adam | epoch: 005 | loss: 0.36204 - acc: 0.8421 -- iter: 096/211
[A[ATraining Step: 32  | total loss: [1m[32m0.39084[0m[0m | time: 78.402s
[2K
| Adam | epoch: 005 | loss: 0.39084 - acc: 0.8539 -- iter: 128/211
[A[ATraining Step: 33  | total loss: [1m[32m0.34934[0m[0m | time: 113.739s
[2K
| Adam | epoch: 005 | loss: 0.34934 - acc: 0.8744 -- iter: 160/211
[A[ATraining Step: 34  | total loss: [1m[32m0.32336[0m[0m | time: 185.753s
[2K
| Adam | epoch: 005 | loss: 0.32336 - acc: 0.8745 -- iter: 192/211
[A[ATraining Step: 35  | total loss: [1m[32m0.29526[0m[0m | time: 205.813s
[2K
| Adam | epoch: 005 | loss: 0.29526 - acc: 0.8877 | val_loss: 3.90536 - val_acc: 0.4478 -- iter: 211/211
--
Training Step: 36  | total loss: [1m[32m0.28431[0m[0m | time: 14.221s
[2K
| Adam | epoch: 006 | loss: 0.28431 - acc: 0.8851 -- iter: 032/211
[A[ATraining Step: 37  | total loss: [1m[32m0.29645[0m[0m | time: 28.332s
[2K
| Adam | epoch: 006 | loss: 0.29645 - acc: 0.8706 -- iter: 064/211
[A[ATraining Step: 38  | total loss: [1m[32m0.30979[0m[0m | time: 42.578s
[2K
| Adam | epoch: 006 | loss: 0.30979 - acc: 0.8531 -- iter: 096/211
[A[ATraining Step: 39  | total loss: [1m[32m0.29559[0m[0m | time: 52.354s
[2K
| Adam | epoch: 006 | loss: 0.29559 - acc: 0.8693 -- iter: 128/211
[A[ATraining Step: 40  | total loss: [1m[32m0.28782[0m[0m | time: 61.311s
[2K
| Adam | epoch: 006 | loss: 0.28782 - acc: 0.8839 -- iter: 160/211
[A[ATraining Step: 41  | total loss: [1m[32m0.25267[0m[0m | time: 75.846s
[2K
| Adam | epoch: 006 | loss: 0.25267 - acc: 0.9052 -- iter: 192/211
[A[ATraining Step: 42  | total loss: [1m[32m0.22936[0m[0m | time: 96.005s
[2K
| Adam | epoch: 006 | loss: 0.22936 - acc: 0.9167 | val_loss: 3.23172 - val_acc: 0.4478 -- iter: 211/211
--
Training Step: 43  | total loss: [1m[32m0.24787[0m[0m | time: 14.410s
[2K
| Adam | epoch: 007 | loss: 0.24787 - acc: 0.9038 -- iter: 032/211
[A[ATraining Step: 44  | total loss: [1m[32m0.23162[0m[0m | time: 28.888s
[2K
| Adam | epoch: 007 | loss: 0.23162 - acc: 0.9150 -- iter: 064/211
[A[ATraining Step: 45  | total loss: [1m[32m0.23895[0m[0m | time: 42.698s
[2K
| Adam | epoch: 007 | loss: 0.23895 - acc: 0.9082 -- iter: 096/211
[A[ATraining Step: 46  | total loss: [1m[32m0.22085[0m[0m | time: 56.943s
[2K
| Adam | epoch: 007 | loss: 0.22085 - acc: 0.9131 -- iter: 128/211
[A[ATraining Step: 47  | total loss: [1m[32m0.21702[0m[0m | time: 66.500s
[2K
| Adam | epoch: 007 | loss: 0.21702 - acc: 0.9171 -- iter: 160/211
[A[ATraining Step: 48  | total loss: [1m[32m0.22682[0m[0m | time: 76.728s
[2K
| Adam | epoch: 007 | loss: 0.22682 - acc: 0.9220 -- iter: 192/211
[A[ATraining Step: 49  | total loss: [1m[32m0.21274[0m[0m | time: 97.167s
[2K
| Adam | epoch: 007 | loss: 0.21274 - acc: 0.9260 | val_loss: 3.52644 - val_acc: 0.4478 -- iter: 211/211
--
Training Step: 50  | total loss: [1m[32m0.21985[0m[0m | time: 14.869s
[2K
| Adam | epoch: 008 | loss: 0.21985 - acc: 0.9278 -- iter: 032/211
[A[ATraining Step: 51  | total loss: [1m[32m0.20724[0m[0m | time: 27.785s
[2K
| Adam | epoch: 008 | loss: 0.20724 - acc: 0.9340 -- iter: 064/211
[A[ATraining Step: 52  | total loss: [1m[32m0.21457[0m[0m | time: 37.953s
[2K
| Adam | epoch: 008 | loss: 0.21457 - acc: 0.9345 -- iter: 096/211
[A[ATraining Step: 53  | total loss: [1m[32m0.22050[0m[0m | time: 47.905s
[2K
| Adam | epoch: 008 | loss: 0.22050 - acc: 0.9350 -- iter: 128/211
[A[ATraining Step: 54  | total loss: [1m[32m0.21284[0m[0m | time: 61.760s
[2K
| Adam | epoch: 008 | loss: 0.21284 - acc: 0.9353 -- iter: 160/211
[A[ATraining Step: 55  | total loss: [1m[32m0.22903[0m[0m | time: 71.724s
[2K
| Adam | epoch: 008 | loss: 0.22903 - acc: 0.9267 -- iter: 192/211
[A[ATraining Step: 56  | total loss: [1m[32m0.22909[0m[0m | time: 86.766s
[2K
| Adam | epoch: 008 | loss: 0.22909 - acc: 0.9222 | val_loss: 1.12053 - val_acc: 0.5522 -- iter: 211/211
--
Training Step: 57  | total loss: [1m[32m0.22245[0m[0m | time: 13.961s
[2K
| Adam | epoch: 009 | loss: 0.22245 - acc: 0.9257 -- iter: 032/211
[A[ATraining Step: 58  | total loss: [1m[32m0.21677[0m[0m | time: 28.339s
[2K
| Adam | epoch: 009 | loss: 0.21677 - acc: 0.9231 -- iter: 064/211
[A[ATraining Step: 59  | total loss: [1m[32m0.20355[0m[0m | time: 42.740s
[2K
| Adam | epoch: 009 | loss: 0.20355 - acc: 0.9334 -- iter: 096/211
[A[ATraining Step: 60  | total loss: [1m[32m0.19797[0m[0m | time: 56.794s
[2K
| Adam | epoch: 009 | loss: 0.19797 - acc: 0.9339 -- iter: 128/211
[A[ATraining Step: 61  | total loss: [1m[32m0.19187[0m[0m | time: 70.925s
[2K
| Adam | epoch: 009 | loss: 0.19187 - acc: 0.9344 -- iter: 160/211
[A[ATraining Step: 62  | total loss: [1m[32m0.21662[0m[0m | time: 85.350s
[2K
| Adam | epoch: 009 | loss: 0.21662 - acc: 0.9187 -- iter: 192/211
[A[ATraining Step: 63  | total loss: [1m[32m0.22649[0m[0m | time: 100.434s
[2K
| Adam | epoch: 009 | loss: 0.22649 - acc: 0.9171 | val_loss: 11.63313 - val_acc: 0.4478 -- iter: 211/211
--
Training Step: 64  | total loss: [1m[32m0.40437[0m[0m | time: 9.649s
[2K
| Adam | epoch: 010 | loss: 0.40437 - acc: 0.8749 -- iter: 032/211
[A[ATraining Step: 65  | total loss: [1m[32m0.38724[0m[0m | time: 24.177s
[2K
| Adam | epoch: 010 | loss: 0.38724 - acc: 0.8708 -- iter: 064/211
[A[ATraining Step: 66  | total loss: [1m[32m0.35482[0m[0m | time: 38.741s
[2K
| Adam | epoch: 010 | loss: 0.35482 - acc: 0.8865 -- iter: 096/211
[A[ATraining Step: 67  | total loss: [1m[32m0.35742[0m[0m | time: 53.146s
[2K
| Adam | epoch: 010 | loss: 0.35742 - acc: 0.8777 -- iter: 128/211
[A[ATraining Step: 68  | total loss: [1m[32m0.33120[0m[0m | time: 67.664s
[2K
| Adam | epoch: 010 | loss: 0.33120 - acc: 0.8884 -- iter: 160/211
[A[ATraining Step: 69  | total loss: [1m[32m0.33741[0m[0m | time: 82.146s
[2K
| Adam | epoch: 010 | loss: 0.33741 - acc: 0.8832 -- iter: 192/211
[A[ATraining Step: 70  | total loss: [1m[32m0.31080[0m[0m | time: 101.302s
[2K
| Adam | epoch: 010 | loss: 0.31080 - acc: 0.8931 | val_loss: 3.17681 - val_acc: 0.4627 -- iter: 211/211
--
Training Step: 71  | total loss: [1m[32m0.30964[0m[0m | time: 9.733s
[2K
| Adam | epoch: 011 | loss: 0.30964 - acc: 0.8981 -- iter: 032/211
[A[ATraining Step: 72  | total loss: [1m[32m0.28838[0m[0m | time: 19.268s
[2K
| Adam | epoch: 011 | loss: 0.28838 - acc: 0.9096 -- iter: 064/211
[A[ATraining Step: 73  | total loss: [1m[32m0.28279[0m[0m | time: 33.767s
[2K
| Adam | epoch: 011 | loss: 0.28279 - acc: 0.9080 -- iter: 096/211
[A[ATraining Step: 74  | total loss: [1m[32m0.26719[0m[0m | time: 48.234s
[2K
| Adam | epoch: 011 | loss: 0.26719 - acc: 0.9181 -- iter: 128/211
[A[ATraining Step: 75  | total loss: [1m[32m0.25032[0m[0m | time: 62.348s
[2K
| Adam | epoch: 011 | loss: 0.25032 - acc: 0.9269 -- iter: 160/211
[A[ATraining Step: 76  | total loss: [1m[32m0.23213[0m[0m | time: 76.376s
[2K
| Adam | epoch: 011 | loss: 0.23213 - acc: 0.9348 -- iter: 192/211
[A[ATraining Step: 77  | total loss: [1m[32m0.21600[0m[0m | time: 96.691s
[2K
| Adam | epoch: 011 | loss: 0.21600 - acc: 0.9417 | val_loss: 0.69901 - val_acc: 0.7612 -- iter: 211/211
--
Training Step: 78  | total loss: [1m[32m0.20792[0m[0m | time: 14.842s
[2K
| Adam | epoch: 012 | loss: 0.20792 - acc: 0.9445 -- iter: 032/211
[A[ATraining Step: 79  | total loss: [1m[32m0.20842[0m[0m | time: 24.687s
[2K
| Adam | epoch: 012 | loss: 0.20842 - acc: 0.9438 -- iter: 064/211
[A[ATraining Step: 80  | total loss: [1m[32m0.23447[0m[0m | time: 32.126s
[2K
| Adam | epoch: 012 | loss: 0.23447 - acc: 0.9442 -- iter: 096/211
[A[ATraining Step: 81  | total loss: [1m[32m0.21685[0m[0m | time: 41.889s
[2K
| Adam | epoch: 012 | loss: 0.21685 - acc: 0.9498 -- iter: 128/211
[A[ATraining Step: 82  | total loss: [1m[32m0.19966[0m[0m | time: 51.804s
[2K
| Adam | epoch: 012 | loss: 0.19966 - acc: 0.9548 -- iter: 160/211
[A[ATraining Step: 83  | total loss: [1m[32m0.20292[0m[0m | time: 65.202s
[2K
| Adam | epoch: 012 | loss: 0.20292 - acc: 0.9468 -- iter: 192/211
[A[ATraining Step: 84  | total loss: [1m[32m0.19824[0m[0m | time: 84.730s
[2K
| Adam | epoch: 012 | loss: 0.19824 - acc: 0.9459 | val_loss: 0.96563 - val_acc: 0.6716 -- iter: 211/211
--
Training Step: 85  | total loss: [1m[32m0.21796[0m[0m | time: 14.154s
[2K
| Adam | epoch: 013 | loss: 0.21796 - acc: 0.9357 -- iter: 032/211
[A[ATraining Step: 86  | total loss: [1m[32m0.19940[0m[0m | time: 28.438s
[2K
| Adam | epoch: 013 | loss: 0.19940 - acc: 0.9421 -- iter: 064/211
[A[ATraining Step: 87  | total loss: [1m[32m0.19058[0m[0m | time: 37.973s
[2K
| Adam | epoch: 013 | loss: 0.19058 - acc: 0.9448 -- iter: 096/211
[A[ATraining Step: 88  | total loss: [1m[32m0.20665[0m[0m | time: 47.314s
[2K
| Adam | epoch: 013 | loss: 0.20665 - acc: 0.9398 -- iter: 128/211
[A[ATraining Step: 89  | total loss: [1m[32m0.18904[0m[0m | time: 61.357s
[2K
| Adam | epoch: 013 | loss: 0.18904 - acc: 0.9458 -- iter: 160/211
[A[ATraining Step: 90  | total loss: [1m[32m0.18829[0m[0m | time: 75.816s
[2K
| Adam | epoch: 013 | loss: 0.18829 - acc: 0.9481 -- iter: 192/211
[A[ATraining Step: 91  | total loss: [1m[32m0.17838[0m[0m | time: 95.230s
[2K
| Adam | epoch: 013 | loss: 0.17838 - acc: 0.9502 | val_loss: 1.92546 - val_acc: 0.6269 -- iter: 211/211
--
Training Step: 92  | total loss: [1m[32m0.20453[0m[0m | time: 13.843s
[2K
| Adam | epoch: 014 | loss: 0.20453 - acc: 0.9426 -- iter: 032/211
[A[ATraining Step: 93  | total loss: [1m[32m0.20497[0m[0m | time: 27.776s
[2K
| Adam | epoch: 014 | loss: 0.20497 - acc: 0.9328 -- iter: 064/211
[A[ATraining Step: 94  | total loss: [1m[32m0.19967[0m[0m | time: 45.334s
[2K
| Adam | epoch: 014 | loss: 0.19967 - acc: 0.9332 -- iter: 096/211
[A[ATraining Step: 95  | total loss: [1m[32m0.18900[0m[0m | time: 54.802s
[2K
| Adam | epoch: 014 | loss: 0.18900 - acc: 0.9368 -- iter: 128/211
[A[ATraining Step: 96  | total loss: [1m[32m0.17204[0m[0m | time: 64.957s
[2K
| Adam | epoch: 014 | loss: 0.17204 - acc: 0.9431 -- iter: 160/211
[A[ATraining Step: 97  | total loss: [1m[32m0.16760[0m[0m | time: 78.918s
[2K
| Adam | epoch: 014 | loss: 0.16760 - acc: 0.9435 -- iter: 192/211
[A[ATraining Step: 98  | total loss: [1m[32m0.17104[0m[0m | time: 98.495s
[2K
| Adam | epoch: 014 | loss: 0.17104 - acc: 0.9429 | val_loss: 1.95027 - val_acc: 0.5075 -- iter: 211/211
--
Training Step: 99  | total loss: [1m[32m0.15606[0m[0m | time: 14.014s
[2K
| Adam | epoch: 015 | loss: 0.15606 - acc: 0.9486 -- iter: 032/211
[A[ATraining Step: 100  | total loss: [1m[32m0.15486[0m[0m | time: 27.735s
[2K
| Adam | epoch: 015 | loss: 0.15486 - acc: 0.9506 -- iter: 064/211
[A[ATraining Step: 101  | total loss: [1m[32m0.15507[0m[0m | time: 42.176s
[2K
| Adam | epoch: 015 | loss: 0.15507 - acc: 0.9525 -- iter: 096/211
[A[ATraining Step: 102  | total loss: [1m[32m0.15041[0m[0m | time: 56.248s
[2K
| Adam | epoch: 015 | loss: 0.15041 - acc: 0.9541 -- iter: 128/211
[A[ATraining Step: 103  | total loss: [1m[32m0.14392[0m[0m | time: 65.667s
[2K
| Adam | epoch: 015 | loss: 0.14392 - acc: 0.9556 -- iter: 160/211
[A[ATraining Step: 104  | total loss: [1m[32m0.16986[0m[0m | time: 75.212s
[2K
| Adam | epoch: 015 | loss: 0.16986 - acc: 0.9547 -- iter: 192/211
[A[ATraining Step: 105  | total loss: [1m[32m0.15569[0m[0m | time: 94.500s
[2K
| Adam | epoch: 015 | loss: 0.15569 - acc: 0.9593 | val_loss: 1.29086 - val_acc: 0.6716 -- iter: 211/211
--
Validation AUC:0.8297297297297298
Validation AUPRC:0.889025990836785
Test AUC:0.8382616487455197
Test AUPRC:0.8011381094477628
BestTestF1Score	0.81	0.62	0.81	0.87	0.75	27	4	27	9	1.0
BestTestMCCScore	0.81	0.62	0.81	0.87	0.75	27	4	27	9	1.0
BestTestAccuracyScore	0.81	0.62	0.81	0.87	0.75	27	4	27	9	1.0
BestValidationF1Score	0.78	0.56	0.78	0.84	0.73	27	5	25	10	1.0
BestValidationMCC	0.78	0.56	0.78	0.84	0.73	27	5	25	10	1.0
BestValidationAccuracy	0.78	0.56	0.78	0.84	0.73	27	5	25	10	1.0
TestPredictions (Threshold:1.0)
CHEMBL1370704,FN,ACT,0.9700000286102295	CHEMBL3639497,TP,ACT,1.0	CHEMBL3650933,TP,ACT,1.0	CHEMBL3650911,TP,ACT,1.0	CHEMBL3824133,TN,INACT,0.9200000166893005	CHEMBL3650885,FP,INACT,1.0	CHEMBL3650931,TP,ACT,1.0	CHEMBL3815086,TP,ACT,1.0	CHEMBL3650906,FN,ACT,0.9900000095367432	CHEMBL3654309,FN,ACT,0.18000000715255737	CHEMBL2430872,TN,INACT,0.8100000023841858	CHEMBL3654293,TP,ACT,1.0	CHEMBL3650891,TP,ACT,1.0	CHEMBL3654294,TP,ACT,1.0	CHEMBL3589469,TN,INACT,0.029999999329447746	CHEMBL3415176,FP,INACT,1.0	CHEMBL3650941,TP,ACT,1.0	CHEMBL1828984,TN,INACT,0.9300000071525574	CHEMBL3654263,TP,ACT,1.0	CHEMBL3650889,TP,ACT,1.0	CHEMBL3110229,TN,INACT,0.8500000238418579	CHEMBL3813929,FN,ACT,0.9599999785423279	CHEMBL3650914,TP,ACT,1.0	CHEMBL98663,TN,INACT,0.2199999988079071	CHEMBL297453,FP,INACT,1.0	CHEMBL3621671,FN,ACT,0.9900000095367432	CHEMBL3813846,TP,ACT,1.0	CHEMBL3654290,FN,ACT,0.8399999737739563	CHEMBL1526788,TN,INACT,0.25	CHEMBL3110247,TN,INACT,0.03999999910593033	CHEMBL3810303,TN,INACT,0.6100000143051147	CHEMBL2436006,TN,INACT,0.800000011920929	CHEMBL3650888,TP,ACT,1.0	CHEMBL3110262,TN,INACT,0.009999999776482582	CHEMBL3108806,TP,ACT,1.0	CHEMBL3621673,TN,INACT,0.23999999463558197	CHEMBL1795623,TN,INACT,0.9100000262260437	CHEMBL2282031,FN,ACT,0.4300000071525574	CHEMBL3589470,TN,INACT,0.6399999856948853	CHEMBL3110246,TN,INACT,0.5299999713897705	CHEMBL3586693,TN,INACT,0.7400000095367432	CHEMBL3650923,TP,ACT,1.0	CHEMBL3654327,FN,ACT,0.07000000029802322	CHEMBL3650896,TP,ACT,1.0	CHEMBL3650918,TP,ACT,1.0	CHEMBL3650893,TP,ACT,1.0	CHEMBL3814896,TP,ACT,1.0	CHEMBL3823551,TN,INACT,0.14000000059604645	CHEMBL3774630,TN,INACT,0.23999999463558197	CHEMBL295316,TN,INACT,0.07000000029802322	CHEMBL504265,TN,INACT,0.8500000238418579	CHEMBL3650924,TP,ACT,1.0	CHEMBL1828980,TN,INACT,0.7799999713897705	CHEMBL3621675,TN,INACT,0.2800000011920929	CHEMBL3650925,TP,ACT,1.0	CHEMBL3770322,TP,ACT,1.0	CHEMBL3353994,TN,INACT,0.0	CHEMBL3775671,TN,INACT,0.9800000190734863	CHEMBL3654273,TP,ACT,1.0	CHEMBL3753048,TP,ACT,1.0	CHEMBL3650956,TN,INACT,0.9900000095367432	CHEMBL3110257,TN,INACT,0.7599999904632568	CHEMBL3650930,TP,ACT,1.0	CHEMBL2177300,FP,INACT,1.0	CHEMBL3650907,FN,ACT,0.9800000190734863	CHEMBL3086885,TP,ACT,1.0	CHEMBL3110258,TN,INACT,0.03999999910593033	

