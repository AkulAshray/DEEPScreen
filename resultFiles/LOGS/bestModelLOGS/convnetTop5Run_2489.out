CNNModel CHEMBL4465 adam 0.0005 30 32 0 0.6 False True
Number of active compounds :	108
Number of inactive compounds :	108
---------------------------------
Run id: CNNModel_CHEMBL4465_adam_0.0005_30_32_0_0.6_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL4465_adam_0.0005_30_32_0.6_True/
---------------------------------
Training samples: 137
Validation samples: 44
--
Training Step: 1  | time: 1.862s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/137
[A[ATraining Step: 2  | total loss: [1m[32m0.62394[0m[0m | time: 3.243s
[2K
| Adam | epoch: 001 | loss: 0.62394 - acc: 0.4219 -- iter: 064/137
[A[ATraining Step: 3  | total loss: [1m[32m0.67925[0m[0m | time: 4.696s
[2K
| Adam | epoch: 001 | loss: 0.67925 - acc: 0.5625 -- iter: 096/137
[A[ATraining Step: 4  | total loss: [1m[32m0.69109[0m[0m | time: 5.844s
[2K
| Adam | epoch: 001 | loss: 0.69109 - acc: 0.4922 -- iter: 128/137
[A[ATraining Step: 5  | total loss: [1m[32m0.68508[0m[0m | time: 7.330s
[2K
| Adam | epoch: 001 | loss: 0.68508 - acc: 0.6058 | val_loss: 0.69018 - val_acc: 0.5227 -- iter: 137/137
--
Training Step: 6  | total loss: [1m[32m0.69389[0m[0m | time: 0.477s
[2K
| Adam | epoch: 002 | loss: 0.69389 - acc: 0.5021 -- iter: 032/137
[A[ATraining Step: 7  | total loss: [1m[32m0.69842[0m[0m | time: 1.819s
[2K
| Adam | epoch: 002 | loss: 0.69842 - acc: 0.4675 -- iter: 064/137
[A[ATraining Step: 8  | total loss: [1m[32m0.69773[0m[0m | time: 3.114s
[2K
| Adam | epoch: 002 | loss: 0.69773 - acc: 0.4682 -- iter: 096/137
[A[ATraining Step: 9  | total loss: [1m[32m0.69642[0m[0m | time: 4.511s
[2K
| Adam | epoch: 002 | loss: 0.69642 - acc: 0.4685 -- iter: 128/137
[A[ATraining Step: 10  | total loss: [1m[32m0.69492[0m[0m | time: 6.802s
[2K
| Adam | epoch: 002 | loss: 0.69492 - acc: 0.4842 | val_loss: 0.69090 - val_acc: 0.5227 -- iter: 137/137
--
Training Step: 11  | total loss: [1m[32m0.69295[0m[0m | time: 0.463s
[2K
| Adam | epoch: 003 | loss: 0.69295 - acc: 0.5065 -- iter: 032/137
[A[ATraining Step: 12  | total loss: [1m[32m0.69317[0m[0m | time: 0.930s
[2K
| Adam | epoch: 003 | loss: 0.69317 - acc: 0.4786 -- iter: 064/137
[A[ATraining Step: 13  | total loss: [1m[32m0.69321[0m[0m | time: 2.062s
[2K
| Adam | epoch: 003 | loss: 0.69321 - acc: 0.4640 -- iter: 096/137
[A[ATraining Step: 14  | total loss: [1m[32m0.69306[0m[0m | time: 2.912s
[2K
| Adam | epoch: 003 | loss: 0.69306 - acc: 0.4659 -- iter: 128/137
[A[ATraining Step: 15  | total loss: [1m[32m0.69190[0m[0m | time: 4.875s
[2K
| Adam | epoch: 003 | loss: 0.69190 - acc: 0.5037 | val_loss: 0.68885 - val_acc: 0.5227 -- iter: 137/137
--
Training Step: 16  | total loss: [1m[32m0.69187[0m[0m | time: 1.351s
[2K
| Adam | epoch: 004 | loss: 0.69187 - acc: 0.4906 -- iter: 032/137
[A[ATraining Step: 17  | total loss: [1m[32m0.69115[0m[0m | time: 1.794s
[2K
| Adam | epoch: 004 | loss: 0.69115 - acc: 0.4940 -- iter: 064/137
[A[ATraining Step: 18  | total loss: [1m[32m0.68894[0m[0m | time: 2.248s
[2K
| Adam | epoch: 004 | loss: 0.68894 - acc: 0.5538 -- iter: 096/137
[A[ATraining Step: 19  | total loss: [1m[32m0.68645[0m[0m | time: 3.489s
[2K
| Adam | epoch: 004 | loss: 0.68645 - acc: 0.5914 -- iter: 128/137
[A[ATraining Step: 20  | total loss: [1m[32m0.68444[0m[0m | time: 5.365s
[2K
| Adam | epoch: 004 | loss: 0.68444 - acc: 0.5821 | val_loss: 0.67182 - val_acc: 0.5227 -- iter: 137/137
--
Training Step: 21  | total loss: [1m[32m0.68596[0m[0m | time: 1.098s
[2K
| Adam | epoch: 005 | loss: 0.68596 - acc: 0.5469 -- iter: 032/137
[A[ATraining Step: 22  | total loss: [1m[32m0.68437[0m[0m | time: 2.000s
[2K
| Adam | epoch: 005 | loss: 0.68437 - acc: 0.5235 -- iter: 064/137
[A[ATraining Step: 23  | total loss: [1m[32m0.67683[0m[0m | time: 2.277s
[2K
| Adam | epoch: 005 | loss: 0.67683 - acc: 0.5257 -- iter: 096/137
[A[ATraining Step: 24  | total loss: [1m[32m0.68591[0m[0m | time: 2.570s
[2K
| Adam | epoch: 005 | loss: 0.68591 - acc: 0.4716 -- iter: 128/137
[A[ATraining Step: 25  | total loss: [1m[32m0.68691[0m[0m | time: 4.798s
[2K
| Adam | epoch: 005 | loss: 0.68691 - acc: 0.4339 | val_loss: 0.65824 - val_acc: 0.9091 -- iter: 137/137
--
Training Step: 26  | total loss: [1m[32m0.68078[0m[0m | time: 1.097s
[2K
| Adam | epoch: 006 | loss: 0.68078 - acc: 0.5176 -- iter: 032/137
[A[ATraining Step: 27  | total loss: [1m[32m0.67834[0m[0m | time: 2.226s
[2K
| Adam | epoch: 006 | loss: 0.67834 - acc: 0.5854 -- iter: 064/137
[A[ATraining Step: 28  | total loss: [1m[32m0.67245[0m[0m | time: 3.515s
[2K
| Adam | epoch: 006 | loss: 0.67245 - acc: 0.6422 -- iter: 096/137
[A[ATraining Step: 29  | total loss: [1m[32m0.66327[0m[0m | time: 3.896s
[2K
| Adam | epoch: 006 | loss: 0.66327 - acc: 0.6912 -- iter: 128/137
[A[ATraining Step: 30  | total loss: [1m[32m0.65049[0m[0m | time: 5.333s
[2K
| Adam | epoch: 006 | loss: 0.65049 - acc: 0.7117 | val_loss: 0.53236 - val_acc: 0.7045 -- iter: 137/137
--
Training Step: 31  | total loss: [1m[32m0.63761[0m[0m | time: 2.741s
[2K
| Adam | epoch: 007 | loss: 0.63761 - acc: 0.7013 -- iter: 032/137
[A[ATraining Step: 32  | total loss: [1m[32m0.61730[0m[0m | time: 4.219s
[2K
| Adam | epoch: 007 | loss: 0.61730 - acc: 0.7052 -- iter: 064/137
[A[ATraining Step: 33  | total loss: [1m[32m0.60474[0m[0m | time: 5.997s
[2K
| Adam | epoch: 007 | loss: 0.60474 - acc: 0.6945 -- iter: 096/137
[A[ATraining Step: 34  | total loss: [1m[32m0.59231[0m[0m | time: 7.937s
[2K
| Adam | epoch: 007 | loss: 0.59231 - acc: 0.7064 -- iter: 128/137
[A[ATraining Step: 35  | total loss: [1m[32m0.56732[0m[0m | time: 9.894s
[2K
| Adam | epoch: 007 | loss: 0.56732 - acc: 0.7417 | val_loss: 0.35221 - val_acc: 0.8409 -- iter: 137/137
--
Training Step: 36  | total loss: [1m[32m0.53417[0m[0m | time: 0.442s
[2K
| Adam | epoch: 008 | loss: 0.53417 - acc: 0.7718 -- iter: 032/137
[A[ATraining Step: 37  | total loss: [1m[32m0.49696[0m[0m | time: 1.528s
[2K
| Adam | epoch: 008 | loss: 0.49696 - acc: 0.7952 -- iter: 064/137
[A[ATraining Step: 38  | total loss: [1m[32m0.50871[0m[0m | time: 2.782s
[2K
| Adam | epoch: 008 | loss: 0.50871 - acc: 0.7680 -- iter: 096/137
[A[ATraining Step: 39  | total loss: [1m[32m0.47517[0m[0m | time: 4.412s
[2K
| Adam | epoch: 008 | loss: 0.47517 - acc: 0.7825 -- iter: 128/137
[A[ATraining Step: 40  | total loss: [1m[32m0.47095[0m[0m | time: 6.972s
[2K
| Adam | epoch: 008 | loss: 0.47095 - acc: 0.7881 | val_loss: 0.28238 - val_acc: 0.9091 -- iter: 137/137
--
Training Step: 41  | total loss: [1m[32m0.45120[0m[0m | time: 0.522s
[2K
| Adam | epoch: 009 | loss: 0.45120 - acc: 0.7984 -- iter: 032/137
[A[ATraining Step: 42  | total loss: [1m[32m0.40093[0m[0m | time: 0.996s
[2K
| Adam | epoch: 009 | loss: 0.40093 - acc: 0.8347 -- iter: 064/137
[A[ATraining Step: 43  | total loss: [1m[32m0.35598[0m[0m | time: 3.019s
[2K
| Adam | epoch: 009 | loss: 0.35598 - acc: 0.8638 -- iter: 096/137
[A[ATraining Step: 44  | total loss: [1m[32m0.33508[0m[0m | time: 6.024s
[2K
| Adam | epoch: 009 | loss: 0.33508 - acc: 0.8712 -- iter: 128/137
[A[ATraining Step: 45  | total loss: [1m[32m0.39074[0m[0m | time: 10.065s
[2K
| Adam | epoch: 009 | loss: 0.39074 - acc: 0.8453 | val_loss: 0.23685 - val_acc: 0.9318 -- iter: 137/137
--
Training Step: 46  | total loss: [1m[32m0.41899[0m[0m | time: 1.428s
[2K
| Adam | epoch: 010 | loss: 0.41899 - acc: 0.8346 -- iter: 032/137
[A[ATraining Step: 47  | total loss: [1m[32m0.39298[0m[0m | time: 1.937s
[2K
| Adam | epoch: 010 | loss: 0.39298 - acc: 0.8463 -- iter: 064/137
[A[ATraining Step: 48  | total loss: [1m[32m0.41644[0m[0m | time: 2.394s
[2K
| Adam | epoch: 010 | loss: 0.41644 - acc: 0.8175 -- iter: 096/137
[A[ATraining Step: 49  | total loss: [1m[32m0.43237[0m[0m | time: 4.106s
[2K
| Adam | epoch: 010 | loss: 0.43237 - acc: 0.7937 -- iter: 128/137
[A[ATraining Step: 50  | total loss: [1m[32m0.40862[0m[0m | time: 6.369s
[2K
| Adam | epoch: 010 | loss: 0.40862 - acc: 0.8014 | val_loss: 0.24751 - val_acc: 0.9318 -- iter: 137/137
--
Training Step: 51  | total loss: [1m[32m0.39673[0m[0m | time: 1.414s
[2K
| Adam | epoch: 011 | loss: 0.39673 - acc: 0.8126 -- iter: 032/137
[A[ATraining Step: 52  | total loss: [1m[32m0.42781[0m[0m | time: 2.846s
[2K
| Adam | epoch: 011 | loss: 0.42781 - acc: 0.8220 -- iter: 064/137
[A[ATraining Step: 53  | total loss: [1m[32m0.41550[0m[0m | time: 3.693s
[2K
| Adam | epoch: 011 | loss: 0.41550 - acc: 0.8206 -- iter: 096/137
[A[ATraining Step: 54  | total loss: [1m[32m0.40278[0m[0m | time: 4.696s
[2K
| Adam | epoch: 011 | loss: 0.40278 - acc: 0.8305 -- iter: 128/137
[A[ATraining Step: 55  | total loss: [1m[32m0.39105[0m[0m | time: 6.815s
[2K
| Adam | epoch: 011 | loss: 0.39105 - acc: 0.8389 | val_loss: 0.24959 - val_acc: 0.9318 -- iter: 137/137
--
Training Step: 56  | total loss: [1m[32m0.37922[0m[0m | time: 1.122s
[2K
| Adam | epoch: 012 | loss: 0.37922 - acc: 0.8483 -- iter: 032/137
[A[ATraining Step: 57  | total loss: [1m[32m0.38213[0m[0m | time: 1.973s
[2K
| Adam | epoch: 012 | loss: 0.38213 - acc: 0.8390 -- iter: 064/137
[A[ATraining Step: 58  | total loss: [1m[32m0.37215[0m[0m | time: 3.042s
[2K
| Adam | epoch: 012 | loss: 0.37215 - acc: 0.8482 -- iter: 096/137
[A[ATraining Step: 59  | total loss: [1m[32m0.36124[0m[0m | time: 3.541s
[2K
| Adam | epoch: 012 | loss: 0.36124 - acc: 0.8602 -- iter: 128/137
[A[ATraining Step: 60  | total loss: [1m[32m0.37777[0m[0m | time: 5.041s
[2K
| Adam | epoch: 012 | loss: 0.37777 - acc: 0.8493 | val_loss: 0.44484 - val_acc: 0.6818 -- iter: 137/137
--
Training Step: 61  | total loss: [1m[32m0.38335[0m[0m | time: 0.999s
[2K
| Adam | epoch: 013 | loss: 0.38335 - acc: 0.8400 -- iter: 032/137
[A[ATraining Step: 62  | total loss: [1m[32m0.38772[0m[0m | time: 2.041s
[2K
| Adam | epoch: 013 | loss: 0.38772 - acc: 0.8244 -- iter: 064/137
[A[ATraining Step: 63  | total loss: [1m[32m0.37823[0m[0m | time: 3.060s
[2K
| Adam | epoch: 013 | loss: 0.37823 - acc: 0.8268 -- iter: 096/137
[A[ATraining Step: 64  | total loss: [1m[32m0.37005[0m[0m | time: 4.328s
[2K
| Adam | epoch: 013 | loss: 0.37005 - acc: 0.8289 -- iter: 128/137
[A[ATraining Step: 65  | total loss: [1m[32m0.35601[0m[0m | time: 5.746s
[2K
| Adam | epoch: 013 | loss: 0.35601 - acc: 0.8385 | val_loss: 0.19033 - val_acc: 0.9318 -- iter: 137/137
--
Training Step: 66  | total loss: [1m[32m0.32902[0m[0m | time: 0.472s
[2K
| Adam | epoch: 014 | loss: 0.32902 - acc: 0.8446 -- iter: 032/137
[A[ATraining Step: 67  | total loss: [1m[32m0.30403[0m[0m | time: 1.724s
[2K
| Adam | epoch: 014 | loss: 0.30403 - acc: 0.8633 -- iter: 064/137
[A[ATraining Step: 68  | total loss: [1m[32m0.28127[0m[0m | time: 2.968s
[2K
| Adam | epoch: 014 | loss: 0.28127 - acc: 0.8795 -- iter: 096/137
[A[ATraining Step: 69  | total loss: [1m[32m0.28033[0m[0m | time: 4.213s
[2K
| Adam | epoch: 014 | loss: 0.28033 - acc: 0.8753 -- iter: 128/137
[A[ATraining Step: 70  | total loss: [1m[32m0.28473[0m[0m | time: 6.453s
[2K
| Adam | epoch: 014 | loss: 0.28473 - acc: 0.8680 | val_loss: 0.30861 - val_acc: 0.7955 -- iter: 137/137
--
Training Step: 71  | total loss: [1m[32m0.27574[0m[0m | time: 0.402s
[2K
| Adam | epoch: 015 | loss: 0.27574 - acc: 0.8759 -- iter: 032/137
[A[ATraining Step: 72  | total loss: [1m[32m0.27361[0m[0m | time: 0.858s
[2K
| Adam | epoch: 015 | loss: 0.27361 - acc: 0.8774 -- iter: 064/137
[A[ATraining Step: 73  | total loss: [1m[32m0.25852[0m[0m | time: 2.162s
[2K
| Adam | epoch: 015 | loss: 0.25852 - acc: 0.8787 -- iter: 096/137
[A[ATraining Step: 74  | total loss: [1m[32m0.26468[0m[0m | time: 3.528s
[2K
| Adam | epoch: 015 | loss: 0.26468 - acc: 0.8783 -- iter: 128/137
[A[ATraining Step: 75  | total loss: [1m[32m0.24731[0m[0m | time: 5.423s
[2K
| Adam | epoch: 015 | loss: 0.24731 - acc: 0.8847 | val_loss: 0.18709 - val_acc: 0.9318 -- iter: 137/137
--
Training Step: 76  | total loss: [1m[32m0.47419[0m[0m | time: 1.068s
[2K
| Adam | epoch: 016 | loss: 0.47419 - acc: 0.8468 -- iter: 032/137
[A[ATraining Step: 77  | total loss: [1m[32m0.44899[0m[0m | time: 1.389s
[2K
| Adam | epoch: 016 | loss: 0.44899 - acc: 0.8498 -- iter: 064/137
[A[ATraining Step: 78  | total loss: [1m[32m0.41025[0m[0m | time: 1.693s
[2K
| Adam | epoch: 016 | loss: 0.41025 - acc: 0.8655 -- iter: 096/137
[A[ATraining Step: 79  | total loss: [1m[32m0.37892[0m[0m | time: 2.931s
[2K
| Adam | epoch: 016 | loss: 0.37892 - acc: 0.8794 -- iter: 128/137
[A[ATraining Step: 80  | total loss: [1m[32m0.36622[0m[0m | time: 5.258s
[2K
| Adam | epoch: 016 | loss: 0.36622 - acc: 0.8854 | val_loss: 0.16592 - val_acc: 0.9545 -- iter: 137/137
--
Training Step: 81  | total loss: [1m[32m0.34491[0m[0m | time: 0.918s
[2K
| Adam | epoch: 017 | loss: 0.34491 - acc: 0.8970 -- iter: 032/137
[A[ATraining Step: 82  | total loss: [1m[32m0.35177[0m[0m | time: 1.949s
[2K
| Adam | epoch: 017 | loss: 0.35177 - acc: 0.8916 -- iter: 064/137
[A[ATraining Step: 83  | total loss: [1m[32m0.33366[0m[0m | time: 2.409s
[2K
| Adam | epoch: 017 | loss: 0.33366 - acc: 0.8962 -- iter: 096/137
[A[ATraining Step: 84  | total loss: [1m[32m0.31293[0m[0m | time: 2.910s
[2K
| Adam | epoch: 017 | loss: 0.31293 - acc: 0.9066 -- iter: 128/137
[A[ATraining Step: 85  | total loss: [1m[32m0.29236[0m[0m | time: 5.240s
[2K
| Adam | epoch: 017 | loss: 0.29236 - acc: 0.9159 | val_loss: 0.15609 - val_acc: 0.9545 -- iter: 137/137
--
Training Step: 86  | total loss: [1m[32m0.28908[0m[0m | time: 0.982s
[2K
| Adam | epoch: 018 | loss: 0.28908 - acc: 0.9119 -- iter: 032/137
[A[ATraining Step: 87  | total loss: [1m[32m0.26958[0m[0m | time: 1.873s
[2K
| Adam | epoch: 018 | loss: 0.26958 - acc: 0.9175 -- iter: 064/137
[A[ATraining Step: 88  | total loss: [1m[32m0.26495[0m[0m | time: 3.032s
[2K
| Adam | epoch: 018 | loss: 0.26495 - acc: 0.9195 -- iter: 096/137
[A[ATraining Step: 89  | total loss: [1m[32m0.25325[0m[0m | time: 3.442s
[2K
| Adam | epoch: 018 | loss: 0.25325 - acc: 0.9276 -- iter: 128/137
[A[ATraining Step: 90  | total loss: [1m[32m0.24110[0m[0m | time: 4.894s
[2K
| Adam | epoch: 018 | loss: 0.24110 - acc: 0.9237 | val_loss: 0.12631 - val_acc: 0.9545 -- iter: 137/137
--
Training Step: 91  | total loss: [1m[32m0.22672[0m[0m | time: 1.032s
[2K
| Adam | epoch: 019 | loss: 0.22672 - acc: 0.9313 -- iter: 032/137
[A[ATraining Step: 92  | total loss: [1m[32m0.20997[0m[0m | time: 2.129s
[2K
| Adam | epoch: 019 | loss: 0.20997 - acc: 0.9382 -- iter: 064/137
[A[ATraining Step: 93  | total loss: [1m[32m0.21302[0m[0m | time: 3.127s
[2K
| Adam | epoch: 019 | loss: 0.21302 - acc: 0.9350 -- iter: 096/137
[A[ATraining Step: 94  | total loss: [1m[32m0.20441[0m[0m | time: 4.155s
[2K
| Adam | epoch: 019 | loss: 0.20441 - acc: 0.9353 -- iter: 128/137
[A[ATraining Step: 95  | total loss: [1m[32m0.19862[0m[0m | time: 5.553s
[2K
| Adam | epoch: 019 | loss: 0.19862 - acc: 0.9386 | val_loss: 0.22122 - val_acc: 0.8864 -- iter: 137/137
--
Training Step: 96  | total loss: [1m[32m0.19794[0m[0m | time: 0.565s
[2K
| Adam | epoch: 020 | loss: 0.19794 - acc: 0.9336 -- iter: 032/137
[A[ATraining Step: 97  | total loss: [1m[32m0.19863[0m[0m | time: 4.446s
[2K
| Adam | epoch: 020 | loss: 0.19863 - acc: 0.9292 -- iter: 064/137
[A[ATraining Step: 98  | total loss: [1m[32m0.18529[0m[0m | time: 8.686s
[2K
| Adam | epoch: 020 | loss: 0.18529 - acc: 0.9362 -- iter: 096/137
[A[ATraining Step: 99  | total loss: [1m[32m0.17822[0m[0m | time: 15.365s
[2K
| Adam | epoch: 020 | loss: 0.17822 - acc: 0.9395 -- iter: 128/137
[A[ATraining Step: 100  | total loss: [1m[32m0.41730[0m[0m | time: 17.565s
[2K
| Adam | epoch: 020 | loss: 0.41730 - acc: 0.8893 | val_loss: 0.69977 - val_acc: 0.7955 -- iter: 137/137
--
Training Step: 101  | total loss: [1m[32m0.45576[0m[0m | time: 0.501s
[2K
| Adam | epoch: 021 | loss: 0.45576 - acc: 0.8722 -- iter: 032/137
[A[ATraining Step: 102  | total loss: [1m[32m0.42814[0m[0m | time: 1.031s
[2K
| Adam | epoch: 021 | loss: 0.42814 - acc: 0.8850 -- iter: 064/137
[A[ATraining Step: 103  | total loss: [1m[32m0.40072[0m[0m | time: 2.588s
[2K
| Adam | epoch: 021 | loss: 0.40072 - acc: 0.8965 -- iter: 096/137
[A[ATraining Step: 104  | total loss: [1m[32m0.41612[0m[0m | time: 3.921s
[2K
| Adam | epoch: 021 | loss: 0.41612 - acc: 0.8850 -- iter: 128/137
[A[ATraining Step: 105  | total loss: [1m[32m0.38346[0m[0m | time: 6.053s
[2K
| Adam | epoch: 021 | loss: 0.38346 - acc: 0.8934 | val_loss: 0.20526 - val_acc: 0.9091 -- iter: 137/137
--
Training Step: 106  | total loss: [1m[32m0.35988[0m[0m | time: 4.131s
[2K
| Adam | epoch: 022 | loss: 0.35988 - acc: 0.9009 -- iter: 032/137
[A[ATraining Step: 107  | total loss: [1m[32m0.34713[0m[0m | time: 4.501s
[2K
| Adam | epoch: 022 | loss: 0.34713 - acc: 0.9046 -- iter: 064/137
[A[ATraining Step: 108  | total loss: [1m[32m0.34686[0m[0m | time: 4.865s
[2K
| Adam | epoch: 022 | loss: 0.34686 - acc: 0.8919 -- iter: 096/137
[A[ATraining Step: 109  | total loss: [1m[32m0.33873[0m[0m | time: 5.960s
[2K
| Adam | epoch: 022 | loss: 0.33873 - acc: 0.8805 -- iter: 128/137
[A[ATraining Step: 110  | total loss: [1m[32m0.31632[0m[0m | time: 8.047s
[2K
| Adam | epoch: 022 | loss: 0.31632 - acc: 0.8924 | val_loss: 0.13297 - val_acc: 0.9545 -- iter: 137/137
--
Training Step: 111  | total loss: [1m[32m0.29637[0m[0m | time: 1.300s
[2K
| Adam | epoch: 023 | loss: 0.29637 - acc: 0.8969 -- iter: 032/137
[A[ATraining Step: 112  | total loss: [1m[32m0.28927[0m[0m | time: 2.464s
[2K
| Adam | epoch: 023 | loss: 0.28927 - acc: 0.9010 -- iter: 064/137
[A[ATraining Step: 113  | total loss: [1m[32m0.26862[0m[0m | time: 2.957s
[2K
| Adam | epoch: 023 | loss: 0.26862 - acc: 0.9078 -- iter: 096/137
[A[ATraining Step: 114  | total loss: [1m[32m0.24515[0m[0m | time: 3.460s
[2K
| Adam | epoch: 023 | loss: 0.24515 - acc: 0.9170 -- iter: 128/137
[A[ATraining Step: 115  | total loss: [1m[32m0.22400[0m[0m | time: 5.980s
[2K
| Adam | epoch: 023 | loss: 0.22400 - acc: 0.9253 | val_loss: 0.10590 - val_acc: 0.9545 -- iter: 137/137
--
Training Step: 116  | total loss: [1m[32m0.20705[0m[0m | time: 2.768s
[2K
| Adam | epoch: 024 | loss: 0.20705 - acc: 0.9328 -- iter: 032/137
[A[ATraining Step: 117  | total loss: [1m[32m0.19253[0m[0m | time: 3.884s
[2K
| Adam | epoch: 024 | loss: 0.19253 - acc: 0.9395 -- iter: 064/137
[A[ATraining Step: 118  | total loss: [1m[32m0.18420[0m[0m | time: 5.143s
[2K
| Adam | epoch: 024 | loss: 0.18420 - acc: 0.9424 -- iter: 096/137
[A[ATraining Step: 119  | total loss: [1m[32m0.17727[0m[0m | time: 5.687s
[2K
| Adam | epoch: 024 | loss: 0.17727 - acc: 0.9450 -- iter: 128/137
[A[ATraining Step: 120  | total loss: [1m[32m0.16396[0m[0m | time: 7.226s
[2K
| Adam | epoch: 024 | loss: 0.16396 - acc: 0.9505 | val_loss: 0.11403 - val_acc: 0.9773 -- iter: 137/137
--
Training Step: 121  | total loss: [1m[32m0.15202[0m[0m | time: 1.272s
[2K
| Adam | epoch: 025 | loss: 0.15202 - acc: 0.9555 -- iter: 032/137
[A[ATraining Step: 122  | total loss: [1m[32m0.14590[0m[0m | time: 2.488s
[2K
| Adam | epoch: 025 | loss: 0.14590 - acc: 0.9568 -- iter: 064/137
[A[ATraining Step: 123  | total loss: [1m[32m0.14140[0m[0m | time: 3.895s
[2K
| Adam | epoch: 025 | loss: 0.14140 - acc: 0.9549 -- iter: 096/137
[A[ATraining Step: 124  | total loss: [1m[32m0.18017[0m[0m | time: 5.266s
[2K
| Adam | epoch: 025 | loss: 0.18017 - acc: 0.9500 -- iter: 128/137
[A[ATraining Step: 125  | total loss: [1m[32m0.16408[0m[0m | time: 6.901s
[2K
| Adam | epoch: 025 | loss: 0.16408 - acc: 0.9550 | val_loss: 0.15404 - val_acc: 0.9318 -- iter: 137/137
--
Training Step: 126  | total loss: [1m[32m0.14810[0m[0m | time: 0.429s
[2K
| Adam | epoch: 026 | loss: 0.14810 - acc: 0.9595 -- iter: 032/137
[A[ATraining Step: 127  | total loss: [1m[32m0.13366[0m[0m | time: 1.651s
[2K
| Adam | epoch: 026 | loss: 0.13366 - acc: 0.9636 -- iter: 064/137
[A[ATraining Step: 128  | total loss: [1m[32m0.12536[0m[0m | time: 2.910s
[2K
| Adam | epoch: 026 | loss: 0.12536 - acc: 0.9672 -- iter: 096/137
[A[ATraining Step: 129  | total loss: [1m[32m0.12463[0m[0m | time: 4.251s
[2K
| Adam | epoch: 026 | loss: 0.12463 - acc: 0.9674 -- iter: 128/137
[A[ATraining Step: 130  | total loss: [1m[32m0.41158[0m[0m | time: 6.620s
[2K
| Adam | epoch: 026 | loss: 0.41158 - acc: 0.9238 | val_loss: 0.06938 - val_acc: 0.9773 -- iter: 137/137
--
Training Step: 131  | total loss: [1m[32m0.37355[0m[0m | time: 0.518s
[2K
| Adam | epoch: 027 | loss: 0.37355 - acc: 0.9314 -- iter: 032/137
[A[ATraining Step: 132  | total loss: [1m[32m0.33884[0m[0m | time: 1.037s
[2K
| Adam | epoch: 027 | loss: 0.33884 - acc: 0.9382 -- iter: 064/137
[A[ATraining Step: 133  | total loss: [1m[32m0.30819[0m[0m | time: 2.280s
[2K
| Adam | epoch: 027 | loss: 0.30819 - acc: 0.9444 -- iter: 096/137
[A[ATraining Step: 134  | total loss: [1m[32m0.29435[0m[0m | time: 3.198s
[2K
| Adam | epoch: 027 | loss: 0.29435 - acc: 0.9468 -- iter: 128/137
[A[ATraining Step: 135  | total loss: [1m[32m0.26908[0m[0m | time: 5.150s
[2K
| Adam | epoch: 027 | loss: 0.26908 - acc: 0.9522 | val_loss: 0.09825 - val_acc: 0.9545 -- iter: 137/137
--
Training Step: 136  | total loss: [1m[32m0.24627[0m[0m | time: 1.368s
[2K
| Adam | epoch: 028 | loss: 0.24627 - acc: 0.9569 -- iter: 032/137
[A[ATraining Step: 137  | total loss: [1m[32m0.22944[0m[0m | time: 1.795s
[2K
| Adam | epoch: 028 | loss: 0.22944 - acc: 0.9581 -- iter: 064/137
[A[ATraining Step: 138  | total loss: [1m[32m0.21374[0m[0m | time: 2.261s
[2K
| Adam | epoch: 028 | loss: 0.21374 - acc: 0.9623 -- iter: 096/137
[A[ATraining Step: 139  | total loss: [1m[32m0.19769[0m[0m | time: 3.231s
[2K
| Adam | epoch: 028 | loss: 0.19769 - acc: 0.9661 -- iter: 128/137
[A[ATraining Step: 140  | total loss: [1m[32m0.18818[0m[0m | time: 5.093s
[2K
| Adam | epoch: 028 | loss: 0.18818 - acc: 0.9663 | val_loss: 0.06684 - val_acc: 0.9773 -- iter: 137/137
--
Training Step: 141  | total loss: [1m[32m0.17271[0m[0m | time: 0.909s
[2K
| Adam | epoch: 029 | loss: 0.17271 - acc: 0.9697 -- iter: 032/137
[A[ATraining Step: 142  | total loss: [1m[32m0.23358[0m[0m | time: 2.055s
[2K
| Adam | epoch: 029 | loss: 0.23358 - acc: 0.9540 -- iter: 064/137
[A[ATraining Step: 143  | total loss: [1m[32m0.21600[0m[0m | time: 2.507s
[2K
| Adam | epoch: 029 | loss: 0.21600 - acc: 0.9555 -- iter: 096/137
[A[ATraining Step: 144  | total loss: [1m[32m0.19789[0m[0m | time: 2.930s
[2K
| Adam | epoch: 029 | loss: 0.19789 - acc: 0.9599 -- iter: 128/137
[A[ATraining Step: 145  | total loss: [1m[32m0.18108[0m[0m | time: 5.271s
[2K
| Adam | epoch: 029 | loss: 0.18108 - acc: 0.9639 | val_loss: 0.09408 - val_acc: 0.9545 -- iter: 137/137
--
Training Step: 146  | total loss: [1m[32m0.16992[0m[0m | time: 1.073s
[2K
| Adam | epoch: 030 | loss: 0.16992 - acc: 0.9644 -- iter: 032/137
[A[ATraining Step: 147  | total loss: [1m[32m0.15560[0m[0m | time: 2.015s
[2K
| Adam | epoch: 030 | loss: 0.15560 - acc: 0.9680 -- iter: 064/137
[A[ATraining Step: 148  | total loss: [1m[32m0.26704[0m[0m | time: 3.263s
[2K
| Adam | epoch: 030 | loss: 0.26704 - acc: 0.9368 -- iter: 096/137
[A[ATraining Step: 149  | total loss: [1m[32m0.24638[0m[0m | time: 3.747s
[2K
| Adam | epoch: 030 | loss: 0.24638 - acc: 0.9431 -- iter: 128/137
[A[ATraining Step: 150  | total loss: [1m[32m0.23293[0m[0m | time: 5.254s
[2K
| Adam | epoch: 030 | loss: 0.23293 - acc: 0.9488 | val_loss: 0.10201 - val_acc: 0.9545 -- iter: 137/137
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:1.0
Validation AUPRC:0.9999999999999998
Test AUC:1.0
Test AUPRC:1.0
BestTestF1Score	1.0	1.0	1.0	1.0	1.0	18	0	26	0	0.25
BestTestMCCScore	1.0	1.0	1.0	1.0	1.0	18	0	26	0	0.25
BestTestAccuracyScore	1.0	1.0	1.0	1.0	1.0	18	0	26	0	0.25
BestValidationF1Score	1.0	1.0	1.0	1.0	1.0	23	0	21	0	0.25
BestValidationMCC	1.0	1.0	1.0	1.0	1.0	23	0	21	0	0.25
BestValidationAccuracy	1.0	1.0	1.0	1.0	1.0	23	0	21	0	0.25
TestPredictions (Threshold:0.25)
CHEMBL8805,TN,INACT,0.10999999940395355	CHEMBL2333812,TP,ACT,0.9200000166893005	CHEMBL180573,TN,INACT,0.03999999910593033	CHEMBL2335520,TN,INACT,0.05000000074505806	CHEMBL2334536,TP,ACT,0.8799999952316284	CHEMBL2335554,TN,INACT,0.07999999821186066	CHEMBL431258,TN,INACT,0.1599999964237213	CHEMBL179229,TP,ACT,0.75	CHEMBL2375599,TP,ACT,0.9900000095367432	CHEMBL1802265,TN,INACT,0.10999999940395355	CHEMBL133095,TN,INACT,0.03999999910593033	CHEMBL2398812,TP,ACT,0.9800000190734863	CHEMBL425926,TN,INACT,0.05000000074505806	CHEMBL44509,TN,INACT,0.05000000074505806	CHEMBL179528,TN,INACT,0.029999999329447746	CHEMBL2396678,TP,ACT,0.9800000190734863	CHEMBL2398811,TP,ACT,0.9800000190734863	CHEMBL2335523,TN,INACT,0.03999999910593033	CHEMBL9499,TN,INACT,0.03999999910593033	CHEMBL1802262,TN,INACT,0.029999999329447746	CHEMBL48361,TN,INACT,0.10000000149011612	CHEMBL2398806,TP,ACT,0.9900000095367432	CHEMBL2334199,TP,ACT,0.9900000095367432	CHEMBL2334538,TP,ACT,0.9300000071525574	CHEMBL2335522,TN,INACT,0.03999999910593033	CHEMBL2178950,TN,INACT,0.07000000029802322	CHEMBL3287865,TP,ACT,0.8799999952316284	CHEMBL15611,TP,ACT,0.28999999165534973	CHEMBL2426749,TN,INACT,0.09000000357627869	CHEMBL3133085,TN,INACT,0.18000000715255737	CHEMBL2335557,TN,INACT,0.05000000074505806	CHEMBL2333809,TP,ACT,0.9900000095367432	CHEMBL2334196,TP,ACT,0.9900000095367432	CHEMBL2375704,TP,ACT,0.5699999928474426	CHEMBL9470,TN,INACT,0.029999999329447746	CHEMBL2375707,TP,ACT,0.9399999976158142	CHEMBL2335521,TN,INACT,0.03999999910593033	CHEMBL2335555,TN,INACT,0.07999999821186066	CHEMBL2335545,TN,INACT,0.05000000074505806	CHEMBL2375693,TP,ACT,0.25	CHEMBL218383,TN,INACT,0.03999999910593033	CHEMBL473982,TN,INACT,0.05000000074505806	CHEMBL22722,TN,INACT,0.05000000074505806	CHEMBL2398802,TP,ACT,0.9800000190734863	

