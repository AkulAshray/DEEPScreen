ImageNetInceptionV2 CHEMBL2366 RMSprop 0.0001 15 0 0 0.6 False True
Number of active compounds :	149
Number of inactive compounds :	149
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL2366_RMSprop_0.0001_15_0_0_0.6_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL2366_RMSprop_0.0001_15_0.6/
---------------------------------
Training samples: 171
Validation samples: 54
--
Training Step: 1  | time: 41.127s
[2K
| RMSProp | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/171
[A[ATraining Step: 2  | total loss: [1m[32m0.64852[0m[0m | time: 49.402s
[2K
| RMSProp | epoch: 001 | loss: 0.64852 - acc: 0.5344 -- iter: 064/171
[A[ATraining Step: 3  | total loss: [1m[32m0.78690[0m[0m | time: 57.411s
[2K
| RMSProp | epoch: 001 | loss: 0.78690 - acc: 0.4295 -- iter: 096/171
[A[ATraining Step: 4  | total loss: [1m[32m0.76747[0m[0m | time: 65.398s
[2K
| RMSProp | epoch: 001 | loss: 0.76747 - acc: 0.5058 -- iter: 128/171
[A[ATraining Step: 5  | total loss: [1m[32m0.76665[0m[0m | time: 73.392s
[2K
| RMSProp | epoch: 001 | loss: 0.76665 - acc: 0.5018 -- iter: 160/171
[A[ATraining Step: 6  | total loss: [1m[32m0.69517[0m[0m | time: 84.829s
[2K
| RMSProp | epoch: 001 | loss: 0.69517 - acc: 0.5810 | val_loss: 0.69549 - val_acc: 0.5000 -- iter: 171/171
--
Training Step: 7  | total loss: [1m[32m0.76474[0m[0m | time: 3.468s
[2K
| RMSProp | epoch: 002 | loss: 0.76474 - acc: 0.4506 -- iter: 032/171
[A[ATraining Step: 8  | total loss: [1m[32m0.77925[0m[0m | time: 11.322s
[2K
| RMSProp | epoch: 002 | loss: 0.77925 - acc: 0.4017 -- iter: 064/171
[A[ATraining Step: 9  | total loss: [1m[32m0.73198[0m[0m | time: 19.314s
[2K
| RMSProp | epoch: 002 | loss: 0.73198 - acc: 0.4372 -- iter: 096/171
[A[ATraining Step: 10  | total loss: [1m[32m0.74340[0m[0m | time: 27.349s
[2K
| RMSProp | epoch: 002 | loss: 0.74340 - acc: 0.4530 -- iter: 128/171
[A[ATraining Step: 11  | total loss: [1m[32m0.74782[0m[0m | time: 35.277s
[2K
| RMSProp | epoch: 002 | loss: 0.74782 - acc: 0.4308 -- iter: 160/171
[A[ATraining Step: 12  | total loss: [1m[32m0.72520[0m[0m | time: 45.736s
[2K
| RMSProp | epoch: 002 | loss: 0.72520 - acc: 0.4760 | val_loss: 0.70208 - val_acc: 0.5000 -- iter: 171/171
--
Training Step: 13  | total loss: [1m[32m0.75958[0m[0m | time: 3.487s
[2K
| RMSProp | epoch: 003 | loss: 0.75958 - acc: 0.4461 -- iter: 032/171
[A[ATraining Step: 14  | total loss: [1m[32m0.72760[0m[0m | time: 6.876s
[2K
| RMSProp | epoch: 003 | loss: 0.72760 - acc: 0.5239 -- iter: 064/171
[A[ATraining Step: 15  | total loss: [1m[32m0.73920[0m[0m | time: 14.849s
[2K
| RMSProp | epoch: 003 | loss: 0.73920 - acc: 0.5324 -- iter: 096/171
[A[ATraining Step: 16  | total loss: [1m[32m0.71507[0m[0m | time: 22.727s
[2K
| RMSProp | epoch: 003 | loss: 0.71507 - acc: 0.5554 -- iter: 128/171
[A[ATraining Step: 17  | total loss: [1m[32m0.70132[0m[0m | time: 30.680s
[2K
| RMSProp | epoch: 003 | loss: 0.70132 - acc: 0.5467 -- iter: 160/171
[A[ATraining Step: 18  | total loss: [1m[32m0.71279[0m[0m | time: 41.211s
[2K
| RMSProp | epoch: 003 | loss: 0.71279 - acc: 0.5413 | val_loss: 0.71138 - val_acc: 0.5000 -- iter: 171/171
--
Training Step: 19  | total loss: [1m[32m0.75605[0m[0m | time: 7.921s
[2K
| RMSProp | epoch: 004 | loss: 0.75605 - acc: 0.4651 -- iter: 032/171
[A[ATraining Step: 20  | total loss: [1m[32m0.73992[0m[0m | time: 11.339s
[2K
| RMSProp | epoch: 004 | loss: 0.73992 - acc: 0.4763 -- iter: 064/171
[A[ATraining Step: 21  | total loss: [1m[32m0.73240[0m[0m | time: 14.765s
[2K
| RMSProp | epoch: 004 | loss: 0.73240 - acc: 0.4978 -- iter: 096/171
[A[ATraining Step: 22  | total loss: [1m[32m0.71922[0m[0m | time: 22.473s
[2K
| RMSProp | epoch: 004 | loss: 0.71922 - acc: 0.5121 -- iter: 128/171
[A[ATraining Step: 23  | total loss: [1m[32m0.73117[0m[0m | time: 30.333s
[2K
| RMSProp | epoch: 004 | loss: 0.73117 - acc: 0.5267 -- iter: 160/171
[A[ATraining Step: 24  | total loss: [1m[32m0.73181[0m[0m | time: 40.740s
[2K
| RMSProp | epoch: 004 | loss: 0.73181 - acc: 0.5280 | val_loss: 0.70614 - val_acc: 0.5000 -- iter: 171/171
--
Training Step: 25  | total loss: [1m[32m0.74399[0m[0m | time: 7.750s
[2K
| RMSProp | epoch: 005 | loss: 0.74399 - acc: 0.5118 -- iter: 032/171
[A[ATraining Step: 26  | total loss: [1m[32m0.72672[0m[0m | time: 15.878s
[2K
| RMSProp | epoch: 005 | loss: 0.72672 - acc: 0.5418 -- iter: 064/171
[A[ATraining Step: 27  | total loss: [1m[32m0.72668[0m[0m | time: 19.217s
[2K
| RMSProp | epoch: 005 | loss: 0.72668 - acc: 0.5230 -- iter: 096/171
[A[ATraining Step: 28  | total loss: [1m[32m0.74783[0m[0m | time: 22.609s
[2K
| RMSProp | epoch: 005 | loss: 0.74783 - acc: 0.4832 -- iter: 128/171
[A[ATraining Step: 29  | total loss: [1m[32m0.74907[0m[0m | time: 30.509s
[2K
| RMSProp | epoch: 005 | loss: 0.74907 - acc: 0.4541 -- iter: 160/171
[A[ATraining Step: 30  | total loss: [1m[32m0.74898[0m[0m | time: 40.860s
[2K
| RMSProp | epoch: 005 | loss: 0.74898 - acc: 0.4798 | val_loss: 0.70281 - val_acc: 0.5000 -- iter: 171/171
--
Training Step: 31  | total loss: [1m[32m0.74148[0m[0m | time: 7.907s
[2K
| RMSProp | epoch: 006 | loss: 0.74148 - acc: 0.4700 -- iter: 032/171
[A[ATraining Step: 32  | total loss: [1m[32m0.72307[0m[0m | time: 15.757s
[2K
| RMSProp | epoch: 006 | loss: 0.72307 - acc: 0.4768 -- iter: 064/171
[A[ATraining Step: 33  | total loss: [1m[32m0.72959[0m[0m | time: 23.816s
[2K
| RMSProp | epoch: 006 | loss: 0.72959 - acc: 0.4681 -- iter: 096/171
[A[ATraining Step: 34  | total loss: [1m[32m0.71282[0m[0m | time: 27.184s
[2K
| RMSProp | epoch: 006 | loss: 0.71282 - acc: 0.4817 -- iter: 128/171
[A[ATraining Step: 35  | total loss: [1m[32m0.74072[0m[0m | time: 30.540s
[2K
| RMSProp | epoch: 006 | loss: 0.74072 - acc: 0.4570 -- iter: 160/171
[A[ATraining Step: 36  | total loss: [1m[32m0.74332[0m[0m | time: 40.977s
[2K
| RMSProp | epoch: 006 | loss: 0.74332 - acc: 0.4751 | val_loss: 0.69854 - val_acc: 0.5000 -- iter: 171/171
--
Training Step: 37  | total loss: [1m[32m0.74688[0m[0m | time: 7.816s
[2K
| RMSProp | epoch: 007 | loss: 0.74688 - acc: 0.4675 -- iter: 032/171
[A[ATraining Step: 38  | total loss: [1m[32m0.73434[0m[0m | time: 15.704s
[2K
| RMSProp | epoch: 007 | loss: 0.73434 - acc: 0.4922 -- iter: 064/171
[A[ATraining Step: 39  | total loss: [1m[32m0.72965[0m[0m | time: 23.756s
[2K
| RMSProp | epoch: 007 | loss: 0.72965 - acc: 0.4818 -- iter: 096/171
[A[ATraining Step: 40  | total loss: [1m[32m0.73363[0m[0m | time: 31.635s
[2K
| RMSProp | epoch: 007 | loss: 0.73363 - acc: 0.4676 -- iter: 128/171
[A[ATraining Step: 41  | total loss: [1m[32m0.73040[0m[0m | time: 34.971s
[2K
| RMSProp | epoch: 007 | loss: 0.73040 - acc: 0.4850 -- iter: 160/171
[A[ATraining Step: 42  | total loss: [1m[32m0.70978[0m[0m | time: 40.863s
[2K
| RMSProp | epoch: 007 | loss: 0.70978 - acc: 0.5123 | val_loss: 0.70076 - val_acc: 0.4815 -- iter: 171/171
--
Training Step: 43  | total loss: [1m[32m0.70240[0m[0m | time: 7.850s
[2K
| RMSProp | epoch: 008 | loss: 0.70240 - acc: 0.5342 -- iter: 032/171
[A[ATraining Step: 44  | total loss: [1m[32m0.70603[0m[0m | time: 15.655s
[2K
| RMSProp | epoch: 008 | loss: 0.70603 - acc: 0.5228 -- iter: 064/171
[A[ATraining Step: 45  | total loss: [1m[32m0.70701[0m[0m | time: 23.531s
[2K
| RMSProp | epoch: 008 | loss: 0.70701 - acc: 0.5190 -- iter: 096/171
[A[ATraining Step: 46  | total loss: [1m[32m0.71185[0m[0m | time: 31.416s
[2K
| RMSProp | epoch: 008 | loss: 0.71185 - acc: 0.5210 -- iter: 128/171
[A[ATraining Step: 47  | total loss: [1m[32m0.71239[0m[0m | time: 39.186s
[2K
| RMSProp | epoch: 008 | loss: 0.71239 - acc: 0.5227 -- iter: 160/171
[A[ATraining Step: 48  | total loss: [1m[32m0.70139[0m[0m | time: 45.045s
[2K
| RMSProp | epoch: 008 | loss: 0.70139 - acc: 0.5492 | val_loss: 0.69776 - val_acc: 0.4815 -- iter: 171/171
--
Training Step: 49  | total loss: [1m[32m0.70649[0m[0m | time: 3.398s
[2K
| RMSProp | epoch: 009 | loss: 0.70649 - acc: 0.5199 -- iter: 032/171
[A[ATraining Step: 50  | total loss: [1m[32m0.69668[0m[0m | time: 11.063s
[2K
| RMSProp | epoch: 009 | loss: 0.69668 - acc: 0.5521 -- iter: 064/171
[A[ATraining Step: 51  | total loss: [1m[32m0.69705[0m[0m | time: 18.960s
[2K
| RMSProp | epoch: 009 | loss: 0.69705 - acc: 0.5441 -- iter: 096/171
[A[ATraining Step: 52  | total loss: [1m[32m0.69440[0m[0m | time: 26.702s
[2K
| RMSProp | epoch: 009 | loss: 0.69440 - acc: 0.5422 -- iter: 128/171
[A[ATraining Step: 53  | total loss: [1m[32m0.70241[0m[0m | time: 34.736s
[2K
| RMSProp | epoch: 009 | loss: 0.70241 - acc: 0.5267 -- iter: 160/171
[A[ATraining Step: 54  | total loss: [1m[32m0.70232[0m[0m | time: 44.965s
[2K
| RMSProp | epoch: 009 | loss: 0.70232 - acc: 0.5183 | val_loss: 0.69545 - val_acc: 0.4630 -- iter: 171/171
--
Training Step: 55  | total loss: [1m[32m0.69903[0m[0m | time: 3.455s
[2K
| RMSProp | epoch: 010 | loss: 0.69903 - acc: 0.5380 -- iter: 032/171
[A[ATraining Step: 56  | total loss: [1m[32m0.69765[0m[0m | time: 6.779s
[2K
| RMSProp | epoch: 010 | loss: 0.69765 - acc: 0.5519 -- iter: 064/171
[A[ATraining Step: 57  | total loss: [1m[32m0.68755[0m[0m | time: 14.641s
[2K
| RMSProp | epoch: 010 | loss: 0.68755 - acc: 0.5510 -- iter: 096/171
[A[ATraining Step: 58  | total loss: [1m[32m0.69297[0m[0m | time: 22.359s
[2K
| RMSProp | epoch: 010 | loss: 0.69297 - acc: 0.5270 -- iter: 128/171
[A[ATraining Step: 59  | total loss: [1m[32m0.69010[0m[0m | time: 30.126s
[2K
| RMSProp | epoch: 010 | loss: 0.69010 - acc: 0.5443 -- iter: 160/171
[A[ATraining Step: 60  | total loss: [1m[32m0.69108[0m[0m | time: 40.300s
[2K
| RMSProp | epoch: 010 | loss: 0.69108 - acc: 0.5343 | val_loss: 0.69328 - val_acc: 0.4444 -- iter: 171/171
--
Training Step: 61  | total loss: [1m[32m0.68785[0m[0m | time: 7.897s
[2K
| RMSProp | epoch: 011 | loss: 0.68785 - acc: 0.5462 -- iter: 032/171
[A[ATraining Step: 62  | total loss: [1m[32m0.69009[0m[0m | time: 11.239s
[2K
| RMSProp | epoch: 011 | loss: 0.69009 - acc: 0.5402 -- iter: 064/171
[A[ATraining Step: 63  | total loss: [1m[32m0.69440[0m[0m | time: 14.619s
[2K
| RMSProp | epoch: 011 | loss: 0.69440 - acc: 0.5063 -- iter: 096/171
[A[ATraining Step: 64  | total loss: [1m[32m0.68430[0m[0m | time: 22.347s
[2K
| RMSProp | epoch: 011 | loss: 0.68430 - acc: 0.5339 -- iter: 128/171
[A[ATraining Step: 65  | total loss: [1m[32m0.68511[0m[0m | time: 30.189s
[2K
| RMSProp | epoch: 011 | loss: 0.68511 - acc: 0.5336 -- iter: 160/171
[A[ATraining Step: 66  | total loss: [1m[32m0.68524[0m[0m | time: 40.313s
[2K
| RMSProp | epoch: 011 | loss: 0.68524 - acc: 0.5409 | val_loss: 0.69108 - val_acc: 0.5185 -- iter: 171/171
--
Training Step: 67  | total loss: [1m[32m0.67901[0m[0m | time: 7.820s
[2K
| RMSProp | epoch: 012 | loss: 0.67901 - acc: 0.5623 -- iter: 032/171
[A[ATraining Step: 68  | total loss: [1m[32m0.68041[0m[0m | time: 15.709s
[2K
| RMSProp | epoch: 012 | loss: 0.68041 - acc: 0.5623 -- iter: 064/171
[A[ATraining Step: 69  | total loss: [1m[32m0.67626[0m[0m | time: 19.126s
[2K
| RMSProp | epoch: 012 | loss: 0.67626 - acc: 0.5769 -- iter: 096/171
[A[ATraining Step: 70  | total loss: [1m[32m0.66934[0m[0m | time: 22.452s
[2K
| RMSProp | epoch: 012 | loss: 0.66934 - acc: 0.5838 -- iter: 128/171
[A[ATraining Step: 71  | total loss: [1m[32m0.66140[0m[0m | time: 30.294s
[2K
| RMSProp | epoch: 012 | loss: 0.66140 - acc: 0.6001 -- iter: 160/171
[A[ATraining Step: 72  | total loss: [1m[32m0.66508[0m[0m | time: 40.789s
[2K
| RMSProp | epoch: 012 | loss: 0.66508 - acc: 0.5853 | val_loss: 0.68434 - val_acc: 0.5185 -- iter: 171/171
--
Training Step: 73  | total loss: [1m[32m0.65786[0m[0m | time: 7.928s
[2K
| RMSProp | epoch: 013 | loss: 0.65786 - acc: 0.6071 -- iter: 032/171
[A[ATraining Step: 74  | total loss: [1m[32m0.65796[0m[0m | time: 15.661s
[2K
| RMSProp | epoch: 013 | loss: 0.65796 - acc: 0.6056 -- iter: 064/171
[A[ATraining Step: 75  | total loss: [1m[32m0.66095[0m[0m | time: 23.477s
[2K
| RMSProp | epoch: 013 | loss: 0.66095 - acc: 0.6077 -- iter: 096/171
[A[ATraining Step: 76  | total loss: [1m[32m0.65841[0m[0m | time: 26.859s
[2K
| RMSProp | epoch: 013 | loss: 0.65841 - acc: 0.6163 -- iter: 128/171
[A[ATraining Step: 77  | total loss: [1m[32m0.66123[0m[0m | time: 30.312s
[2K
| RMSProp | epoch: 013 | loss: 0.66123 - acc: 0.6088 -- iter: 160/171
[A[ATraining Step: 78  | total loss: [1m[32m0.66303[0m[0m | time: 40.615s
[2K
| RMSProp | epoch: 013 | loss: 0.66303 - acc: 0.6212 | val_loss: 0.68319 - val_acc: 0.5370 -- iter: 171/171
--
Training Step: 79  | total loss: [1m[32m0.65782[0m[0m | time: 7.901s
[2K
| RMSProp | epoch: 014 | loss: 0.65782 - acc: 0.6248 -- iter: 032/171
[A[ATraining Step: 80  | total loss: [1m[32m0.65286[0m[0m | time: 15.769s
[2K
| RMSProp | epoch: 014 | loss: 0.65286 - acc: 0.6344 -- iter: 064/171
[A[ATraining Step: 81  | total loss: [1m[32m0.65555[0m[0m | time: 23.575s
[2K
| RMSProp | epoch: 014 | loss: 0.65555 - acc: 0.6272 -- iter: 096/171
[A[ATraining Step: 82  | total loss: [1m[32m0.65431[0m[0m | time: 31.581s
[2K
| RMSProp | epoch: 014 | loss: 0.65431 - acc: 0.6269 -- iter: 128/171
[A[ATraining Step: 83  | total loss: [1m[32m0.65298[0m[0m | time: 34.887s
[2K
| RMSProp | epoch: 014 | loss: 0.65298 - acc: 0.6361 -- iter: 160/171
[A[ATraining Step: 84  | total loss: [1m[32m0.65640[0m[0m | time: 40.700s
[2K
| RMSProp | epoch: 014 | loss: 0.65640 - acc: 0.6361 | val_loss: 0.69653 - val_acc: 0.5000 -- iter: 171/171
--
Training Step: 85  | total loss: [1m[32m0.64413[0m[0m | time: 7.757s
[2K
| RMSProp | epoch: 015 | loss: 0.64413 - acc: 0.6543 -- iter: 032/171
[A[ATraining Step: 86  | total loss: [1m[32m0.64576[0m[0m | time: 15.469s
[2K
| RMSProp | epoch: 015 | loss: 0.64576 - acc: 0.6452 -- iter: 064/171
[A[ATraining Step: 87  | total loss: [1m[32m0.64048[0m[0m | time: 23.020s
[2K
| RMSProp | epoch: 015 | loss: 0.64048 - acc: 0.6556 -- iter: 096/171
[A[ATraining Step: 88  | total loss: [1m[32m0.63873[0m[0m | time: 30.846s
[2K
| RMSProp | epoch: 015 | loss: 0.63873 - acc: 0.6588 -- iter: 128/171
[A[ATraining Step: 89  | total loss: [1m[32m0.63283[0m[0m | time: 38.580s
[2K
| RMSProp | epoch: 015 | loss: 0.63283 - acc: 0.6773 -- iter: 160/171
[A[ATraining Step: 90  | total loss: [1m[32m0.62650[0m[0m | time: 44.378s
[2K
| RMSProp | epoch: 015 | loss: 0.62650 - acc: 0.6908 | val_loss: 0.73572 - val_acc: 0.5185 -- iter: 171/171
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.6063100137174211
Validation AUPRC:0.6388774767373198
Test AUC:0.6486111111111111
Test AUPRC:0.5823890264049044
BestTestF1Score	0.67	0.3	0.57	0.51	0.96	23	22	8	1	0.6
BestTestMCCScore	0.67	0.3	0.57	0.51	0.96	23	22	8	1	0.6
BestTestAccuracyScore	0.67	0.3	0.57	0.51	0.96	23	22	8	1	0.6
BestValidationF1Score	0.7	0.29	0.61	0.57	0.93	25	19	8	2	0.6
BestValidationMCC	0.7	0.29	0.61	0.57	0.93	25	19	8	2	0.6
BestValidationAccuracy	0.7	0.29	0.61	0.57	0.93	25	19	8	2	0.6
TestPredictions (Threshold:0.6)
CHEMBL1574542,FP,INACT,0.7699999809265137	CHEMBL1340445,FP,INACT,0.6600000262260437	CHEMBL182220,TP,ACT,0.7099999785423279	CHEMBL1160813,FP,INACT,0.8100000023841858	CHEMBL128443,FP,INACT,0.6800000071525574	CHEMBL331908,FP,INACT,0.6800000071525574	CHEMBL1436748,TP,ACT,0.699999988079071	CHEMBL1444617,TP,ACT,0.6899999976158142	CHEMBL1499903,TP,ACT,0.75	CHEMBL2315238,TP,ACT,0.800000011920929	CHEMBL1339887,FP,INACT,0.6000000238418579	CHEMBL46120,TN,INACT,0.5099999904632568	CHEMBL557616,FP,INACT,0.7300000190734863	CHEMBL322542,TN,INACT,0.5699999928474426	CHEMBL1441135,FP,INACT,0.7400000095367432	CHEMBL2315234,TP,ACT,0.7099999785423279	CHEMBL3325623,FN,ACT,0.5600000023841858	CHEMBL151,FP,INACT,0.7099999785423279	CHEMBL2315235,TP,ACT,0.7799999713897705	CHEMBL553963,TN,INACT,0.5899999737739563	CHEMBL3193715,TP,ACT,0.7599999904632568	CHEMBL1313808,TN,INACT,0.5600000023841858	CHEMBL1388377,TP,ACT,0.6100000143051147	CHEMBL1454924,TP,ACT,0.6899999976158142	CHEMBL1508257,TN,INACT,0.5899999737739563	CHEMBL2371636,FP,INACT,0.6700000166893005	CHEMBL1453858,TP,ACT,0.6800000071525574	CHEMBL1496854,TP,ACT,0.6800000071525574	CHEMBL1457908,TP,ACT,0.7099999785423279	CHEMBL61933,FP,INACT,0.6200000047683716	CHEMBL38512,FP,INACT,0.6700000166893005	CHEMBL1561078,TP,ACT,0.7699999809265137	CHEMBL1451569,FP,INACT,0.699999988079071	CHEMBL83205,FP,INACT,0.7400000095367432	CHEMBL1348562,FP,INACT,0.6299999952316284	CHEMBL185345,TP,ACT,0.7400000095367432	CHEMBL1596326,TP,ACT,0.6299999952316284	CHEMBL1349755,TP,ACT,0.6899999976158142	CHEMBL1406272,TP,ACT,0.6499999761581421	CHEMBL1523930,TP,ACT,0.6299999952316284	CHEMBL1449269,FP,INACT,0.6700000166893005	CHEMBL544131,FP,INACT,0.6399999856948853	CHEMBL356296,TN,INACT,0.550000011920929	CHEMBL1523678,FP,INACT,0.7799999713897705	CHEMBL415479,FP,INACT,0.7699999809265137	CHEMBL320512,FP,INACT,0.6800000071525574	CHEMBL3735263,FP,INACT,0.7300000190734863	CHEMBL1532234,TN,INACT,0.47999998927116394	CHEMBL3190607,TP,ACT,0.6000000238418579	CHEMBL342914,TN,INACT,0.5699999928474426	CHEMBL539815,FP,INACT,0.6100000143051147	CHEMBL1407834,TP,ACT,0.6399999856948853	CHEMBL2315244,TP,ACT,0.8199999928474426	CHEMBL1200847,TP,ACT,0.699999988079071	

