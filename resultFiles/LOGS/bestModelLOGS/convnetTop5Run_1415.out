ImageNetInceptionV2 CHEMBL4506 adam 0.0001 15 0 0 0.6 False True
Number of active compounds :	226
Number of inactive compounds :	226
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL4506_adam_0.0001_15_0_0_0.6_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL4506_adam_0.0001_15_0.6/
---------------------------------
Training samples: 288
Validation samples: 91
--
Training Step: 1  | time: 36.212s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/288
[A[ATraining Step: 2  | total loss: [1m[32m0.68004[0m[0m | time: 44.117s
[2K
| Adam | epoch: 001 | loss: 0.68004 - acc: 0.4219 -- iter: 064/288
[A[ATraining Step: 3  | total loss: [1m[32m0.70275[0m[0m | time: 52.104s
[2K
| Adam | epoch: 001 | loss: 0.70275 - acc: 0.4858 -- iter: 096/288
[A[ATraining Step: 4  | total loss: [1m[32m0.65479[0m[0m | time: 59.996s
[2K
| Adam | epoch: 001 | loss: 0.65479 - acc: 0.5199 -- iter: 128/288
[A[ATraining Step: 5  | total loss: [1m[32m0.65950[0m[0m | time: 67.733s
[2K
| Adam | epoch: 001 | loss: 0.65950 - acc: 0.6143 -- iter: 160/288
[A[ATraining Step: 6  | total loss: [1m[32m0.64008[0m[0m | time: 75.748s
[2K
| Adam | epoch: 001 | loss: 0.64008 - acc: 0.6212 -- iter: 192/288
[A[ATraining Step: 7  | total loss: [1m[32m0.67336[0m[0m | time: 83.584s
[2K
| Adam | epoch: 001 | loss: 0.67336 - acc: 0.6422 -- iter: 224/288
[A[ATraining Step: 8  | total loss: [1m[32m0.53728[0m[0m | time: 91.454s
[2K
| Adam | epoch: 001 | loss: 0.53728 - acc: 0.7732 -- iter: 256/288
[A[ATraining Step: 9  | total loss: [1m[32m0.55328[0m[0m | time: 108.903s
[2K
| Adam | epoch: 001 | loss: 0.55328 - acc: 0.7278 | val_loss: 0.81748 - val_acc: 0.4945 -- iter: 288/288
--
Training Step: 10  | total loss: [1m[32m0.53703[0m[0m | time: 8.112s
[2K
| Adam | epoch: 002 | loss: 0.53703 - acc: 0.7233 -- iter: 032/288
[A[ATraining Step: 11  | total loss: [1m[32m0.50613[0m[0m | time: 16.106s
[2K
| Adam | epoch: 002 | loss: 0.50613 - acc: 0.7063 -- iter: 064/288
[A[ATraining Step: 12  | total loss: [1m[32m0.56597[0m[0m | time: 24.055s
[2K
| Adam | epoch: 002 | loss: 0.56597 - acc: 0.6838 -- iter: 096/288
[A[ATraining Step: 13  | total loss: [1m[32m0.51109[0m[0m | time: 32.156s
[2K
| Adam | epoch: 002 | loss: 0.51109 - acc: 0.7256 -- iter: 128/288
[A[ATraining Step: 14  | total loss: [1m[32m0.49803[0m[0m | time: 40.104s
[2K
| Adam | epoch: 002 | loss: 0.49803 - acc: 0.7611 -- iter: 160/288
[A[ATraining Step: 15  | total loss: [1m[32m0.47003[0m[0m | time: 48.106s
[2K
| Adam | epoch: 002 | loss: 0.47003 - acc: 0.7690 -- iter: 192/288
[A[ATraining Step: 16  | total loss: [1m[32m0.43840[0m[0m | time: 55.990s
[2K
| Adam | epoch: 002 | loss: 0.43840 - acc: 0.7853 -- iter: 224/288
[A[ATraining Step: 17  | total loss: [1m[32m0.46372[0m[0m | time: 64.057s
[2K
| Adam | epoch: 002 | loss: 0.46372 - acc: 0.7726 -- iter: 256/288
[A[ATraining Step: 18  | total loss: [1m[32m0.43724[0m[0m | time: 76.041s
[2K
| Adam | epoch: 002 | loss: 0.43724 - acc: 0.7864 | val_loss: 1.15610 - val_acc: 0.4945 -- iter: 288/288
--
Training Step: 19  | total loss: [1m[32m0.40149[0m[0m | time: 9.729s
[2K
| Adam | epoch: 003 | loss: 0.40149 - acc: 0.8055 -- iter: 032/288
[A[ATraining Step: 20  | total loss: [1m[32m0.37418[0m[0m | time: 18.711s
[2K
| Adam | epoch: 003 | loss: 0.37418 - acc: 0.8178 -- iter: 064/288
[A[ATraining Step: 21  | total loss: [1m[32m0.34376[0m[0m | time: 28.325s
[2K
| Adam | epoch: 003 | loss: 0.34376 - acc: 0.8550 -- iter: 096/288
[A[ATraining Step: 22  | total loss: [1m[32m0.32351[0m[0m | time: 38.326s
[2K
| Adam | epoch: 003 | loss: 0.32351 - acc: 0.8610 -- iter: 128/288
[A[ATraining Step: 23  | total loss: [1m[32m0.31530[0m[0m | time: 47.241s
[2K
| Adam | epoch: 003 | loss: 0.31530 - acc: 0.8741 -- iter: 160/288
[A[ATraining Step: 24  | total loss: [1m[32m0.26766[0m[0m | time: 56.472s
[2K
| Adam | epoch: 003 | loss: 0.26766 - acc: 0.9007 -- iter: 192/288
[A[ATraining Step: 25  | total loss: [1m[32m0.26330[0m[0m | time: 66.837s
[2K
| Adam | epoch: 003 | loss: 0.26330 - acc: 0.9108 -- iter: 224/288
[A[ATraining Step: 26  | total loss: [1m[32m0.27655[0m[0m | time: 76.040s
[2K
| Adam | epoch: 003 | loss: 0.27655 - acc: 0.9096 -- iter: 256/288
[A[ATraining Step: 27  | total loss: [1m[32m0.28488[0m[0m | time: 90.776s
[2K
| Adam | epoch: 003 | loss: 0.28488 - acc: 0.9087 | val_loss: 1.26417 - val_acc: 0.4945 -- iter: 288/288
--
Training Step: 28  | total loss: [1m[32m0.24876[0m[0m | time: 9.260s
[2K
| Adam | epoch: 004 | loss: 0.24876 - acc: 0.9315 -- iter: 032/288
[A[ATraining Step: 29  | total loss: [1m[32m0.21854[0m[0m | time: 19.873s
[2K
| Adam | epoch: 004 | loss: 0.21854 - acc: 0.9406 -- iter: 064/288
[A[ATraining Step: 30  | total loss: [1m[32m0.24879[0m[0m | time: 30.646s
[2K
| Adam | epoch: 004 | loss: 0.24879 - acc: 0.9251 -- iter: 096/288
[A[ATraining Step: 31  | total loss: [1m[32m0.23816[0m[0m | time: 39.355s
[2K
| Adam | epoch: 004 | loss: 0.23816 - acc: 0.9207 -- iter: 128/288
[A[ATraining Step: 32  | total loss: [1m[32m0.22848[0m[0m | time: 49.512s
[2K
| Adam | epoch: 004 | loss: 0.22848 - acc: 0.9175 -- iter: 160/288
[A[ATraining Step: 33  | total loss: [1m[32m0.25462[0m[0m | time: 58.565s
[2K
| Adam | epoch: 004 | loss: 0.25462 - acc: 0.9081 -- iter: 192/288
[A[ATraining Step: 34  | total loss: [1m[32m0.23184[0m[0m | time: 67.702s
[2K
| Adam | epoch: 004 | loss: 0.23184 - acc: 0.9211 -- iter: 224/288
[A[ATraining Step: 35  | total loss: [1m[32m0.20945[0m[0m | time: 77.286s
[2K
| Adam | epoch: 004 | loss: 0.20945 - acc: 0.9311 -- iter: 256/288
[A[ATraining Step: 36  | total loss: [1m[32m0.18323[0m[0m | time: 91.811s
[2K
| Adam | epoch: 004 | loss: 0.18323 - acc: 0.9452 | val_loss: 1.22834 - val_acc: 0.4945 -- iter: 288/288
--
Training Step: 37  | total loss: [1m[32m0.17482[0m[0m | time: 7.773s
[2K
| Adam | epoch: 005 | loss: 0.17482 - acc: 0.9499 -- iter: 032/288
[A[ATraining Step: 38  | total loss: [1m[32m0.18652[0m[0m | time: 15.752s
[2K
| Adam | epoch: 005 | loss: 0.18652 - acc: 0.9475 -- iter: 064/288
[A[ATraining Step: 39  | total loss: [1m[32m0.18818[0m[0m | time: 23.761s
[2K
| Adam | epoch: 005 | loss: 0.18818 - acc: 0.9456 -- iter: 096/288
[A[ATraining Step: 40  | total loss: [1m[32m0.16085[0m[0m | time: 31.617s
[2K
| Adam | epoch: 005 | loss: 0.16085 - acc: 0.9558 -- iter: 128/288
[A[ATraining Step: 41  | total loss: [1m[32m0.13901[0m[0m | time: 39.348s
[2K
| Adam | epoch: 005 | loss: 0.13901 - acc: 0.9639 -- iter: 160/288
[A[ATraining Step: 42  | total loss: [1m[32m0.16512[0m[0m | time: 47.166s
[2K
| Adam | epoch: 005 | loss: 0.16512 - acc: 0.9591 -- iter: 192/288
[A[ATraining Step: 43  | total loss: [1m[32m0.15425[0m[0m | time: 54.935s
[2K
| Adam | epoch: 005 | loss: 0.15425 - acc: 0.9608 -- iter: 224/288
[A[ATraining Step: 44  | total loss: [1m[32m0.14439[0m[0m | time: 62.946s
[2K
| Adam | epoch: 005 | loss: 0.14439 - acc: 0.9622 -- iter: 256/288
[A[ATraining Step: 45  | total loss: [1m[32m0.13958[0m[0m | time: 74.764s
[2K
| Adam | epoch: 005 | loss: 0.13958 - acc: 0.9633 | val_loss: 0.74656 - val_acc: 0.5165 -- iter: 288/288
--
Training Step: 46  | total loss: [1m[32m0.17455[0m[0m | time: 7.783s
[2K
| Adam | epoch: 006 | loss: 0.17455 - acc: 0.9486 -- iter: 032/288
[A[ATraining Step: 47  | total loss: [1m[32m0.15853[0m[0m | time: 15.561s
[2K
| Adam | epoch: 006 | loss: 0.15853 - acc: 0.9570 -- iter: 064/288
[A[ATraining Step: 48  | total loss: [1m[32m0.16756[0m[0m | time: 23.422s
[2K
| Adam | epoch: 006 | loss: 0.16756 - acc: 0.9539 -- iter: 096/288
[A[ATraining Step: 49  | total loss: [1m[32m0.14538[0m[0m | time: 31.237s
[2K
| Adam | epoch: 006 | loss: 0.14538 - acc: 0.9612 -- iter: 128/288
[A[ATraining Step: 50  | total loss: [1m[32m0.14462[0m[0m | time: 38.993s
[2K
| Adam | epoch: 006 | loss: 0.14462 - acc: 0.9623 -- iter: 160/288
[A[ATraining Step: 51  | total loss: [1m[32m0.14064[0m[0m | time: 46.739s
[2K
| Adam | epoch: 006 | loss: 0.14064 - acc: 0.9633 -- iter: 192/288
[A[ATraining Step: 52  | total loss: [1m[32m0.12682[0m[0m | time: 54.493s
[2K
| Adam | epoch: 006 | loss: 0.12682 - acc: 0.9688 -- iter: 224/288
[A[ATraining Step: 53  | total loss: [1m[32m0.11273[0m[0m | time: 62.254s
[2K
| Adam | epoch: 006 | loss: 0.11273 - acc: 0.9734 -- iter: 256/288
[A[ATraining Step: 54  | total loss: [1m[32m0.10637[0m[0m | time: 74.008s
[2K
| Adam | epoch: 006 | loss: 0.10637 - acc: 0.9727 | val_loss: 1.08724 - val_acc: 0.5385 -- iter: 288/288
--
Training Step: 55  | total loss: [1m[32m0.12365[0m[0m | time: 7.785s
[2K
| Adam | epoch: 007 | loss: 0.12365 - acc: 0.9722 -- iter: 032/288
[A[ATraining Step: 56  | total loss: [1m[32m0.12895[0m[0m | time: 15.510s
[2K
| Adam | epoch: 007 | loss: 0.12895 - acc: 0.9629 -- iter: 064/288
[A[ATraining Step: 57  | total loss: [1m[32m0.12673[0m[0m | time: 23.313s
[2K
| Adam | epoch: 007 | loss: 0.12673 - acc: 0.9637 -- iter: 096/288
[A[ATraining Step: 58  | total loss: [1m[32m0.12600[0m[0m | time: 31.233s
[2K
| Adam | epoch: 007 | loss: 0.12600 - acc: 0.9601 -- iter: 128/288
[A[ATraining Step: 59  | total loss: [1m[32m0.11631[0m[0m | time: 39.063s
[2K
| Adam | epoch: 007 | loss: 0.11631 - acc: 0.9655 -- iter: 160/288
[A[ATraining Step: 60  | total loss: [1m[32m0.10335[0m[0m | time: 46.888s
[2K
| Adam | epoch: 007 | loss: 0.10335 - acc: 0.9701 -- iter: 192/288
[A[ATraining Step: 61  | total loss: [1m[32m0.09211[0m[0m | time: 54.881s
[2K
| Adam | epoch: 007 | loss: 0.09211 - acc: 0.9740 -- iter: 224/288
[A[ATraining Step: 62  | total loss: [1m[32m0.08806[0m[0m | time: 62.854s
[2K
| Adam | epoch: 007 | loss: 0.08806 - acc: 0.9733 -- iter: 256/288
[A[ATraining Step: 63  | total loss: [1m[32m0.10837[0m[0m | time: 74.918s
[2K
| Adam | epoch: 007 | loss: 0.10837 - acc: 0.9648 | val_loss: 0.93295 - val_acc: 0.6374 -- iter: 288/288
--
Training Step: 64  | total loss: [1m[32m0.10111[0m[0m | time: 7.894s
[2K
| Adam | epoch: 008 | loss: 0.10111 - acc: 0.9653 -- iter: 032/288
[A[ATraining Step: 65  | total loss: [1m[32m0.09206[0m[0m | time: 15.743s
[2K
| Adam | epoch: 008 | loss: 0.09206 - acc: 0.9696 -- iter: 064/288
[A[ATraining Step: 66  | total loss: [1m[32m0.08463[0m[0m | time: 23.645s
[2K
| Adam | epoch: 008 | loss: 0.08463 - acc: 0.9733 -- iter: 096/288
[A[ATraining Step: 67  | total loss: [1m[32m0.08927[0m[0m | time: 31.545s
[2K
| Adam | epoch: 008 | loss: 0.08927 - acc: 0.9727 -- iter: 128/288
[A[ATraining Step: 68  | total loss: [1m[32m0.09894[0m[0m | time: 39.299s
[2K
| Adam | epoch: 008 | loss: 0.09894 - acc: 0.9723 -- iter: 160/288
[A[ATraining Step: 69  | total loss: [1m[32m0.08878[0m[0m | time: 47.240s
[2K
| Adam | epoch: 008 | loss: 0.08878 - acc: 0.9755 -- iter: 192/288
[A[ATraining Step: 70  | total loss: [1m[32m0.08302[0m[0m | time: 55.196s
[2K
| Adam | epoch: 008 | loss: 0.08302 - acc: 0.9783 -- iter: 224/288
[A[ATraining Step: 71  | total loss: [1m[32m0.07630[0m[0m | time: 62.933s
[2K
| Adam | epoch: 008 | loss: 0.07630 - acc: 0.9808 -- iter: 256/288
[A[ATraining Step: 72  | total loss: [1m[32m0.07828[0m[0m | time: 74.653s
[2K
| Adam | epoch: 008 | loss: 0.07828 - acc: 0.9794 | val_loss: 2.14646 - val_acc: 0.5165 -- iter: 288/288
--
Training Step: 73  | total loss: [1m[32m0.07066[0m[0m | time: 7.838s
[2K
| Adam | epoch: 009 | loss: 0.07066 - acc: 0.9817 -- iter: 032/288
[A[ATraining Step: 74  | total loss: [1m[32m0.06639[0m[0m | time: 16.745s
[2K
| Adam | epoch: 009 | loss: 0.06639 - acc: 0.9837 -- iter: 064/288
[A[ATraining Step: 75  | total loss: [1m[32m0.07365[0m[0m | time: 29.939s
[2K
| Adam | epoch: 009 | loss: 0.07365 - acc: 0.9821 -- iter: 096/288
[A[ATraining Step: 76  | total loss: [1m[32m0.07972[0m[0m | time: 43.266s
[2K
| Adam | epoch: 009 | loss: 0.07972 - acc: 0.9807 -- iter: 128/288
[A[ATraining Step: 77  | total loss: [1m[32m0.10561[0m[0m | time: 66.199s
[2K
| Adam | epoch: 009 | loss: 0.10561 - acc: 0.9761 -- iter: 160/288
[A[ATraining Step: 78  | total loss: [1m[32m0.15074[0m[0m | time: 103.069s
[2K
| Adam | epoch: 009 | loss: 0.15074 - acc: 0.9655 -- iter: 192/288
[A[ATraining Step: 79  | total loss: [1m[32m0.15756[0m[0m | time: 139.456s
[2K
| Adam | epoch: 009 | loss: 0.15756 - acc: 0.9626 -- iter: 224/288
[A[ATraining Step: 80  | total loss: [1m[32m0.14218[0m[0m | time: 178.519s
[2K
| Adam | epoch: 009 | loss: 0.14218 - acc: 0.9664 -- iter: 256/288
[A[ATraining Step: 81  | total loss: [1m[32m0.12861[0m[0m | time: 197.679s
[2K
| Adam | epoch: 009 | loss: 0.12861 - acc: 0.9698 | val_loss: 4.60131 - val_acc: 0.4945 -- iter: 288/288
--
Training Step: 82  | total loss: [1m[32m0.11658[0m[0m | time: 12.853s
[2K
| Adam | epoch: 010 | loss: 0.11658 - acc: 0.9729 -- iter: 032/288
[A[ATraining Step: 83  | total loss: [1m[32m0.11045[0m[0m | time: 24.608s
[2K
| Adam | epoch: 010 | loss: 0.11045 - acc: 0.9724 -- iter: 064/288
[A[ATraining Step: 84  | total loss: [1m[32m0.10791[0m[0m | time: 45.798s
[2K
| Adam | epoch: 010 | loss: 0.10791 - acc: 0.9721 -- iter: 096/288
[A[ATraining Step: 85  | total loss: [1m[32m0.11560[0m[0m | time: 59.938s
[2K
| Adam | epoch: 010 | loss: 0.11560 - acc: 0.9655 -- iter: 128/288
[A[ATraining Step: 86  | total loss: [1m[32m0.10576[0m[0m | time: 88.895s
[2K
| Adam | epoch: 010 | loss: 0.10576 - acc: 0.9689 -- iter: 160/288
[A[ATraining Step: 87  | total loss: [1m[32m0.10753[0m[0m | time: 104.139s
[2K
| Adam | epoch: 010 | loss: 0.10753 - acc: 0.9689 -- iter: 192/288
[A[ATraining Step: 88  | total loss: [1m[32m0.12015[0m[0m | time: 127.438s
[2K
| Adam | epoch: 010 | loss: 0.12015 - acc: 0.9658 -- iter: 224/288
[A[ATraining Step: 89  | total loss: [1m[32m0.10945[0m[0m | time: 140.188s
[2K
| Adam | epoch: 010 | loss: 0.10945 - acc: 0.9692 -- iter: 256/288
[A[ATraining Step: 90  | total loss: [1m[32m0.09928[0m[0m | time: 169.664s
[2K
| Adam | epoch: 010 | loss: 0.09928 - acc: 0.9723 | val_loss: 1.70034 - val_acc: 0.6154 -- iter: 288/288
--
Training Step: 91  | total loss: [1m[32m0.09004[0m[0m | time: 11.838s
[2K
| Adam | epoch: 011 | loss: 0.09004 - acc: 0.9751 -- iter: 032/288
[A[ATraining Step: 92  | total loss: [1m[32m0.09145[0m[0m | time: 24.368s
[2K
| Adam | epoch: 011 | loss: 0.09145 - acc: 0.9744 -- iter: 064/288
[A[ATraining Step: 93  | total loss: [1m[32m0.08673[0m[0m | time: 36.895s
[2K
| Adam | epoch: 011 | loss: 0.08673 - acc: 0.9770 -- iter: 096/288
[A[ATraining Step: 94  | total loss: [1m[32m0.08479[0m[0m | time: 46.742s
[2K
| Adam | epoch: 011 | loss: 0.08479 - acc: 0.9762 -- iter: 128/288
[A[ATraining Step: 95  | total loss: [1m[32m0.08354[0m[0m | time: 56.152s
[2K
| Adam | epoch: 011 | loss: 0.08354 - acc: 0.9754 -- iter: 160/288
[A[ATraining Step: 96  | total loss: [1m[32m0.09422[0m[0m | time: 65.585s
[2K
| Adam | epoch: 011 | loss: 0.09422 - acc: 0.9685 -- iter: 192/288
[A[ATraining Step: 97  | total loss: [1m[32m0.09772[0m[0m | time: 74.926s
[2K
| Adam | epoch: 011 | loss: 0.09772 - acc: 0.9685 -- iter: 224/288
[A[ATraining Step: 98  | total loss: [1m[32m0.08949[0m[0m | time: 84.363s
[2K
| Adam | epoch: 011 | loss: 0.08949 - acc: 0.9717 -- iter: 256/288
[A[ATraining Step: 99  | total loss: [1m[32m0.08376[0m[0m | time: 98.864s
[2K
| Adam | epoch: 011 | loss: 0.08376 - acc: 0.9745 | val_loss: 1.45075 - val_acc: 0.6593 -- iter: 288/288
--
Training Step: 100  | total loss: [1m[32m0.08264[0m[0m | time: 9.627s
[2K
| Adam | epoch: 012 | loss: 0.08264 - acc: 0.9739 -- iter: 032/288
[A[ATraining Step: 101  | total loss: [1m[32m0.07690[0m[0m | time: 19.355s
[2K
| Adam | epoch: 012 | loss: 0.07690 - acc: 0.9765 -- iter: 064/288
[A[ATraining Step: 102  | total loss: [1m[32m0.07237[0m[0m | time: 29.099s
[2K
| Adam | epoch: 012 | loss: 0.07237 - acc: 0.9789 -- iter: 096/288
[A[ATraining Step: 103  | total loss: [1m[32m0.07086[0m[0m | time: 38.675s
[2K
| Adam | epoch: 012 | loss: 0.07086 - acc: 0.9779 -- iter: 128/288
[A[ATraining Step: 104  | total loss: [1m[32m0.06706[0m[0m | time: 48.116s
[2K
| Adam | epoch: 012 | loss: 0.06706 - acc: 0.9801 -- iter: 160/288
[A[ATraining Step: 105  | total loss: [1m[32m0.06200[0m[0m | time: 57.734s
[2K
| Adam | epoch: 012 | loss: 0.06200 - acc: 0.9821 -- iter: 192/288
[A[ATraining Step: 106  | total loss: [1m[32m0.06022[0m[0m | time: 67.345s
[2K
| Adam | epoch: 012 | loss: 0.06022 - acc: 0.9839 -- iter: 224/288
[A[ATraining Step: 107  | total loss: [1m[32m0.05752[0m[0m | time: 76.960s
[2K
| Adam | epoch: 012 | loss: 0.05752 - acc: 0.9824 -- iter: 256/288
[A[ATraining Step: 108  | total loss: [1m[32m0.05330[0m[0m | time: 91.805s
[2K
| Adam | epoch: 012 | loss: 0.05330 - acc: 0.9841 | val_loss: 1.04561 - val_acc: 0.6923 -- iter: 288/288
--
Training Step: 109  | total loss: [1m[32m0.04936[0m[0m | time: 9.588s
[2K
| Adam | epoch: 013 | loss: 0.04936 - acc: 0.9857 -- iter: 032/288
[A[ATraining Step: 110  | total loss: [1m[32m0.05991[0m[0m | time: 18.817s
[2K
| Adam | epoch: 013 | loss: 0.05991 - acc: 0.9840 -- iter: 064/288
[A[ATraining Step: 111  | total loss: [1m[32m0.06447[0m[0m | time: 26.738s
[2K
| Adam | epoch: 013 | loss: 0.06447 - acc: 0.9825 -- iter: 096/288
[A[ATraining Step: 112  | total loss: [1m[32m0.07141[0m[0m | time: 34.567s
[2K
| Adam | epoch: 013 | loss: 0.07141 - acc: 0.9811 -- iter: 128/288
[A[ATraining Step: 113  | total loss: [1m[32m0.06581[0m[0m | time: 42.486s
[2K
| Adam | epoch: 013 | loss: 0.06581 - acc: 0.9830 -- iter: 160/288
[A[ATraining Step: 114  | total loss: [1m[32m0.05993[0m[0m | time: 50.422s
[2K
| Adam | epoch: 013 | loss: 0.05993 - acc: 0.9847 -- iter: 192/288
[A[ATraining Step: 115  | total loss: [1m[32m0.05490[0m[0m | time: 58.219s
[2K
| Adam | epoch: 013 | loss: 0.05490 - acc: 0.9862 -- iter: 224/288
[A[ATraining Step: 116  | total loss: [1m[32m0.05130[0m[0m | time: 66.061s
[2K
| Adam | epoch: 013 | loss: 0.05130 - acc: 0.9876 -- iter: 256/288
[A[ATraining Step: 117  | total loss: [1m[32m0.04755[0m[0m | time: 77.784s
[2K
| Adam | epoch: 013 | loss: 0.04755 - acc: 0.9888 | val_loss: 1.08785 - val_acc: 0.6703 -- iter: 288/288
--
Training Step: 118  | total loss: [1m[32m0.04369[0m[0m | time: 7.832s
[2K
| Adam | epoch: 014 | loss: 0.04369 - acc: 0.9900 -- iter: 032/288
[A[ATraining Step: 119  | total loss: [1m[32m0.04713[0m[0m | time: 15.644s
[2K
| Adam | epoch: 014 | loss: 0.04713 - acc: 0.9847 -- iter: 064/288
[A[ATraining Step: 120  | total loss: [1m[32m0.04293[0m[0m | time: 23.351s
[2K
| Adam | epoch: 014 | loss: 0.04293 - acc: 0.9862 -- iter: 096/288
[A[ATraining Step: 121  | total loss: [1m[32m0.03916[0m[0m | time: 31.240s
[2K
| Adam | epoch: 014 | loss: 0.03916 - acc: 0.9876 -- iter: 128/288
[A[ATraining Step: 122  | total loss: [1m[32m0.03595[0m[0m | time: 38.988s
[2K
| Adam | epoch: 014 | loss: 0.03595 - acc: 0.9889 -- iter: 160/288
[A[ATraining Step: 123  | total loss: [1m[32m0.03775[0m[0m | time: 46.778s
[2K
| Adam | epoch: 014 | loss: 0.03775 - acc: 0.9868 -- iter: 192/288
[A[ATraining Step: 124  | total loss: [1m[32m0.03522[0m[0m | time: 54.394s
[2K
| Adam | epoch: 014 | loss: 0.03522 - acc: 0.9882 -- iter: 224/288
[A[ATraining Step: 125  | total loss: [1m[32m0.03299[0m[0m | time: 62.302s
[2K
| Adam | epoch: 014 | loss: 0.03299 - acc: 0.9893 -- iter: 256/288
[A[ATraining Step: 126  | total loss: [1m[32m0.03021[0m[0m | time: 74.194s
[2K
| Adam | epoch: 014 | loss: 0.03021 - acc: 0.9904 | val_loss: 1.09507 - val_acc: 0.6593 -- iter: 288/288
--
Training Step: 127  | total loss: [1m[32m0.03978[0m[0m | time: 7.891s
[2K
| Adam | epoch: 015 | loss: 0.03978 - acc: 0.9882 -- iter: 032/288
[A[ATraining Step: 128  | total loss: [1m[32m0.03716[0m[0m | time: 15.754s
[2K
| Adam | epoch: 015 | loss: 0.03716 - acc: 0.9894 -- iter: 064/288
[A[ATraining Step: 129  | total loss: [1m[32m0.03414[0m[0m | time: 23.724s
[2K
| Adam | epoch: 015 | loss: 0.03414 - acc: 0.9905 -- iter: 096/288
[A[ATraining Step: 130  | total loss: [1m[32m0.03112[0m[0m | time: 31.504s
[2K
| Adam | epoch: 015 | loss: 0.03112 - acc: 0.9914 -- iter: 128/288
[A[ATraining Step: 131  | total loss: [1m[32m0.02833[0m[0m | time: 39.433s
[2K
| Adam | epoch: 015 | loss: 0.02833 - acc: 0.9923 -- iter: 160/288
[A[ATraining Step: 132  | total loss: [1m[32m0.03319[0m[0m | time: 47.169s
[2K
| Adam | epoch: 015 | loss: 0.03319 - acc: 0.9899 -- iter: 192/288
[A[ATraining Step: 133  | total loss: [1m[32m0.03146[0m[0m | time: 54.949s
[2K
| Adam | epoch: 015 | loss: 0.03146 - acc: 0.9909 -- iter: 224/288
[A[ATraining Step: 134  | total loss: [1m[32m0.02924[0m[0m | time: 62.720s
[2K
| Adam | epoch: 015 | loss: 0.02924 - acc: 0.9918 -- iter: 256/288
[A[ATraining Step: 135  | total loss: [1m[32m0.02661[0m[0m | time: 74.662s
[2K
| Adam | epoch: 015 | loss: 0.02661 - acc: 0.9927 | val_loss: 1.22088 - val_acc: 0.7253 -- iter: 288/288
--
Validation AUC:0.7502415458937198
Validation AUPRC:0.7247277760263939
Test AUC:0.7810077519379846
Test AUPRC:0.7771146885928121
BestTestF1Score	0.76	0.47	0.74	0.72	0.81	39	15	28	9	0.56
BestTestMCCScore	0.76	0.47	0.74	0.72	0.81	39	15	28	9	0.56
BestTestAccuracyScore	0.78	0.51	0.76	0.75	0.81	39	13	30	9	0.68
BestValidationF1Score	0.76	0.48	0.74	0.7	0.83	38	16	29	8	0.56
BestValidationMCC	0.76	0.48	0.74	0.7	0.83	38	16	29	8	0.56
BestValidationAccuracy	0.75	0.47	0.74	0.72	0.78	36	14	31	10	0.68
TestPredictions (Threshold:0.56)
CHEMBL199180,FP,INACT,0.9200000166893005	CHEMBL2152612,TN,INACT,0.38999998569488525	CHEMBL2203538,TN,INACT,0.07999999821186066	CHEMBL373134,FP,INACT,0.8700000047683716	CHEMBL1254613,TP,ACT,0.9800000190734863	CHEMBL595354,TN,INACT,0.009999999776482582	CHEMBL2181515,TN,INACT,0.0	CHEMBL2333661,TP,ACT,0.7400000095367432	CHEMBL507676,TP,ACT,1.0	CHEMBL2171191,FN,ACT,0.07000000029802322	CHEMBL1254033,TP,ACT,1.0	CHEMBL3219831,TP,ACT,1.0	CHEMBL470835,TP,ACT,0.7400000095367432	CHEMBL3427754,FN,ACT,0.17000000178813934	CHEMBL3311074,TN,INACT,0.3199999928474426	CHEMBL200762,TN,INACT,0.0	CHEMBL412723,FN,ACT,0.38999998569488525	CHEMBL483942,FN,ACT,0.3700000047683716	CHEMBL2203543,FN,ACT,0.33000001311302185	CHEMBL2333651,FP,INACT,0.9100000262260437	CHEMBL3109656,FP,INACT,0.800000011920929	CHEMBL3805929,TP,ACT,0.949999988079071	CHEMBL425294,TN,INACT,0.4399999976158142	CHEMBL2333666,TP,ACT,0.9900000095367432	CHEMBL489527,TN,INACT,0.019999999552965164	CHEMBL2171192,TN,INACT,0.07000000029802322	CHEMBL490091,TP,ACT,1.0	CHEMBL473881,TP,ACT,0.6899999976158142	CHEMBL3233742,TN,INACT,0.009999999776482582	CHEMBL449883,TP,ACT,1.0	CHEMBL3608442,TP,ACT,1.0	CHEMBL3770860,FP,INACT,0.5899999737739563	CHEMBL3805072,TP,ACT,1.0	CHEMBL570186,TN,INACT,0.019999999552965164	CHEMBL3311067,TN,INACT,0.09000000357627869	CHEMBL3754317,TN,INACT,0.009999999776482582	CHEMBL1140,TN,INACT,0.46000000834465027	CHEMBL2181513,TN,INACT,0.009999999776482582	CHEMBL3408827,FP,INACT,0.699999988079071	CHEMBL522821,TP,ACT,0.8700000047683716	CHEMBL3601362,FP,INACT,1.0	CHEMBL1835304,TP,ACT,1.0	CHEMBL3311082,TN,INACT,0.30000001192092896	CHEMBL492308,TP,ACT,1.0	CHEMBL2017559,TN,INACT,0.5299999713897705	CHEMBL3805427,TP,ACT,1.0	CHEMBL2181517,TN,INACT,0.09000000357627869	CHEMBL524124,TP,ACT,0.7699999809265137	CHEMBL2152572,FP,INACT,0.6200000047683716	CHEMBL3608443,TP,ACT,0.9900000095367432	CHEMBL3219833,TP,ACT,1.0	CHEMBL2203542,FP,INACT,0.7400000095367432	CHEMBL3608446,TP,ACT,0.8899999856948853	CHEMBL1578365,TN,INACT,0.36000001430511475	CHEMBL3311065,TN,INACT,0.03999999910593033	CHEMBL368127,FN,ACT,0.009999999776482582	CHEMBL3343532,TP,ACT,0.7200000286102295	CHEMBL3311077,TN,INACT,0.009999999776482582	CHEMBL3747698,FN,ACT,0.0	CHEMBL523676,TP,ACT,1.0	CHEMBL3805740,TP,ACT,0.949999988079071	CHEMBL3805950,TP,ACT,1.0	CHEMBL1945254,TN,INACT,0.07999999821186066	CHEMBL3343654,TN,INACT,0.5099999904632568	CHEMBL3698622,FN,ACT,0.019999999552965164	CHEMBL224864,FP,INACT,0.8199999928474426	CHEMBL3805632,TP,ACT,1.0	CHEMBL1795371,TN,INACT,0.009999999776482582	CHEMBL3311064,TN,INACT,0.5299999713897705	CHEMBL3805035,TP,ACT,1.0	CHEMBL3805778,TP,ACT,1.0	CHEMBL596655,FP,INACT,0.6899999976158142	CHEMBL1254217,FP,INACT,1.0	CHEMBL3343531,FP,INACT,1.0	CHEMBL482552,TP,ACT,0.7900000214576721	CHEMBL3310816,TN,INACT,0.0	CHEMBL3781485,TP,ACT,1.0	CHEMBL2333655,TP,ACT,1.0	CHEMBL3427770,FN,ACT,0.12999999523162842	CHEMBL3427744,TP,ACT,1.0	CHEMBL2332052,TP,ACT,1.0	CHEMBL1945672,TN,INACT,0.009999999776482582	CHEMBL2332042,TP,ACT,0.949999988079071	CHEMBL3805478,TP,ACT,0.9700000286102295	CHEMBL430574,TN,INACT,0.4099999964237213	CHEMBL1254380,FP,INACT,1.0	CHEMBL371432,FP,INACT,1.0	CHEMBL3427761,TP,ACT,1.0	CHEMBL1835307,TP,ACT,0.9700000286102295	CHEMBL2333665,TP,ACT,0.6899999976158142	CHEMBL2333663,TP,ACT,0.8199999928474426	

