ImageNetInceptionV2 CHEMBL1955 adam 0.001 15 0 0 0.8 False True
Number of active compounds :	277
Number of inactive compounds :	277
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL1955_adam_0.001_15_0_0_0.8_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL1955_adam_0.001_15_0.8/
---------------------------------
Training samples: 268
Validation samples: 84
--
Training Step: 1  | time: 38.705s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/268
[A[ATraining Step: 2  | total loss: [1m[32m0.64484[0m[0m | time: 47.298s
[2K
| Adam | epoch: 001 | loss: 0.64484 - acc: 0.3937 -- iter: 064/268
[A[ATraining Step: 3  | total loss: [1m[32m0.67598[0m[0m | time: 55.733s
[2K
| Adam | epoch: 001 | loss: 0.67598 - acc: 0.5830 -- iter: 096/268
[A[ATraining Step: 4  | total loss: [1m[32m0.67194[0m[0m | time: 64.094s
[2K
| Adam | epoch: 001 | loss: 0.67194 - acc: 0.6379 -- iter: 128/268
[A[ATraining Step: 5  | total loss: [1m[32m0.74980[0m[0m | time: 73.050s
[2K
| Adam | epoch: 001 | loss: 0.74980 - acc: 0.5208 -- iter: 160/268
[A[ATraining Step: 6  | total loss: [1m[32m0.73866[0m[0m | time: 81.670s
[2K
| Adam | epoch: 001 | loss: 0.73866 - acc: 0.5476 -- iter: 192/268
[A[ATraining Step: 7  | total loss: [1m[32m0.68966[0m[0m | time: 90.522s
[2K
| Adam | epoch: 001 | loss: 0.68966 - acc: 0.5940 -- iter: 224/268
[A[ATraining Step: 8  | total loss: [1m[32m0.64958[0m[0m | time: 98.503s
[2K
| Adam | epoch: 001 | loss: 0.64958 - acc: 0.6642 -- iter: 256/268
[A[ATraining Step: 9  | total loss: [1m[32m0.62889[0m[0m | time: 112.516s
[2K
| Adam | epoch: 001 | loss: 0.62889 - acc: 0.6434 | val_loss: 3.67837 - val_acc: 0.3571 -- iter: 268/268
--
Training Step: 10  | total loss: [1m[32m0.65881[0m[0m | time: 3.898s
[2K
| Adam | epoch: 002 | loss: 0.65881 - acc: 0.6134 -- iter: 032/268
[A[ATraining Step: 11  | total loss: [1m[32m0.49528[0m[0m | time: 11.661s
[2K
| Adam | epoch: 002 | loss: 0.49528 - acc: 0.7570 -- iter: 064/268
[A[ATraining Step: 12  | total loss: [1m[32m0.53106[0m[0m | time: 19.479s
[2K
| Adam | epoch: 002 | loss: 0.53106 - acc: 0.7539 -- iter: 096/268
[A[ATraining Step: 13  | total loss: [1m[32m0.50434[0m[0m | time: 27.312s
[2K
| Adam | epoch: 002 | loss: 0.50434 - acc: 0.8192 -- iter: 128/268
[A[ATraining Step: 14  | total loss: [1m[32m0.59392[0m[0m | time: 35.083s
[2K
| Adam | epoch: 002 | loss: 0.59392 - acc: 0.7909 -- iter: 160/268
[A[ATraining Step: 15  | total loss: [1m[32m0.59413[0m[0m | time: 42.985s
[2K
| Adam | epoch: 002 | loss: 0.59413 - acc: 0.7993 -- iter: 192/268
[A[ATraining Step: 16  | total loss: [1m[32m0.53832[0m[0m | time: 50.891s
[2K
| Adam | epoch: 002 | loss: 0.53832 - acc: 0.8160 -- iter: 224/268
[A[ATraining Step: 17  | total loss: [1m[32m0.50415[0m[0m | time: 58.938s
[2K
| Adam | epoch: 002 | loss: 0.50415 - acc: 0.8035 -- iter: 256/268
[A[ATraining Step: 18  | total loss: [1m[32m0.44914[0m[0m | time: 70.495s
[2K
| Adam | epoch: 002 | loss: 0.44914 - acc: 0.8391 | val_loss: 2.48511 - val_acc: 0.3571 -- iter: 268/268
--
Training Step: 19  | total loss: [1m[32m0.45146[0m[0m | time: 3.607s
[2K
| Adam | epoch: 003 | loss: 0.45146 - acc: 0.8406 -- iter: 032/268
[A[ATraining Step: 20  | total loss: [1m[32m0.42502[0m[0m | time: 7.196s
[2K
| Adam | epoch: 003 | loss: 0.42502 - acc: 0.8383 -- iter: 064/268
[A[ATraining Step: 21  | total loss: [1m[32m0.35767[0m[0m | time: 15.127s
[2K
| Adam | epoch: 003 | loss: 0.35767 - acc: 0.8626 -- iter: 096/268
[A[ATraining Step: 22  | total loss: [1m[32m0.36642[0m[0m | time: 22.953s
[2K
| Adam | epoch: 003 | loss: 0.36642 - acc: 0.8382 -- iter: 128/268
[A[ATraining Step: 23  | total loss: [1m[32m0.44997[0m[0m | time: 30.675s
[2K
| Adam | epoch: 003 | loss: 0.44997 - acc: 0.8035 -- iter: 160/268
[A[ATraining Step: 24  | total loss: [1m[32m0.42063[0m[0m | time: 38.681s
[2K
| Adam | epoch: 003 | loss: 0.42063 - acc: 0.8060 -- iter: 192/268
[A[ATraining Step: 25  | total loss: [1m[32m0.43054[0m[0m | time: 46.567s
[2K
| Adam | epoch: 003 | loss: 0.43054 - acc: 0.8249 -- iter: 224/268
[A[ATraining Step: 26  | total loss: [1m[32m0.52844[0m[0m | time: 54.413s
[2K
| Adam | epoch: 003 | loss: 0.52844 - acc: 0.7802 -- iter: 256/268
[A[ATraining Step: 27  | total loss: [1m[32m0.56778[0m[0m | time: 65.913s
[2K
| Adam | epoch: 003 | loss: 0.56778 - acc: 0.7644 | val_loss: 5.67466 - val_acc: 0.3571 -- iter: 268/268
--
Training Step: 28  | total loss: [1m[32m0.50664[0m[0m | time: 7.710s
[2K
| Adam | epoch: 004 | loss: 0.50664 - acc: 0.7999 -- iter: 032/268
[A[ATraining Step: 29  | total loss: [1m[32m0.44667[0m[0m | time: 11.201s
[2K
| Adam | epoch: 004 | loss: 0.44667 - acc: 0.8334 -- iter: 064/268
[A[ATraining Step: 30  | total loss: [1m[32m0.42858[0m[0m | time: 14.668s
[2K
| Adam | epoch: 004 | loss: 0.42858 - acc: 0.8136 -- iter: 096/268
[A[ATraining Step: 31  | total loss: [1m[32m0.34629[0m[0m | time: 22.599s
[2K
| Adam | epoch: 004 | loss: 0.34629 - acc: 0.8566 -- iter: 128/268
[A[ATraining Step: 32  | total loss: [1m[32m0.36701[0m[0m | time: 30.254s
[2K
| Adam | epoch: 004 | loss: 0.36701 - acc: 0.8537 -- iter: 160/268
[A[ATraining Step: 33  | total loss: [1m[32m0.44573[0m[0m | time: 38.177s
[2K
| Adam | epoch: 004 | loss: 0.44573 - acc: 0.8172 -- iter: 192/268
[A[ATraining Step: 34  | total loss: [1m[32m0.45488[0m[0m | time: 45.811s
[2K
| Adam | epoch: 004 | loss: 0.45488 - acc: 0.8162 -- iter: 224/268
[A[ATraining Step: 35  | total loss: [1m[32m0.47104[0m[0m | time: 53.832s
[2K
| Adam | epoch: 004 | loss: 0.47104 - acc: 0.8089 -- iter: 256/268
[A[ATraining Step: 36  | total loss: [1m[32m0.43844[0m[0m | time: 65.428s
[2K
| Adam | epoch: 004 | loss: 0.43844 - acc: 0.8352 | val_loss: 3.48946 - val_acc: 0.3571 -- iter: 268/268
--
Training Step: 37  | total loss: [1m[32m0.40427[0m[0m | time: 7.934s
[2K
| Adam | epoch: 005 | loss: 0.40427 - acc: 0.8494 -- iter: 032/268
[A[ATraining Step: 38  | total loss: [1m[32m0.38484[0m[0m | time: 15.793s
[2K
| Adam | epoch: 005 | loss: 0.38484 - acc: 0.8605 -- iter: 064/268
[A[ATraining Step: 39  | total loss: [1m[32m0.36609[0m[0m | time: 19.292s
[2K
| Adam | epoch: 005 | loss: 0.36609 - acc: 0.8573 -- iter: 096/268
[A[ATraining Step: 40  | total loss: [1m[32m0.32681[0m[0m | time: 22.767s
[2K
| Adam | epoch: 005 | loss: 0.32681 - acc: 0.8684 -- iter: 128/268
[A[ATraining Step: 41  | total loss: [1m[32m0.27415[0m[0m | time: 30.706s
[2K
| Adam | epoch: 005 | loss: 0.27415 - acc: 0.8926 -- iter: 160/268
[A[ATraining Step: 42  | total loss: [1m[32m0.26982[0m[0m | time: 38.370s
[2K
| Adam | epoch: 005 | loss: 0.26982 - acc: 0.8951 -- iter: 192/268
[A[ATraining Step: 43  | total loss: [1m[32m0.23928[0m[0m | time: 46.196s
[2K
| Adam | epoch: 005 | loss: 0.23928 - acc: 0.9136 -- iter: 224/268
[A[ATraining Step: 44  | total loss: [1m[32m0.23427[0m[0m | time: 53.994s
[2K
| Adam | epoch: 005 | loss: 0.23427 - acc: 0.9123 -- iter: 256/268
[A[ATraining Step: 45  | total loss: [1m[32m0.23377[0m[0m | time: 65.587s
[2K
| Adam | epoch: 005 | loss: 0.23377 - acc: 0.9113 | val_loss: 1.48100 - val_acc: 0.6786 -- iter: 268/268
--
Training Step: 46  | total loss: [1m[32m0.22374[0m[0m | time: 7.799s
[2K
| Adam | epoch: 006 | loss: 0.22374 - acc: 0.9104 -- iter: 032/268
[A[ATraining Step: 47  | total loss: [1m[32m0.22395[0m[0m | time: 15.577s
[2K
| Adam | epoch: 006 | loss: 0.22395 - acc: 0.9200 -- iter: 064/268
[A[ATraining Step: 48  | total loss: [1m[32m0.19752[0m[0m | time: 23.379s
[2K
| Adam | epoch: 006 | loss: 0.19752 - acc: 0.9328 -- iter: 096/268
[A[ATraining Step: 49  | total loss: [1m[32m0.17618[0m[0m | time: 26.931s
[2K
| Adam | epoch: 006 | loss: 0.17618 - acc: 0.9385 -- iter: 128/268
[A[ATraining Step: 50  | total loss: [1m[32m0.16620[0m[0m | time: 30.474s
[2K
| Adam | epoch: 006 | loss: 0.16620 - acc: 0.9351 -- iter: 160/268
[A[ATraining Step: 51  | total loss: [1m[32m0.14441[0m[0m | time: 38.497s
[2K
| Adam | epoch: 006 | loss: 0.14441 - acc: 0.9450 -- iter: 192/268
[A[ATraining Step: 52  | total loss: [1m[32m0.16783[0m[0m | time: 46.415s
[2K
| Adam | epoch: 006 | loss: 0.16783 - acc: 0.9345 -- iter: 224/268
[A[ATraining Step: 53  | total loss: [1m[32m0.14963[0m[0m | time: 54.266s
[2K
| Adam | epoch: 006 | loss: 0.14963 - acc: 0.9442 -- iter: 256/268
[A[ATraining Step: 54  | total loss: [1m[32m0.13415[0m[0m | time: 65.801s
[2K
| Adam | epoch: 006 | loss: 0.13415 - acc: 0.9523 | val_loss: 0.97582 - val_acc: 0.7381 -- iter: 268/268
--
Training Step: 55  | total loss: [1m[32m0.20794[0m[0m | time: 7.776s
[2K
| Adam | epoch: 007 | loss: 0.20794 - acc: 0.9412 -- iter: 032/268
[A[ATraining Step: 56  | total loss: [1m[32m0.21277[0m[0m | time: 15.464s
[2K
| Adam | epoch: 007 | loss: 0.21277 - acc: 0.9363 -- iter: 064/268
[A[ATraining Step: 57  | total loss: [1m[32m0.19238[0m[0m | time: 24.338s
[2K
| Adam | epoch: 007 | loss: 0.19238 - acc: 0.9408 -- iter: 096/268
[A[ATraining Step: 58  | total loss: [1m[32m0.20712[0m[0m | time: 34.029s
[2K
| Adam | epoch: 007 | loss: 0.20712 - acc: 0.9361 -- iter: 128/268
[A[ATraining Step: 59  | total loss: [1m[32m0.19467[0m[0m | time: 38.537s
[2K
| Adam | epoch: 007 | loss: 0.19467 - acc: 0.9363 -- iter: 160/268
[A[ATraining Step: 60  | total loss: [1m[32m0.17975[0m[0m | time: 42.877s
[2K
| Adam | epoch: 007 | loss: 0.17975 - acc: 0.9447 -- iter: 192/268
[A[ATraining Step: 61  | total loss: [1m[32m0.16016[0m[0m | time: 52.838s
[2K
| Adam | epoch: 007 | loss: 0.16016 - acc: 0.9519 -- iter: 224/268
[A[ATraining Step: 62  | total loss: [1m[32m0.15126[0m[0m | time: 61.301s
[2K
| Adam | epoch: 007 | loss: 0.15126 - acc: 0.9501 -- iter: 256/268
[A[ATraining Step: 63  | total loss: [1m[32m0.13751[0m[0m | time: 75.761s
[2K
| Adam | epoch: 007 | loss: 0.13751 - acc: 0.9564 | val_loss: 1.00114 - val_acc: 0.7500 -- iter: 268/268
--
Training Step: 64  | total loss: [1m[32m0.14255[0m[0m | time: 9.304s
[2K
| Adam | epoch: 008 | loss: 0.14255 - acc: 0.9540 -- iter: 032/268
[A[ATraining Step: 65  | total loss: [1m[32m0.15586[0m[0m | time: 18.504s
[2K
| Adam | epoch: 008 | loss: 0.15586 - acc: 0.9481 -- iter: 064/268
[A[ATraining Step: 66  | total loss: [1m[32m0.14483[0m[0m | time: 28.140s
[2K
| Adam | epoch: 008 | loss: 0.14483 - acc: 0.9507 -- iter: 096/268
[A[ATraining Step: 67  | total loss: [1m[32m0.13181[0m[0m | time: 37.448s
[2K
| Adam | epoch: 008 | loss: 0.13181 - acc: 0.9566 -- iter: 128/268
[A[ATraining Step: 68  | total loss: [1m[32m0.13413[0m[0m | time: 47.339s
[2K
| Adam | epoch: 008 | loss: 0.13413 - acc: 0.9543 -- iter: 160/268
[A[ATraining Step: 69  | total loss: [1m[32m0.13635[0m[0m | time: 51.076s
[2K
| Adam | epoch: 008 | loss: 0.13635 - acc: 0.9524 -- iter: 192/268
[A[ATraining Step: 70  | total loss: [1m[32m0.12782[0m[0m | time: 55.298s
[2K
| Adam | epoch: 008 | loss: 0.12782 - acc: 0.9578 -- iter: 224/268
[A[ATraining Step: 71  | total loss: [1m[32m0.11512[0m[0m | time: 65.305s
[2K
| Adam | epoch: 008 | loss: 0.11512 - acc: 0.9627 -- iter: 256/268
[A[ATraining Step: 72  | total loss: [1m[32m0.10450[0m[0m | time: 79.510s
[2K
| Adam | epoch: 008 | loss: 0.10450 - acc: 0.9669 | val_loss: 0.80934 - val_acc: 0.7500 -- iter: 268/268
--
Training Step: 73  | total loss: [1m[32m0.10161[0m[0m | time: 9.700s
[2K
| Adam | epoch: 009 | loss: 0.10161 - acc: 0.9671 -- iter: 032/268
[A[ATraining Step: 74  | total loss: [1m[32m0.09793[0m[0m | time: 19.451s
[2K
| Adam | epoch: 009 | loss: 0.09793 - acc: 0.9707 -- iter: 064/268
[A[ATraining Step: 75  | total loss: [1m[32m0.10371[0m[0m | time: 28.238s
[2K
| Adam | epoch: 009 | loss: 0.10371 - acc: 0.9671 -- iter: 096/268
[A[ATraining Step: 76  | total loss: [1m[32m0.10726[0m[0m | time: 37.674s
[2K
| Adam | epoch: 009 | loss: 0.10726 - acc: 0.9639 -- iter: 128/268
[A[ATraining Step: 77  | total loss: [1m[32m0.11364[0m[0m | time: 47.424s
[2K
| Adam | epoch: 009 | loss: 0.11364 - acc: 0.9578 -- iter: 160/268
[A[ATraining Step: 78  | total loss: [1m[32m0.12691[0m[0m | time: 57.226s
[2K
| Adam | epoch: 009 | loss: 0.12691 - acc: 0.9557 -- iter: 192/268
[A[ATraining Step: 79  | total loss: [1m[32m0.12613[0m[0m | time: 61.621s
[2K
| Adam | epoch: 009 | loss: 0.12613 - acc: 0.9570 -- iter: 224/268
[A[ATraining Step: 80  | total loss: [1m[32m0.14999[0m[0m | time: 65.314s
[2K
| Adam | epoch: 009 | loss: 0.14999 - acc: 0.9444 -- iter: 256/268
[A[ATraining Step: 81  | total loss: [1m[32m0.14034[0m[0m | time: 79.283s
[2K
| Adam | epoch: 009 | loss: 0.14034 - acc: 0.9500 | val_loss: 0.86608 - val_acc: 0.7143 -- iter: 268/268
--
Training Step: 82  | total loss: [1m[32m0.13851[0m[0m | time: 9.745s
[2K
| Adam | epoch: 010 | loss: 0.13851 - acc: 0.9488 -- iter: 032/268
[A[ATraining Step: 83  | total loss: [1m[32m0.12730[0m[0m | time: 19.581s
[2K
| Adam | epoch: 010 | loss: 0.12730 - acc: 0.9539 -- iter: 064/268
[A[ATraining Step: 84  | total loss: [1m[32m0.12826[0m[0m | time: 28.818s
[2K
| Adam | epoch: 010 | loss: 0.12826 - acc: 0.9554 -- iter: 096/268
[A[ATraining Step: 85  | total loss: [1m[32m0.12204[0m[0m | time: 38.771s
[2K
| Adam | epoch: 010 | loss: 0.12204 - acc: 0.9598 -- iter: 128/268
[A[ATraining Step: 86  | total loss: [1m[32m0.11705[0m[0m | time: 48.461s
[2K
| Adam | epoch: 010 | loss: 0.11705 - acc: 0.9638 -- iter: 160/268
[A[ATraining Step: 87  | total loss: [1m[32m0.11356[0m[0m | time: 58.130s
[2K
| Adam | epoch: 010 | loss: 0.11356 - acc: 0.9643 -- iter: 192/268
[A[ATraining Step: 88  | total loss: [1m[32m0.12562[0m[0m | time: 67.363s
[2K
| Adam | epoch: 010 | loss: 0.12562 - acc: 0.9617 -- iter: 224/268
[A[ATraining Step: 89  | total loss: [1m[32m0.12386[0m[0m | time: 71.484s
[2K
| Adam | epoch: 010 | loss: 0.12386 - acc: 0.9592 -- iter: 256/268
[A[ATraining Step: 90  | total loss: [1m[32m0.11816[0m[0m | time: 80.384s
[2K
| Adam | epoch: 010 | loss: 0.11816 - acc: 0.9550 | val_loss: 2.20687 - val_acc: 0.6429 -- iter: 268/268
--
Training Step: 91  | total loss: [1m[32m0.10722[0m[0m | time: 9.816s
[2K
| Adam | epoch: 011 | loss: 0.10722 - acc: 0.9595 -- iter: 032/268
[A[ATraining Step: 92  | total loss: [1m[32m0.10845[0m[0m | time: 19.614s
[2K
| Adam | epoch: 011 | loss: 0.10845 - acc: 0.9542 -- iter: 064/268
[A[ATraining Step: 93  | total loss: [1m[32m0.11137[0m[0m | time: 28.556s
[2K
| Adam | epoch: 011 | loss: 0.11137 - acc: 0.9525 -- iter: 096/268
[A[ATraining Step: 94  | total loss: [1m[32m0.11270[0m[0m | time: 38.457s
[2K
| Adam | epoch: 011 | loss: 0.11270 - acc: 0.9541 -- iter: 128/268
[A[ATraining Step: 95  | total loss: [1m[32m0.11590[0m[0m | time: 47.391s
[2K
| Adam | epoch: 011 | loss: 0.11590 - acc: 0.9556 -- iter: 160/268
[A[ATraining Step: 96  | total loss: [1m[32m0.12797[0m[0m | time: 57.312s
[2K
| Adam | epoch: 011 | loss: 0.12797 - acc: 0.9569 -- iter: 192/268
[A[ATraining Step: 97  | total loss: [1m[32m0.12657[0m[0m | time: 67.038s
[2K
| Adam | epoch: 011 | loss: 0.12657 - acc: 0.9550 -- iter: 224/268
[A[ATraining Step: 98  | total loss: [1m[32m0.12519[0m[0m | time: 77.026s
[2K
| Adam | epoch: 011 | loss: 0.12519 - acc: 0.9563 -- iter: 256/268
[A[ATraining Step: 99  | total loss: [1m[32m0.11390[0m[0m | time: 85.003s
[2K
| Adam | epoch: 011 | loss: 0.11390 - acc: 0.9607 | val_loss: 0.75423 - val_acc: 0.7976 -- iter: 268/268
--
Training Step: 100  | total loss: [1m[32m0.11526[0m[0m | time: 4.191s
[2K
| Adam | epoch: 012 | loss: 0.11526 - acc: 0.9563 -- iter: 032/268
[A[ATraining Step: 101  | total loss: [1m[32m0.10432[0m[0m | time: 14.264s
[2K
| Adam | epoch: 012 | loss: 0.10432 - acc: 0.9607 -- iter: 064/268
[A[ATraining Step: 102  | total loss: [1m[32m0.10365[0m[0m | time: 24.133s
[2K
| Adam | epoch: 012 | loss: 0.10365 - acc: 0.9615 -- iter: 096/268
[A[ATraining Step: 103  | total loss: [1m[32m0.12893[0m[0m | time: 32.323s
[2K
| Adam | epoch: 012 | loss: 0.12893 - acc: 0.9560 -- iter: 128/268
[A[ATraining Step: 104  | total loss: [1m[32m0.13695[0m[0m | time: 41.883s
[2K
| Adam | epoch: 012 | loss: 0.13695 - acc: 0.9479 -- iter: 160/268
[A[ATraining Step: 105  | total loss: [1m[32m0.13781[0m[0m | time: 51.826s
[2K
| Adam | epoch: 012 | loss: 0.13781 - acc: 0.9499 -- iter: 192/268
[A[ATraining Step: 106  | total loss: [1m[32m0.13755[0m[0m | time: 61.547s
[2K
| Adam | epoch: 012 | loss: 0.13755 - acc: 0.9518 -- iter: 224/268
[A[ATraining Step: 107  | total loss: [1m[32m0.15452[0m[0m | time: 70.614s
[2K
| Adam | epoch: 012 | loss: 0.15452 - acc: 0.9441 -- iter: 256/268
[A[ATraining Step: 108  | total loss: [1m[32m0.17204[0m[0m | time: 84.893s
[2K
| Adam | epoch: 012 | loss: 0.17204 - acc: 0.9466 | val_loss: 5.49179 - val_acc: 0.3571 -- iter: 268/268
--
Training Step: 109  | total loss: [1m[32m0.15900[0m[0m | time: 4.276s
[2K
| Adam | epoch: 013 | loss: 0.15900 - acc: 0.9519 -- iter: 032/268
[A[ATraining Step: 110  | total loss: [1m[32m0.14456[0m[0m | time: 8.474s
[2K
| Adam | epoch: 013 | loss: 0.14456 - acc: 0.9568 -- iter: 064/268
[A[ATraining Step: 111  | total loss: [1m[32m0.13155[0m[0m | time: 18.605s
[2K
| Adam | epoch: 013 | loss: 0.13155 - acc: 0.9611 -- iter: 096/268
[A[ATraining Step: 112  | total loss: [1m[32m0.12516[0m[0m | time: 27.571s
[2K
| Adam | epoch: 013 | loss: 0.12516 - acc: 0.9618 -- iter: 128/268
[A[ATraining Step: 113  | total loss: [1m[32m0.12094[0m[0m | time: 37.146s
[2K
| Adam | epoch: 013 | loss: 0.12094 - acc: 0.9625 -- iter: 160/268
[A[ATraining Step: 114  | total loss: [1m[32m0.12390[0m[0m | time: 45.850s
[2K
| Adam | epoch: 013 | loss: 0.12390 - acc: 0.9600 -- iter: 192/268
[A[ATraining Step: 115  | total loss: [1m[32m0.12776[0m[0m | time: 55.614s
[2K
| Adam | epoch: 013 | loss: 0.12776 - acc: 0.9609 -- iter: 224/268
[A[ATraining Step: 116  | total loss: [1m[32m0.13593[0m[0m | time: 65.173s
[2K
| Adam | epoch: 013 | loss: 0.13593 - acc: 0.9554 -- iter: 256/268
[A[ATraining Step: 117  | total loss: [1m[32m0.13200[0m[0m | time: 79.264s
[2K
| Adam | epoch: 013 | loss: 0.13200 - acc: 0.9568 | val_loss: 1.20659 - val_acc: 0.7024 -- iter: 268/268
--
Training Step: 118  | total loss: [1m[32m0.13243[0m[0m | time: 8.975s
[2K
| Adam | epoch: 014 | loss: 0.13243 - acc: 0.9580 -- iter: 032/268
[A[ATraining Step: 119  | total loss: [1m[32m0.12478[0m[0m | time: 13.078s
[2K
| Adam | epoch: 014 | loss: 0.12478 - acc: 0.9622 -- iter: 064/268
[A[ATraining Step: 120  | total loss: [1m[32m0.11468[0m[0m | time: 17.429s
[2K
| Adam | epoch: 014 | loss: 0.11468 - acc: 0.9660 -- iter: 096/268
[A[ATraining Step: 121  | total loss: [1m[32m0.10479[0m[0m | time: 27.281s
[2K
| Adam | epoch: 014 | loss: 0.10479 - acc: 0.9694 -- iter: 128/268
[A[ATraining Step: 122  | total loss: [1m[32m0.09701[0m[0m | time: 36.856s
[2K
| Adam | epoch: 014 | loss: 0.09701 - acc: 0.9724 -- iter: 160/268
[A[ATraining Step: 123  | total loss: [1m[32m0.12019[0m[0m | time: 46.393s
[2K
| Adam | epoch: 014 | loss: 0.12019 - acc: 0.9596 -- iter: 192/268
[A[ATraining Step: 124  | total loss: [1m[32m0.11724[0m[0m | time: 55.789s
[2K
| Adam | epoch: 014 | loss: 0.11724 - acc: 0.9605 -- iter: 224/268
[A[ATraining Step: 125  | total loss: [1m[32m0.12737[0m[0m | time: 65.243s
[2K
| Adam | epoch: 014 | loss: 0.12737 - acc: 0.9519 -- iter: 256/268
[A[ATraining Step: 126  | total loss: [1m[32m0.11609[0m[0m | time: 79.512s
[2K
| Adam | epoch: 014 | loss: 0.11609 - acc: 0.9567 | val_loss: 2.86301 - val_acc: 0.5357 -- iter: 268/268
--
Training Step: 127  | total loss: [1m[32m0.10701[0m[0m | time: 9.639s
[2K
| Adam | epoch: 015 | loss: 0.10701 - acc: 0.9611 -- iter: 032/268
[A[ATraining Step: 128  | total loss: [1m[32m0.12608[0m[0m | time: 18.831s
[2K
| Adam | epoch: 015 | loss: 0.12608 - acc: 0.9618 -- iter: 064/268
[A[ATraining Step: 129  | total loss: [1m[32m0.11996[0m[0m | time: 23.062s
[2K
| Adam | epoch: 015 | loss: 0.11996 - acc: 0.9625 -- iter: 096/268
[A[ATraining Step: 130  | total loss: [1m[32m0.11103[0m[0m | time: 27.304s
[2K
| Adam | epoch: 015 | loss: 0.11103 - acc: 0.9663 -- iter: 128/268
[A[ATraining Step: 131  | total loss: [1m[32m0.10058[0m[0m | time: 37.612s
[2K
| Adam | epoch: 015 | loss: 0.10058 - acc: 0.9696 -- iter: 160/268
[A[ATraining Step: 132  | total loss: [1m[32m0.09522[0m[0m | time: 46.309s
[2K
| Adam | epoch: 015 | loss: 0.09522 - acc: 0.9727 -- iter: 192/268
[A[ATraining Step: 133  | total loss: [1m[32m0.08747[0m[0m | time: 56.130s
[2K
| Adam | epoch: 015 | loss: 0.08747 - acc: 0.9754 -- iter: 224/268
[A[ATraining Step: 134  | total loss: [1m[32m0.08180[0m[0m | time: 64.844s
[2K
| Adam | epoch: 015 | loss: 0.08180 - acc: 0.9779 -- iter: 256/268
[A[ATraining Step: 135  | total loss: [1m[32m0.07906[0m[0m | time: 79.461s
[2K
| Adam | epoch: 015 | loss: 0.07906 - acc: 0.9801 | val_loss: 0.77292 - val_acc: 0.7500 -- iter: 268/268
--
Validation AUC:0.875925925925926
Validation AUPRC:0.7333171850848961
Test AUC:0.8846153846153846
Test AUPRC:0.7005958101590098
BestTestF1Score	0.74	0.61	0.82	0.68	0.81	21	10	48	5	0.79
BestTestMCCScore	0.74	0.61	0.82	0.68	0.81	21	10	48	5	0.79
BestTestAccuracyScore	0.71	0.58	0.81	0.67	0.77	20	10	48	6	0.85
BestValidationF1Score	0.79	0.67	0.83	0.71	0.9	27	11	43	3	0.79
BestValidationMCC	0.79	0.67	0.83	0.71	0.9	27	11	43	3	0.79
BestValidationAccuracy	0.77	0.64	0.83	0.75	0.8	24	8	46	6	0.85
TestPredictions (Threshold:0.79)
CHEMBL525538,FP,INACT,0.9800000190734863	CHEMBL2380834,TP,ACT,1.0	CHEMBL1922121,TN,INACT,0.0	CHEMBL113996,TN,INACT,0.05000000074505806	CHEMBL40583,TN,INACT,0.029999999329447746	CHEMBL1940275,TP,ACT,1.0	CHEMBL406381,TP,ACT,0.9900000095367432	CHEMBL1721885,TP,ACT,0.9900000095367432	CHEMBL456796,TN,INACT,0.1599999964237213	CHEMBL520515,TN,INACT,0.0	CHEMBL3623375,FN,ACT,0.5400000214576721	CHEMBL50,TN,INACT,0.11999999731779099	CHEMBL1940108,TP,ACT,1.0	CHEMBL569880,TP,ACT,0.9900000095367432	CHEMBL1828883,TN,INACT,0.009999999776482582	CHEMBL600048,TN,INACT,0.009999999776482582	CHEMBL590568,TN,INACT,0.0	CHEMBL3133827,TN,INACT,0.11999999731779099	CHEMBL1242845,FP,INACT,0.8500000238418579	CHEMBL428647,FP,INACT,1.0	CHEMBL514499,TP,ACT,0.8500000238418579	CHEMBL557321,TN,INACT,0.029999999329447746	CHEMBL280998,TN,INACT,0.0	CHEMBL1829271,TN,INACT,0.0	CHEMBL430845,TN,INACT,0.0	CHEMBL498248,TN,INACT,0.05999999865889549	CHEMBL53898,TN,INACT,0.009999999776482582	CHEMBL1828884,TN,INACT,0.009999999776482582	CHEMBL529663,FN,ACT,0.30000001192092896	CHEMBL395664,TN,INACT,0.6600000262260437	CHEMBL55993,TN,INACT,0.0	CHEMBL3644863,TP,ACT,1.0	CHEMBL293749,TN,INACT,0.009999999776482582	CHEMBL3689079,FP,INACT,1.0	CHEMBL1945644,TN,INACT,0.09000000357627869	CHEMBL1922120,TN,INACT,0.0	CHEMBL7699,FP,INACT,0.9100000262260437	CHEMBL589119,TN,INACT,0.5899999737739563	CHEMBL1080271,FP,INACT,1.0	CHEMBL3628798,TP,ACT,0.9900000095367432	CHEMBL606027,TN,INACT,0.0	CHEMBL101779,FP,INACT,0.9700000286102295	CHEMBL246356,FP,INACT,1.0	CHEMBL55979,TN,INACT,0.7599999904632568	CHEMBL504075,FN,ACT,0.10000000149011612	CHEMBL67655,TN,INACT,0.019999999552965164	CHEMBL395665,TN,INACT,0.019999999552965164	CHEMBL2420911,TN,INACT,0.0	CHEMBL490241,TN,INACT,0.6899999976158142	CHEMBL1094835,TP,ACT,1.0	CHEMBL44,TN,INACT,0.009999999776482582	CHEMBL564829,FN,ACT,0.6499999761581421	CHEMBL591437,TN,INACT,0.0	CHEMBL602472,TN,INACT,0.7300000190734863	CHEMBL1095154,TP,ACT,1.0	CHEMBL2337363,TN,INACT,0.7699999809265137	CHEMBL245966,TN,INACT,0.009999999776482582	CHEMBL456113,TN,INACT,0.019999999552965164	CHEMBL3644871,TP,ACT,0.949999988079071	CHEMBL1094195,TP,ACT,1.0	CHEMBL450786,TP,ACT,1.0	CHEMBL1171273,TN,INACT,0.0	CHEMBL1683952,TN,INACT,0.10999999940395355	CHEMBL486487,TN,INACT,0.03999999910593033	CHEMBL208433,TN,INACT,0.25999999046325684	CHEMBL456378,TN,INACT,0.07999999821186066	CHEMBL1681932,TP,ACT,0.9900000095367432	CHEMBL1095957,FN,ACT,0.699999988079071	CHEMBL525530,TN,INACT,0.0	CHEMBL1668419,TP,ACT,0.9900000095367432	CHEMBL593274,TP,ACT,0.9900000095367432	CHEMBL259084,TP,ACT,1.0	CHEMBL20926,FP,INACT,1.0	CHEMBL113902,TN,INACT,0.009999999776482582	CHEMBL1088348,TN,INACT,0.7400000095367432	CHEMBL1258913,TP,ACT,1.0	CHEMBL1940103,TP,ACT,0.8399999737739563	CHEMBL56731,TN,INACT,0.0	CHEMBL366831,TN,INACT,0.6399999856948853	CHEMBL595406,TP,ACT,0.9900000095367432	CHEMBL132399,TN,INACT,0.029999999329447746	CHEMBL487526,TN,INACT,0.15000000596046448	CHEMBL1910278,TN,INACT,0.0	CHEMBL315546,FP,INACT,0.9200000166893005	

