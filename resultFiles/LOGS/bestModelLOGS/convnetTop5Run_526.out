CNNModel CHEMBL331 RMSprop 0.0005 15 256 0 0.8 False True
Number of active compounds :	1026
Number of inactive compounds :	1026
---------------------------------
Run id: CNNModel_CHEMBL331_RMSprop_0.0005_15_256_0_0.8_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL331_RMSprop_0.0005_15_256_0.8_True/
---------------------------------
Training samples: 1292
Validation samples: 405
--
Training Step: 1  | time: 0.774s
[2K
| RMSProp | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 0032/1292
[A[ATraining Step: 2  | total loss: [1m[32m0.62373[0m[0m | time: 1.388s
[2K
| RMSProp | epoch: 001 | loss: 0.62373 - acc: 0.4781 -- iter: 0064/1292
[A[ATraining Step: 3  | total loss: [1m[32m0.68052[0m[0m | time: 1.991s
[2K
| RMSProp | epoch: 001 | loss: 0.68052 - acc: 0.4960 -- iter: 0096/1292
[A[ATraining Step: 4  | total loss: [1m[32m0.69015[0m[0m | time: 2.578s
[2K
| RMSProp | epoch: 001 | loss: 0.69015 - acc: 0.4287 -- iter: 0128/1292
[A[ATraining Step: 5  | total loss: [1m[32m0.69256[0m[0m | time: 3.186s
[2K
| RMSProp | epoch: 001 | loss: 0.69256 - acc: 0.4132 -- iter: 0160/1292
[A[ATraining Step: 6  | total loss: [1m[32m0.69298[0m[0m | time: 3.774s
[2K
| RMSProp | epoch: 001 | loss: 0.69298 - acc: 0.4891 -- iter: 0192/1292
[A[ATraining Step: 7  | total loss: [1m[32m0.69331[0m[0m | time: 4.403s
[2K
| RMSProp | epoch: 001 | loss: 0.69331 - acc: 0.4581 -- iter: 0224/1292
[A[ATraining Step: 8  | total loss: [1m[32m0.69329[0m[0m | time: 5.000s
[2K
| RMSProp | epoch: 001 | loss: 0.69329 - acc: 0.4817 -- iter: 0256/1292
[A[ATraining Step: 9  | total loss: [1m[32m0.69317[0m[0m | time: 5.600s
[2K
| RMSProp | epoch: 001 | loss: 0.69317 - acc: 0.4748 -- iter: 0288/1292
[A[ATraining Step: 10  | total loss: [1m[32m0.69321[0m[0m | time: 6.204s
[2K
| RMSProp | epoch: 001 | loss: 0.69321 - acc: 0.4718 -- iter: 0320/1292
[A[ATraining Step: 11  | total loss: [1m[32m0.69321[0m[0m | time: 6.819s
[2K
| RMSProp | epoch: 001 | loss: 0.69321 - acc: 0.4852 -- iter: 0352/1292
[A[ATraining Step: 12  | total loss: [1m[32m0.69323[0m[0m | time: 7.425s
[2K
| RMSProp | epoch: 001 | loss: 0.69323 - acc: 0.4356 -- iter: 0384/1292
[A[ATraining Step: 13  | total loss: [1m[32m0.69316[0m[0m | time: 8.032s
[2K
| RMSProp | epoch: 001 | loss: 0.69316 - acc: 0.5034 -- iter: 0416/1292
[A[ATraining Step: 14  | total loss: [1m[32m0.69339[0m[0m | time: 8.641s
[2K
| RMSProp | epoch: 001 | loss: 0.69339 - acc: 0.4253 -- iter: 0448/1292
[A[ATraining Step: 15  | total loss: [1m[32m0.69325[0m[0m | time: 9.241s
[2K
| RMSProp | epoch: 001 | loss: 0.69325 - acc: 0.4912 -- iter: 0480/1292
[A[ATraining Step: 16  | total loss: [1m[32m0.69313[0m[0m | time: 9.842s
[2K
| RMSProp | epoch: 001 | loss: 0.69313 - acc: 0.5297 -- iter: 0512/1292
[A[ATraining Step: 17  | total loss: [1m[32m0.69317[0m[0m | time: 10.467s
[2K
| RMSProp | epoch: 001 | loss: 0.69317 - acc: 0.4965 -- iter: 0544/1292
[A[ATraining Step: 18  | total loss: [1m[32m0.69313[0m[0m | time: 11.066s
[2K
| RMSProp | epoch: 001 | loss: 0.69313 - acc: 0.5193 -- iter: 0576/1292
[A[ATraining Step: 19  | total loss: [1m[32m0.69317[0m[0m | time: 11.681s
[2K
| RMSProp | epoch: 001 | loss: 0.69317 - acc: 0.5233 -- iter: 0608/1292
[A[ATraining Step: 20  | total loss: [1m[32m0.69313[0m[0m | time: 12.300s
[2K
| RMSProp | epoch: 001 | loss: 0.69313 - acc: 0.4957 -- iter: 0640/1292
[A[ATraining Step: 21  | total loss: [1m[32m0.69311[0m[0m | time: 12.919s
[2K
| RMSProp | epoch: 001 | loss: 0.69311 - acc: 0.4971 -- iter: 0672/1292
[A[ATraining Step: 22  | total loss: [1m[32m0.69316[0m[0m | time: 13.541s
[2K
| RMSProp | epoch: 001 | loss: 0.69316 - acc: 0.5073 -- iter: 0704/1292
[A[ATraining Step: 23  | total loss: [1m[32m0.69304[0m[0m | time: 14.160s
[2K
| RMSProp | epoch: 001 | loss: 0.69304 - acc: 0.5324 -- iter: 0736/1292
[A[ATraining Step: 24  | total loss: [1m[32m0.69318[0m[0m | time: 14.774s
[2K
| RMSProp | epoch: 001 | loss: 0.69318 - acc: 0.5233 -- iter: 0768/1292
[A[ATraining Step: 25  | total loss: [1m[32m0.69318[0m[0m | time: 15.387s
[2K
| RMSProp | epoch: 001 | loss: 0.69318 - acc: 0.5255 -- iter: 0800/1292
[A[ATraining Step: 26  | total loss: [1m[32m0.69312[0m[0m | time: 16.018s
[2K
| RMSProp | epoch: 001 | loss: 0.69312 - acc: 0.5187 -- iter: 0832/1292
[A[ATraining Step: 27  | total loss: [1m[32m0.69310[0m[0m | time: 16.630s
[2K
| RMSProp | epoch: 001 | loss: 0.69310 - acc: 0.5380 -- iter: 0864/1292
[A[ATraining Step: 28  | total loss: [1m[32m0.69315[0m[0m | time: 17.264s
[2K
| RMSProp | epoch: 001 | loss: 0.69315 - acc: 0.5207 -- iter: 0896/1292
[A[ATraining Step: 29  | total loss: [1m[32m0.69317[0m[0m | time: 17.883s
[2K
| RMSProp | epoch: 001 | loss: 0.69317 - acc: 0.5157 -- iter: 0928/1292
[A[ATraining Step: 30  | total loss: [1m[32m0.69326[0m[0m | time: 18.489s
[2K
| RMSProp | epoch: 001 | loss: 0.69326 - acc: 0.5120 -- iter: 0960/1292
[A[ATraining Step: 31  | total loss: [1m[32m0.69326[0m[0m | time: 19.131s
[2K
| RMSProp | epoch: 001 | loss: 0.69326 - acc: 0.5020 -- iter: 0992/1292
[A[ATraining Step: 32  | total loss: [1m[32m0.69322[0m[0m | time: 19.733s
[2K
| RMSProp | epoch: 001 | loss: 0.69322 - acc: 0.5015 -- iter: 1024/1292
[A[ATraining Step: 33  | total loss: [1m[32m0.69327[0m[0m | time: 20.359s
[2K
| RMSProp | epoch: 001 | loss: 0.69327 - acc: 0.4738 -- iter: 1056/1292
[A[ATraining Step: 34  | total loss: [1m[32m0.69328[0m[0m | time: 20.966s
[2K
| RMSProp | epoch: 001 | loss: 0.69328 - acc: 0.4928 -- iter: 1088/1292
[A[ATraining Step: 35  | total loss: [1m[32m0.69324[0m[0m | time: 21.582s
[2K
| RMSProp | epoch: 001 | loss: 0.69324 - acc: 0.5008 -- iter: 1120/1292
[A[ATraining Step: 36  | total loss: [1m[32m0.69326[0m[0m | time: 22.200s
[2K
| RMSProp | epoch: 001 | loss: 0.69326 - acc: 0.4815 -- iter: 1152/1292
[A[ATraining Step: 37  | total loss: [1m[32m0.69323[0m[0m | time: 22.820s
[2K
| RMSProp | epoch: 001 | loss: 0.69323 - acc: 0.4727 -- iter: 1184/1292
[A[ATraining Step: 38  | total loss: [1m[32m0.69320[0m[0m | time: 23.458s
[2K
| RMSProp | epoch: 001 | loss: 0.69320 - acc: 0.4841 -- iter: 1216/1292
[A[ATraining Step: 39  | total loss: [1m[32m0.69323[0m[0m | time: 24.084s
[2K
| RMSProp | epoch: 001 | loss: 0.69323 - acc: 0.4513 -- iter: 1248/1292
[A[ATraining Step: 40  | total loss: [1m[32m0.69321[0m[0m | time: 24.752s
[2K
| RMSProp | epoch: 001 | loss: 0.69321 - acc: 0.4663 -- iter: 1280/1292
[A[ATraining Step: 41  | total loss: [1m[32m0.69325[0m[0m | time: 27.432s
[2K
| RMSProp | epoch: 001 | loss: 0.69325 - acc: 0.4667 | val_loss: 0.69306 - val_acc: 0.5333 -- iter: 1292/1292
--
Training Step: 42  | total loss: [1m[32m0.69314[0m[0m | time: 0.359s
[2K
| RMSProp | epoch: 002 | loss: 0.69314 - acc: 0.5177 -- iter: 0032/1292
[A[ATraining Step: 43  | total loss: [1m[32m0.69305[0m[0m | time: 1.224s
[2K
| RMSProp | epoch: 002 | loss: 0.69305 - acc: 0.5293 -- iter: 0064/1292
[A[ATraining Step: 44  | total loss: [1m[32m0.69306[0m[0m | time: 2.163s
[2K
| RMSProp | epoch: 002 | loss: 0.69306 - acc: 0.5242 -- iter: 0096/1292
[A[ATraining Step: 45  | total loss: [1m[32m0.69310[0m[0m | time: 3.127s
[2K
| RMSProp | epoch: 002 | loss: 0.69310 - acc: 0.5148 -- iter: 0128/1292
[A[ATraining Step: 46  | total loss: [1m[32m0.69309[0m[0m | time: 4.132s
[2K
| RMSProp | epoch: 002 | loss: 0.69309 - acc: 0.5280 -- iter: 0160/1292
[A[ATraining Step: 47  | total loss: [1m[32m0.69318[0m[0m | time: 5.226s
[2K
| RMSProp | epoch: 002 | loss: 0.69318 - acc: 0.5029 -- iter: 0192/1292
[A[ATraining Step: 48  | total loss: [1m[32m0.69316[0m[0m | time: 6.202s
[2K
| RMSProp | epoch: 002 | loss: 0.69316 - acc: 0.5175 -- iter: 0224/1292
[A[ATraining Step: 49  | total loss: [1m[32m0.69316[0m[0m | time: 7.142s
[2K
| RMSProp | epoch: 002 | loss: 0.69316 - acc: 0.5246 -- iter: 0256/1292
[A[ATraining Step: 50  | total loss: [1m[32m0.69321[0m[0m | time: 8.190s
[2K
| RMSProp | epoch: 002 | loss: 0.69321 - acc: 0.5063 -- iter: 0288/1292
[A[ATraining Step: 51  | total loss: [1m[32m0.69327[0m[0m | time: 9.254s
[2K
| RMSProp | epoch: 002 | loss: 0.69327 - acc: 0.4862 -- iter: 0320/1292
[A[ATraining Step: 52  | total loss: [1m[32m0.69326[0m[0m | time: 10.136s
[2K
| RMSProp | epoch: 002 | loss: 0.69326 - acc: 0.4836 -- iter: 0352/1292
[A[ATraining Step: 53  | total loss: [1m[32m0.69324[0m[0m | time: 11.045s
[2K
| RMSProp | epoch: 002 | loss: 0.69324 - acc: 0.4860 -- iter: 0384/1292
[A[ATraining Step: 54  | total loss: [1m[32m0.69322[0m[0m | time: 11.993s
[2K
| RMSProp | epoch: 002 | loss: 0.69322 - acc: 0.4926 -- iter: 0416/1292
[A[ATraining Step: 55  | total loss: [1m[32m0.69322[0m[0m | time: 13.001s
[2K
| RMSProp | epoch: 002 | loss: 0.69322 - acc: 0.4937 -- iter: 0448/1292
[A[ATraining Step: 56  | total loss: [1m[32m0.69323[0m[0m | time: 14.043s
[2K
| RMSProp | epoch: 002 | loss: 0.69323 - acc: 0.4814 -- iter: 0480/1292
[A[ATraining Step: 57  | total loss: [1m[32m0.69322[0m[0m | time: 15.134s
[2K
| RMSProp | epoch: 002 | loss: 0.69322 - acc: 0.4969 -- iter: 0512/1292
[A[ATraining Step: 58  | total loss: [1m[32m0.69319[0m[0m | time: 15.978s
[2K
| RMSProp | epoch: 002 | loss: 0.69319 - acc: 0.5144 -- iter: 0544/1292
[A[ATraining Step: 59  | total loss: [1m[32m0.69318[0m[0m | time: 17.047s
[2K
| RMSProp | epoch: 002 | loss: 0.69318 - acc: 0.5209 -- iter: 0576/1292
[A[ATraining Step: 60  | total loss: [1m[32m0.69319[0m[0m | time: 18.120s
[2K
| RMSProp | epoch: 002 | loss: 0.69319 - acc: 0.5140 -- iter: 0608/1292
[A[ATraining Step: 61  | total loss: [1m[32m0.69314[0m[0m | time: 19.049s
[2K
| RMSProp | epoch: 002 | loss: 0.69314 - acc: 0.5244 -- iter: 0640/1292
[A[ATraining Step: 62  | total loss: [1m[32m0.69317[0m[0m | time: 19.932s
[2K
| RMSProp | epoch: 002 | loss: 0.69317 - acc: 0.5172 -- iter: 0672/1292
[A[ATraining Step: 63  | total loss: [1m[32m0.69315[0m[0m | time: 20.862s
[2K
| RMSProp | epoch: 002 | loss: 0.69315 - acc: 0.5190 -- iter: 0704/1292
[A[ATraining Step: 64  | total loss: [1m[32m0.69318[0m[0m | time: 21.732s
[2K
| RMSProp | epoch: 002 | loss: 0.69318 - acc: 0.5166 -- iter: 0736/1292
[A[ATraining Step: 65  | total loss: [1m[32m0.69329[0m[0m | time: 22.720s
[2K
| RMSProp | epoch: 002 | loss: 0.69329 - acc: 0.4953 -- iter: 0768/1292
[A[ATraining Step: 66  | total loss: [1m[32m0.69332[0m[0m | time: 23.785s
[2K
| RMSProp | epoch: 002 | loss: 0.69332 - acc: 0.4921 -- iter: 0800/1292
[A[ATraining Step: 67  | total loss: [1m[32m0.69327[0m[0m | time: 24.628s
[2K
| RMSProp | epoch: 002 | loss: 0.69327 - acc: 0.5080 -- iter: 0832/1292
[A[ATraining Step: 68  | total loss: [1m[32m0.69324[0m[0m | time: 25.653s
[2K
| RMSProp | epoch: 002 | loss: 0.69324 - acc: 0.5182 -- iter: 0864/1292
[A[ATraining Step: 69  | total loss: [1m[32m0.69322[0m[0m | time: 26.731s
[2K
| RMSProp | epoch: 002 | loss: 0.69322 - acc: 0.5270 -- iter: 0896/1292
[A[ATraining Step: 70  | total loss: [1m[32m0.69317[0m[0m | time: 27.683s
[2K
| RMSProp | epoch: 002 | loss: 0.69317 - acc: 0.5419 -- iter: 0928/1292
[A[ATraining Step: 71  | total loss: [1m[32m0.69316[0m[0m | time: 28.533s
[2K
| RMSProp | epoch: 002 | loss: 0.69316 - acc: 0.5371 -- iter: 0960/1292
[A[ATraining Step: 72  | total loss: [1m[32m0.69317[0m[0m | time: 29.551s
[2K
| RMSProp | epoch: 002 | loss: 0.69317 - acc: 0.5224 -- iter: 0992/1292
[A[ATraining Step: 73  | total loss: [1m[32m0.69317[0m[0m | time: 30.495s
[2K
| RMSProp | epoch: 002 | loss: 0.69317 - acc: 0.5199 -- iter: 1024/1292
[A[ATraining Step: 74  | total loss: [1m[32m0.69319[0m[0m | time: 31.584s
[2K
| RMSProp | epoch: 002 | loss: 0.69319 - acc: 0.5143 -- iter: 1056/1292
[A[ATraining Step: 75  | total loss: [1m[32m0.69320[0m[0m | time: 32.624s
[2K
| RMSProp | epoch: 002 | loss: 0.69320 - acc: 0.5060 -- iter: 1088/1292
[A[ATraining Step: 76  | total loss: [1m[32m0.69314[0m[0m | time: 33.531s
[2K
| RMSProp | epoch: 002 | loss: 0.69314 - acc: 0.5321 -- iter: 1120/1292
[A[ATraining Step: 77  | total loss: [1m[32m0.69313[0m[0m | time: 34.600s
[2K
| RMSProp | epoch: 002 | loss: 0.69313 - acc: 0.5320 -- iter: 1152/1292
[A[ATraining Step: 78  | total loss: [1m[32m0.69314[0m[0m | time: 35.648s
[2K
| RMSProp | epoch: 002 | loss: 0.69314 - acc: 0.5189 -- iter: 1184/1292
[A[ATraining Step: 79  | total loss: [1m[32m0.69309[0m[0m | time: 36.588s
[2K
| RMSProp | epoch: 002 | loss: 0.69309 - acc: 0.5299 -- iter: 1216/1292
[A[ATraining Step: 80  | total loss: [1m[32m0.69320[0m[0m | time: 37.568s
[2K
| RMSProp | epoch: 002 | loss: 0.69320 - acc: 0.5076 -- iter: 1248/1292
[A[ATraining Step: 81  | total loss: [1m[32m0.69318[0m[0m | time: 38.533s
[2K
| RMSProp | epoch: 002 | loss: 0.69318 - acc: 0.5195 -- iter: 1280/1292
[A[ATraining Step: 82  | total loss: [1m[32m0.69314[0m[0m | time: 41.583s
[2K
| RMSProp | epoch: 002 | loss: 0.69314 - acc: 0.5238 | val_loss: 0.69321 - val_acc: 0.4667 -- iter: 1292/1292
--
Training Step: 83  | total loss: [1m[32m0.69316[0m[0m | time: 0.446s
[2K
| RMSProp | epoch: 003 | loss: 0.69316 - acc: 0.5183 -- iter: 0032/1292
[A[ATraining Step: 84  | total loss: [1m[32m0.69311[0m[0m | time: 0.784s
[2K
| RMSProp | epoch: 003 | loss: 0.69311 - acc: 0.5248 -- iter: 0064/1292
[A[ATraining Step: 85  | total loss: [1m[32m0.69306[0m[0m | time: 1.648s
[2K
| RMSProp | epoch: 003 | loss: 0.69306 - acc: 0.5306 -- iter: 0096/1292
[A[ATraining Step: 86  | total loss: [1m[32m0.69307[0m[0m | time: 2.721s
[2K
| RMSProp | epoch: 003 | loss: 0.69307 - acc: 0.5307 -- iter: 0128/1292
[A[ATraining Step: 87  | total loss: [1m[32m0.69298[0m[0m | time: 3.780s
[2K
| RMSProp | epoch: 003 | loss: 0.69298 - acc: 0.5370 -- iter: 0160/1292
[A[ATraining Step: 88  | total loss: [1m[32m0.69293[0m[0m | time: 4.663s
[2K
| RMSProp | epoch: 003 | loss: 0.69293 - acc: 0.5396 -- iter: 0192/1292
[A[ATraining Step: 89  | total loss: [1m[32m0.69280[0m[0m | time: 5.560s
[2K
| RMSProp | epoch: 003 | loss: 0.69280 - acc: 0.5481 -- iter: 0224/1292
[A[ATraining Step: 90  | total loss: [1m[32m0.69300[0m[0m | time: 6.517s
[2K
| RMSProp | epoch: 003 | loss: 0.69300 - acc: 0.5339 -- iter: 0256/1292
[A[ATraining Step: 91  | total loss: [1m[32m0.69297[0m[0m | time: 7.477s
[2K
| RMSProp | epoch: 003 | loss: 0.69297 - acc: 0.5337 -- iter: 0288/1292
[A[ATraining Step: 92  | total loss: [1m[32m0.69319[0m[0m | time: 8.449s
[2K
| RMSProp | epoch: 003 | loss: 0.69319 - acc: 0.5178 -- iter: 0320/1292
[A[ATraining Step: 93  | total loss: [1m[32m0.69305[0m[0m | time: 9.561s
[2K
| RMSProp | epoch: 003 | loss: 0.69305 - acc: 0.5285 -- iter: 0352/1292
[A[ATraining Step: 94  | total loss: [1m[32m0.69310[0m[0m | time: 10.461s
[2K
| RMSProp | epoch: 003 | loss: 0.69310 - acc: 0.5225 -- iter: 0384/1292
[A[ATraining Step: 95  | total loss: [1m[32m0.69321[0m[0m | time: 11.449s
[2K
| RMSProp | epoch: 003 | loss: 0.69321 - acc: 0.5109 -- iter: 0416/1292
[A[ATraining Step: 96  | total loss: [1m[32m0.69329[0m[0m | time: 12.523s
[2K
| RMSProp | epoch: 003 | loss: 0.69329 - acc: 0.5036 -- iter: 0448/1292
[A[ATraining Step: 97  | total loss: [1m[32m0.69334[0m[0m | time: 13.543s
[2K
| RMSProp | epoch: 003 | loss: 0.69334 - acc: 0.4970 -- iter: 0480/1292
[A[ATraining Step: 98  | total loss: [1m[32m0.69332[0m[0m | time: 14.334s
[2K
| RMSProp | epoch: 003 | loss: 0.69332 - acc: 0.5004 -- iter: 0512/1292
[A[ATraining Step: 99  | total loss: [1m[32m0.69334[0m[0m | time: 15.278s
[2K
| RMSProp | epoch: 003 | loss: 0.69334 - acc: 0.4941 -- iter: 0544/1292
[A[ATraining Step: 100  | total loss: [1m[32m0.69332[0m[0m | time: 16.217s
[2K
| RMSProp | epoch: 003 | loss: 0.69332 - acc: 0.4978 -- iter: 0576/1292
[A[ATraining Step: 101  | total loss: [1m[32m0.69330[0m[0m | time: 17.188s
[2K
| RMSProp | epoch: 003 | loss: 0.69330 - acc: 0.5012 -- iter: 0608/1292
[A[ATraining Step: 102  | total loss: [1m[32m0.69317[0m[0m | time: 18.198s
[2K
| RMSProp | epoch: 003 | loss: 0.69317 - acc: 0.5135 -- iter: 0640/1292
[A[ATraining Step: 103  | total loss: [1m[32m0.69317[0m[0m | time: 19.254s
[2K
| RMSProp | epoch: 003 | loss: 0.69317 - acc: 0.5122 -- iter: 0672/1292
[A[ATraining Step: 104  | total loss: [1m[32m0.69343[0m[0m | time: 20.104s
[2K
| RMSProp | epoch: 003 | loss: 0.69343 - acc: 0.4922 -- iter: 0704/1292
[A[ATraining Step: 105  | total loss: [1m[32m0.69336[0m[0m | time: 21.167s
[2K
| RMSProp | epoch: 003 | loss: 0.69336 - acc: 0.5086 -- iter: 0736/1292
[A[ATraining Step: 106  | total loss: [1m[32m0.69327[0m[0m | time: 22.219s
[2K
| RMSProp | epoch: 003 | loss: 0.69327 - acc: 0.5203 -- iter: 0768/1292
[A[ATraining Step: 107  | total loss: [1m[32m0.69346[0m[0m | time: 23.275s
[2K
| RMSProp | epoch: 003 | loss: 0.69346 - acc: 0.5026 -- iter: 0800/1292
[A[ATraining Step: 108  | total loss: [1m[32m0.69337[0m[0m | time: 24.084s
[2K
| RMSProp | epoch: 003 | loss: 0.69337 - acc: 0.5086 -- iter: 0832/1292
[A[ATraining Step: 109  | total loss: [1m[32m0.69340[0m[0m | time: 25.021s
[2K
| RMSProp | epoch: 003 | loss: 0.69340 - acc: 0.4984 -- iter: 0864/1292
[A[ATraining Step: 110  | total loss: [1m[32m0.69334[0m[0m | time: 25.943s
[2K
| RMSProp | epoch: 003 | loss: 0.69334 - acc: 0.4985 -- iter: 0896/1292
[A[ATraining Step: 111  | total loss: [1m[32m0.69320[0m[0m | time: 26.919s
[2K
| RMSProp | epoch: 003 | loss: 0.69320 - acc: 0.5143 -- iter: 0928/1292
[A[ATraining Step: 112  | total loss: [1m[32m0.69320[0m[0m | time: 27.907s
[2K
| RMSProp | epoch: 003 | loss: 0.69320 - acc: 0.5129 -- iter: 0960/1292
[A[ATraining Step: 113  | total loss: [1m[32m0.69289[0m[0m | time: 29.027s
[2K
| RMSProp | epoch: 003 | loss: 0.69289 - acc: 0.5303 -- iter: 0992/1292
[A[ATraining Step: 114  | total loss: [1m[32m0.69325[0m[0m | time: 29.919s
[2K
| RMSProp | epoch: 003 | loss: 0.69325 - acc: 0.5148 -- iter: 1024/1292
[A[ATraining Step: 115  | total loss: [1m[32m0.69308[0m[0m | time: 31.001s
[2K
| RMSProp | epoch: 003 | loss: 0.69308 - acc: 0.5196 -- iter: 1056/1292
[A[ATraining Step: 116  | total loss: [1m[32m0.69290[0m[0m | time: 32.080s
[2K
| RMSProp | epoch: 003 | loss: 0.69290 - acc: 0.5270 -- iter: 1088/1292
[A[ATraining Step: 117  | total loss: [1m[32m0.69284[0m[0m | time: 33.080s
[2K
| RMSProp | epoch: 003 | loss: 0.69284 - acc: 0.5274 -- iter: 1120/1292
[A[ATraining Step: 118  | total loss: [1m[32m0.69289[0m[0m | time: 33.947s
[2K
| RMSProp | epoch: 003 | loss: 0.69289 - acc: 0.5247 -- iter: 1152/1292
[A[ATraining Step: 119  | total loss: [1m[32m0.69317[0m[0m | time: 34.888s
[2K
| RMSProp | epoch: 003 | loss: 0.69317 - acc: 0.5128 -- iter: 1184/1292
[A[ATraining Step: 120  | total loss: [1m[32m0.69347[0m[0m | time: 35.884s
[2K
| RMSProp | epoch: 003 | loss: 0.69347 - acc: 0.4990 -- iter: 1216/1292
[A[ATraining Step: 121  | total loss: [1m[32m0.69339[0m[0m | time: 36.844s
[2K
| RMSProp | epoch: 003 | loss: 0.69339 - acc: 0.5023 -- iter: 1248/1292
[A[ATraining Step: 122  | total loss: [1m[32m0.69340[0m[0m | time: 37.990s
[2K
| RMSProp | epoch: 003 | loss: 0.69340 - acc: 0.4989 -- iter: 1280/1292
[A[ATraining Step: 123  | total loss: [1m[32m0.69323[0m[0m | time: 40.756s
[2K
| RMSProp | epoch: 003 | loss: 0.69323 - acc: 0.5115 | val_loss: 0.69407 - val_acc: 0.4667 -- iter: 1292/1292
--
Training Step: 124  | total loss: [1m[32m0.69307[0m[0m | time: 1.072s
[2K
| RMSProp | epoch: 004 | loss: 0.69307 - acc: 0.5166 -- iter: 0032/1292
[A[ATraining Step: 125  | total loss: [1m[32m0.69308[0m[0m | time: 1.532s
[2K
| RMSProp | epoch: 004 | loss: 0.69308 - acc: 0.5150 -- iter: 0064/1292
[A[ATraining Step: 126  | total loss: [1m[32m0.69261[0m[0m | time: 1.891s
[2K
| RMSProp | epoch: 004 | loss: 0.69261 - acc: 0.5301 -- iter: 0096/1292
[A[ATraining Step: 127  | total loss: [1m[32m0.69193[0m[0m | time: 2.721s
[2K
| RMSProp | epoch: 004 | loss: 0.69193 - acc: 0.5438 -- iter: 0128/1292
[A[ATraining Step: 128  | total loss: [1m[32m0.69118[0m[0m | time: 3.737s
[2K
| RMSProp | epoch: 004 | loss: 0.69118 - acc: 0.5550 -- iter: 0160/1292
[A[ATraining Step: 129  | total loss: [1m[32m0.69146[0m[0m | time: 4.729s
[2K
| RMSProp | epoch: 004 | loss: 0.69146 - acc: 0.5495 -- iter: 0192/1292
[A[ATraining Step: 130  | total loss: [1m[32m0.69172[0m[0m | time: 5.794s
[2K
| RMSProp | epoch: 004 | loss: 0.69172 - acc: 0.5446 -- iter: 0224/1292
[A[ATraining Step: 131  | total loss: [1m[32m0.69237[0m[0m | time: 6.984s
[2K
| RMSProp | epoch: 004 | loss: 0.69237 - acc: 0.5339 -- iter: 0256/1292
[A[ATraining Step: 132  | total loss: [1m[32m0.69367[0m[0m | time: 7.860s
[2K
| RMSProp | epoch: 004 | loss: 0.69367 - acc: 0.5086 -- iter: 0288/1292
[A[ATraining Step: 133  | total loss: [1m[32m0.69353[0m[0m | time: 8.876s
[2K
| RMSProp | epoch: 004 | loss: 0.69353 - acc: 0.5109 -- iter: 0320/1292
[A[ATraining Step: 134  | total loss: [1m[32m0.69287[0m[0m | time: 9.931s
[2K
| RMSProp | epoch: 004 | loss: 0.69287 - acc: 0.5317 -- iter: 0352/1292
[A[ATraining Step: 135  | total loss: [1m[32m0.69235[0m[0m | time: 10.950s
[2K
| RMSProp | epoch: 004 | loss: 0.69235 - acc: 0.5410 -- iter: 0384/1292
[A[ATraining Step: 136  | total loss: [1m[32m0.69197[0m[0m | time: 11.822s
[2K
| RMSProp | epoch: 004 | loss: 0.69197 - acc: 0.5463 -- iter: 0416/1292
[A[ATraining Step: 137  | total loss: [1m[32m0.69163[0m[0m | time: 12.757s
[2K
| RMSProp | epoch: 004 | loss: 0.69163 - acc: 0.5479 -- iter: 0448/1292
[A[ATraining Step: 138  | total loss: [1m[32m0.69108[0m[0m | time: 13.680s
[2K
| RMSProp | epoch: 004 | loss: 0.69108 - acc: 0.5525 -- iter: 0480/1292
[A[ATraining Step: 139  | total loss: [1m[32m0.69208[0m[0m | time: 14.584s
[2K
| RMSProp | epoch: 004 | loss: 0.69208 - acc: 0.5410 -- iter: 0512/1292
[A[ATraining Step: 140  | total loss: [1m[32m0.69174[0m[0m | time: 15.585s
[2K
| RMSProp | epoch: 004 | loss: 0.69174 - acc: 0.5431 -- iter: 0544/1292
[A[ATraining Step: 141  | total loss: [1m[32m0.69200[0m[0m | time: 16.656s
[2K
| RMSProp | epoch: 004 | loss: 0.69200 - acc: 0.5388 -- iter: 0576/1292
[A[ATraining Step: 142  | total loss: [1m[32m0.69192[0m[0m | time: 17.562s
[2K
| RMSProp | epoch: 004 | loss: 0.69192 - acc: 0.5381 -- iter: 0608/1292
[A[ATraining Step: 143  | total loss: [1m[32m0.69188[0m[0m | time: 18.640s
[2K
| RMSProp | epoch: 004 | loss: 0.69188 - acc: 0.5374 -- iter: 0640/1292
[A[ATraining Step: 144  | total loss: [1m[32m0.69188[0m[0m | time: 19.710s
[2K
| RMSProp | epoch: 004 | loss: 0.69188 - acc: 0.5368 -- iter: 0672/1292
[A[ATraining Step: 145  | total loss: [1m[32m0.69285[0m[0m | time: 20.711s
[2K
| RMSProp | epoch: 004 | loss: 0.69285 - acc: 0.5237 -- iter: 0704/1292
[A[ATraining Step: 146  | total loss: [1m[32m0.69386[0m[0m | time: 21.569s
[2K
| RMSProp | epoch: 004 | loss: 0.69386 - acc: 0.5026 -- iter: 0736/1292
[A[ATraining Step: 147  | total loss: [1m[32m0.69369[0m[0m | time: 22.508s
[2K
| RMSProp | epoch: 004 | loss: 0.69369 - acc: 0.5055 -- iter: 0768/1292
[A[ATraining Step: 148  | total loss: [1m[32m0.69357[0m[0m | time: 23.503s
[2K
| RMSProp | epoch: 004 | loss: 0.69357 - acc: 0.5080 -- iter: 0800/1292
[A[ATraining Step: 149  | total loss: [1m[32m0.69377[0m[0m | time: 24.463s
[2K
| RMSProp | epoch: 004 | loss: 0.69377 - acc: 0.4979 -- iter: 0832/1292
[A[ATraining Step: 150  | total loss: [1m[32m0.69367[0m[0m | time: 25.487s
[2K
| RMSProp | epoch: 004 | loss: 0.69367 - acc: 0.5043 -- iter: 0864/1292
[A[ATraining Step: 151  | total loss: [1m[32m0.69349[0m[0m | time: 26.541s
[2K
| RMSProp | epoch: 004 | loss: 0.69349 - acc: 0.5101 -- iter: 0896/1292
[A[ATraining Step: 152  | total loss: [1m[32m0.69310[0m[0m | time: 27.475s
[2K
| RMSProp | epoch: 004 | loss: 0.69310 - acc: 0.5216 -- iter: 0928/1292
[A[ATraining Step: 153  | total loss: [1m[32m0.69353[0m[0m | time: 28.620s
[2K
| RMSProp | epoch: 004 | loss: 0.69353 - acc: 0.5101 -- iter: 0960/1292
[A[ATraining Step: 154  | total loss: [1m[32m0.69348[0m[0m | time: 29.660s
[2K
| RMSProp | epoch: 004 | loss: 0.69348 - acc: 0.5091 -- iter: 0992/1292
[A[ATraining Step: 155  | total loss: [1m[32m0.69304[0m[0m | time: 30.589s
[2K
| RMSProp | epoch: 004 | loss: 0.69304 - acc: 0.5238 -- iter: 1024/1292
[A[ATraining Step: 156  | total loss: [1m[32m0.69359[0m[0m | time: 31.480s
[2K
| RMSProp | epoch: 004 | loss: 0.69359 - acc: 0.5089 -- iter: 1056/1292
[A[ATraining Step: 157  | total loss: [1m[32m0.69366[0m[0m | time: 32.444s
[2K
| RMSProp | epoch: 004 | loss: 0.69366 - acc: 0.5049 -- iter: 1088/1292
[A[ATraining Step: 158  | total loss: [1m[32m0.69376[0m[0m | time: 33.423s
[2K
| RMSProp | epoch: 004 | loss: 0.69376 - acc: 0.4982 -- iter: 1120/1292
[A[ATraining Step: 159  | total loss: [1m[32m0.69393[0m[0m | time: 34.386s
[2K
| RMSProp | epoch: 004 | loss: 0.69393 - acc: 0.4858 -- iter: 1152/1292
[A[ATraining Step: 160  | total loss: [1m[32m0.69384[0m[0m | time: 35.573s
[2K
| RMSProp | epoch: 004 | loss: 0.69384 - acc: 0.4841 -- iter: 1184/1292
[A[ATraining Step: 161  | total loss: [1m[32m0.69381[0m[0m | time: 36.521s
[2K
| RMSProp | epoch: 004 | loss: 0.69381 - acc: 0.4638 -- iter: 1216/1292
[A[ATraining Step: 162  | total loss: [1m[32m0.69377[0m[0m | time: 37.465s
[2K
| RMSProp | epoch: 004 | loss: 0.69377 - acc: 0.4675 -- iter: 1248/1292
[A[ATraining Step: 163  | total loss: [1m[32m0.69338[0m[0m | time: 38.542s
[2K
| RMSProp | epoch: 004 | loss: 0.69338 - acc: 0.4832 -- iter: 1280/1292
[A[ATraining Step: 164  | total loss: [1m[32m0.69279[0m[0m | time: 41.170s
[2K
| RMSProp | epoch: 004 | loss: 0.69279 - acc: 0.4974 | val_loss: 0.69471 - val_acc: 0.4667 -- iter: 1292/1292
--
Training Step: 165  | total loss: [1m[32m0.69338[0m[0m | time: 1.016s
[2K
| RMSProp | epoch: 005 | loss: 0.69338 - acc: 0.4914 -- iter: 0032/1292
[A[ATraining Step: 166  | total loss: [1m[32m0.69368[0m[0m | time: 1.977s
[2K
| RMSProp | epoch: 005 | loss: 0.69368 - acc: 0.4860 -- iter: 0064/1292
[A[ATraining Step: 167  | total loss: [1m[32m0.69420[0m[0m | time: 2.368s
[2K
| RMSProp | epoch: 005 | loss: 0.69420 - acc: 0.4687 -- iter: 0096/1292
[A[ATraining Step: 168  | total loss: [1m[32m0.69424[0m[0m | time: 2.751s
[2K
| RMSProp | epoch: 005 | loss: 0.69424 - acc: 0.4635 -- iter: 0128/1292
[A[ATraining Step: 169  | total loss: [1m[32m0.69420[0m[0m | time: 3.828s
[2K
| RMSProp | epoch: 005 | loss: 0.69420 - acc: 0.4588 -- iter: 0160/1292
[A[ATraining Step: 170  | total loss: [1m[32m0.69406[0m[0m | time: 4.839s
[2K
| RMSProp | epoch: 005 | loss: 0.69406 - acc: 0.4942 -- iter: 0192/1292
[A[ATraining Step: 171  | total loss: [1m[32m0.69394[0m[0m | time: 5.719s
[2K
| RMSProp | epoch: 005 | loss: 0.69394 - acc: 0.4979 -- iter: 0224/1292
[A[ATraining Step: 172  | total loss: [1m[32m0.69387[0m[0m | time: 6.761s
[2K
| RMSProp | epoch: 005 | loss: 0.69387 - acc: 0.4981 -- iter: 0256/1292
[A[ATraining Step: 173  | total loss: [1m[32m0.69372[0m[0m | time: 7.792s
[2K
| RMSProp | epoch: 005 | loss: 0.69372 - acc: 0.5045 -- iter: 0288/1292
[A[ATraining Step: 174  | total loss: [1m[32m0.69350[0m[0m | time: 8.712s
[2K
| RMSProp | epoch: 005 | loss: 0.69350 - acc: 0.5134 -- iter: 0320/1292
[A[ATraining Step: 175  | total loss: [1m[32m0.69329[0m[0m | time: 9.547s
[2K
| RMSProp | epoch: 005 | loss: 0.69329 - acc: 0.5183 -- iter: 0352/1292
[A[ATraining Step: 176  | total loss: [1m[32m0.69375[0m[0m | time: 10.468s
[2K
| RMSProp | epoch: 005 | loss: 0.69375 - acc: 0.5040 -- iter: 0384/1292
[A[ATraining Step: 177  | total loss: [1m[32m0.69322[0m[0m | time: 11.397s
[2K
| RMSProp | epoch: 005 | loss: 0.69322 - acc: 0.5286 -- iter: 0416/1292
[A[ATraining Step: 178  | total loss: [1m[32m0.69365[0m[0m | time: 12.342s
[2K
| RMSProp | epoch: 005 | loss: 0.69365 - acc: 0.5133 -- iter: 0448/1292
[A[ATraining Step: 179  | total loss: [1m[32m0.69397[0m[0m | time: 13.383s
[2K
| RMSProp | epoch: 005 | loss: 0.69397 - acc: 0.4963 -- iter: 0480/1292
[A[ATraining Step: 180  | total loss: [1m[32m0.69380[0m[0m | time: 14.412s
[2K
| RMSProp | epoch: 005 | loss: 0.69380 - acc: 0.5029 -- iter: 0512/1292
[A[ATraining Step: 181  | total loss: [1m[32m0.69355[0m[0m | time: 15.245s
[2K
| RMSProp | epoch: 005 | loss: 0.69355 - acc: 0.5120 -- iter: 0544/1292
[A[ATraining Step: 182  | total loss: [1m[32m0.69335[0m[0m | time: 16.373s
[2K
| RMSProp | epoch: 005 | loss: 0.69335 - acc: 0.5171 -- iter: 0576/1292
[A[ATraining Step: 183  | total loss: [1m[32m0.69333[0m[0m | time: 17.448s
[2K
| RMSProp | epoch: 005 | loss: 0.69333 - acc: 0.5153 -- iter: 0608/1292
[A[ATraining Step: 184  | total loss: [1m[32m0.69338[0m[0m | time: 18.425s
[2K
| RMSProp | epoch: 005 | loss: 0.69338 - acc: 0.5107 -- iter: 0640/1292
[A[ATraining Step: 185  | total loss: [1m[32m0.69376[0m[0m | time: 19.273s
[2K
| RMSProp | epoch: 005 | loss: 0.69376 - acc: 0.4909 -- iter: 0672/1292
[A[ATraining Step: 186  | total loss: [1m[32m0.69374[0m[0m | time: 20.211s
[2K
| RMSProp | epoch: 005 | loss: 0.69374 - acc: 0.4824 -- iter: 0704/1292
[A[ATraining Step: 187  | total loss: [1m[32m0.69373[0m[0m | time: 21.183s
[2K
| RMSProp | epoch: 005 | loss: 0.69373 - acc: 0.4810 -- iter: 0736/1292
[A[ATraining Step: 188  | total loss: [1m[32m0.69376[0m[0m | time: 22.208s
[2K
| RMSProp | epoch: 005 | loss: 0.69376 - acc: 0.4767 -- iter: 0768/1292
[A[ATraining Step: 189  | total loss: [1m[32m0.69369[0m[0m | time: 23.240s
[2K
| RMSProp | epoch: 005 | loss: 0.69369 - acc: 0.4821 -- iter: 0800/1292
[A[ATraining Step: 190  | total loss: [1m[32m0.69371[0m[0m | time: 24.175s
[2K
| RMSProp | epoch: 005 | loss: 0.69371 - acc: 0.4746 -- iter: 0832/1292
[A[ATraining Step: 191  | total loss: [1m[32m0.69377[0m[0m | time: 25.090s
[2K
| RMSProp | epoch: 005 | loss: 0.69377 - acc: 0.4615 -- iter: 0864/1292
[A[ATraining Step: 192  | total loss: [1m[32m0.69353[0m[0m | time: 26.218s
[2K
| RMSProp | epoch: 005 | loss: 0.69353 - acc: 0.4778 -- iter: 0896/1292
[A[ATraining Step: 193  | total loss: [1m[32m0.69375[0m[0m | time: 27.298s
[2K
| RMSProp | epoch: 005 | loss: 0.69375 - acc: 0.4707 -- iter: 0928/1292
[A[ATraining Step: 194  | total loss: [1m[32m0.69376[0m[0m | time: 28.173s
[2K
| RMSProp | epoch: 005 | loss: 0.69376 - acc: 0.4705 -- iter: 0960/1292
[A[ATraining Step: 195  | total loss: [1m[32m0.69380[0m[0m | time: 29.047s
[2K
| RMSProp | epoch: 005 | loss: 0.69380 - acc: 0.4672 -- iter: 0992/1292
[A[ATraining Step: 196  | total loss: [1m[32m0.69374[0m[0m | time: 29.966s
[2K
| RMSProp | epoch: 005 | loss: 0.69374 - acc: 0.4705 -- iter: 1024/1292
[A[ATraining Step: 197  | total loss: [1m[32m0.69362[0m[0m | time: 30.888s
[2K
| RMSProp | epoch: 005 | loss: 0.69362 - acc: 0.4797 -- iter: 1056/1292
[A[ATraining Step: 198  | total loss: [1m[32m0.69343[0m[0m | time: 31.869s
[2K
| RMSProp | epoch: 005 | loss: 0.69343 - acc: 0.4911 -- iter: 1088/1292
[A[ATraining Step: 199  | total loss: [1m[32m0.69349[0m[0m | time: 32.993s
[2K
| RMSProp | epoch: 005 | loss: 0.69349 - acc: 0.4888 -- iter: 1120/1292
[A[ATraining Step: 200  | total loss: [1m[32m0.69365[0m[0m | time: 35.793s
[2K
| RMSProp | epoch: 005 | loss: 0.69365 - acc: 0.4806 | val_loss: 0.69337 - val_acc: 0.4667 -- iter: 1152/1292
--
Training Step: 201  | total loss: [1m[32m0.69373[0m[0m | time: 36.917s
[2K
| RMSProp | epoch: 005 | loss: 0.69373 - acc: 0.4731 -- iter: 1184/1292
[A[ATraining Step: 202  | total loss: [1m[32m0.69375[0m[0m | time: 37.783s
[2K
| RMSProp | epoch: 005 | loss: 0.69375 - acc: 0.4665 -- iter: 1216/1292
[A[ATraining Step: 203  | total loss: [1m[32m0.69368[0m[0m | time: 38.729s
[2K
| RMSProp | epoch: 005 | loss: 0.69368 - acc: 0.4729 -- iter: 1248/1292
[A[ATraining Step: 204  | total loss: [1m[32m0.69359[0m[0m | time: 39.712s
[2K
| RMSProp | epoch: 005 | loss: 0.69359 - acc: 0.4788 -- iter: 1280/1292
[A[ATraining Step: 205  | total loss: [1m[32m0.69361[0m[0m | time: 42.651s
[2K
| RMSProp | epoch: 005 | loss: 0.69361 - acc: 0.4778 | val_loss: 0.69359 - val_acc: 0.4667 -- iter: 1292/1292
--
Training Step: 206  | total loss: [1m[32m0.69353[0m[0m | time: 0.906s
[2K
| RMSProp | epoch: 006 | loss: 0.69353 - acc: 0.4831 -- iter: 0032/1292
[A[ATraining Step: 207  | total loss: [1m[32m0.69354[0m[0m | time: 1.933s
[2K
| RMSProp | epoch: 006 | loss: 0.69354 - acc: 0.4817 -- iter: 0064/1292
[A[ATraining Step: 208  | total loss: [1m[32m0.69350[0m[0m | time: 3.002s
[2K
| RMSProp | epoch: 006 | loss: 0.69350 - acc: 0.4835 -- iter: 0096/1292
[A[ATraining Step: 209  | total loss: [1m[32m0.69346[0m[0m | time: 3.419s
[2K
| RMSProp | epoch: 006 | loss: 0.69346 - acc: 0.4852 -- iter: 0128/1292
[A[ATraining Step: 210  | total loss: [1m[32m0.69341[0m[0m | time: 3.883s
[2K
| RMSProp | epoch: 006 | loss: 0.69341 - acc: 0.4866 -- iter: 0160/1292
[A[ATraining Step: 211  | total loss: [1m[32m0.69337[0m[0m | time: 4.702s
[2K
| RMSProp | epoch: 006 | loss: 0.69337 - acc: 0.4880 -- iter: 0192/1292
[A[ATraining Step: 212  | total loss: [1m[32m0.69323[0m[0m | time: 5.687s
[2K
| RMSProp | epoch: 006 | loss: 0.69323 - acc: 0.5017 -- iter: 0224/1292
[A[ATraining Step: 213  | total loss: [1m[32m0.69301[0m[0m | time: 6.589s
[2K
| RMSProp | epoch: 006 | loss: 0.69301 - acc: 0.5109 -- iter: 0256/1292
[A[ATraining Step: 214  | total loss: [1m[32m0.69277[0m[0m | time: 7.504s
[2K
| RMSProp | epoch: 006 | loss: 0.69277 - acc: 0.5160 -- iter: 0288/1292
[A[ATraining Step: 215  | total loss: [1m[32m0.69176[0m[0m | time: 8.547s
[2K
| RMSProp | epoch: 006 | loss: 0.69176 - acc: 0.5301 -- iter: 0320/1292
[A[ATraining Step: 216  | total loss: [1m[32m0.69213[0m[0m | time: 9.647s
[2K
| RMSProp | epoch: 006 | loss: 0.69213 - acc: 0.5302 -- iter: 0352/1292
[A[ATraining Step: 217  | total loss: [1m[32m0.69174[0m[0m | time: 10.512s
[2K
| RMSProp | epoch: 006 | loss: 0.69174 - acc: 0.5334 -- iter: 0384/1292
[A[ATraining Step: 218  | total loss: [1m[32m0.69017[0m[0m | time: 11.556s
[2K
| RMSProp | epoch: 006 | loss: 0.69017 - acc: 0.5457 -- iter: 0416/1292
[A[ATraining Step: 219  | total loss: [1m[32m0.69017[0m[0m | time: 12.533s
[2K
| RMSProp | epoch: 006 | loss: 0.69017 - acc: 0.5474 -- iter: 0448/1292
[A[ATraining Step: 220  | total loss: [1m[32m0.69249[0m[0m | time: 13.587s
[2K
| RMSProp | epoch: 006 | loss: 0.69249 - acc: 0.5333 -- iter: 0480/1292
[A[ATraining Step: 221  | total loss: [1m[32m0.69216[0m[0m | time: 14.607s
[2K
| RMSProp | epoch: 006 | loss: 0.69216 - acc: 0.5362 -- iter: 0512/1292
[A[ATraining Step: 222  | total loss: [1m[32m0.69279[0m[0m | time: 15.602s
[2K
| RMSProp | epoch: 006 | loss: 0.69279 - acc: 0.5263 -- iter: 0544/1292
[A[ATraining Step: 223  | total loss: [1m[32m0.69297[0m[0m | time: 16.619s
[2K
| RMSProp | epoch: 006 | loss: 0.69297 - acc: 0.5206 -- iter: 0576/1292
[A[ATraining Step: 224  | total loss: [1m[32m0.69351[0m[0m | time: 17.590s
[2K
| RMSProp | epoch: 006 | loss: 0.69351 - acc: 0.5060 -- iter: 0608/1292
[A[ATraining Step: 225  | total loss: [1m[32m0.69332[0m[0m | time: 18.592s
[2K
| RMSProp | epoch: 006 | loss: 0.69332 - acc: 0.5117 -- iter: 0640/1292
[A[ATraining Step: 226  | total loss: [1m[32m0.69340[0m[0m | time: 19.617s
[2K
| RMSProp | epoch: 006 | loss: 0.69340 - acc: 0.5074 -- iter: 0672/1292
[A[ATraining Step: 227  | total loss: [1m[32m0.69337[0m[0m | time: 20.593s
[2K
| RMSProp | epoch: 006 | loss: 0.69337 - acc: 0.5066 -- iter: 0704/1292
[A[ATraining Step: 228  | total loss: [1m[32m0.69348[0m[0m | time: 21.315s
[2K
| RMSProp | epoch: 006 | loss: 0.69348 - acc: 0.4997 -- iter: 0736/1292
[A[ATraining Step: 229  | total loss: [1m[32m0.69332[0m[0m | time: 21.898s
[2K
| RMSProp | epoch: 006 | loss: 0.69332 - acc: 0.5091 -- iter: 0768/1292
[A[ATraining Step: 230  | total loss: [1m[32m0.69346[0m[0m | time: 22.515s
[2K
| RMSProp | epoch: 006 | loss: 0.69346 - acc: 0.5020 -- iter: 0800/1292
[A[ATraining Step: 231  | total loss: [1m[32m0.69347[0m[0m | time: 23.130s
[2K
| RMSProp | epoch: 006 | loss: 0.69347 - acc: 0.4986 -- iter: 0832/1292
[A[ATraining Step: 232  | total loss: [1m[32m0.69330[0m[0m | time: 23.754s
[2K
| RMSProp | epoch: 006 | loss: 0.69330 - acc: 0.5050 -- iter: 0864/1292
[A[ATraining Step: 233  | total loss: [1m[32m0.69305[0m[0m | time: 24.362s
[2K
| RMSProp | epoch: 006 | loss: 0.69305 - acc: 0.5108 -- iter: 0896/1292
[A[ATraining Step: 234  | total loss: [1m[32m0.69250[0m[0m | time: 24.947s
[2K
| RMSProp | epoch: 006 | loss: 0.69250 - acc: 0.5191 -- iter: 0928/1292
[A[ATraining Step: 235  | total loss: [1m[32m0.69205[0m[0m | time: 25.577s
[2K
| RMSProp | epoch: 006 | loss: 0.69205 - acc: 0.5234 -- iter: 0960/1292
[A[ATraining Step: 236  | total loss: [1m[32m0.69142[0m[0m | time: 26.184s
[2K
| RMSProp | epoch: 006 | loss: 0.69142 - acc: 0.5273 -- iter: 0992/1292
[A[ATraining Step: 237  | total loss: [1m[32m0.69314[0m[0m | time: 26.777s
[2K
| RMSProp | epoch: 006 | loss: 0.69314 - acc: 0.5215 -- iter: 1024/1292
[A[ATraining Step: 238  | total loss: [1m[32m0.69380[0m[0m | time: 27.384s
[2K
| RMSProp | epoch: 006 | loss: 0.69380 - acc: 0.5068 -- iter: 1056/1292
[A[ATraining Step: 239  | total loss: [1m[32m0.69356[0m[0m | time: 27.992s
[2K
| RMSProp | epoch: 006 | loss: 0.69356 - acc: 0.5124 -- iter: 1088/1292
[A[ATraining Step: 240  | total loss: [1m[32m0.69315[0m[0m | time: 28.624s
[2K
| RMSProp | epoch: 006 | loss: 0.69315 - acc: 0.5205 -- iter: 1120/1292
[A[ATraining Step: 241  | total loss: [1m[32m0.69273[0m[0m | time: 29.221s
[2K
| RMSProp | epoch: 006 | loss: 0.69273 - acc: 0.5247 -- iter: 1152/1292
[A[ATraining Step: 242  | total loss: [1m[32m0.69148[0m[0m | time: 29.849s
[2K
| RMSProp | epoch: 006 | loss: 0.69148 - acc: 0.5410 -- iter: 1184/1292
[A[ATraining Step: 243  | total loss: [1m[32m0.69222[0m[0m | time: 30.467s
[2K
| RMSProp | epoch: 006 | loss: 0.69222 - acc: 0.5338 -- iter: 1216/1292
[A[ATraining Step: 244  | total loss: [1m[32m0.69360[0m[0m | time: 31.100s
[2K
| RMSProp | epoch: 006 | loss: 0.69360 - acc: 0.5148 -- iter: 1248/1292
[A[ATraining Step: 245  | total loss: [1m[32m0.69343[0m[0m | time: 31.721s
[2K
| RMSProp | epoch: 006 | loss: 0.69343 - acc: 0.5164 -- iter: 1280/1292
[A[ATraining Step: 246  | total loss: [1m[32m0.69315[0m[0m | time: 33.611s
[2K
| RMSProp | epoch: 006 | loss: 0.69315 - acc: 0.5179 | val_loss: 0.69271 - val_acc: 0.4667 -- iter: 1292/1292
--
Training Step: 247  | total loss: [1m[32m0.69324[0m[0m | time: 0.602s
[2K
| RMSProp | epoch: 007 | loss: 0.69324 - acc: 0.5130 -- iter: 0032/1292
[A[ATraining Step: 248  | total loss: [1m[32m0.69327[0m[0m | time: 1.244s
[2K
| RMSProp | epoch: 007 | loss: 0.69327 - acc: 0.5054 -- iter: 0064/1292
[A[ATraining Step: 249  | total loss: [1m[32m0.69312[0m[0m | time: 1.868s
[2K
| RMSProp | epoch: 007 | loss: 0.69312 - acc: 0.5080 -- iter: 0096/1292
[A[ATraining Step: 250  | total loss: [1m[32m0.69304[0m[0m | time: 2.482s
[2K
| RMSProp | epoch: 007 | loss: 0.69304 - acc: 0.5072 -- iter: 0128/1292
[A[ATraining Step: 251  | total loss: [1m[32m0.69229[0m[0m | time: 2.725s
[2K
| RMSProp | epoch: 007 | loss: 0.69229 - acc: 0.5221 -- iter: 0160/1292
[A[ATraining Step: 252  | total loss: [1m[32m0.69069[0m[0m | time: 2.983s
[2K
| RMSProp | epoch: 007 | loss: 0.69069 - acc: 0.5366 -- iter: 0192/1292
[A[ATraining Step: 253  | total loss: [1m[32m0.68715[0m[0m | time: 3.575s
[2K
| RMSProp | epoch: 007 | loss: 0.68715 - acc: 0.5496 -- iter: 0224/1292
[A[ATraining Step: 254  | total loss: [1m[32m0.69985[0m[0m | time: 4.229s
[2K
| RMSProp | epoch: 007 | loss: 0.69985 - acc: 0.5415 -- iter: 0256/1292
[A[ATraining Step: 255  | total loss: [1m[32m0.69833[0m[0m | time: 4.845s
[2K
| RMSProp | epoch: 007 | loss: 0.69833 - acc: 0.5436 -- iter: 0288/1292
[A[ATraining Step: 256  | total loss: [1m[32m0.69823[0m[0m | time: 5.441s
[2K
| RMSProp | epoch: 007 | loss: 0.69823 - acc: 0.5361 -- iter: 0320/1292
[A[ATraining Step: 257  | total loss: [1m[32m0.69793[0m[0m | time: 6.072s
[2K
| RMSProp | epoch: 007 | loss: 0.69793 - acc: 0.5294 -- iter: 0352/1292
[A[ATraining Step: 258  | total loss: [1m[32m0.69635[0m[0m | time: 6.693s
[2K
| RMSProp | epoch: 007 | loss: 0.69635 - acc: 0.5358 -- iter: 0384/1292
[A[ATraining Step: 259  | total loss: [1m[32m0.69523[0m[0m | time: 7.298s
[2K
| RMSProp | epoch: 007 | loss: 0.69523 - acc: 0.5385 -- iter: 0416/1292
[A[ATraining Step: 260  | total loss: [1m[32m0.69506[0m[0m | time: 7.909s
[2K
| RMSProp | epoch: 007 | loss: 0.69506 - acc: 0.5346 -- iter: 0448/1292
[A[ATraining Step: 261  | total loss: [1m[32m0.69473[0m[0m | time: 8.522s
[2K
| RMSProp | epoch: 007 | loss: 0.69473 - acc: 0.5343 -- iter: 0480/1292
[A[ATraining Step: 262  | total loss: [1m[32m0.69553[0m[0m | time: 9.139s
[2K
| RMSProp | epoch: 007 | loss: 0.69553 - acc: 0.5184 -- iter: 0512/1292
[A[ATraining Step: 263  | total loss: [1m[32m0.69473[0m[0m | time: 9.754s
[2K
| RMSProp | epoch: 007 | loss: 0.69473 - acc: 0.5134 -- iter: 0544/1292
[A[ATraining Step: 264  | total loss: [1m[32m0.69394[0m[0m | time: 10.367s
[2K
| RMSProp | epoch: 007 | loss: 0.69394 - acc: 0.5152 -- iter: 0576/1292
[A[ATraining Step: 265  | total loss: [1m[32m0.69286[0m[0m | time: 10.969s
[2K
| RMSProp | epoch: 007 | loss: 0.69286 - acc: 0.5137 -- iter: 0608/1292
[A[ATraining Step: 266  | total loss: [1m[32m0.69228[0m[0m | time: 11.585s
[2K
| RMSProp | epoch: 007 | loss: 0.69228 - acc: 0.5279 -- iter: 0640/1292
[A[ATraining Step: 267  | total loss: [1m[32m0.69028[0m[0m | time: 12.231s
[2K
| RMSProp | epoch: 007 | loss: 0.69028 - acc: 0.5376 -- iter: 0672/1292
[A[ATraining Step: 268  | total loss: [1m[32m0.69002[0m[0m | time: 12.838s
[2K
| RMSProp | epoch: 007 | loss: 0.69002 - acc: 0.5370 -- iter: 0704/1292
[A[ATraining Step: 269  | total loss: [1m[32m0.68885[0m[0m | time: 13.525s
[2K
| RMSProp | epoch: 007 | loss: 0.68885 - acc: 0.5395 -- iter: 0736/1292
[A[ATraining Step: 270  | total loss: [1m[32m0.68920[0m[0m | time: 14.647s
[2K
| RMSProp | epoch: 007 | loss: 0.68920 - acc: 0.5325 -- iter: 0768/1292
[A[ATraining Step: 271  | total loss: [1m[32m0.68762[0m[0m | time: 15.723s
[2K
| RMSProp | epoch: 007 | loss: 0.68762 - acc: 0.5355 -- iter: 0800/1292
[A[ATraining Step: 272  | total loss: [1m[32m0.68672[0m[0m | time: 16.660s
[2K
| RMSProp | epoch: 007 | loss: 0.68672 - acc: 0.5382 -- iter: 0832/1292
[A[ATraining Step: 273  | total loss: [1m[32m0.68500[0m[0m | time: 17.567s
[2K
| RMSProp | epoch: 007 | loss: 0.68500 - acc: 0.5406 -- iter: 0864/1292
[A[ATraining Step: 274  | total loss: [1m[32m0.68115[0m[0m | time: 18.533s
[2K
| RMSProp | epoch: 007 | loss: 0.68115 - acc: 0.5490 -- iter: 0896/1292
[A[ATraining Step: 275  | total loss: [1m[32m0.68262[0m[0m | time: 19.464s
[2K
| RMSProp | epoch: 007 | loss: 0.68262 - acc: 0.5473 -- iter: 0928/1292
[A[ATraining Step: 276  | total loss: [1m[32m0.67943[0m[0m | time: 20.403s
[2K
| RMSProp | epoch: 007 | loss: 0.67943 - acc: 0.5800 -- iter: 0960/1292
[A[ATraining Step: 277  | total loss: [1m[32m0.67629[0m[0m | time: 21.419s
[2K
| RMSProp | epoch: 007 | loss: 0.67629 - acc: 0.5877 -- iter: 0992/1292
[A[ATraining Step: 278  | total loss: [1m[32m0.67776[0m[0m | time: 22.395s
[2K
| RMSProp | epoch: 007 | loss: 0.67776 - acc: 0.5820 -- iter: 1024/1292
[A[ATraining Step: 279  | total loss: [1m[32m0.68443[0m[0m | time: 23.380s
[2K
| RMSProp | epoch: 007 | loss: 0.68443 - acc: 0.5644 -- iter: 1056/1292
[A[ATraining Step: 280  | total loss: [1m[32m0.68337[0m[0m | time: 24.436s
[2K
| RMSProp | epoch: 007 | loss: 0.68337 - acc: 0.5674 -- iter: 1088/1292
[A[ATraining Step: 281  | total loss: [1m[32m0.68211[0m[0m | time: 25.455s
[2K
| RMSProp | epoch: 007 | loss: 0.68211 - acc: 0.5700 -- iter: 1120/1292
[A[ATraining Step: 282  | total loss: [1m[32m0.68051[0m[0m | time: 26.323s
[2K
| RMSProp | epoch: 007 | loss: 0.68051 - acc: 0.5724 -- iter: 1152/1292
[A[ATraining Step: 283  | total loss: [1m[32m0.68311[0m[0m | time: 27.158s
[2K
| RMSProp | epoch: 007 | loss: 0.68311 - acc: 0.5620 -- iter: 1184/1292
[A[ATraining Step: 284  | total loss: [1m[32m0.67801[0m[0m | time: 28.064s
[2K
| RMSProp | epoch: 007 | loss: 0.67801 - acc: 0.5839 -- iter: 1216/1292
[A[ATraining Step: 285  | total loss: [1m[32m0.67588[0m[0m | time: 29.013s
[2K
| RMSProp | epoch: 007 | loss: 0.67588 - acc: 0.5912 -- iter: 1248/1292
[A[ATraining Step: 286  | total loss: [1m[32m0.67037[0m[0m | time: 29.957s
[2K
| RMSProp | epoch: 007 | loss: 0.67037 - acc: 0.6164 -- iter: 1280/1292
[A[ATraining Step: 287  | total loss: [1m[32m0.66530[0m[0m | time: 32.864s
[2K
| RMSProp | epoch: 007 | loss: 0.66530 - acc: 0.6298 | val_loss: 0.58219 - val_acc: 0.7407 -- iter: 1292/1292
--
Training Step: 288  | total loss: [1m[32m0.65733[0m[0m | time: 1.032s
[2K
| RMSProp | epoch: 008 | loss: 0.65733 - acc: 0.6387 -- iter: 0032/1292
[A[ATraining Step: 289  | total loss: [1m[32m0.64943[0m[0m | time: 2.106s
[2K
| RMSProp | epoch: 008 | loss: 0.64943 - acc: 0.6404 -- iter: 0064/1292
[A[ATraining Step: 290  | total loss: [1m[32m0.64740[0m[0m | time: 2.983s
[2K
| RMSProp | epoch: 008 | loss: 0.64740 - acc: 0.6295 -- iter: 0096/1292
[A[ATraining Step: 291  | total loss: [1m[32m0.67866[0m[0m | time: 3.916s
[2K
| RMSProp | epoch: 008 | loss: 0.67866 - acc: 0.6103 -- iter: 0128/1292
[A[ATraining Step: 292  | total loss: [1m[32m0.67818[0m[0m | time: 4.844s
[2K
| RMSProp | epoch: 008 | loss: 0.67818 - acc: 0.6180 -- iter: 0160/1292
[A[ATraining Step: 293  | total loss: [1m[32m0.67418[0m[0m | time: 5.221s
[2K
| RMSProp | epoch: 008 | loss: 0.67418 - acc: 0.6250 -- iter: 0192/1292
[A[ATraining Step: 294  | total loss: [1m[32m0.66482[0m[0m | time: 5.599s
[2K
| RMSProp | epoch: 008 | loss: 0.66482 - acc: 0.6458 -- iter: 0224/1292
[A[ATraining Step: 295  | total loss: [1m[32m0.65166[0m[0m | time: 6.664s
[2K
| RMSProp | epoch: 008 | loss: 0.65166 - acc: 0.6562 -- iter: 0256/1292
[A[ATraining Step: 296  | total loss: [1m[32m0.64844[0m[0m | time: 7.681s
[2K
| RMSProp | epoch: 008 | loss: 0.64844 - acc: 0.6625 -- iter: 0288/1292
[A[ATraining Step: 297  | total loss: [1m[32m0.64572[0m[0m | time: 8.560s
[2K
| RMSProp | epoch: 008 | loss: 0.64572 - acc: 0.6712 -- iter: 0320/1292
[A[ATraining Step: 298  | total loss: [1m[32m0.64284[0m[0m | time: 9.644s
[2K
| RMSProp | epoch: 008 | loss: 0.64284 - acc: 0.6729 -- iter: 0352/1292
[A[ATraining Step: 299  | total loss: [1m[32m0.64180[0m[0m | time: 10.669s
[2K
| RMSProp | epoch: 008 | loss: 0.64180 - acc: 0.6650 -- iter: 0384/1292
[A[ATraining Step: 300  | total loss: [1m[32m0.65091[0m[0m | time: 11.662s
[2K
| RMSProp | epoch: 008 | loss: 0.65091 - acc: 0.6547 -- iter: 0416/1292
[A[ATraining Step: 301  | total loss: [1m[32m0.65019[0m[0m | time: 12.561s
[2K
| RMSProp | epoch: 008 | loss: 0.65019 - acc: 0.6455 -- iter: 0448/1292
[A[ATraining Step: 302  | total loss: [1m[32m0.64908[0m[0m | time: 13.441s
[2K
| RMSProp | epoch: 008 | loss: 0.64908 - acc: 0.6466 -- iter: 0480/1292
[A[ATraining Step: 303  | total loss: [1m[32m0.64148[0m[0m | time: 14.356s
[2K
| RMSProp | epoch: 008 | loss: 0.64148 - acc: 0.6569 -- iter: 0512/1292
[A[ATraining Step: 304  | total loss: [1m[32m0.63794[0m[0m | time: 15.294s
[2K
| RMSProp | epoch: 008 | loss: 0.63794 - acc: 0.6568 -- iter: 0544/1292
[A[ATraining Step: 305  | total loss: [1m[32m0.63357[0m[0m | time: 16.373s
[2K
| RMSProp | epoch: 008 | loss: 0.63357 - acc: 0.6693 -- iter: 0576/1292
[A[ATraining Step: 306  | total loss: [1m[32m0.61525[0m[0m | time: 17.261s
[2K
| RMSProp | epoch: 008 | loss: 0.61525 - acc: 0.6930 -- iter: 0608/1292
[A[ATraining Step: 307  | total loss: [1m[32m0.61176[0m[0m | time: 18.265s
[2K
| RMSProp | epoch: 008 | loss: 0.61176 - acc: 0.6956 -- iter: 0640/1292
[A[ATraining Step: 308  | total loss: [1m[32m0.65827[0m[0m | time: 19.335s
[2K
| RMSProp | epoch: 008 | loss: 0.65827 - acc: 0.6573 -- iter: 0672/1292
[A[ATraining Step: 309  | total loss: [1m[32m0.64868[0m[0m | time: 20.323s
[2K
| RMSProp | epoch: 008 | loss: 0.64868 - acc: 0.6634 -- iter: 0704/1292
[A[ATraining Step: 310  | total loss: [1m[32m0.64011[0m[0m | time: 21.196s
[2K
| RMSProp | epoch: 008 | loss: 0.64011 - acc: 0.6658 -- iter: 0736/1292
[A[ATraining Step: 311  | total loss: [1m[32m0.62320[0m[0m | time: 22.101s
[2K
| RMSProp | epoch: 008 | loss: 0.62320 - acc: 0.6867 -- iter: 0768/1292
[A[ATraining Step: 312  | total loss: [1m[32m0.61718[0m[0m | time: 23.028s
[2K
| RMSProp | epoch: 008 | loss: 0.61718 - acc: 0.6931 -- iter: 0800/1292
[A[ATraining Step: 313  | total loss: [1m[32m0.61135[0m[0m | time: 24.030s
[2K
| RMSProp | epoch: 008 | loss: 0.61135 - acc: 0.7019 -- iter: 0832/1292
[A[ATraining Step: 314  | total loss: [1m[32m0.61468[0m[0m | time: 25.052s
[2K
| RMSProp | epoch: 008 | loss: 0.61468 - acc: 0.7004 -- iter: 0864/1292
[A[ATraining Step: 315  | total loss: [1m[32m0.60981[0m[0m | time: 25.898s
[2K
| RMSProp | epoch: 008 | loss: 0.60981 - acc: 0.6991 -- iter: 0896/1292
[A[ATraining Step: 316  | total loss: [1m[32m0.59897[0m[0m | time: 26.882s
[2K
| RMSProp | epoch: 008 | loss: 0.59897 - acc: 0.7074 -- iter: 0928/1292
[A[ATraining Step: 317  | total loss: [1m[32m0.58222[0m[0m | time: 27.917s
[2K
| RMSProp | epoch: 008 | loss: 0.58222 - acc: 0.7147 -- iter: 0960/1292
[A[ATraining Step: 318  | total loss: [1m[32m0.58234[0m[0m | time: 28.976s
[2K
| RMSProp | epoch: 008 | loss: 0.58234 - acc: 0.7151 -- iter: 0992/1292
[A[ATraining Step: 319  | total loss: [1m[32m0.59053[0m[0m | time: 29.797s
[2K
| RMSProp | epoch: 008 | loss: 0.59053 - acc: 0.7061 -- iter: 1024/1292
[A[ATraining Step: 320  | total loss: [1m[32m0.58352[0m[0m | time: 30.738s
[2K
| RMSProp | epoch: 008 | loss: 0.58352 - acc: 0.7136 -- iter: 1056/1292
[A[ATraining Step: 321  | total loss: [1m[32m0.57708[0m[0m | time: 31.682s
[2K
| RMSProp | epoch: 008 | loss: 0.57708 - acc: 0.7173 -- iter: 1088/1292
[A[ATraining Step: 322  | total loss: [1m[32m0.57826[0m[0m | time: 32.611s
[2K
| RMSProp | epoch: 008 | loss: 0.57826 - acc: 0.7206 -- iter: 1120/1292
[A[ATraining Step: 323  | total loss: [1m[32m0.58618[0m[0m | time: 33.614s
[2K
| RMSProp | epoch: 008 | loss: 0.58618 - acc: 0.7141 -- iter: 1152/1292
[A[ATraining Step: 324  | total loss: [1m[32m0.58021[0m[0m | time: 34.654s
[2K
| RMSProp | epoch: 008 | loss: 0.58021 - acc: 0.7240 -- iter: 1184/1292
[A[ATraining Step: 325  | total loss: [1m[32m0.57616[0m[0m | time: 35.490s
[2K
| RMSProp | epoch: 008 | loss: 0.57616 - acc: 0.7203 -- iter: 1216/1292
[A[ATraining Step: 326  | total loss: [1m[32m0.56068[0m[0m | time: 36.523s
[2K
| RMSProp | epoch: 008 | loss: 0.56068 - acc: 0.7295 -- iter: 1248/1292
[A[ATraining Step: 327  | total loss: [1m[32m0.55146[0m[0m | time: 37.610s
[2K
| RMSProp | epoch: 008 | loss: 0.55146 - acc: 0.7410 -- iter: 1280/1292
[A[ATraining Step: 328  | total loss: [1m[32m0.52953[0m[0m | time: 40.218s
[2K
| RMSProp | epoch: 008 | loss: 0.52953 - acc: 0.7575 | val_loss: 0.64647 - val_acc: 0.6543 -- iter: 1292/1292
--
Training Step: 329  | total loss: [1m[32m0.53414[0m[0m | time: 0.925s
[2K
| RMSProp | epoch: 009 | loss: 0.53414 - acc: 0.7505 -- iter: 0032/1292
[A[ATraining Step: 330  | total loss: [1m[32m0.52455[0m[0m | time: 1.850s
[2K
| RMSProp | epoch: 009 | loss: 0.52455 - acc: 0.7567 -- iter: 0064/1292
[A[ATraining Step: 331  | total loss: [1m[32m0.53441[0m[0m | time: 2.819s
[2K
| RMSProp | epoch: 009 | loss: 0.53441 - acc: 0.7498 -- iter: 0096/1292
[A[ATraining Step: 332  | total loss: [1m[32m0.53502[0m[0m | time: 3.877s
[2K
| RMSProp | epoch: 009 | loss: 0.53502 - acc: 0.7467 -- iter: 0128/1292
[A[ATraining Step: 333  | total loss: [1m[32m0.51803[0m[0m | time: 4.818s
[2K
| RMSProp | epoch: 009 | loss: 0.51803 - acc: 0.7657 -- iter: 0160/1292
[A[ATraining Step: 334  | total loss: [1m[32m0.51940[0m[0m | time: 5.831s
[2K
| RMSProp | epoch: 009 | loss: 0.51940 - acc: 0.7579 -- iter: 0192/1292
[A[ATraining Step: 335  | total loss: [1m[32m0.52344[0m[0m | time: 6.271s
[2K
| RMSProp | epoch: 009 | loss: 0.52344 - acc: 0.7571 -- iter: 0224/1292
[A[ATraining Step: 336  | total loss: [1m[32m0.53529[0m[0m | time: 6.733s
[2K
| RMSProp | epoch: 009 | loss: 0.53529 - acc: 0.7564 -- iter: 0256/1292
[A[ATraining Step: 337  | total loss: [1m[32m0.54174[0m[0m | time: 7.771s
[2K
| RMSProp | epoch: 009 | loss: 0.54174 - acc: 0.7474 -- iter: 0288/1292
[A[ATraining Step: 338  | total loss: [1m[32m0.53509[0m[0m | time: 8.662s
[2K
| RMSProp | epoch: 009 | loss: 0.53509 - acc: 0.7477 -- iter: 0320/1292
[A[ATraining Step: 339  | total loss: [1m[32m0.54228[0m[0m | time: 9.605s
[2K
| RMSProp | epoch: 009 | loss: 0.54228 - acc: 0.7354 -- iter: 0352/1292
[A[ATraining Step: 340  | total loss: [1m[32m0.55035[0m[0m | time: 10.611s
[2K
| RMSProp | epoch: 009 | loss: 0.55035 - acc: 0.7244 -- iter: 0384/1292
[A[ATraining Step: 341  | total loss: [1m[32m0.55926[0m[0m | time: 11.568s
[2K
| RMSProp | epoch: 009 | loss: 0.55926 - acc: 0.7144 -- iter: 0416/1292
[A[ATraining Step: 342  | total loss: [1m[32m0.55537[0m[0m | time: 12.610s
[2K
| RMSProp | epoch: 009 | loss: 0.55537 - acc: 0.7180 -- iter: 0448/1292
[A[ATraining Step: 343  | total loss: [1m[32m0.54743[0m[0m | time: 13.694s
[2K
| RMSProp | epoch: 009 | loss: 0.54743 - acc: 0.7212 -- iter: 0480/1292
[A[ATraining Step: 344  | total loss: [1m[32m0.55775[0m[0m | time: 14.574s
[2K
| RMSProp | epoch: 009 | loss: 0.55775 - acc: 0.7022 -- iter: 0512/1292
[A[ATraining Step: 345  | total loss: [1m[32m0.55669[0m[0m | time: 15.527s
[2K
| RMSProp | epoch: 009 | loss: 0.55669 - acc: 0.7070 -- iter: 0544/1292
[A[ATraining Step: 346  | total loss: [1m[32m0.55184[0m[0m | time: 16.582s
[2K
| RMSProp | epoch: 009 | loss: 0.55184 - acc: 0.7113 -- iter: 0576/1292
[A[ATraining Step: 347  | total loss: [1m[32m0.54687[0m[0m | time: 17.659s
[2K
| RMSProp | epoch: 009 | loss: 0.54687 - acc: 0.7152 -- iter: 0608/1292
[A[ATraining Step: 348  | total loss: [1m[32m0.53208[0m[0m | time: 18.496s
[2K
| RMSProp | epoch: 009 | loss: 0.53208 - acc: 0.7280 -- iter: 0640/1292
[A[ATraining Step: 349  | total loss: [1m[32m0.51434[0m[0m | time: 19.395s
[2K
| RMSProp | epoch: 009 | loss: 0.51434 - acc: 0.7427 -- iter: 0672/1292
[A[ATraining Step: 350  | total loss: [1m[32m0.50734[0m[0m | time: 20.356s
[2K
| RMSProp | epoch: 009 | loss: 0.50734 - acc: 0.7466 -- iter: 0704/1292
[A[ATraining Step: 351  | total loss: [1m[32m0.52170[0m[0m | time: 21.323s
[2K
| RMSProp | epoch: 009 | loss: 0.52170 - acc: 0.7313 -- iter: 0736/1292
[A[ATraining Step: 352  | total loss: [1m[32m0.52219[0m[0m | time: 22.384s
[2K
| RMSProp | epoch: 009 | loss: 0.52219 - acc: 0.7332 -- iter: 0768/1292
[A[ATraining Step: 353  | total loss: [1m[32m0.51461[0m[0m | time: 23.458s
[2K
| RMSProp | epoch: 009 | loss: 0.51461 - acc: 0.7411 -- iter: 0800/1292
[A[ATraining Step: 354  | total loss: [1m[32m0.50310[0m[0m | time: 24.362s
[2K
| RMSProp | epoch: 009 | loss: 0.50310 - acc: 0.7514 -- iter: 0832/1292
[A[ATraining Step: 355  | total loss: [1m[32m0.49881[0m[0m | time: 25.426s
[2K
| RMSProp | epoch: 009 | loss: 0.49881 - acc: 0.7543 -- iter: 0864/1292
[A[ATraining Step: 356  | total loss: [1m[32m0.51773[0m[0m | time: 26.533s
[2K
| RMSProp | epoch: 009 | loss: 0.51773 - acc: 0.7414 -- iter: 0896/1292
[A[ATraining Step: 357  | total loss: [1m[32m0.50672[0m[0m | time: 27.504s
[2K
| RMSProp | epoch: 009 | loss: 0.50672 - acc: 0.7516 -- iter: 0928/1292
[A[ATraining Step: 358  | total loss: [1m[32m0.50325[0m[0m | time: 28.345s
[2K
| RMSProp | epoch: 009 | loss: 0.50325 - acc: 0.7609 -- iter: 0960/1292
[A[ATraining Step: 359  | total loss: [1m[32m0.49674[0m[0m | time: 29.280s
[2K
| RMSProp | epoch: 009 | loss: 0.49674 - acc: 0.7660 -- iter: 0992/1292
[A[ATraining Step: 360  | total loss: [1m[32m0.49886[0m[0m | time: 30.204s
[2K
| RMSProp | epoch: 009 | loss: 0.49886 - acc: 0.7644 -- iter: 1024/1292
[A[ATraining Step: 361  | total loss: [1m[32m0.48319[0m[0m | time: 31.130s
[2K
| RMSProp | epoch: 009 | loss: 0.48319 - acc: 0.7786 -- iter: 1056/1292
[A[ATraining Step: 362  | total loss: [1m[32m0.47114[0m[0m | time: 32.158s
[2K
| RMSProp | epoch: 009 | loss: 0.47114 - acc: 0.7882 -- iter: 1088/1292
[A[ATraining Step: 363  | total loss: [1m[32m0.47280[0m[0m | time: 33.174s
[2K
| RMSProp | epoch: 009 | loss: 0.47280 - acc: 0.7813 -- iter: 1120/1292
[A[ATraining Step: 364  | total loss: [1m[32m0.47333[0m[0m | time: 34.014s
[2K
| RMSProp | epoch: 009 | loss: 0.47333 - acc: 0.7813 -- iter: 1152/1292
[A[ATraining Step: 365  | total loss: [1m[32m0.49314[0m[0m | time: 35.047s
[2K
| RMSProp | epoch: 009 | loss: 0.49314 - acc: 0.7719 -- iter: 1184/1292
[A[ATraining Step: 366  | total loss: [1m[32m0.49784[0m[0m | time: 36.112s
[2K
| RMSProp | epoch: 009 | loss: 0.49784 - acc: 0.7635 -- iter: 1216/1292
[A[ATraining Step: 367  | total loss: [1m[32m0.49941[0m[0m | time: 37.126s
[2K
| RMSProp | epoch: 009 | loss: 0.49941 - acc: 0.7621 -- iter: 1248/1292
[A[ATraining Step: 368  | total loss: [1m[32m0.49819[0m[0m | time: 37.939s
[2K
| RMSProp | epoch: 009 | loss: 0.49819 - acc: 0.7609 -- iter: 1280/1292
[A[ATraining Step: 369  | total loss: [1m[32m0.49161[0m[0m | time: 40.712s
[2K
| RMSProp | epoch: 009 | loss: 0.49161 - acc: 0.7661 | val_loss: 0.45752 - val_acc: 0.7630 -- iter: 1292/1292
--
Training Step: 370  | total loss: [1m[32m0.48355[0m[0m | time: 1.086s
[2K
| RMSProp | epoch: 010 | loss: 0.48355 - acc: 0.7738 -- iter: 0032/1292
[A[ATraining Step: 371  | total loss: [1m[32m0.46991[0m[0m | time: 2.142s
[2K
| RMSProp | epoch: 010 | loss: 0.46991 - acc: 0.7777 -- iter: 0064/1292
[A[ATraining Step: 372  | total loss: [1m[32m0.45918[0m[0m | time: 2.941s
[2K
| RMSProp | epoch: 010 | loss: 0.45918 - acc: 0.7906 -- iter: 0096/1292
[A[ATraining Step: 373  | total loss: [1m[32m0.46555[0m[0m | time: 4.027s
[2K
| RMSProp | epoch: 010 | loss: 0.46555 - acc: 0.7834 -- iter: 0128/1292
[A[ATraining Step: 374  | total loss: [1m[32m0.47800[0m[0m | time: 5.152s
[2K
| RMSProp | epoch: 010 | loss: 0.47800 - acc: 0.7675 -- iter: 0160/1292
[A[ATraining Step: 375  | total loss: [1m[32m0.48069[0m[0m | time: 6.153s
[2K
| RMSProp | epoch: 010 | loss: 0.48069 - acc: 0.7658 -- iter: 0192/1292
[A[ATraining Step: 376  | total loss: [1m[32m0.47248[0m[0m | time: 7.027s
[2K
| RMSProp | epoch: 010 | loss: 0.47248 - acc: 0.7767 -- iter: 0224/1292
[A[ATraining Step: 377  | total loss: [1m[32m0.46400[0m[0m | time: 7.416s
[2K
| RMSProp | epoch: 010 | loss: 0.46400 - acc: 0.7865 -- iter: 0256/1292
[A[ATraining Step: 378  | total loss: [1m[32m0.47358[0m[0m | time: 7.798s
[2K
| RMSProp | epoch: 010 | loss: 0.47358 - acc: 0.7829 -- iter: 0288/1292
[A[ATraining Step: 379  | total loss: [1m[32m0.47401[0m[0m | time: 8.787s
[2K
| RMSProp | epoch: 010 | loss: 0.47401 - acc: 0.7879 -- iter: 0320/1292
[A[ATraining Step: 380  | total loss: [1m[32m0.47976[0m[0m | time: 9.729s
[2K
| RMSProp | epoch: 010 | loss: 0.47976 - acc: 0.7810 -- iter: 0352/1292
[A[ATraining Step: 381  | total loss: [1m[32m0.47095[0m[0m | time: 10.877s
[2K
| RMSProp | epoch: 010 | loss: 0.47095 - acc: 0.7873 -- iter: 0384/1292
[A[ATraining Step: 382  | total loss: [1m[32m0.46752[0m[0m | time: 11.899s
[2K
| RMSProp | epoch: 010 | loss: 0.46752 - acc: 0.7836 -- iter: 0416/1292
[A[ATraining Step: 383  | total loss: [1m[32m0.47763[0m[0m | time: 12.793s
[2K
| RMSProp | epoch: 010 | loss: 0.47763 - acc: 0.7708 -- iter: 0448/1292
[A[ATraining Step: 384  | total loss: [1m[32m0.46996[0m[0m | time: 13.893s
[2K
| RMSProp | epoch: 010 | loss: 0.46996 - acc: 0.7781 -- iter: 0480/1292
[A[ATraining Step: 385  | total loss: [1m[32m0.46112[0m[0m | time: 14.985s
[2K
| RMSProp | epoch: 010 | loss: 0.46112 - acc: 0.7847 -- iter: 0512/1292
[A[ATraining Step: 386  | total loss: [1m[32m0.47290[0m[0m | time: 15.887s
[2K
| RMSProp | epoch: 010 | loss: 0.47290 - acc: 0.7750 -- iter: 0544/1292
[A[ATraining Step: 387  | total loss: [1m[32m0.46797[0m[0m | time: 16.737s
[2K
| RMSProp | epoch: 010 | loss: 0.46797 - acc: 0.7818 -- iter: 0576/1292
[A[ATraining Step: 388  | total loss: [1m[32m0.47565[0m[0m | time: 17.652s
[2K
| RMSProp | epoch: 010 | loss: 0.47565 - acc: 0.7724 -- iter: 0608/1292
[A[ATraining Step: 389  | total loss: [1m[32m0.47224[0m[0m | time: 18.607s
[2K
| RMSProp | epoch: 010 | loss: 0.47224 - acc: 0.7733 -- iter: 0640/1292
[A[ATraining Step: 390  | total loss: [1m[32m0.47152[0m[0m | time: 19.620s
[2K
| RMSProp | epoch: 010 | loss: 0.47152 - acc: 0.7772 -- iter: 0672/1292
[A[ATraining Step: 391  | total loss: [1m[32m0.47578[0m[0m | time: 20.683s
[2K
| RMSProp | epoch: 010 | loss: 0.47578 - acc: 0.7714 -- iter: 0704/1292
[A[ATraining Step: 392  | total loss: [1m[32m0.48197[0m[0m | time: 21.556s
[2K
| RMSProp | epoch: 010 | loss: 0.48197 - acc: 0.7661 -- iter: 0736/1292
[A[ATraining Step: 393  | total loss: [1m[32m0.47082[0m[0m | time: 22.559s
[2K
| RMSProp | epoch: 010 | loss: 0.47082 - acc: 0.7739 -- iter: 0768/1292
[A[ATraining Step: 394  | total loss: [1m[32m0.45858[0m[0m | time: 23.580s
[2K
| RMSProp | epoch: 010 | loss: 0.45858 - acc: 0.7809 -- iter: 0800/1292
[A[ATraining Step: 395  | total loss: [1m[32m0.44618[0m[0m | time: 24.637s
[2K
| RMSProp | epoch: 010 | loss: 0.44618 - acc: 0.7903 -- iter: 0832/1292
[A[ATraining Step: 396  | total loss: [1m[32m0.44534[0m[0m | time: 25.457s
[2K
| RMSProp | epoch: 010 | loss: 0.44534 - acc: 0.7894 -- iter: 0864/1292
[A[ATraining Step: 397  | total loss: [1m[32m0.46608[0m[0m | time: 26.406s
[2K
| RMSProp | epoch: 010 | loss: 0.46608 - acc: 0.7854 -- iter: 0896/1292
[A[ATraining Step: 398  | total loss: [1m[32m0.47964[0m[0m | time: 27.329s
[2K
| RMSProp | epoch: 010 | loss: 0.47964 - acc: 0.7725 -- iter: 0928/1292
[A[ATraining Step: 399  | total loss: [1m[32m0.46785[0m[0m | time: 28.267s
[2K
| RMSProp | epoch: 010 | loss: 0.46785 - acc: 0.7859 -- iter: 0960/1292
[A[ATraining Step: 400  | total loss: [1m[32m0.46143[0m[0m | time: 31.375s
[2K
| RMSProp | epoch: 010 | loss: 0.46143 - acc: 0.7917 | val_loss: 0.47076 - val_acc: 0.7877 -- iter: 0992/1292
--
Training Step: 401  | total loss: [1m[32m0.45705[0m[0m | time: 32.394s
[2K
| RMSProp | epoch: 010 | loss: 0.45705 - acc: 0.7938 -- iter: 1024/1292
[A[ATraining Step: 402  | total loss: [1m[32m0.45026[0m[0m | time: 33.469s
[2K
| RMSProp | epoch: 010 | loss: 0.45026 - acc: 0.7988 -- iter: 1056/1292
[A[ATraining Step: 403  | total loss: [1m[32m0.44274[0m[0m | time: 34.524s
[2K
| RMSProp | epoch: 010 | loss: 0.44274 - acc: 0.7970 -- iter: 1088/1292
[A[ATraining Step: 404  | total loss: [1m[32m0.44763[0m[0m | time: 35.364s
[2K
| RMSProp | epoch: 010 | loss: 0.44763 - acc: 0.7954 -- iter: 1120/1292
[A[ATraining Step: 405  | total loss: [1m[32m0.43941[0m[0m | time: 36.259s
[2K
| RMSProp | epoch: 010 | loss: 0.43941 - acc: 0.7940 -- iter: 1152/1292
[A[ATraining Step: 406  | total loss: [1m[32m0.44998[0m[0m | time: 37.236s
[2K
| RMSProp | epoch: 010 | loss: 0.44998 - acc: 0.7896 -- iter: 1184/1292
[A[ATraining Step: 407  | total loss: [1m[32m0.43988[0m[0m | time: 38.180s
[2K
| RMSProp | epoch: 010 | loss: 0.43988 - acc: 0.7919 -- iter: 1216/1292
[A[ATraining Step: 408  | total loss: [1m[32m0.42456[0m[0m | time: 39.268s
[2K
| RMSProp | epoch: 010 | loss: 0.42456 - acc: 0.8065 -- iter: 1248/1292
[A[ATraining Step: 409  | total loss: [1m[32m0.42415[0m[0m | time: 40.357s
[2K
| RMSProp | epoch: 010 | loss: 0.42415 - acc: 0.8039 -- iter: 1280/1292
[A[ATraining Step: 410  | total loss: [1m[32m0.41932[0m[0m | time: 43.137s
[2K
| RMSProp | epoch: 010 | loss: 0.41932 - acc: 0.8079 | val_loss: 0.42823 - val_acc: 0.8049 -- iter: 1292/1292
--
Training Step: 411  | total loss: [1m[32m0.41505[0m[0m | time: 0.972s
[2K
| RMSProp | epoch: 011 | loss: 0.41505 - acc: 0.8084 -- iter: 0032/1292
[A[ATraining Step: 412  | total loss: [1m[32m0.41004[0m[0m | time: 1.810s
[2K
| RMSProp | epoch: 011 | loss: 0.41004 - acc: 0.8119 -- iter: 0064/1292
[A[ATraining Step: 413  | total loss: [1m[32m0.45002[0m[0m | time: 2.761s
[2K
| RMSProp | epoch: 011 | loss: 0.45002 - acc: 0.7901 -- iter: 0096/1292
[A[ATraining Step: 414  | total loss: [1m[32m0.45124[0m[0m | time: 3.681s
[2K
| RMSProp | epoch: 011 | loss: 0.45124 - acc: 0.7892 -- iter: 0128/1292
[A[ATraining Step: 415  | total loss: [1m[32m0.44517[0m[0m | time: 4.629s
[2K
| RMSProp | epoch: 011 | loss: 0.44517 - acc: 0.7947 -- iter: 0160/1292
[A[ATraining Step: 416  | total loss: [1m[32m0.43316[0m[0m | time: 5.758s
[2K
| RMSProp | epoch: 011 | loss: 0.43316 - acc: 0.8058 -- iter: 0192/1292
[A[ATraining Step: 417  | total loss: [1m[32m0.42858[0m[0m | time: 6.720s
[2K
| RMSProp | epoch: 011 | loss: 0.42858 - acc: 0.8034 -- iter: 0224/1292
[A[ATraining Step: 418  | total loss: [1m[32m0.42763[0m[0m | time: 7.642s
[2K
| RMSProp | epoch: 011 | loss: 0.42763 - acc: 0.8043 -- iter: 0256/1292
[A[ATraining Step: 419  | total loss: [1m[32m0.41263[0m[0m | time: 8.080s
[2K
| RMSProp | epoch: 011 | loss: 0.41263 - acc: 0.8145 -- iter: 0288/1292
[A[ATraining Step: 420  | total loss: [1m[32m0.40006[0m[0m | time: 8.546s
[2K
| RMSProp | epoch: 011 | loss: 0.40006 - acc: 0.8247 -- iter: 0320/1292
[A[ATraining Step: 421  | total loss: [1m[32m0.38483[0m[0m | time: 9.612s
[2K
| RMSProp | epoch: 011 | loss: 0.38483 - acc: 0.8339 -- iter: 0352/1292
[A[ATraining Step: 422  | total loss: [1m[32m0.39722[0m[0m | time: 10.500s
[2K
| RMSProp | epoch: 011 | loss: 0.39722 - acc: 0.8286 -- iter: 0384/1292
[A[ATraining Step: 423  | total loss: [1m[32m0.38996[0m[0m | time: 11.362s
[2K
| RMSProp | epoch: 011 | loss: 0.38996 - acc: 0.8270 -- iter: 0416/1292
[A[ATraining Step: 424  | total loss: [1m[32m0.38315[0m[0m | time: 12.299s
[2K
| RMSProp | epoch: 011 | loss: 0.38315 - acc: 0.8349 -- iter: 0448/1292
[A[ATraining Step: 425  | total loss: [1m[32m0.37416[0m[0m | time: 13.254s
[2K
| RMSProp | epoch: 011 | loss: 0.37416 - acc: 0.8421 -- iter: 0480/1292
[A[ATraining Step: 426  | total loss: [1m[32m0.37691[0m[0m | time: 14.177s
[2K
| RMSProp | epoch: 011 | loss: 0.37691 - acc: 0.8391 -- iter: 0512/1292
[A[ATraining Step: 427  | total loss: [1m[32m0.39396[0m[0m | time: 15.201s
[2K
| RMSProp | epoch: 011 | loss: 0.39396 - acc: 0.8271 -- iter: 0544/1292
[A[ATraining Step: 428  | total loss: [1m[32m0.39734[0m[0m | time: 16.201s
[2K
| RMSProp | epoch: 011 | loss: 0.39734 - acc: 0.8256 -- iter: 0576/1292
[A[ATraining Step: 429  | total loss: [1m[32m0.39956[0m[0m | time: 17.130s
[2K
| RMSProp | epoch: 011 | loss: 0.39956 - acc: 0.8212 -- iter: 0608/1292
[A[ATraining Step: 430  | total loss: [1m[32m0.39649[0m[0m | time: 18.184s
[2K
| RMSProp | epoch: 011 | loss: 0.39649 - acc: 0.8297 -- iter: 0640/1292
[A[ATraining Step: 431  | total loss: [1m[32m0.39405[0m[0m | time: 19.204s
[2K
| RMSProp | epoch: 011 | loss: 0.39405 - acc: 0.8280 -- iter: 0672/1292
[A[ATraining Step: 432  | total loss: [1m[32m0.38792[0m[0m | time: 20.104s
[2K
| RMSProp | epoch: 011 | loss: 0.38792 - acc: 0.8358 -- iter: 0704/1292
[A[ATraining Step: 433  | total loss: [1m[32m0.37178[0m[0m | time: 21.004s
[2K
| RMSProp | epoch: 011 | loss: 0.37178 - acc: 0.8428 -- iter: 0736/1292
[A[ATraining Step: 434  | total loss: [1m[32m0.36654[0m[0m | time: 21.935s
[2K
| RMSProp | epoch: 011 | loss: 0.36654 - acc: 0.8461 -- iter: 0768/1292
[A[ATraining Step: 435  | total loss: [1m[32m0.34571[0m[0m | time: 22.854s
[2K
| RMSProp | epoch: 011 | loss: 0.34571 - acc: 0.8583 -- iter: 0800/1292
[A[ATraining Step: 436  | total loss: [1m[32m0.37046[0m[0m | time: 23.780s
[2K
| RMSProp | epoch: 011 | loss: 0.37046 - acc: 0.8350 -- iter: 0832/1292
[A[ATraining Step: 437  | total loss: [1m[32m0.36691[0m[0m | time: 24.785s
[2K
| RMSProp | epoch: 011 | loss: 0.36691 - acc: 0.8390 -- iter: 0864/1292
[A[ATraining Step: 438  | total loss: [1m[32m0.36410[0m[0m | time: 25.710s
[2K
| RMSProp | epoch: 011 | loss: 0.36410 - acc: 0.8395 -- iter: 0896/1292
[A[ATraining Step: 439  | total loss: [1m[32m0.36714[0m[0m | time: 26.592s
[2K
| RMSProp | epoch: 011 | loss: 0.36714 - acc: 0.8305 -- iter: 0928/1292
[A[ATraining Step: 440  | total loss: [1m[32m0.36767[0m[0m | time: 27.694s
[2K
| RMSProp | epoch: 011 | loss: 0.36767 - acc: 0.8318 -- iter: 0960/1292
[A[ATraining Step: 441  | total loss: [1m[32m0.35168[0m[0m | time: 28.769s
[2K
| RMSProp | epoch: 011 | loss: 0.35168 - acc: 0.8424 -- iter: 0992/1292
[A[ATraining Step: 442  | total loss: [1m[32m0.34052[0m[0m | time: 29.640s
[2K
| RMSProp | epoch: 011 | loss: 0.34052 - acc: 0.8519 -- iter: 1024/1292
[A[ATraining Step: 443  | total loss: [1m[32m0.35100[0m[0m | time: 30.584s
[2K
| RMSProp | epoch: 011 | loss: 0.35100 - acc: 0.8449 -- iter: 1056/1292
[A[ATraining Step: 444  | total loss: [1m[32m0.34516[0m[0m | time: 31.519s
[2K
| RMSProp | epoch: 011 | loss: 0.34516 - acc: 0.8510 -- iter: 1088/1292
[A[ATraining Step: 445  | total loss: [1m[32m0.34596[0m[0m | time: 32.473s
[2K
| RMSProp | epoch: 011 | loss: 0.34596 - acc: 0.8503 -- iter: 1120/1292
[A[ATraining Step: 446  | total loss: [1m[32m0.33889[0m[0m | time: 33.447s
[2K
| RMSProp | epoch: 011 | loss: 0.33889 - acc: 0.8559 -- iter: 1152/1292
[A[ATraining Step: 447  | total loss: [1m[32m0.34267[0m[0m | time: 34.560s
[2K
| RMSProp | epoch: 011 | loss: 0.34267 - acc: 0.8453 -- iter: 1184/1292
[A[ATraining Step: 448  | total loss: [1m[32m0.36490[0m[0m | time: 35.449s
[2K
| RMSProp | epoch: 011 | loss: 0.36490 - acc: 0.8326 -- iter: 1216/1292
[A[ATraining Step: 449  | total loss: [1m[32m0.35543[0m[0m | time: 36.371s
[2K
| RMSProp | epoch: 011 | loss: 0.35543 - acc: 0.8369 -- iter: 1248/1292
[A[ATraining Step: 450  | total loss: [1m[32m0.35427[0m[0m | time: 37.341s
[2K
| RMSProp | epoch: 011 | loss: 0.35427 - acc: 0.8407 -- iter: 1280/1292
[A[ATraining Step: 451  | total loss: [1m[32m0.36524[0m[0m | time: 40.401s
[2K
| RMSProp | epoch: 011 | loss: 0.36524 - acc: 0.8379 | val_loss: 0.57638 - val_acc: 0.6914 -- iter: 1292/1292
--
Training Step: 452  | total loss: [1m[32m0.37789[0m[0m | time: 0.993s
[2K
| RMSProp | epoch: 012 | loss: 0.37789 - acc: 0.8291 -- iter: 0032/1292
[A[ATraining Step: 453  | total loss: [1m[32m0.39984[0m[0m | time: 2.032s
[2K
| RMSProp | epoch: 012 | loss: 0.39984 - acc: 0.8149 -- iter: 0064/1292
[A[ATraining Step: 454  | total loss: [1m[32m0.40214[0m[0m | time: 3.110s
[2K
| RMSProp | epoch: 012 | loss: 0.40214 - acc: 0.8116 -- iter: 0096/1292
[A[ATraining Step: 455  | total loss: [1m[32m0.39856[0m[0m | time: 4.115s
[2K
| RMSProp | epoch: 012 | loss: 0.39856 - acc: 0.8116 -- iter: 0128/1292
[A[ATraining Step: 456  | total loss: [1m[32m0.39577[0m[0m | time: 5.097s
[2K
| RMSProp | epoch: 012 | loss: 0.39577 - acc: 0.8117 -- iter: 0160/1292
[A[ATraining Step: 457  | total loss: [1m[32m0.41514[0m[0m | time: 5.849s
[2K
| RMSProp | epoch: 012 | loss: 0.41514 - acc: 0.8056 -- iter: 0192/1292
[A[ATraining Step: 458  | total loss: [1m[32m0.41862[0m[0m | time: 6.458s
[2K
| RMSProp | epoch: 012 | loss: 0.41862 - acc: 0.8031 -- iter: 0224/1292
[A[ATraining Step: 459  | total loss: [1m[32m0.40894[0m[0m | time: 7.086s
[2K
| RMSProp | epoch: 012 | loss: 0.40894 - acc: 0.8103 -- iter: 0256/1292
[A[ATraining Step: 460  | total loss: [1m[32m0.40243[0m[0m | time: 7.711s
[2K
| RMSProp | epoch: 012 | loss: 0.40243 - acc: 0.8168 -- iter: 0288/1292
[A[ATraining Step: 461  | total loss: [1m[32m0.39238[0m[0m | time: 7.952s
[2K
| RMSProp | epoch: 012 | loss: 0.39238 - acc: 0.8195 -- iter: 0320/1292
[A[ATraining Step: 462  | total loss: [1m[32m0.38264[0m[0m | time: 8.217s
[2K
| RMSProp | epoch: 012 | loss: 0.38264 - acc: 0.8209 -- iter: 0352/1292
[A[ATraining Step: 463  | total loss: [1m[32m0.36579[0m[0m | time: 8.816s
[2K
| RMSProp | epoch: 012 | loss: 0.36579 - acc: 0.8304 -- iter: 0384/1292
[A[ATraining Step: 464  | total loss: [1m[32m0.36830[0m[0m | time: 9.430s
[2K
| RMSProp | epoch: 012 | loss: 0.36830 - acc: 0.8318 -- iter: 0416/1292
[A[ATraining Step: 465  | total loss: [1m[32m0.35683[0m[0m | time: 10.051s
[2K
| RMSProp | epoch: 012 | loss: 0.35683 - acc: 0.8392 -- iter: 0448/1292
[A[ATraining Step: 466  | total loss: [1m[32m0.34472[0m[0m | time: 10.668s
[2K
| RMSProp | epoch: 012 | loss: 0.34472 - acc: 0.8459 -- iter: 0480/1292
[A[ATraining Step: 467  | total loss: [1m[32m0.34624[0m[0m | time: 11.293s
[2K
| RMSProp | epoch: 012 | loss: 0.34624 - acc: 0.8457 -- iter: 0512/1292
[A[ATraining Step: 468  | total loss: [1m[32m0.38130[0m[0m | time: 11.888s
[2K
| RMSProp | epoch: 012 | loss: 0.38130 - acc: 0.8268 -- iter: 0544/1292
[A[ATraining Step: 469  | total loss: [1m[32m0.39503[0m[0m | time: 12.491s
[2K
| RMSProp | epoch: 012 | loss: 0.39503 - acc: 0.8160 -- iter: 0576/1292
[A[ATraining Step: 470  | total loss: [1m[32m0.39180[0m[0m | time: 13.103s
[2K
| RMSProp | epoch: 012 | loss: 0.39180 - acc: 0.8125 -- iter: 0608/1292
[A[ATraining Step: 471  | total loss: [1m[32m0.40932[0m[0m | time: 13.728s
[2K
| RMSProp | epoch: 012 | loss: 0.40932 - acc: 0.7937 -- iter: 0640/1292
[A[ATraining Step: 472  | total loss: [1m[32m0.42976[0m[0m | time: 14.342s
[2K
| RMSProp | epoch: 012 | loss: 0.42976 - acc: 0.7956 -- iter: 0672/1292
[A[ATraining Step: 473  | total loss: [1m[32m0.44169[0m[0m | time: 14.947s
[2K
| RMSProp | epoch: 012 | loss: 0.44169 - acc: 0.7848 -- iter: 0704/1292
[A[ATraining Step: 474  | total loss: [1m[32m0.42367[0m[0m | time: 15.556s
[2K
| RMSProp | epoch: 012 | loss: 0.42367 - acc: 0.8063 -- iter: 0736/1292
[A[ATraining Step: 475  | total loss: [1m[32m0.40503[0m[0m | time: 16.180s
[2K
| RMSProp | epoch: 012 | loss: 0.40503 - acc: 0.8163 -- iter: 0768/1292
[A[ATraining Step: 476  | total loss: [1m[32m0.39659[0m[0m | time: 16.804s
[2K
| RMSProp | epoch: 012 | loss: 0.39659 - acc: 0.8222 -- iter: 0800/1292
[A[ATraining Step: 477  | total loss: [1m[32m0.39807[0m[0m | time: 17.396s
[2K
| RMSProp | epoch: 012 | loss: 0.39807 - acc: 0.8243 -- iter: 0832/1292
[A[ATraining Step: 478  | total loss: [1m[32m0.38296[0m[0m | time: 18.015s
[2K
| RMSProp | epoch: 012 | loss: 0.38296 - acc: 0.8325 -- iter: 0864/1292
[A[ATraining Step: 479  | total loss: [1m[32m0.39024[0m[0m | time: 18.619s
[2K
| RMSProp | epoch: 012 | loss: 0.39024 - acc: 0.8305 -- iter: 0896/1292
[A[ATraining Step: 480  | total loss: [1m[32m0.38029[0m[0m | time: 19.228s
[2K
| RMSProp | epoch: 012 | loss: 0.38029 - acc: 0.8350 -- iter: 0928/1292
[A[ATraining Step: 481  | total loss: [1m[32m0.36767[0m[0m | time: 19.824s
[2K
| RMSProp | epoch: 012 | loss: 0.36767 - acc: 0.8421 -- iter: 0960/1292
[A[ATraining Step: 482  | total loss: [1m[32m0.35393[0m[0m | time: 20.418s
[2K
| RMSProp | epoch: 012 | loss: 0.35393 - acc: 0.8516 -- iter: 0992/1292
[A[ATraining Step: 483  | total loss: [1m[32m0.33680[0m[0m | time: 21.017s
[2K
| RMSProp | epoch: 012 | loss: 0.33680 - acc: 0.8602 -- iter: 1024/1292
[A[ATraining Step: 484  | total loss: [1m[32m0.32649[0m[0m | time: 21.641s
[2K
| RMSProp | epoch: 012 | loss: 0.32649 - acc: 0.8680 -- iter: 1056/1292
[A[ATraining Step: 485  | total loss: [1m[32m0.32554[0m[0m | time: 22.254s
[2K
| RMSProp | epoch: 012 | loss: 0.32554 - acc: 0.8687 -- iter: 1088/1292
[A[ATraining Step: 486  | total loss: [1m[32m0.39784[0m[0m | time: 23.046s
[2K
| RMSProp | epoch: 012 | loss: 0.39784 - acc: 0.8318 -- iter: 1120/1292
[A[ATraining Step: 487  | total loss: [1m[32m0.39626[0m[0m | time: 24.133s
[2K
| RMSProp | epoch: 012 | loss: 0.39626 - acc: 0.8330 -- iter: 1152/1292
[A[ATraining Step: 488  | total loss: [1m[32m0.38418[0m[0m | time: 25.240s
[2K
| RMSProp | epoch: 012 | loss: 0.38418 - acc: 0.8466 -- iter: 1184/1292
[A[ATraining Step: 489  | total loss: [1m[32m0.38296[0m[0m | time: 26.143s
[2K
| RMSProp | epoch: 012 | loss: 0.38296 - acc: 0.8494 -- iter: 1216/1292
[A[ATraining Step: 490  | total loss: [1m[32m0.38350[0m[0m | time: 27.071s
[2K
| RMSProp | epoch: 012 | loss: 0.38350 - acc: 0.8488 -- iter: 1248/1292
[A[ATraining Step: 491  | total loss: [1m[32m0.41084[0m[0m | time: 28.074s
[2K
| RMSProp | epoch: 012 | loss: 0.41084 - acc: 0.8358 -- iter: 1280/1292
[A[ATraining Step: 492  | total loss: [1m[32m0.40644[0m[0m | time: 31.093s
[2K
| RMSProp | epoch: 012 | loss: 0.40644 - acc: 0.8398 | val_loss: 0.32439 - val_acc: 0.8617 -- iter: 1292/1292
--
Training Step: 493  | total loss: [1m[32m0.39158[0m[0m | time: 0.849s
[2K
| RMSProp | epoch: 013 | loss: 0.39158 - acc: 0.8495 -- iter: 0032/1292
[A[ATraining Step: 494  | total loss: [1m[32m0.39228[0m[0m | time: 1.957s
[2K
| RMSProp | epoch: 013 | loss: 0.39228 - acc: 0.8427 -- iter: 0064/1292
[A[ATraining Step: 495  | total loss: [1m[32m0.38481[0m[0m | time: 3.014s
[2K
| RMSProp | epoch: 013 | loss: 0.38481 - acc: 0.8428 -- iter: 0096/1292
[A[ATraining Step: 496  | total loss: [1m[32m0.38395[0m[0m | time: 3.969s
[2K
| RMSProp | epoch: 013 | loss: 0.38395 - acc: 0.8398 -- iter: 0128/1292
[A[ATraining Step: 497  | total loss: [1m[32m0.38867[0m[0m | time: 4.899s
[2K
| RMSProp | epoch: 013 | loss: 0.38867 - acc: 0.8370 -- iter: 0160/1292
[A[ATraining Step: 498  | total loss: [1m[32m0.37100[0m[0m | time: 5.859s
[2K
| RMSProp | epoch: 013 | loss: 0.37100 - acc: 0.8440 -- iter: 0192/1292
[A[ATraining Step: 499  | total loss: [1m[32m0.38960[0m[0m | time: 6.787s
[2K
| RMSProp | epoch: 013 | loss: 0.38960 - acc: 0.8346 -- iter: 0224/1292
[A[ATraining Step: 500  | total loss: [1m[32m0.37336[0m[0m | time: 7.824s
[2K
| RMSProp | epoch: 013 | loss: 0.37336 - acc: 0.8417 -- iter: 0256/1292
[A[ATraining Step: 501  | total loss: [1m[32m0.35185[0m[0m | time: 8.953s
[2K
| RMSProp | epoch: 013 | loss: 0.35185 - acc: 0.8544 -- iter: 0288/1292
[A[ATraining Step: 502  | total loss: [1m[32m0.32946[0m[0m | time: 9.829s
[2K
| RMSProp | epoch: 013 | loss: 0.32946 - acc: 0.8659 -- iter: 0320/1292
[A[ATraining Step: 503  | total loss: [1m[32m0.32607[0m[0m | time: 10.214s
[2K
| RMSProp | epoch: 013 | loss: 0.32607 - acc: 0.8668 -- iter: 0352/1292
[A[ATraining Step: 504  | total loss: [1m[32m0.31833[0m[0m | time: 10.640s
[2K
| RMSProp | epoch: 013 | loss: 0.31833 - acc: 0.8634 -- iter: 0384/1292
[A[ATraining Step: 505  | total loss: [1m[32m0.31838[0m[0m | time: 11.735s
[2K
| RMSProp | epoch: 013 | loss: 0.31838 - acc: 0.8521 -- iter: 0416/1292
[A[ATraining Step: 506  | total loss: [1m[32m0.31810[0m[0m | time: 12.826s
[2K
| RMSProp | epoch: 013 | loss: 0.31810 - acc: 0.8575 -- iter: 0448/1292
[A[ATraining Step: 507  | total loss: [1m[32m0.32463[0m[0m | time: 13.663s
[2K
| RMSProp | epoch: 013 | loss: 0.32463 - acc: 0.8530 -- iter: 0480/1292
[A[ATraining Step: 508  | total loss: [1m[32m0.32131[0m[0m | time: 14.570s
[2K
| RMSProp | epoch: 013 | loss: 0.32131 - acc: 0.8521 -- iter: 0512/1292
[A[ATraining Step: 509  | total loss: [1m[32m0.31086[0m[0m | time: 15.545s
[2K
| RMSProp | epoch: 013 | loss: 0.31086 - acc: 0.8606 -- iter: 0544/1292
[A[ATraining Step: 510  | total loss: [1m[32m0.31174[0m[0m | time: 16.522s
[2K
| RMSProp | epoch: 013 | loss: 0.31174 - acc: 0.8589 -- iter: 0576/1292
[A[ATraining Step: 511  | total loss: [1m[32m0.32872[0m[0m | time: 17.576s
[2K
| RMSProp | epoch: 013 | loss: 0.32872 - acc: 0.8387 -- iter: 0608/1292
[A[ATraining Step: 512  | total loss: [1m[32m0.32746[0m[0m | time: 18.602s
[2K
| RMSProp | epoch: 013 | loss: 0.32746 - acc: 0.8454 -- iter: 0640/1292
[A[ATraining Step: 513  | total loss: [1m[32m0.31353[0m[0m | time: 19.503s
[2K
| RMSProp | epoch: 013 | loss: 0.31353 - acc: 0.8546 -- iter: 0672/1292
[A[ATraining Step: 514  | total loss: [1m[32m0.31515[0m[0m | time: 20.565s
[2K
| RMSProp | epoch: 013 | loss: 0.31515 - acc: 0.8535 -- iter: 0704/1292
[A[ATraining Step: 515  | total loss: [1m[32m0.29632[0m[0m | time: 21.578s
[2K
| RMSProp | epoch: 013 | loss: 0.29632 - acc: 0.8651 -- iter: 0736/1292
[A[ATraining Step: 516  | total loss: [1m[32m0.28061[0m[0m | time: 22.596s
[2K
| RMSProp | epoch: 013 | loss: 0.28061 - acc: 0.8754 -- iter: 0768/1292
[A[ATraining Step: 517  | total loss: [1m[32m0.27141[0m[0m | time: 23.407s
[2K
| RMSProp | epoch: 013 | loss: 0.27141 - acc: 0.8848 -- iter: 0800/1292
[A[ATraining Step: 518  | total loss: [1m[32m0.29228[0m[0m | time: 24.367s
[2K
| RMSProp | epoch: 013 | loss: 0.29228 - acc: 0.8744 -- iter: 0832/1292
[A[ATraining Step: 519  | total loss: [1m[32m0.30786[0m[0m | time: 25.313s
[2K
| RMSProp | epoch: 013 | loss: 0.30786 - acc: 0.8713 -- iter: 0864/1292
[A[ATraining Step: 520  | total loss: [1m[32m0.31286[0m[0m | time: 26.232s
[2K
| RMSProp | epoch: 013 | loss: 0.31286 - acc: 0.8717 -- iter: 0896/1292
[A[ATraining Step: 521  | total loss: [1m[32m0.31147[0m[0m | time: 27.279s
[2K
| RMSProp | epoch: 013 | loss: 0.31147 - acc: 0.8752 -- iter: 0928/1292
[A[ATraining Step: 522  | total loss: [1m[32m0.31173[0m[0m | time: 28.331s
[2K
| RMSProp | epoch: 013 | loss: 0.31173 - acc: 0.8752 -- iter: 0960/1292
[A[ATraining Step: 523  | total loss: [1m[32m0.31755[0m[0m | time: 29.157s
[2K
| RMSProp | epoch: 013 | loss: 0.31755 - acc: 0.8720 -- iter: 0992/1292
[A[ATraining Step: 524  | total loss: [1m[32m0.35738[0m[0m | time: 30.213s
[2K
| RMSProp | epoch: 013 | loss: 0.35738 - acc: 0.8567 -- iter: 1024/1292
[A[ATraining Step: 525  | total loss: [1m[32m0.34835[0m[0m | time: 31.320s
[2K
| RMSProp | epoch: 013 | loss: 0.34835 - acc: 0.8616 -- iter: 1056/1292
[A[ATraining Step: 526  | total loss: [1m[32m0.34212[0m[0m | time: 32.218s
[2K
| RMSProp | epoch: 013 | loss: 0.34212 - acc: 0.8661 -- iter: 1088/1292
[A[ATraining Step: 527  | total loss: [1m[32m0.32681[0m[0m | time: 33.077s
[2K
| RMSProp | epoch: 013 | loss: 0.32681 - acc: 0.8732 -- iter: 1120/1292
[A[ATraining Step: 528  | total loss: [1m[32m0.31432[0m[0m | time: 33.980s
[2K
| RMSProp | epoch: 013 | loss: 0.31432 - acc: 0.8797 -- iter: 1152/1292
[A[ATraining Step: 529  | total loss: [1m[32m0.30302[0m[0m | time: 34.876s
[2K
| RMSProp | epoch: 013 | loss: 0.30302 - acc: 0.8823 -- iter: 1184/1292
[A[ATraining Step: 530  | total loss: [1m[32m0.27925[0m[0m | time: 35.885s
[2K
| RMSProp | epoch: 013 | loss: 0.27925 - acc: 0.8941 -- iter: 1216/1292
[A[ATraining Step: 531  | total loss: [1m[32m0.28166[0m[0m | time: 36.945s
[2K
| RMSProp | epoch: 013 | loss: 0.28166 - acc: 0.8953 -- iter: 1248/1292
[A[ATraining Step: 532  | total loss: [1m[32m0.27360[0m[0m | time: 37.823s
[2K
| RMSProp | epoch: 013 | loss: 0.27360 - acc: 0.8995 -- iter: 1280/1292
[A[ATraining Step: 533  | total loss: [1m[32m0.27623[0m[0m | time: 40.797s
[2K
| RMSProp | epoch: 013 | loss: 0.27623 - acc: 0.8971 | val_loss: 0.29867 - val_acc: 0.8765 -- iter: 1292/1292
--
Training Step: 534  | total loss: [1m[32m0.27811[0m[0m | time: 0.849s
[2K
| RMSProp | epoch: 014 | loss: 0.27811 - acc: 0.8949 -- iter: 0032/1292
[A[ATraining Step: 535  | total loss: [1m[32m0.28328[0m[0m | time: 1.728s
[2K
| RMSProp | epoch: 014 | loss: 0.28328 - acc: 0.8898 -- iter: 0064/1292
[A[ATraining Step: 536  | total loss: [1m[32m0.28426[0m[0m | time: 2.646s
[2K
| RMSProp | epoch: 014 | loss: 0.28426 - acc: 0.8852 -- iter: 0096/1292
[A[ATraining Step: 537  | total loss: [1m[32m0.28038[0m[0m | time: 3.595s
[2K
| RMSProp | epoch: 014 | loss: 0.28038 - acc: 0.8873 -- iter: 0128/1292
[A[ATraining Step: 538  | total loss: [1m[32m0.27272[0m[0m | time: 4.613s
[2K
| RMSProp | epoch: 014 | loss: 0.27272 - acc: 0.8923 -- iter: 0160/1292
[A[ATraining Step: 539  | total loss: [1m[32m0.26738[0m[0m | time: 5.468s
[2K
| RMSProp | epoch: 014 | loss: 0.26738 - acc: 0.8937 -- iter: 0192/1292
[A[ATraining Step: 540  | total loss: [1m[32m0.26304[0m[0m | time: 6.468s
[2K
| RMSProp | epoch: 014 | loss: 0.26304 - acc: 0.8918 -- iter: 0224/1292
[A[ATraining Step: 541  | total loss: [1m[32m0.26299[0m[0m | time: 7.510s
[2K
| RMSProp | epoch: 014 | loss: 0.26299 - acc: 0.8870 -- iter: 0256/1292
[A[ATraining Step: 542  | total loss: [1m[32m0.27193[0m[0m | time: 8.543s
[2K
| RMSProp | epoch: 014 | loss: 0.27193 - acc: 0.8764 -- iter: 0288/1292
[A[ATraining Step: 543  | total loss: [1m[32m0.25715[0m[0m | time: 9.402s
[2K
| RMSProp | epoch: 014 | loss: 0.25715 - acc: 0.8888 -- iter: 0320/1292
[A[ATraining Step: 544  | total loss: [1m[32m0.25545[0m[0m | time: 10.331s
[2K
| RMSProp | epoch: 014 | loss: 0.25545 - acc: 0.8874 -- iter: 0352/1292
[A[ATraining Step: 545  | total loss: [1m[32m0.26272[0m[0m | time: 10.735s
[2K
| RMSProp | epoch: 014 | loss: 0.26272 - acc: 0.8862 -- iter: 0384/1292
[A[ATraining Step: 546  | total loss: [1m[32m0.27421[0m[0m | time: 11.151s
[2K
| RMSProp | epoch: 014 | loss: 0.27421 - acc: 0.8809 -- iter: 0416/1292
[A[ATraining Step: 547  | total loss: [1m[32m0.27131[0m[0m | time: 12.066s
[2K
| RMSProp | epoch: 014 | loss: 0.27131 - acc: 0.8845 -- iter: 0448/1292
[A[ATraining Step: 548  | total loss: [1m[32m0.27508[0m[0m | time: 13.143s
[2K
| RMSProp | epoch: 014 | loss: 0.27508 - acc: 0.8835 -- iter: 0480/1292
[A[ATraining Step: 549  | total loss: [1m[32m0.26943[0m[0m | time: 14.209s
[2K
| RMSProp | epoch: 014 | loss: 0.26943 - acc: 0.8889 -- iter: 0512/1292
[A[ATraining Step: 550  | total loss: [1m[32m0.26944[0m[0m | time: 15.002s
[2K
| RMSProp | epoch: 014 | loss: 0.26944 - acc: 0.8875 -- iter: 0544/1292
[A[ATraining Step: 551  | total loss: [1m[32m0.25390[0m[0m | time: 16.108s
[2K
| RMSProp | epoch: 014 | loss: 0.25390 - acc: 0.8956 -- iter: 0576/1292
[A[ATraining Step: 552  | total loss: [1m[32m0.27187[0m[0m | time: 17.219s
[2K
| RMSProp | epoch: 014 | loss: 0.27187 - acc: 0.8873 -- iter: 0608/1292
[A[ATraining Step: 553  | total loss: [1m[32m0.26460[0m[0m | time: 18.157s
[2K
| RMSProp | epoch: 014 | loss: 0.26460 - acc: 0.8923 -- iter: 0640/1292
[A[ATraining Step: 554  | total loss: [1m[32m0.27026[0m[0m | time: 18.969s
[2K
| RMSProp | epoch: 014 | loss: 0.27026 - acc: 0.8937 -- iter: 0672/1292
[A[ATraining Step: 555  | total loss: [1m[32m0.27845[0m[0m | time: 19.922s
[2K
| RMSProp | epoch: 014 | loss: 0.27845 - acc: 0.8950 -- iter: 0704/1292
[A[ATraining Step: 556  | total loss: [1m[32m0.27928[0m[0m | time: 20.896s
[2K
| RMSProp | epoch: 014 | loss: 0.27928 - acc: 0.8930 -- iter: 0736/1292
[A[ATraining Step: 557  | total loss: [1m[32m0.26877[0m[0m | time: 21.813s
[2K
| RMSProp | epoch: 014 | loss: 0.26877 - acc: 0.9006 -- iter: 0768/1292
[A[ATraining Step: 558  | total loss: [1m[32m0.26574[0m[0m | time: 22.922s
[2K
| RMSProp | epoch: 014 | loss: 0.26574 - acc: 0.9043 -- iter: 0800/1292
[A[ATraining Step: 559  | total loss: [1m[32m0.24713[0m[0m | time: 23.943s
[2K
| RMSProp | epoch: 014 | loss: 0.24713 - acc: 0.9138 -- iter: 0832/1292
[A[ATraining Step: 560  | total loss: [1m[32m0.23422[0m[0m | time: 24.759s
[2K
| RMSProp | epoch: 014 | loss: 0.23422 - acc: 0.9162 -- iter: 0864/1292
[A[ATraining Step: 561  | total loss: [1m[32m0.22429[0m[0m | time: 25.846s
[2K
| RMSProp | epoch: 014 | loss: 0.22429 - acc: 0.9246 -- iter: 0896/1292
[A[ATraining Step: 562  | total loss: [1m[32m0.23797[0m[0m | time: 26.922s
[2K
| RMSProp | epoch: 014 | loss: 0.23797 - acc: 0.9196 -- iter: 0928/1292
[A[ATraining Step: 563  | total loss: [1m[32m0.24386[0m[0m | time: 27.876s
[2K
| RMSProp | epoch: 014 | loss: 0.24386 - acc: 0.9120 -- iter: 0960/1292
[A[ATraining Step: 564  | total loss: [1m[32m0.23065[0m[0m | time: 28.720s
[2K
| RMSProp | epoch: 014 | loss: 0.23065 - acc: 0.9177 -- iter: 0992/1292
[A[ATraining Step: 565  | total loss: [1m[32m0.22236[0m[0m | time: 29.620s
[2K
| RMSProp | epoch: 014 | loss: 0.22236 - acc: 0.9259 -- iter: 1024/1292
[A[ATraining Step: 566  | total loss: [1m[32m0.23345[0m[0m | time: 30.539s
[2K
| RMSProp | epoch: 014 | loss: 0.23345 - acc: 0.9177 -- iter: 1056/1292
[A[ATraining Step: 567  | total loss: [1m[32m0.23470[0m[0m | time: 31.505s
[2K
| RMSProp | epoch: 014 | loss: 0.23470 - acc: 0.9134 -- iter: 1088/1292
[A[ATraining Step: 568  | total loss: [1m[32m0.22899[0m[0m | time: 32.532s
[2K
| RMSProp | epoch: 014 | loss: 0.22899 - acc: 0.9159 -- iter: 1120/1292
[A[ATraining Step: 569  | total loss: [1m[32m0.22967[0m[0m | time: 33.600s
[2K
| RMSProp | epoch: 014 | loss: 0.22967 - acc: 0.9211 -- iter: 1152/1292
[A[ATraining Step: 570  | total loss: [1m[32m0.21354[0m[0m | time: 34.396s
[2K
| RMSProp | epoch: 014 | loss: 0.21354 - acc: 0.9290 -- iter: 1184/1292
[A[ATraining Step: 571  | total loss: [1m[32m0.22171[0m[0m | time: 35.550s
[2K
| RMSProp | epoch: 014 | loss: 0.22171 - acc: 0.9267 -- iter: 1216/1292
[A[ATraining Step: 572  | total loss: [1m[32m0.21296[0m[0m | time: 36.610s
[2K
| RMSProp | epoch: 014 | loss: 0.21296 - acc: 0.9309 -- iter: 1248/1292
[A[ATraining Step: 573  | total loss: [1m[32m0.20464[0m[0m | time: 37.578s
[2K
| RMSProp | epoch: 014 | loss: 0.20464 - acc: 0.9316 -- iter: 1280/1292
[A[ATraining Step: 574  | total loss: [1m[32m0.22388[0m[0m | time: 40.395s
[2K
| RMSProp | epoch: 014 | loss: 0.22388 - acc: 0.9291 | val_loss: 0.33065 - val_acc: 0.8543 -- iter: 1292/1292
--
Training Step: 575  | total loss: [1m[32m0.24126[0m[0m | time: 0.915s
[2K
| RMSProp | epoch: 015 | loss: 0.24126 - acc: 0.9174 -- iter: 0032/1292
[A[ATraining Step: 576  | total loss: [1m[32m0.23929[0m[0m | time: 1.986s
[2K
| RMSProp | epoch: 015 | loss: 0.23929 - acc: 0.9225 -- iter: 0064/1292
[A[ATraining Step: 577  | total loss: [1m[32m0.22393[0m[0m | time: 2.933s
[2K
| RMSProp | epoch: 015 | loss: 0.22393 - acc: 0.9303 -- iter: 0096/1292
[A[ATraining Step: 578  | total loss: [1m[32m0.22033[0m[0m | time: 3.896s
[2K
| RMSProp | epoch: 015 | loss: 0.22033 - acc: 0.9310 -- iter: 0128/1292
[A[ATraining Step: 579  | total loss: [1m[32m0.21848[0m[0m | time: 4.980s
[2K
| RMSProp | epoch: 015 | loss: 0.21848 - acc: 0.9285 -- iter: 0160/1292
[A[ATraining Step: 580  | total loss: [1m[32m0.24166[0m[0m | time: 6.037s
[2K
| RMSProp | epoch: 015 | loss: 0.24166 - acc: 0.9169 -- iter: 0192/1292
[A[ATraining Step: 581  | total loss: [1m[32m0.23825[0m[0m | time: 6.864s
[2K
| RMSProp | epoch: 015 | loss: 0.23825 - acc: 0.9127 -- iter: 0224/1292
[A[ATraining Step: 582  | total loss: [1m[32m0.23232[0m[0m | time: 7.775s
[2K
| RMSProp | epoch: 015 | loss: 0.23232 - acc: 0.9152 -- iter: 0256/1292
[A[ATraining Step: 583  | total loss: [1m[32m0.23417[0m[0m | time: 8.765s
[2K
| RMSProp | epoch: 015 | loss: 0.23417 - acc: 0.9143 -- iter: 0288/1292
[A[ATraining Step: 584  | total loss: [1m[32m0.23421[0m[0m | time: 9.745s
[2K
| RMSProp | epoch: 015 | loss: 0.23421 - acc: 0.9104 -- iter: 0320/1292
[A[ATraining Step: 585  | total loss: [1m[32m0.22893[0m[0m | time: 10.750s
[2K
| RMSProp | epoch: 015 | loss: 0.22893 - acc: 0.9131 -- iter: 0352/1292
[A[ATraining Step: 586  | total loss: [1m[32m0.22708[0m[0m | time: 11.866s
[2K
| RMSProp | epoch: 015 | loss: 0.22708 - acc: 0.9155 -- iter: 0384/1292
[A[ATraining Step: 587  | total loss: [1m[32m0.22699[0m[0m | time: 12.260s
[2K
| RMSProp | epoch: 015 | loss: 0.22699 - acc: 0.9115 -- iter: 0416/1292
[A[ATraining Step: 588  | total loss: [1m[32m0.23543[0m[0m | time: 12.623s
[2K
| RMSProp | epoch: 015 | loss: 0.23543 - acc: 0.9120 -- iter: 0448/1292
[A[ATraining Step: 589  | total loss: [1m[32m0.23695[0m[0m | time: 13.585s
[2K
| RMSProp | epoch: 015 | loss: 0.23695 - acc: 0.9125 -- iter: 0480/1292
[A[ATraining Step: 590  | total loss: [1m[32m0.25259[0m[0m | time: 14.639s
[2K
| RMSProp | epoch: 015 | loss: 0.25259 - acc: 0.9056 -- iter: 0512/1292
[A[ATraining Step: 591  | total loss: [1m[32m0.24490[0m[0m | time: 15.675s
[2K
| RMSProp | epoch: 015 | loss: 0.24490 - acc: 0.9088 -- iter: 0544/1292
[A[ATraining Step: 592  | total loss: [1m[32m0.23684[0m[0m | time: 16.460s
[2K
| RMSProp | epoch: 015 | loss: 0.23684 - acc: 0.9117 -- iter: 0576/1292
[A[ATraining Step: 593  | total loss: [1m[32m0.24015[0m[0m | time: 17.395s
[2K
| RMSProp | epoch: 015 | loss: 0.24015 - acc: 0.9080 -- iter: 0608/1292
[A[ATraining Step: 594  | total loss: [1m[32m0.25494[0m[0m | time: 18.389s
[2K
| RMSProp | epoch: 015 | loss: 0.25494 - acc: 0.9016 -- iter: 0640/1292
[A[ATraining Step: 595  | total loss: [1m[32m0.26448[0m[0m | time: 19.307s
[2K
| RMSProp | epoch: 015 | loss: 0.26448 - acc: 0.9020 -- iter: 0672/1292
[A[ATraining Step: 596  | total loss: [1m[32m0.26392[0m[0m | time: 20.358s
[2K
| RMSProp | epoch: 015 | loss: 0.26392 - acc: 0.8993 -- iter: 0704/1292
[A[ATraining Step: 597  | total loss: [1m[32m0.25044[0m[0m | time: 21.321s
[2K
| RMSProp | epoch: 015 | loss: 0.25044 - acc: 0.9031 -- iter: 0736/1292
[A[ATraining Step: 598  | total loss: [1m[32m0.23999[0m[0m | time: 22.176s
[2K
| RMSProp | epoch: 015 | loss: 0.23999 - acc: 0.9097 -- iter: 0768/1292
[A[ATraining Step: 599  | total loss: [1m[32m0.23257[0m[0m | time: 23.188s
[2K
| RMSProp | epoch: 015 | loss: 0.23257 - acc: 0.9156 -- iter: 0800/1292
[A[ATraining Step: 600  | total loss: [1m[32m0.21655[0m[0m | time: 26.112s
[2K
| RMSProp | epoch: 015 | loss: 0.21655 - acc: 0.9241 | val_loss: 0.29661 - val_acc: 0.8889 -- iter: 0832/1292
--
Training Step: 601  | total loss: [1m[32m0.20022[0m[0m | time: 27.047s
[2K
| RMSProp | epoch: 015 | loss: 0.20022 - acc: 0.9316 -- iter: 0864/1292
[A[ATraining Step: 602  | total loss: [1m[32m0.20083[0m[0m | time: 27.991s
[2K
| RMSProp | epoch: 015 | loss: 0.20083 - acc: 0.9291 -- iter: 0896/1292
[A[ATraining Step: 603  | total loss: [1m[32m0.20081[0m[0m | time: 28.899s
[2K
| RMSProp | epoch: 015 | loss: 0.20081 - acc: 0.9206 -- iter: 0928/1292
[A[ATraining Step: 604  | total loss: [1m[32m0.22443[0m[0m | time: 29.867s
[2K
| RMSProp | epoch: 015 | loss: 0.22443 - acc: 0.9098 -- iter: 0960/1292
[A[ATraining Step: 605  | total loss: [1m[32m0.21726[0m[0m | time: 30.921s
[2K
| RMSProp | epoch: 015 | loss: 0.21726 - acc: 0.9188 -- iter: 0992/1292
[A[ATraining Step: 606  | total loss: [1m[32m0.20580[0m[0m | time: 31.809s
[2K
| RMSProp | epoch: 015 | loss: 0.20580 - acc: 0.9238 -- iter: 1024/1292
[A[ATraining Step: 607  | total loss: [1m[32m0.20557[0m[0m | time: 32.770s
[2K
| RMSProp | epoch: 015 | loss: 0.20557 - acc: 0.9283 -- iter: 1056/1292
[A[ATraining Step: 608  | total loss: [1m[32m0.19978[0m[0m | time: 33.855s
[2K
| RMSProp | epoch: 015 | loss: 0.19978 - acc: 0.9292 -- iter: 1088/1292
[A[ATraining Step: 609  | total loss: [1m[32m0.18731[0m[0m | time: 34.920s
[2K
| RMSProp | epoch: 015 | loss: 0.18731 - acc: 0.9363 -- iter: 1120/1292
[A[ATraining Step: 610  | total loss: [1m[32m0.17955[0m[0m | time: 35.816s
[2K
| RMSProp | epoch: 015 | loss: 0.17955 - acc: 0.9395 -- iter: 1152/1292
[A[ATraining Step: 611  | total loss: [1m[32m0.18143[0m[0m | time: 36.766s
[2K
| RMSProp | epoch: 015 | loss: 0.18143 - acc: 0.9393 -- iter: 1184/1292
[A[ATraining Step: 612  | total loss: [1m[32m0.19540[0m[0m | time: 37.726s
[2K
| RMSProp | epoch: 015 | loss: 0.19540 - acc: 0.9298 -- iter: 1216/1292
[A[ATraining Step: 613  | total loss: [1m[32m0.18448[0m[0m | time: 38.624s
[2K
| RMSProp | epoch: 015 | loss: 0.18448 - acc: 0.9368 -- iter: 1248/1292
[A[ATraining Step: 614  | total loss: [1m[32m0.18269[0m[0m | time: 39.630s
[2K
| RMSProp | epoch: 015 | loss: 0.18269 - acc: 0.9369 -- iter: 1280/1292
[A[ATraining Step: 615  | total loss: [1m[32m0.18116[0m[0m | time: 42.336s
[2K
| RMSProp | epoch: 015 | loss: 0.18116 - acc: 0.9338 | val_loss: 0.28052 - val_acc: 0.8691 -- iter: 1292/1292
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.952527924750147
Validation AUPRC:0.969176108126684
Test AUC:0.9584763212079616
Test AUPRC:0.9668686540695505
BestTestF1Score	0.91	0.83	0.92	0.93	0.89	167	13	204	21	0.81
BestTestMCCScore	0.91	0.83	0.92	0.93	0.89	167	13	204	21	0.81
BestTestAccuracyScore	0.91	0.83	0.92	0.93	0.89	167	13	204	21	0.81
BestValidationF1Score	0.91	0.83	0.91	0.95	0.88	189	9	180	27	0.81
BestValidationMCC	0.91	0.83	0.91	0.95	0.88	189	9	180	27	0.81
BestValidationAccuracy	0.91	0.83	0.91	0.95	0.88	189	9	180	27	0.81
TestPredictions (Threshold:0.81)
CHEMBL3702086,TP,ACT,1.0	CHEMBL192489,TN,INACT,0.47999998927116394	CHEMBL2392385,TN,INACT,0.15000000596046448	CHEMBL1336927,TN,INACT,0.14000000059604645	CHEMBL328164,TN,INACT,0.27000001072883606	CHEMBL1420829,TN,INACT,0.09000000357627869	CHEMBL399121,TN,INACT,0.1599999964237213	CHEMBL509435,TN,INACT,0.6800000071525574	CHEMBL1363706,TN,INACT,0.36000001430511475	CHEMBL3702046,TP,ACT,1.0	CHEMBL1171700,TN,INACT,0.7300000190734863	CHEMBL495545,TP,ACT,0.9300000071525574	CHEMBL3702185,TP,ACT,1.0	CHEMBL3642001,TP,ACT,1.0	CHEMBL485878,FP,INACT,0.9900000095367432	CHEMBL3109401,TN,INACT,0.17000000178813934	CHEMBL1400863,TN,INACT,0.3700000047683716	CHEMBL3189536,TN,INACT,0.14000000059604645	CHEMBL3641911,TP,ACT,0.9900000095367432	CHEMBL572922,TN,INACT,0.4000000059604645	CHEMBL1650116,TN,INACT,0.07999999821186066	CHEMBL332342,TN,INACT,0.09000000357627869	CHEMBL120127,TN,INACT,0.23000000417232513	CHEMBL3702175,TP,ACT,0.9900000095367432	CHEMBL3698685,TP,ACT,1.0	CHEMBL1170125,TN,INACT,0.10000000149011612	CHEMBL477860,TP,ACT,0.9800000190734863	CHEMBL2426399,FN,ACT,0.6800000071525574	CHEMBL480583,TN,INACT,0.10999999940395355	CHEMBL1555880,TN,INACT,0.38999998569488525	CHEMBL3691633,TP,ACT,0.9900000095367432	CHEMBL1093745,TN,INACT,0.11999999731779099	CHEMBL524059,TP,ACT,0.9900000095367432	CHEMBL1320813,FP,INACT,0.8700000047683716	CHEMBL132369,TN,INACT,0.25	CHEMBL477863,TP,ACT,0.9399999976158142	CHEMBL3641994,TP,ACT,1.0	CHEMBL1310505,TN,INACT,0.05000000074505806	CHEMBL3702054,TP,ACT,0.9700000286102295	CHEMBL2403074,FP,INACT,0.9399999976158142	CHEMBL509635,TN,INACT,0.10999999940395355	CHEMBL2392383,TN,INACT,0.05000000074505806	CHEMBL515051,TN,INACT,0.6100000143051147	CHEMBL3702036,TP,ACT,1.0	CHEMBL3691603,TP,ACT,0.9900000095367432	CHEMBL46817,FN,ACT,0.6299999952316284	CHEMBL1462031,TN,INACT,0.07000000029802322	CHEMBL345348,TN,INACT,0.05999999865889549	CHEMBL190201,TN,INACT,0.05999999865889549	CHEMBL3642015,TP,ACT,1.0	CHEMBL521459,TP,ACT,0.9900000095367432	CHEMBL2059876,TN,INACT,0.1599999964237213	CHEMBL3691557,TP,ACT,0.9900000095367432	CHEMBL3641939,TP,ACT,0.9900000095367432	CHEMBL1650113,TN,INACT,0.10000000149011612	CHEMBL1472407,TN,INACT,0.12999999523162842	CHEMBL456965,TN,INACT,0.05999999865889549	CHEMBL3641908,TP,ACT,1.0	CHEMBL1349172,TN,INACT,0.10999999940395355	CHEMBL362722,TP,ACT,0.9700000286102295	CHEMBL291324,FN,ACT,0.05000000074505806	CHEMBL1414184,TN,INACT,0.17000000178813934	CHEMBL2071613,TN,INACT,0.28999999165534973	CHEMBL3691565,TP,ACT,0.9900000095367432	CHEMBL3684984,TP,ACT,0.9800000190734863	CHEMBL3698704,TP,ACT,1.0	CHEMBL3702038,TP,ACT,1.0	CHEMBL510863,TN,INACT,0.18000000715255737	CHEMBL316485,TN,INACT,0.12999999523162842	CHEMBL3120991,TN,INACT,0.6000000238418579	CHEMBL1382399,TN,INACT,0.25999999046325684	CHEMBL323015,FP,INACT,0.8100000023841858	CHEMBL468125,TN,INACT,0.30000001192092896	CHEMBL2024526,TN,INACT,0.03999999910593033	CHEMBL1950043,TN,INACT,0.6600000262260437	CHEMBL3702116,TP,ACT,0.9900000095367432	CHEMBL3702122,TP,ACT,0.9900000095367432	CHEMBL3691612,TP,ACT,0.9900000095367432	CHEMBL1275916,TN,INACT,0.17000000178813934	CHEMBL3691652,TP,ACT,0.9200000166893005	CHEMBL1681997,TN,INACT,0.07999999821186066	CHEMBL3702246,TP,ACT,1.0	CHEMBL3698651,TP,ACT,1.0	CHEMBL2392227,TN,INACT,0.20999999344348907	CHEMBL3398592,TN,INACT,0.23999999463558197	CHEMBL105350,TN,INACT,0.05000000074505806	CHEMBL3684980,TP,ACT,0.9800000190734863	CHEMBL1171699,TN,INACT,0.5699999928474426	CHEMBL1576272,TN,INACT,0.17000000178813934	CHEMBL519113,TN,INACT,0.09000000357627869	CHEMBL46181,TP,ACT,0.9900000095367432	CHEMBL3642021,TP,ACT,1.0	CHEMBL1084969,TP,ACT,0.9399999976158142	CHEMBL1170124,TN,INACT,0.14000000059604645	CHEMBL3237719,TP,ACT,0.9900000095367432	CHEMBL477063,TP,ACT,0.9800000190734863	CHEMBL517068,TN,INACT,0.7599999904632568	CHEMBL3698684,TP,ACT,1.0	CHEMBL3641965,TP,ACT,1.0	CHEMBL3702227,TP,ACT,0.9900000095367432	CHEMBL3691651,TP,ACT,0.9700000286102295	CHEMBL2164716,TN,INACT,0.05999999865889549	CHEMBL1922210,TN,INACT,0.03999999910593033	CHEMBL3641925,TP,ACT,1.0	CHEMBL3691585,TP,ACT,0.9900000095367432	CHEMBL258741,TN,INACT,0.09000000357627869	CHEMBL3700411,TN,INACT,0.1899999976158142	CHEMBL522590,TN,INACT,0.27000001072883606	CHEMBL3684981,TP,ACT,0.9800000190734863	CHEMBL3702032,TP,ACT,1.0	CHEMBL1561032,TN,INACT,0.38999998569488525	CHEMBL3702109,TP,ACT,0.9900000095367432	CHEMBL3691645,TP,ACT,0.9800000190734863	CHEMBL359999,TP,ACT,0.9900000095367432	CHEMBL509032,FN,ACT,0.5	CHEMBL2392358,TN,INACT,0.18000000715255737	CHEMBL99779,TN,INACT,0.029999999329447746	CHEMBL524820,TN,INACT,0.47999998927116394	CHEMBL456964,TN,INACT,0.05000000074505806	CHEMBL2163999,TN,INACT,0.47999998927116394	CHEMBL1784656,TN,INACT,0.07000000029802322	CHEMBL1540545,TN,INACT,0.07000000029802322	CHEMBL3702106,TP,ACT,0.9800000190734863	CHEMBL214253,FN,ACT,0.28999999165534973	CHEMBL51709,FN,ACT,0.28999999165534973	CHEMBL2392249,TN,INACT,0.07999999821186066	CHEMBL414128,FP,INACT,0.9200000166893005	CHEMBL456113,TN,INACT,0.44999998807907104	CHEMBL384831,TN,INACT,0.11999999731779099	CHEMBL347134,TN,INACT,0.05999999865889549	CHEMBL3702195,TP,ACT,1.0	CHEMBL551318,TN,INACT,0.10000000149011612	CHEMBL1411504,FP,INACT,0.9200000166893005	CHEMBL2163608,TN,INACT,0.6100000143051147	CHEMBL1643146,TN,INACT,0.25	CHEMBL2435287,TN,INACT,0.07999999821186066	CHEMBL1230170,TP,ACT,0.9800000190734863	CHEMBL3698719,TP,ACT,1.0	CHEMBL3702040,TP,ACT,0.9399999976158142	CHEMBL295398,FN,ACT,0.5400000214576721	CHEMBL1301070,TN,INACT,0.3700000047683716	CHEMBL77262,TN,INACT,0.15000000596046448	CHEMBL3691637,TP,ACT,0.9399999976158142	CHEMBL98335,TP,ACT,0.9800000190734863	CHEMBL455389,TN,INACT,0.05000000074505806	CHEMBL189136,TP,ACT,0.9900000095367432	CHEMBL3698697,TP,ACT,1.0	CHEMBL489646,TP,ACT,0.9399999976158142	CHEMBL67655,TN,INACT,0.10999999940395355	CHEMBL497564,TP,ACT,0.9900000095367432	CHEMBL101019,TP,ACT,0.9800000190734863	CHEMBL2392233,TN,INACT,0.05000000074505806	CHEMBL477861,TP,ACT,0.9399999976158142	CHEMBL2059869,TN,INACT,0.10999999940395355	CHEMBL3702245,TP,ACT,1.0	CHEMBL312065,TN,INACT,0.5799999833106995	CHEMBL3691564,TP,ACT,0.9900000095367432	CHEMBL3698737,TP,ACT,1.0	CHEMBL3702129,TP,ACT,1.0	CHEMBL1545746,TN,INACT,0.14000000059604645	CHEMBL1547674,FP,INACT,0.8600000143051147	CHEMBL101156,TP,ACT,0.9599999785423279	CHEMBL3654258,TP,ACT,0.9599999785423279	CHEMBL3691625,TP,ACT,0.9900000095367432	CHEMBL1414543,TN,INACT,0.23999999463558197	CHEMBL103355,FN,ACT,0.6499999761581421	CHEMBL1569232,TN,INACT,0.12999999523162842	CHEMBL3641986,TP,ACT,1.0	CHEMBL482967,TP,ACT,0.9800000190734863	CHEMBL3698695,TP,ACT,0.9900000095367432	CHEMBL206683,TN,INACT,0.6200000047683716	CHEMBL498520,TN,INACT,0.20000000298023224	CHEMBL419534,TP,ACT,0.9800000190734863	CHEMBL1241935,TN,INACT,0.7200000286102295	CHEMBL486286,TP,ACT,0.9900000095367432	CHEMBL487327,TP,ACT,0.9900000095367432	CHEMBL3698688,TP,ACT,1.0	CHEMBL3684988,TP,ACT,0.9800000190734863	CHEMBL1922219,FN,ACT,0.03999999910593033	CHEMBL602472,TN,INACT,0.11999999731779099	CHEMBL1375423,TN,INACT,0.33000001311302185	CHEMBL489244,TP,ACT,0.9800000190734863	CHEMBL1083786,TP,ACT,0.949999988079071	CHEMBL1922120,TN,INACT,0.05999999865889549	CHEMBL2087005,TN,INACT,0.11999999731779099	CHEMBL477077,TP,ACT,0.9800000190734863	CHEMBL1371386,TN,INACT,0.05000000074505806	CHEMBL496360,TP,ACT,0.9900000095367432	CHEMBL3702134,TP,ACT,0.9900000095367432	CHEMBL462343,TN,INACT,0.800000011920929	CHEMBL2321969,TN,INACT,0.4399999976158142	CHEMBL183158,FN,ACT,0.6800000071525574	CHEMBL1414671,TN,INACT,0.20999999344348907	CHEMBL518732,TN,INACT,0.3400000035762787	CHEMBL2420584,TN,INACT,0.07000000029802322	CHEMBL3361128,TN,INACT,0.3499999940395355	CHEMBL1454779,TN,INACT,0.12999999523162842	CHEMBL3237710,TP,ACT,0.9900000095367432	CHEMBL468963,FN,ACT,0.6899999976158142	CHEMBL3702211,TP,ACT,1.0	CHEMBL3109933,TN,INACT,0.7200000286102295	CHEMBL3691569,TP,ACT,0.9900000095367432	CHEMBL521922,TP,ACT,0.9800000190734863	CHEMBL563143,TN,INACT,0.20999999344348907	CHEMBL472566,TN,INACT,0.07999999821186066	CHEMBL3199093,TN,INACT,0.09000000357627869	CHEMBL142327,TN,INACT,0.38999998569488525	CHEMBL191632,TN,INACT,0.03999999910593033	CHEMBL3641962,TP,ACT,1.0	CHEMBL3641978,TP,ACT,0.9900000095367432	CHEMBL228862,TN,INACT,0.09000000357627869	CHEMBL477073,TP,ACT,0.9900000095367432	CHEMBL188282,TN,INACT,0.5899999737739563	CHEMBL1457783,TN,INACT,0.029999999329447746	CHEMBL3702110,TP,ACT,0.9800000190734863	CHEMBL486540,TN,INACT,0.09000000357627869	CHEMBL2426395,FN,ACT,0.7200000286102295	CHEMBL1349023,TN,INACT,0.7699999809265137	CHEMBL1499974,TN,INACT,0.7300000190734863	CHEMBL101558,TP,ACT,0.949999988079071	CHEMBL3628252,TN,INACT,0.1599999964237213	CHEMBL568708,TN,INACT,0.23999999463558197	CHEMBL3675375,TN,INACT,0.09000000357627869	CHEMBL318188,FP,INACT,0.8299999833106995	CHEMBL3684991,TP,ACT,0.9599999785423279	CHEMBL1481517,TN,INACT,0.14000000059604645	CHEMBL487326,TP,ACT,0.9900000095367432	CHEMBL1511773,TN,INACT,0.6700000166893005	CHEMBL364408,TP,ACT,0.8500000238418579	CHEMBL1306899,TN,INACT,0.41999998688697815	CHEMBL1545185,TN,INACT,0.10000000149011612	CHEMBL486285,FP,INACT,0.9700000286102295	CHEMBL418793,TP,ACT,0.9399999976158142	CHEMBL3702107,TP,ACT,1.0	CHEMBL3698746,TP,ACT,1.0	CHEMBL1517409,TN,INACT,0.05999999865889549	CHEMBL3109955,TN,INACT,0.1599999964237213	CHEMBL1762116,TN,INACT,0.17000000178813934	CHEMBL3641947,TP,ACT,1.0	CHEMBL2312646,TN,INACT,0.07000000029802322	CHEMBL3609564,TN,INACT,0.11999999731779099	CHEMBL3102933,TN,INACT,0.05000000074505806	CHEMBL485502,TP,ACT,0.9399999976158142	CHEMBL1425417,TN,INACT,0.10999999940395355	CHEMBL3698739,TP,ACT,1.0	CHEMBL100391,TP,ACT,0.9800000190734863	CHEMBL3798296,TN,INACT,0.23999999463558197	CHEMBL2152967,TN,INACT,0.10000000149011612	CHEMBL104264,TN,INACT,0.17000000178813934	CHEMBL3702069,TP,ACT,1.0	CHEMBL3641996,TP,ACT,1.0	CHEMBL557003,TN,INACT,0.5899999737739563	CHEMBL456796,TN,INACT,0.1599999964237213	CHEMBL2420911,TN,INACT,0.11999999731779099	CHEMBL3641993,TP,ACT,0.9700000286102295	CHEMBL1093390,TN,INACT,0.10999999940395355	CHEMBL3698713,TP,ACT,1.0	CHEMBL1351746,TN,INACT,0.17000000178813934	CHEMBL1668418,FP,INACT,0.8899999856948853	CHEMBL26551,TN,INACT,0.1599999964237213	CHEMBL202684,TN,INACT,0.38999998569488525	CHEMBL2312652,TN,INACT,0.05000000074505806	CHEMBL1241849,FN,ACT,0.3400000035762787	CHEMBL3641946,TP,ACT,0.9900000095367432	CHEMBL3698638,TP,ACT,1.0	CHEMBL1431441,TN,INACT,0.5099999904632568	CHEMBL3691668,TP,ACT,0.9900000095367432	CHEMBL477064,TP,ACT,0.9300000071525574	CHEMBL3698680,TP,ACT,1.0	CHEMBL450786,TP,ACT,0.8799999952316284	CHEMBL142788,FN,ACT,0.07000000029802322	CHEMBL3702135,TP,ACT,1.0	CHEMBL2153264,TN,INACT,0.10000000149011612	CHEMBL3702055,TP,ACT,1.0	CHEMBL460472,TN,INACT,0.27000001072883606	CHEMBL2381608,TN,INACT,0.019999999552965164	CHEMBL3680385,TP,ACT,0.9900000095367432	CHEMBL3698641,TP,ACT,1.0	CHEMBL2064537,TN,INACT,0.5299999713897705	CHEMBL3702145,TP,ACT,1.0	CHEMBL1287887,TN,INACT,0.10000000149011612	CHEMBL142789,FN,ACT,0.07000000029802322	CHEMBL3691658,TP,ACT,0.9900000095367432	CHEMBL2392235,TN,INACT,0.05000000074505806	CHEMBL3641950,TP,ACT,1.0	CHEMBL482919,FP,INACT,0.9100000262260437	CHEMBL3702162,TP,ACT,1.0	CHEMBL3702216,TP,ACT,0.9900000095367432	CHEMBL1641792,TN,INACT,0.07999999821186066	CHEMBL193966,FP,INACT,0.8199999928474426	CHEMBL1372214,TN,INACT,0.46000000834465027	CHEMBL504574,TN,INACT,0.17000000178813934	CHEMBL3702095,TP,ACT,1.0	CHEMBL2392355,TN,INACT,0.17000000178813934	CHEMBL2071607,TN,INACT,0.6600000262260437	CHEMBL469770,TN,INACT,0.20999999344348907	CHEMBL1083528,TN,INACT,0.6299999952316284	CHEMBL3691562,TP,ACT,0.9900000095367432	CHEMBL47132,TP,ACT,0.9100000262260437	CHEMBL362558,FN,ACT,0.7900000214576721	CHEMBL415789,TP,ACT,0.9300000071525574	CHEMBL3702181,TP,ACT,1.0	CHEMBL1403868,TN,INACT,0.12999999523162842	CHEMBL488645,TN,INACT,0.07000000029802322	CHEMBL1379677,TN,INACT,0.49000000953674316	CHEMBL3642026,TP,ACT,1.0	CHEMBL2057832,TN,INACT,0.6200000047683716	CHEMBL488955,TP,ACT,0.9900000095367432	CHEMBL233349,TN,INACT,0.5799999833106995	CHEMBL1801620,TN,INACT,0.18000000715255737	CHEMBL1288069,TN,INACT,0.28999999165534973	CHEMBL3701217,TN,INACT,0.1899999976158142	CHEMBL3086109,TN,INACT,0.20999999344348907	CHEMBL255718,TP,ACT,0.9900000095367432	CHEMBL3746916,TN,INACT,0.41999998688697815	CHEMBL3698689,TP,ACT,0.9399999976158142	CHEMBL1337355,TN,INACT,0.20999999344348907	CHEMBL496575,TP,ACT,0.9700000286102295	CHEMBL3641942,TP,ACT,1.0	CHEMBL178179,TN,INACT,0.5299999713897705	CHEMBL3702182,TP,ACT,1.0	CHEMBL402370,TP,ACT,0.9900000095367432	CHEMBL3698717,TP,ACT,1.0	CHEMBL450161,TN,INACT,0.6399999856948853	CHEMBL560393,TN,INACT,0.05000000074505806	CHEMBL496543,TN,INACT,0.6200000047683716	CHEMBL3087481,TN,INACT,0.6499999761581421	CHEMBL2087024,TN,INACT,0.33000001311302185	CHEMBL222841,TN,INACT,0.05999999865889549	CHEMBL3691655,TP,ACT,0.9900000095367432	CHEMBL1454635,TN,INACT,0.09000000357627869	CHEMBL1498547,TN,INACT,0.17000000178813934	CHEMBL1401280,TN,INACT,0.05000000074505806	CHEMBL1908397,TP,ACT,0.8600000143051147	CHEMBL3691550,TP,ACT,0.9900000095367432	CHEMBL38694,FN,ACT,0.07000000029802322	CHEMBL515949,TP,ACT,0.9900000095367432	CHEMBL2334797,TN,INACT,0.07999999821186066	CHEMBL3702199,TP,ACT,1.0	CHEMBL3698661,TP,ACT,0.9900000095367432	CHEMBL3698731,TP,ACT,1.0	CHEMBL2392241,TN,INACT,0.03999999910593033	CHEMBL3702155,TP,ACT,1.0	CHEMBL1929238,FP,INACT,0.8600000143051147	CHEMBL463383,FN,ACT,0.3199999928474426	CHEMBL105712,TN,INACT,0.12999999523162842	CHEMBL75680,TN,INACT,0.1599999964237213	CHEMBL2334794,TN,INACT,0.09000000357627869	CHEMBL186101,TP,ACT,0.9800000190734863	CHEMBL2392238,TN,INACT,0.03999999910593033	CHEMBL185914,TP,ACT,0.8700000047683716	CHEMBL101473,TP,ACT,0.9800000190734863	CHEMBL172973,TN,INACT,0.03999999910593033	CHEMBL3702223,TP,ACT,1.0	CHEMBL3691647,TP,ACT,0.9800000190734863	CHEMBL3702072,TP,ACT,1.0	CHEMBL518185,TN,INACT,0.23000000417232513	CHEMBL3700566,TN,INACT,0.07000000029802322	CHEMBL322640,TP,ACT,0.949999988079071	CHEMBL101810,TP,ACT,0.9300000071525574	CHEMBL1480110,TN,INACT,0.5199999809265137	CHEMBL1372854,TN,INACT,0.6399999856948853	CHEMBL317378,TP,ACT,0.9800000190734863	CHEMBL428968,TN,INACT,0.5400000214576721	CHEMBL3628249,TN,INACT,0.10000000149011612	CHEMBL3698668,TP,ACT,1.0	CHEMBL559683,TN,INACT,0.09000000357627869	CHEMBL3193799,TN,INACT,0.11999999731779099	CHEMBL1093079,TN,INACT,0.14000000059604645	CHEMBL1288067,TN,INACT,0.2199999988079071	CHEMBL259041,TN,INACT,0.07999999821186066	CHEMBL349810,TN,INACT,0.46000000834465027	CHEMBL1276308,TN,INACT,0.23999999463558197	CHEMBL1083785,TP,ACT,0.9599999785423279	CHEMBL1365626,TN,INACT,0.10000000149011612	CHEMBL1529808,TN,INACT,0.20000000298023224	CHEMBL1458153,TN,INACT,0.25	CHEMBL2312654,TN,INACT,0.23999999463558197	CHEMBL574738,FN,ACT,0.4699999988079071	CHEMBL3197208,TN,INACT,0.2199999988079071	CHEMBL3691608,TP,ACT,0.9900000095367432	CHEMBL490251,FN,ACT,0.7900000214576721	CHEMBL524901,TP,ACT,0.949999988079071	CHEMBL3641963,TP,ACT,1.0	CHEMBL1494345,TN,INACT,0.07999999821186066	CHEMBL476907,TP,ACT,0.9900000095367432	CHEMBL3642003,TP,ACT,0.9800000190734863	CHEMBL211705,TN,INACT,0.03999999910593033	CHEMBL3702169,TP,ACT,1.0	CHEMBL2334791,TN,INACT,0.12999999523162842	CHEMBL3641977,TP,ACT,1.0	CHEMBL1459123,TN,INACT,0.09000000357627869	CHEMBL395839,TN,INACT,0.5400000214576721	CHEMBL3641930,TP,ACT,0.9900000095367432	CHEMBL1083150,FN,ACT,0.6499999761581421	CHEMBL460003,TN,INACT,0.1899999976158142	CHEMBL1873703,TN,INACT,0.25	CHEMBL3702143,TP,ACT,1.0	CHEMBL432116,TP,ACT,0.9800000190734863	CHEMBL513336,TN,INACT,0.05999999865889549	CHEMBL3641929,TP,ACT,1.0	CHEMBL3702157,TP,ACT,1.0	CHEMBL1084629,TP,ACT,0.9900000095367432	CHEMBL3698663,TP,ACT,0.9700000286102295	CHEMBL77732,TN,INACT,0.20000000298023224	

