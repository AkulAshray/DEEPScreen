ImageNetInceptionV2 CHEMBL3923 adam 0.001 15 0 0 0.8 False True
Number of active compounds :	164
Number of inactive compounds :	164
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL3923_adam_0.001_15_0_0_0.8_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL3923_adam_0.001_15_0.8/
---------------------------------
Training samples: 204
Validation samples: 64
--
Training Step: 1  | time: 51.268s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/204
[A[ATraining Step: 2  | total loss: [1m[32m0.64414[0m[0m | time: 64.607s
[2K
| Adam | epoch: 001 | loss: 0.64414 - acc: 0.3375 -- iter: 064/204
[A[ATraining Step: 3  | total loss: [1m[32m0.60818[0m[0m | time: 77.730s
[2K
| Adam | epoch: 001 | loss: 0.60818 - acc: 0.6494 -- iter: 096/204
[A[ATraining Step: 4  | total loss: [1m[32m0.50256[0m[0m | time: 97.975s
[2K
| Adam | epoch: 001 | loss: 0.50256 - acc: 0.7249 -- iter: 128/204
[A[ATraining Step: 5  | total loss: [1m[32m0.55491[0m[0m | time: 123.432s
[2K
| Adam | epoch: 001 | loss: 0.55491 - acc: 0.7206 -- iter: 160/204
[A[ATraining Step: 6  | total loss: [1m[32m0.37087[0m[0m | time: 144.066s
[2K
| Adam | epoch: 001 | loss: 0.37087 - acc: 0.7998 -- iter: 192/204
[A[ATraining Step: 7  | total loss: [1m[32m0.30600[0m[0m | time: 174.841s
[2K
| Adam | epoch: 001 | loss: 0.30600 - acc: 0.8449 | val_loss: 0.78428 - val_acc: 0.5469 -- iter: 204/204
--
Training Step: 8  | total loss: [1m[32m0.75959[0m[0m | time: 6.481s
[2K
| Adam | epoch: 002 | loss: 0.75959 - acc: 0.7446 -- iter: 032/204
[A[ATraining Step: 9  | total loss: [1m[32m0.64386[0m[0m | time: 23.275s
[2K
| Adam | epoch: 002 | loss: 0.64386 - acc: 0.7475 -- iter: 064/204
[A[ATraining Step: 10  | total loss: [1m[32m0.52807[0m[0m | time: 90.440s
[2K
| Adam | epoch: 002 | loss: 0.52807 - acc: 0.8112 -- iter: 096/204
[A[ATraining Step: 11  | total loss: [1m[32m0.41170[0m[0m | time: 178.979s
[2K
| Adam | epoch: 002 | loss: 0.41170 - acc: 0.8710 -- iter: 128/204
[A[ATraining Step: 12  | total loss: [1m[32m0.36158[0m[0m | time: 275.412s
[2K
| Adam | epoch: 002 | loss: 0.36158 - acc: 0.8869 -- iter: 160/204
[A[ATraining Step: 13  | total loss: [1m[32m0.26863[0m[0m | time: 305.288s
[2K
| Adam | epoch: 002 | loss: 0.26863 - acc: 0.9220 -- iter: 192/204
[A[ATraining Step: 14  | total loss: [1m[32m0.29864[0m[0m | time: 324.024s
[2K
| Adam | epoch: 002 | loss: 0.29864 - acc: 0.8900 | val_loss: 0.95605 - val_acc: 0.5469 -- iter: 204/204
--
Training Step: 15  | total loss: [1m[32m0.33434[0m[0m | time: 9.612s
[2K
| Adam | epoch: 003 | loss: 0.33434 - acc: 0.8719 -- iter: 032/204
[A[ATraining Step: 16  | total loss: [1m[32m0.65112[0m[0m | time: 18.965s
[2K
| Adam | epoch: 003 | loss: 0.65112 - acc: 0.8887 -- iter: 064/204
[A[ATraining Step: 17  | total loss: [1m[32m0.50384[0m[0m | time: 49.612s
[2K
| Adam | epoch: 003 | loss: 0.50384 - acc: 0.9288 -- iter: 096/204
[A[ATraining Step: 18  | total loss: [1m[32m0.38257[0m[0m | time: 67.308s
[2K
| Adam | epoch: 003 | loss: 0.38257 - acc: 0.9426 -- iter: 128/204
[A[ATraining Step: 19  | total loss: [1m[32m0.31900[0m[0m | time: 81.031s
[2K
| Adam | epoch: 003 | loss: 0.31900 - acc: 0.9409 -- iter: 160/204
[A[ATraining Step: 20  | total loss: [1m[32m0.28109[0m[0m | time: 96.213s
[2K
| Adam | epoch: 003 | loss: 0.28109 - acc: 0.9298 -- iter: 192/204
[A[ATraining Step: 21  | total loss: [1m[32m0.25517[0m[0m | time: 132.398s
[2K
| Adam | epoch: 003 | loss: 0.25517 - acc: 0.9419 | val_loss: 0.68323 - val_acc: 0.5469 -- iter: 204/204
--
Training Step: 22  | total loss: [1m[32m0.24978[0m[0m | time: 15.988s
[2K
| Adam | epoch: 004 | loss: 0.24978 - acc: 0.9218 -- iter: 032/204
[A[ATraining Step: 23  | total loss: [1m[32m0.21856[0m[0m | time: 22.207s
[2K
| Adam | epoch: 004 | loss: 0.21856 - acc: 0.9445 -- iter: 064/204
[A[ATraining Step: 24  | total loss: [1m[32m0.28218[0m[0m | time: 27.960s
[2K
| Adam | epoch: 004 | loss: 0.28218 - acc: 0.9367 -- iter: 096/204
[A[ATraining Step: 25  | total loss: [1m[32m0.21653[0m[0m | time: 46.296s
[2K
| Adam | epoch: 004 | loss: 0.21653 - acc: 0.9539 -- iter: 128/204
[A[ATraining Step: 26  | total loss: [1m[32m0.19668[0m[0m | time: 68.183s
[2K
| Adam | epoch: 004 | loss: 0.19668 - acc: 0.9579 -- iter: 160/204
[A[ATraining Step: 27  | total loss: [1m[32m0.17503[0m[0m | time: 90.229s
[2K
| Adam | epoch: 004 | loss: 0.17503 - acc: 0.9526 -- iter: 192/204
[A[ATraining Step: 28  | total loss: [1m[32m0.14556[0m[0m | time: 116.603s
[2K
| Adam | epoch: 004 | loss: 0.14556 - acc: 0.9645 | val_loss: 0.69045 - val_acc: 0.4688 -- iter: 204/204
--
Training Step: 29  | total loss: [1m[32m0.12126[0m[0m | time: 23.269s
[2K
| Adam | epoch: 005 | loss: 0.12126 - acc: 0.9731 -- iter: 032/204
[A[ATraining Step: 30  | total loss: [1m[32m0.12775[0m[0m | time: 42.279s
[2K
| Adam | epoch: 005 | loss: 0.12775 - acc: 0.9647 -- iter: 064/204
[A[ATraining Step: 31  | total loss: [1m[32m0.15074[0m[0m | time: 51.387s
[2K
| Adam | epoch: 005 | loss: 0.15074 - acc: 0.9512 -- iter: 096/204
[A[ATraining Step: 32  | total loss: [1m[32m0.12147[0m[0m | time: 63.614s
[2K
| Adam | epoch: 005 | loss: 0.12147 - acc: 0.9622 -- iter: 128/204
[A[ATraining Step: 33  | total loss: [1m[32m0.34090[0m[0m | time: 81.069s
[2K
| Adam | epoch: 005 | loss: 0.34090 - acc: 0.9339 -- iter: 160/204
[A[ATraining Step: 34  | total loss: [1m[32m0.27619[0m[0m | time: 96.000s
[2K
| Adam | epoch: 005 | loss: 0.27619 - acc: 0.9481 -- iter: 192/204
[A[ATraining Step: 35  | total loss: [1m[32m0.25262[0m[0m | time: 120.345s
[2K
| Adam | epoch: 005 | loss: 0.25262 - acc: 0.9328 | val_loss: 0.70213 - val_acc: 0.6562 -- iter: 204/204
--
Training Step: 36  | total loss: [1m[32m0.21580[0m[0m | time: 83.125s
[2K
| Adam | epoch: 006 | loss: 0.21580 - acc: 0.9401 -- iter: 032/204
[A[ATraining Step: 37  | total loss: [1m[32m0.18155[0m[0m | time: 97.744s
[2K
| Adam | epoch: 006 | loss: 0.18155 - acc: 0.9521 -- iter: 064/204
[A[ATraining Step: 38  | total loss: [1m[32m0.15834[0m[0m | time: 110.971s
[2K
| Adam | epoch: 006 | loss: 0.15834 - acc: 0.9615 -- iter: 096/204
[A[ATraining Step: 39  | total loss: [1m[32m0.13838[0m[0m | time: 118.374s
[2K
| Adam | epoch: 006 | loss: 0.13838 - acc: 0.9689 -- iter: 128/204
[A[ATraining Step: 40  | total loss: [1m[32m0.11796[0m[0m | time: 127.845s
[2K
| Adam | epoch: 006 | loss: 0.11796 - acc: 0.9747 -- iter: 160/204
[A[ATraining Step: 41  | total loss: [1m[32m0.10288[0m[0m | time: 149.209s
[2K
| Adam | epoch: 006 | loss: 0.10288 - acc: 0.9793 -- iter: 192/204
[A[ATraining Step: 42  | total loss: [1m[32m0.09876[0m[0m | time: 175.163s
[2K
| Adam | epoch: 006 | loss: 0.09876 - acc: 0.9774 | val_loss: 1.34619 - val_acc: 0.2812 -- iter: 204/204
--
Training Step: 43  | total loss: [1m[32m0.08475[0m[0m | time: 13.644s
[2K
| Adam | epoch: 007 | loss: 0.08475 - acc: 0.9814 -- iter: 032/204
[A[ATraining Step: 44  | total loss: [1m[32m0.07393[0m[0m | time: 29.262s
[2K
| Adam | epoch: 007 | loss: 0.07393 - acc: 0.9846 -- iter: 064/204
[A[ATraining Step: 45  | total loss: [1m[32m0.07838[0m[0m | time: 74.569s
[2K
| Adam | epoch: 007 | loss: 0.07838 - acc: 0.9766 -- iter: 096/204
[A[ATraining Step: 46  | total loss: [1m[32m0.07992[0m[0m | time: 94.857s
[2K
| Adam | epoch: 007 | loss: 0.07992 - acc: 0.9701 -- iter: 128/204
[A[ATraining Step: 47  | total loss: [1m[32m0.06875[0m[0m | time: 103.344s
[2K
| Adam | epoch: 007 | loss: 0.06875 - acc: 0.9750 -- iter: 160/204
[A[ATraining Step: 48  | total loss: [1m[32m0.22177[0m[0m | time: 112.300s
[2K
| Adam | epoch: 007 | loss: 0.22177 - acc: 0.9522 -- iter: 192/204
[A[ATraining Step: 49  | total loss: [1m[32m0.19061[0m[0m | time: 136.309s
[2K
| Adam | epoch: 007 | loss: 0.19061 - acc: 0.9598 | val_loss: 1.51024 - val_acc: 0.6094 -- iter: 204/204
--
Training Step: 50  | total loss: [1m[32m0.17191[0m[0m | time: 17.650s
[2K
| Adam | epoch: 008 | loss: 0.17191 - acc: 0.9612 -- iter: 032/204
[A[ATraining Step: 51  | total loss: [1m[32m0.16189[0m[0m | time: 43.635s
[2K
| Adam | epoch: 008 | loss: 0.16189 - acc: 0.9576 -- iter: 064/204
[A[ATraining Step: 52  | total loss: [1m[32m0.14088[0m[0m | time: 69.742s
[2K
| Adam | epoch: 008 | loss: 0.14088 - acc: 0.9639 -- iter: 096/204
[A[ATraining Step: 53  | total loss: [1m[32m0.12444[0m[0m | time: 88.502s
[2K
| Adam | epoch: 008 | loss: 0.12444 - acc: 0.9692 -- iter: 128/204
[A[ATraining Step: 54  | total loss: [1m[32m0.11350[0m[0m | time: 101.786s
[2K
| Adam | epoch: 008 | loss: 0.11350 - acc: 0.9737 -- iter: 160/204
[A[ATraining Step: 55  | total loss: [1m[32m0.12033[0m[0m | time: 108.029s
[2K
| Adam | epoch: 008 | loss: 0.12033 - acc: 0.9641 -- iter: 192/204
[A[ATraining Step: 56  | total loss: [1m[32m0.11215[0m[0m | time: 119.497s
[2K
| Adam | epoch: 008 | loss: 0.11215 - acc: 0.9691 | val_loss: 0.53355 - val_acc: 0.8594 -- iter: 204/204
--
Training Step: 57  | total loss: [1m[32m0.09784[0m[0m | time: 33.172s
[2K
| Adam | epoch: 009 | loss: 0.09784 - acc: 0.9734 -- iter: 032/204
[A[ATraining Step: 58  | total loss: [1m[32m0.08706[0m[0m | time: 47.326s
[2K
| Adam | epoch: 009 | loss: 0.08706 - acc: 0.9770 -- iter: 064/204
[A[ATraining Step: 59  | total loss: [1m[32m0.08033[0m[0m | time: 60.402s
[2K
| Adam | epoch: 009 | loss: 0.08033 - acc: 0.9801 -- iter: 096/204
[A[ATraining Step: 60  | total loss: [1m[32m0.07759[0m[0m | time: 103.855s
[2K
| Adam | epoch: 009 | loss: 0.07759 - acc: 0.9786 -- iter: 128/204
[A[ATraining Step: 61  | total loss: [1m[32m0.07521[0m[0m | time: 146.473s
[2K
| Adam | epoch: 009 | loss: 0.07521 - acc: 0.9773 -- iter: 160/204
[A[ATraining Step: 62  | total loss: [1m[32m0.06754[0m[0m | time: 186.408s
[2K
| Adam | epoch: 009 | loss: 0.06754 - acc: 0.9802 -- iter: 192/204
[A[ATraining Step: 63  | total loss: [1m[32m0.06075[0m[0m | time: 201.555s
[2K
| Adam | epoch: 009 | loss: 0.06075 - acc: 0.9827 | val_loss: 3.01120 - val_acc: 0.5312 -- iter: 204/204
--
Training Step: 64  | total loss: [1m[32m0.15904[0m[0m | time: 6.282s
[2K
| Adam | epoch: 010 | loss: 0.15904 - acc: 0.9537 -- iter: 032/204
[A[ATraining Step: 65  | total loss: [1m[32m0.16088[0m[0m | time: 19.438s
[2K
| Adam | epoch: 010 | loss: 0.16088 - acc: 0.9388 -- iter: 064/204
[A[ATraining Step: 66  | total loss: [1m[32m0.14650[0m[0m | time: 90.258s
[2K
| Adam | epoch: 010 | loss: 0.14650 - acc: 0.9463 -- iter: 096/204
[A[ATraining Step: 67  | total loss: [1m[32m0.15786[0m[0m | time: 131.148s
[2K
| Adam | epoch: 010 | loss: 0.15786 - acc: 0.9452 -- iter: 128/204
[A[ATraining Step: 68  | total loss: [1m[32m0.15462[0m[0m | time: 154.103s
[2K
| Adam | epoch: 010 | loss: 0.15462 - acc: 0.9443 -- iter: 160/204
[A[ATraining Step: 69  | total loss: [1m[32m0.15820[0m[0m | time: 188.911s
[2K
| Adam | epoch: 010 | loss: 0.15820 - acc: 0.9435 -- iter: 192/204
[A[ATraining Step: 70  | total loss: [1m[32m0.15690[0m[0m | time: 209.872s
[2K
| Adam | epoch: 010 | loss: 0.15690 - acc: 0.9464 | val_loss: 0.42077 - val_acc: 0.8438 -- iter: 204/204
--
Training Step: 71  | total loss: [1m[32m0.14153[0m[0m | time: 8.546s
[2K
| Adam | epoch: 011 | loss: 0.14153 - acc: 0.9525 -- iter: 032/204
[A[ATraining Step: 72  | total loss: [1m[32m0.26540[0m[0m | time: 18.114s
[2K
| Adam | epoch: 011 | loss: 0.26540 - acc: 0.9485 -- iter: 064/204
[A[ATraining Step: 73  | total loss: [1m[32m0.24062[0m[0m | time: 36.662s
[2K
| Adam | epoch: 011 | loss: 0.24062 - acc: 0.9542 -- iter: 096/204
[A[ATraining Step: 74  | total loss: [1m[32m0.22662[0m[0m | time: 55.112s
[2K
| Adam | epoch: 011 | loss: 0.22662 - acc: 0.9524 -- iter: 128/204
[A[ATraining Step: 75  | total loss: [1m[32m0.21145[0m[0m | time: 74.005s
[2K
| Adam | epoch: 011 | loss: 0.21145 - acc: 0.9542 -- iter: 160/204
[A[ATraining Step: 76  | total loss: [1m[32m0.19469[0m[0m | time: 87.236s
[2K
| Adam | epoch: 011 | loss: 0.19469 - acc: 0.9557 -- iter: 192/204
[A[ATraining Step: 77  | total loss: [1m[32m0.17688[0m[0m | time: 107.549s
[2K
| Adam | epoch: 011 | loss: 0.17688 - acc: 0.9604 | val_loss: 0.31402 - val_acc: 0.8750 -- iter: 204/204
--
Training Step: 78  | total loss: [1m[32m0.16241[0m[0m | time: 17.822s
[2K
| Adam | epoch: 012 | loss: 0.16241 - acc: 0.9645 -- iter: 032/204
[A[ATraining Step: 79  | total loss: [1m[32m0.15020[0m[0m | time: 24.017s
[2K
| Adam | epoch: 012 | loss: 0.15020 - acc: 0.9682 -- iter: 064/204
[A[ATraining Step: 80  | total loss: [1m[32m0.13743[0m[0m | time: 29.760s
[2K
| Adam | epoch: 012 | loss: 0.13743 - acc: 0.9715 -- iter: 096/204
[A[ATraining Step: 81  | total loss: [1m[32m0.12590[0m[0m | time: 43.148s
[2K
| Adam | epoch: 012 | loss: 0.12590 - acc: 0.9744 -- iter: 128/204
[A[ATraining Step: 82  | total loss: [1m[32m0.11749[0m[0m | time: 59.739s
[2K
| Adam | epoch: 012 | loss: 0.11749 - acc: 0.9769 -- iter: 160/204
[A[ATraining Step: 83  | total loss: [1m[32m0.10899[0m[0m | time: 72.565s
[2K
| Adam | epoch: 012 | loss: 0.10899 - acc: 0.9792 -- iter: 192/204
[A[ATraining Step: 84  | total loss: [1m[32m0.10032[0m[0m | time: 89.694s
[2K
| Adam | epoch: 012 | loss: 0.10032 - acc: 0.9813 | val_loss: 0.51262 - val_acc: 0.8906 -- iter: 204/204
--
Training Step: 85  | total loss: [1m[32m0.09469[0m[0m | time: 12.583s
[2K
| Adam | epoch: 013 | loss: 0.09469 - acc: 0.9800 -- iter: 032/204
[A[ATraining Step: 86  | total loss: [1m[32m0.08608[0m[0m | time: 25.264s
[2K
| Adam | epoch: 013 | loss: 0.08608 - acc: 0.9820 -- iter: 064/204
[A[ATraining Step: 87  | total loss: [1m[32m0.08156[0m[0m | time: 30.900s
[2K
| Adam | epoch: 013 | loss: 0.08156 - acc: 0.9838 -- iter: 096/204
[A[ATraining Step: 88  | total loss: [1m[32m0.07447[0m[0m | time: 36.970s
[2K
| Adam | epoch: 013 | loss: 0.07447 - acc: 0.9855 -- iter: 128/204
[A[ATraining Step: 89  | total loss: [1m[32m0.06808[0m[0m | time: 49.036s
[2K
| Adam | epoch: 013 | loss: 0.06808 - acc: 0.9869 -- iter: 160/204
[A[ATraining Step: 90  | total loss: [1m[32m0.06239[0m[0m | time: 61.332s
[2K
| Adam | epoch: 013 | loss: 0.06239 - acc: 0.9882 -- iter: 192/204
[A[ATraining Step: 91  | total loss: [1m[32m0.05656[0m[0m | time: 78.442s
[2K
| Adam | epoch: 013 | loss: 0.05656 - acc: 0.9894 | val_loss: 0.47048 - val_acc: 0.8906 -- iter: 204/204
--
Training Step: 92  | total loss: [1m[32m0.05130[0m[0m | time: 12.897s
[2K
| Adam | epoch: 014 | loss: 0.05130 - acc: 0.9905 -- iter: 032/204
[A[ATraining Step: 93  | total loss: [1m[32m0.04665[0m[0m | time: 25.707s
[2K
| Adam | epoch: 014 | loss: 0.04665 - acc: 0.9914 -- iter: 064/204
[A[ATraining Step: 94  | total loss: [1m[32m0.04246[0m[0m | time: 38.617s
[2K
| Adam | epoch: 014 | loss: 0.04246 - acc: 0.9923 -- iter: 096/204
[A[ATraining Step: 95  | total loss: [1m[32m0.03941[0m[0m | time: 44.237s
[2K
| Adam | epoch: 014 | loss: 0.03941 - acc: 0.9930 -- iter: 128/204
[A[ATraining Step: 96  | total loss: [1m[32m0.10278[0m[0m | time: 49.945s
[2K
| Adam | epoch: 014 | loss: 0.10278 - acc: 0.9854 -- iter: 160/204
[A[ATraining Step: 97  | total loss: [1m[32m0.09385[0m[0m | time: 62.796s
[2K
| Adam | epoch: 014 | loss: 0.09385 - acc: 0.9869 -- iter: 192/204
[A[ATraining Step: 98  | total loss: [1m[32m0.08501[0m[0m | time: 80.464s
[2K
| Adam | epoch: 014 | loss: 0.08501 - acc: 0.9882 | val_loss: 1.09478 - val_acc: 0.7344 -- iter: 204/204
--
Training Step: 99  | total loss: [1m[32m0.07824[0m[0m | time: 13.661s
[2K
| Adam | epoch: 015 | loss: 0.07824 - acc: 0.9894 -- iter: 032/204
[A[ATraining Step: 100  | total loss: [1m[32m0.07224[0m[0m | time: 27.595s
[2K
| Adam | epoch: 015 | loss: 0.07224 - acc: 0.9904 -- iter: 064/204
[A[ATraining Step: 101  | total loss: [1m[32m0.06729[0m[0m | time: 41.479s
[2K
| Adam | epoch: 015 | loss: 0.06729 - acc: 0.9914 -- iter: 096/204
[A[ATraining Step: 102  | total loss: [1m[32m0.06300[0m[0m | time: 55.556s
[2K
| Adam | epoch: 015 | loss: 0.06300 - acc: 0.9922 -- iter: 128/204
[A[ATraining Step: 103  | total loss: [1m[32m0.06199[0m[0m | time: 62.083s
[2K
| Adam | epoch: 015 | loss: 0.06199 - acc: 0.9899 -- iter: 160/204
[A[ATraining Step: 104  | total loss: [1m[32m0.11025[0m[0m | time: 68.889s
[2K
| Adam | epoch: 015 | loss: 0.11025 - acc: 0.9826 -- iter: 192/204
[A[ATraining Step: 105  | total loss: [1m[32m0.09957[0m[0m | time: 87.614s
[2K
| Adam | epoch: 015 | loss: 0.09957 - acc: 0.9843 | val_loss: 0.50957 - val_acc: 0.8750 -- iter: 204/204
--
Validation AUC:0.9793103448275863
Validation AUPRC:0.9785474822112753
Test AUC:0.9726295210166178
Test AUPRC:0.9751287670913279
BestTestF1Score	0.92	0.85	0.92	0.97	0.88	29	1	30	4	1.0
BestTestMCCScore	0.92	0.85	0.92	0.97	0.88	29	1	30	4	1.0
BestTestAccuracyScore	0.92	0.85	0.92	0.97	0.88	29	1	30	4	1.0
BestValidationF1Score	0.93	0.88	0.94	1.0	0.86	25	0	35	4	1.0
BestValidationMCC	0.93	0.88	0.94	1.0	0.86	25	0	35	4	1.0
BestValidationAccuracy	0.93	0.88	0.94	1.0	0.86	25	0	35	4	1.0
TestPredictions (Threshold:1.0)
CHEMBL295234,TP,ACT,1.0	CHEMBL277570,TP,ACT,1.0	CHEMBL14358,TP,ACT,1.0	CHEMBL276868,TP,ACT,1.0	CHEMBL14140,TP,ACT,1.0	CHEMBL302376,TN,INACT,0.05000000074505806	CHEMBL267801,TP,ACT,1.0	CHEMBL52011,TN,INACT,0.019999999552965164	CHEMBL210829,TN,INACT,0.0	CHEMBL1824685,TN,INACT,0.029999999329447746	CHEMBL468161,FN,ACT,0.5600000023841858	CHEMBL266861,TP,ACT,1.0	CHEMBL3753533,TN,INACT,0.009999999776482582	CHEMBL14168,TP,ACT,1.0	CHEMBL1641684,TN,INACT,0.9700000286102295	CHEMBL1726592,TN,INACT,0.8799999952316284	CHEMBL2179395,TP,ACT,1.0	CHEMBL275777,TP,ACT,1.0	CHEMBL268945,TP,ACT,1.0	CHEMBL13955,TP,ACT,1.0	CHEMBL289480,TP,ACT,1.0	CHEMBL13620,FN,ACT,0.9399999976158142	CHEMBL10316,TP,ACT,1.0	CHEMBL497458,FP,INACT,1.0	CHEMBL513060,TN,INACT,0.0	CHEMBL14016,TP,ACT,1.0	CHEMBL3116319,TN,INACT,0.019999999552965164	CHEMBL404505,FN,ACT,0.9599999785423279	CHEMBL1824700,TN,INACT,0.019999999552965164	CHEMBL13918,TP,ACT,1.0	CHEMBL1079,TP,ACT,1.0	CHEMBL1910033,TN,INACT,0.0	CHEMBL489,TN,INACT,0.0	CHEMBL502001,TN,INACT,0.0	CHEMBL275970,TP,ACT,1.0	CHEMBL280022,TP,ACT,1.0	CHEMBL13689,TP,ACT,1.0	CHEMBL3753265,TN,INACT,0.009999999776482582	CHEMBL611107,TN,INACT,0.0	CHEMBL13343,TP,ACT,1.0	CHEMBL261619,TN,INACT,0.029999999329447746	CHEMBL1269668,TN,INACT,0.9100000262260437	CHEMBL1824680,TN,INACT,0.019999999552965164	CHEMBL13886,TP,ACT,1.0	CHEMBL862,TP,ACT,1.0	CHEMBL14410,TP,ACT,1.0	CHEMBL13879,TP,ACT,1.0	CHEMBL3680809,TN,INACT,0.0	CHEMBL3343699,TN,INACT,0.9900000095367432	CHEMBL3601059,TN,INACT,0.8700000047683716	CHEMBL266863,TP,ACT,1.0	CHEMBL266155,TP,ACT,1.0	CHEMBL1672008,TN,INACT,0.10999999940395355	CHEMBL1350432,TN,INACT,0.9900000095367432	CHEMBL232751,TN,INACT,0.009999999776482582	CHEMBL1928333,TN,INACT,0.18000000715255737	CHEMBL3577585,TN,INACT,0.0	CHEMBL1824691,TN,INACT,0.07000000029802322	CHEMBL58343,TN,INACT,0.9900000095367432	CHEMBL3600999,TN,INACT,0.3499999940395355	CHEMBL146997,FN,ACT,0.9900000095367432	CHEMBL93845,TP,ACT,1.0	CHEMBL2029101,TN,INACT,0.0	CHEMBL14448,TP,ACT,1.0	

