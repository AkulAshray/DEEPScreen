ImageNetInceptionV2 CHEMBL4860 RMSprop 0.0005 5 0 0 0.6 False True
Number of active compounds :	462
Number of inactive compounds :	308
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL4860_RMSprop_0.0005_5_0_0_0.6_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL4860_RMSprop_0.0005_5_0.6/
---------------------------------
Training samples: 492
Validation samples: 154
--
Training Step: 1  | time: 36.736s
[2K
| RMSProp | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/492
[A[ATraining Step: 2  | total loss: [1m[32m0.67408[0m[0m | time: 44.670s
[2K
| RMSProp | epoch: 001 | loss: 0.67408 - acc: 0.4500 -- iter: 064/492
[A[ATraining Step: 3  | total loss: [1m[32m0.71936[0m[0m | time: 52.639s
[2K
| RMSProp | epoch: 001 | loss: 0.71936 - acc: 0.3886 -- iter: 096/492
[A[ATraining Step: 4  | total loss: [1m[32m0.74418[0m[0m | time: 60.534s
[2K
| RMSProp | epoch: 001 | loss: 0.74418 - acc: 0.4018 -- iter: 128/492
[A[ATraining Step: 5  | total loss: [1m[32m0.73743[0m[0m | time: 68.436s
[2K
| RMSProp | epoch: 001 | loss: 0.73743 - acc: 0.3833 -- iter: 160/492
[A[ATraining Step: 6  | total loss: [1m[32m0.72721[0m[0m | time: 76.284s
[2K
| RMSProp | epoch: 001 | loss: 0.72721 - acc: 0.4583 -- iter: 192/492
[A[ATraining Step: 7  | total loss: [1m[32m0.72735[0m[0m | time: 84.115s
[2K
| RMSProp | epoch: 001 | loss: 0.72735 - acc: 0.3896 -- iter: 224/492
[A[ATraining Step: 8  | total loss: [1m[32m0.73000[0m[0m | time: 91.750s
[2K
| RMSProp | epoch: 001 | loss: 0.73000 - acc: 0.4165 -- iter: 256/492
[A[ATraining Step: 9  | total loss: [1m[32m0.71785[0m[0m | time: 99.438s
[2K
| RMSProp | epoch: 001 | loss: 0.71785 - acc: 0.5104 -- iter: 288/492
[A[ATraining Step: 10  | total loss: [1m[32m0.72273[0m[0m | time: 107.361s
[2K
| RMSProp | epoch: 001 | loss: 0.72273 - acc: 0.4583 -- iter: 320/492
[A[ATraining Step: 11  | total loss: [1m[32m0.68871[0m[0m | time: 115.399s
[2K
| RMSProp | epoch: 001 | loss: 0.68871 - acc: 0.5077 -- iter: 352/492
[A[ATraining Step: 12  | total loss: [1m[32m0.70678[0m[0m | time: 123.177s
[2K
| RMSProp | epoch: 001 | loss: 0.70678 - acc: 0.4761 -- iter: 384/492
[A[ATraining Step: 13  | total loss: [1m[32m0.68976[0m[0m | time: 131.056s
[2K
| RMSProp | epoch: 001 | loss: 0.68976 - acc: 0.5131 -- iter: 416/492
[A[ATraining Step: 14  | total loss: [1m[32m0.68756[0m[0m | time: 138.841s
[2K
| RMSProp | epoch: 001 | loss: 0.68756 - acc: 0.5205 -- iter: 448/492
[A[ATraining Step: 15  | total loss: [1m[32m0.69407[0m[0m | time: 146.694s
[2K
| RMSProp | epoch: 001 | loss: 0.69407 - acc: 0.4880 -- iter: 480/492
[A[ATraining Step: 16  | total loss: [1m[32m0.69269[0m[0m | time: 163.155s
[2K
| RMSProp | epoch: 001 | loss: 0.69269 - acc: 0.5394 | val_loss: 0.67630 - val_acc: 0.6104 -- iter: 492/492
--
Training Step: 17  | total loss: [1m[32m0.71252[0m[0m | time: 3.500s
[2K
| RMSProp | epoch: 002 | loss: 0.71252 - acc: 0.5552 -- iter: 032/492
[A[ATraining Step: 18  | total loss: [1m[32m0.71843[0m[0m | time: 11.542s
[2K
| RMSProp | epoch: 002 | loss: 0.71843 - acc: 0.5361 -- iter: 064/492
[A[ATraining Step: 19  | total loss: [1m[32m0.70240[0m[0m | time: 19.321s
[2K
| RMSProp | epoch: 002 | loss: 0.70240 - acc: 0.5553 -- iter: 096/492
[A[ATraining Step: 20  | total loss: [1m[32m0.68693[0m[0m | time: 27.344s
[2K
| RMSProp | epoch: 002 | loss: 0.68693 - acc: 0.5375 -- iter: 128/492
[A[ATraining Step: 21  | total loss: [1m[32m0.69771[0m[0m | time: 35.058s
[2K
| RMSProp | epoch: 002 | loss: 0.69771 - acc: 0.5259 -- iter: 160/492
[A[ATraining Step: 22  | total loss: [1m[32m0.66850[0m[0m | time: 43.153s
[2K
| RMSProp | epoch: 002 | loss: 0.66850 - acc: 0.5744 -- iter: 192/492
[A[ATraining Step: 23  | total loss: [1m[32m0.68580[0m[0m | time: 51.034s
[2K
| RMSProp | epoch: 002 | loss: 0.68580 - acc: 0.5165 -- iter: 224/492
[A[ATraining Step: 24  | total loss: [1m[32m0.65432[0m[0m | time: 58.883s
[2K
| RMSProp | epoch: 002 | loss: 0.65432 - acc: 0.5734 -- iter: 256/492
[A[ATraining Step: 25  | total loss: [1m[32m0.67767[0m[0m | time: 66.759s
[2K
| RMSProp | epoch: 002 | loss: 0.67767 - acc: 0.5278 -- iter: 288/492
[A[ATraining Step: 26  | total loss: [1m[32m0.67669[0m[0m | time: 74.824s
[2K
| RMSProp | epoch: 002 | loss: 0.67669 - acc: 0.5535 -- iter: 320/492
[A[ATraining Step: 27  | total loss: [1m[32m0.65935[0m[0m | time: 82.758s
[2K
| RMSProp | epoch: 002 | loss: 0.65935 - acc: 0.5880 -- iter: 352/492
[A[ATraining Step: 28  | total loss: [1m[32m0.67467[0m[0m | time: 90.648s
[2K
| RMSProp | epoch: 002 | loss: 0.67467 - acc: 0.5660 -- iter: 384/492
[A[ATraining Step: 29  | total loss: [1m[32m0.67795[0m[0m | time: 98.750s
[2K
| RMSProp | epoch: 002 | loss: 0.67795 - acc: 0.5575 -- iter: 416/492
[A[ATraining Step: 30  | total loss: [1m[32m0.67780[0m[0m | time: 106.806s
[2K
| RMSProp | epoch: 002 | loss: 0.67780 - acc: 0.5883 -- iter: 448/492
[A[ATraining Step: 31  | total loss: [1m[32m0.67082[0m[0m | time: 114.853s
[2K
| RMSProp | epoch: 002 | loss: 0.67082 - acc: 0.5968 -- iter: 480/492
[A[ATraining Step: 32  | total loss: [1m[32m0.66219[0m[0m | time: 129.775s
[2K
| RMSProp | epoch: 002 | loss: 0.66219 - acc: 0.6031 | val_loss: 0.69183 - val_acc: 0.5195 -- iter: 492/492
--
Training Step: 33  | total loss: [1m[32m0.66499[0m[0m | time: 3.587s
[2K
| RMSProp | epoch: 003 | loss: 0.66499 - acc: 0.5942 -- iter: 032/492
[A[ATraining Step: 34  | total loss: [1m[32m0.66131[0m[0m | time: 7.018s
[2K
| RMSProp | epoch: 003 | loss: 0.66131 - acc: 0.6276 -- iter: 064/492
[A[ATraining Step: 35  | total loss: [1m[32m0.66030[0m[0m | time: 15.045s
[2K
| RMSProp | epoch: 003 | loss: 0.66030 - acc: 0.6358 -- iter: 096/492
[A[ATraining Step: 36  | total loss: [1m[32m0.66027[0m[0m | time: 22.905s
[2K
| RMSProp | epoch: 003 | loss: 0.66027 - acc: 0.6272 -- iter: 128/492
[A[ATraining Step: 37  | total loss: [1m[32m0.65364[0m[0m | time: 30.777s
[2K
| RMSProp | epoch: 003 | loss: 0.65364 - acc: 0.6330 -- iter: 160/492
[A[ATraining Step: 38  | total loss: [1m[32m0.63569[0m[0m | time: 38.529s
[2K
| RMSProp | epoch: 003 | loss: 0.63569 - acc: 0.6375 -- iter: 192/492
[A[ATraining Step: 39  | total loss: [1m[32m0.62572[0m[0m | time: 46.380s
[2K
| RMSProp | epoch: 003 | loss: 0.62572 - acc: 0.6351 -- iter: 224/492
[A[ATraining Step: 40  | total loss: [1m[32m0.61620[0m[0m | time: 54.277s
[2K
| RMSProp | epoch: 003 | loss: 0.61620 - acc: 0.6567 -- iter: 256/492
[A[ATraining Step: 41  | total loss: [1m[32m0.59814[0m[0m | time: 62.020s
[2K
| RMSProp | epoch: 003 | loss: 0.59814 - acc: 0.6796 -- iter: 288/492
[A[ATraining Step: 42  | total loss: [1m[32m0.62386[0m[0m | time: 69.902s
[2K
| RMSProp | epoch: 003 | loss: 0.62386 - acc: 0.6585 -- iter: 320/492
[A[ATraining Step: 43  | total loss: [1m[32m0.60904[0m[0m | time: 77.891s
[2K
| RMSProp | epoch: 003 | loss: 0.60904 - acc: 0.6636 -- iter: 352/492
[A[ATraining Step: 44  | total loss: [1m[32m0.62016[0m[0m | time: 85.863s
[2K
| RMSProp | epoch: 003 | loss: 0.62016 - acc: 0.6515 -- iter: 384/492
[A[ATraining Step: 45  | total loss: [1m[32m0.60723[0m[0m | time: 93.547s
[2K
| RMSProp | epoch: 003 | loss: 0.60723 - acc: 0.6735 -- iter: 416/492
[A[ATraining Step: 46  | total loss: [1m[32m0.61413[0m[0m | time: 101.543s
[2K
| RMSProp | epoch: 003 | loss: 0.61413 - acc: 0.6863 -- iter: 448/492
[A[ATraining Step: 47  | total loss: [1m[32m0.60629[0m[0m | time: 109.538s
[2K
| RMSProp | epoch: 003 | loss: 0.60629 - acc: 0.7069 -- iter: 480/492
[A[ATraining Step: 48  | total loss: [1m[32m0.59628[0m[0m | time: 124.315s
[2K
| RMSProp | epoch: 003 | loss: 0.59628 - acc: 0.7189 | val_loss: 0.62390 - val_acc: 0.8377 -- iter: 492/492
--
Training Step: 49  | total loss: [1m[32m0.59137[0m[0m | time: 7.696s
[2K
| RMSProp | epoch: 004 | loss: 0.59137 - acc: 0.7041 -- iter: 032/492
[A[ATraining Step: 50  | total loss: [1m[32m0.59469[0m[0m | time: 11.139s
[2K
| RMSProp | epoch: 004 | loss: 0.59469 - acc: 0.7257 -- iter: 064/492
[A[ATraining Step: 51  | total loss: [1m[32m0.60271[0m[0m | time: 14.663s
[2K
| RMSProp | epoch: 004 | loss: 0.60271 - acc: 0.7421 -- iter: 096/492
[A[ATraining Step: 52  | total loss: [1m[32m0.61117[0m[0m | time: 22.427s
[2K
| RMSProp | epoch: 004 | loss: 0.61117 - acc: 0.7433 -- iter: 128/492
[A[ATraining Step: 53  | total loss: [1m[32m0.59456[0m[0m | time: 30.054s
[2K
| RMSProp | epoch: 004 | loss: 0.59456 - acc: 0.7628 -- iter: 160/492
[A[ATraining Step: 54  | total loss: [1m[32m0.61460[0m[0m | time: 38.046s
[2K
| RMSProp | epoch: 004 | loss: 0.61460 - acc: 0.7291 -- iter: 192/492
[A[ATraining Step: 55  | total loss: [1m[32m0.59324[0m[0m | time: 45.863s
[2K
| RMSProp | epoch: 004 | loss: 0.59324 - acc: 0.7500 -- iter: 224/492
[A[ATraining Step: 56  | total loss: [1m[32m0.58797[0m[0m | time: 53.661s
[2K
| RMSProp | epoch: 004 | loss: 0.58797 - acc: 0.7544 -- iter: 256/492
[A[ATraining Step: 57  | total loss: [1m[32m0.57195[0m[0m | time: 61.485s
[2K
| RMSProp | epoch: 004 | loss: 0.57195 - acc: 0.7624 -- iter: 288/492
[A[ATraining Step: 58  | total loss: [1m[32m0.57046[0m[0m | time: 69.214s
[2K
| RMSProp | epoch: 004 | loss: 0.57046 - acc: 0.7650 -- iter: 320/492
[A[ATraining Step: 59  | total loss: [1m[32m0.55488[0m[0m | time: 76.853s
[2K
| RMSProp | epoch: 004 | loss: 0.55488 - acc: 0.7798 -- iter: 352/492
[A[ATraining Step: 60  | total loss: [1m[32m0.55086[0m[0m | time: 84.702s
[2K
| RMSProp | epoch: 004 | loss: 0.55086 - acc: 0.7882 -- iter: 384/492
[A[ATraining Step: 61  | total loss: [1m[32m0.54151[0m[0m | time: 92.454s
[2K
| RMSProp | epoch: 004 | loss: 0.54151 - acc: 0.7873 -- iter: 416/492
[A[ATraining Step: 62  | total loss: [1m[32m0.52244[0m[0m | time: 99.960s
[2K
| RMSProp | epoch: 004 | loss: 0.52244 - acc: 0.7946 -- iter: 448/492
[A[ATraining Step: 63  | total loss: [1m[32m0.52418[0m[0m | time: 107.798s
[2K
| RMSProp | epoch: 004 | loss: 0.52418 - acc: 0.7889 -- iter: 480/492
[A[ATraining Step: 64  | total loss: [1m[32m0.51348[0m[0m | time: 122.400s
[2K
| RMSProp | epoch: 004 | loss: 0.51348 - acc: 0.7919 | val_loss: 0.70386 - val_acc: 0.6234 -- iter: 492/492
--
Training Step: 65  | total loss: [1m[32m0.49414[0m[0m | time: 8.021s
[2K
| RMSProp | epoch: 005 | loss: 0.49414 - acc: 0.8021 -- iter: 032/492
[A[ATraining Step: 66  | total loss: [1m[32m0.48170[0m[0m | time: 15.576s
[2K
| RMSProp | epoch: 005 | loss: 0.48170 - acc: 0.8034 -- iter: 064/492
[A[ATraining Step: 67  | total loss: [1m[32m0.47137[0m[0m | time: 18.995s
[2K
| RMSProp | epoch: 005 | loss: 0.47137 - acc: 0.8045 -- iter: 096/492
[A[ATraining Step: 68  | total loss: [1m[32m0.52866[0m[0m | time: 22.452s
[2K
| RMSProp | epoch: 005 | loss: 0.52866 - acc: 0.7980 -- iter: 128/492
[A[ATraining Step: 69  | total loss: [1m[32m0.52981[0m[0m | time: 30.243s
[2K
| RMSProp | epoch: 005 | loss: 0.52981 - acc: 0.7924 -- iter: 160/492
[A[ATraining Step: 70  | total loss: [1m[32m0.51802[0m[0m | time: 37.971s
[2K
| RMSProp | epoch: 005 | loss: 0.51802 - acc: 0.7983 -- iter: 192/492
[A[ATraining Step: 71  | total loss: [1m[32m0.49592[0m[0m | time: 45.623s
[2K
| RMSProp | epoch: 005 | loss: 0.49592 - acc: 0.8106 -- iter: 224/492
[A[ATraining Step: 72  | total loss: [1m[32m0.48220[0m[0m | time: 53.338s
[2K
| RMSProp | epoch: 005 | loss: 0.48220 - acc: 0.8179 -- iter: 256/492
[A[ATraining Step: 73  | total loss: [1m[32m0.46400[0m[0m | time: 60.974s
[2K
| RMSProp | epoch: 005 | loss: 0.46400 - acc: 0.8242 -- iter: 288/492
[A[ATraining Step: 74  | total loss: [1m[32m0.46050[0m[0m | time: 68.590s
[2K
| RMSProp | epoch: 005 | loss: 0.46050 - acc: 0.8229 -- iter: 320/492
[A[ATraining Step: 75  | total loss: [1m[32m0.46193[0m[0m | time: 76.342s
[2K
| RMSProp | epoch: 005 | loss: 0.46193 - acc: 0.8252 -- iter: 352/492
[A[ATraining Step: 76  | total loss: [1m[32m0.45559[0m[0m | time: 84.193s
[2K
| RMSProp | epoch: 005 | loss: 0.45559 - acc: 0.8272 -- iter: 384/492
[A[ATraining Step: 77  | total loss: [1m[32m0.44317[0m[0m | time: 92.136s
[2K
| RMSProp | epoch: 005 | loss: 0.44317 - acc: 0.8289 -- iter: 416/492
[A[ATraining Step: 78  | total loss: [1m[32m0.42733[0m[0m | time: 99.901s
[2K
| RMSProp | epoch: 005 | loss: 0.42733 - acc: 0.8338 -- iter: 448/492
[A[ATraining Step: 79  | total loss: [1m[32m0.43983[0m[0m | time: 107.555s
[2K
| RMSProp | epoch: 005 | loss: 0.43983 - acc: 0.8283 -- iter: 480/492
[A[ATraining Step: 80  | total loss: [1m[32m0.43346[0m[0m | time: 122.073s
[2K
| RMSProp | epoch: 005 | loss: 0.43346 - acc: 0.8299 | val_loss: 0.76471 - val_acc: 0.6494 -- iter: 492/492
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.902127659574468
Validation AUPRC:0.9199306071715776
Test AUC:0.9069148936170213
Test AUPRC:0.9328930371758031
BestTestF1Score	0.88	0.71	0.86	0.91	0.85	80	8	52	14	0.93
BestTestMCCScore	0.88	0.71	0.86	0.91	0.85	80	8	52	14	0.93
BestTestAccuracyScore	0.88	0.71	0.86	0.91	0.85	80	8	52	14	0.93
BestValidationF1Score	0.89	0.72	0.86	0.91	0.86	81	8	52	13	0.93
BestValidationMCC	0.89	0.72	0.86	0.91	0.86	81	8	52	13	0.93
BestValidationAccuracy	0.89	0.72	0.86	0.91	0.86	81	8	52	13	0.93
TestPredictions (Threshold:0.93)
CHEMBL269166,FP,INACT,0.949999988079071	CHEMBL3654116,TP,ACT,1.0	CHEMBL3426311,TN,INACT,0.6499999761581421	CHEMBL3654156,TP,ACT,1.0	CHEMBL3654038,TP,ACT,1.0	CHEMBL3126335,TN,INACT,0.5699999928474426	CHEMBL1824824,TN,INACT,0.8399999737739563	CHEMBL2180319,FN,ACT,0.6000000238418579	CHEMBL3654159,TP,ACT,1.0	CHEMBL3780094,TN,INACT,0.6200000047683716	CHEMBL1271992,TN,INACT,0.5899999737739563	CHEMBL2430578,FP,INACT,0.9900000095367432	CHEMBL51529,TN,INACT,0.8700000047683716	CHEMBL1098763,FP,INACT,0.9599999785423279	CHEMBL3654025,TP,ACT,1.0	CHEMBL3654016,TP,ACT,1.0	CHEMBL1094197,TN,INACT,0.8500000238418579	CHEMBL2314513,TN,INACT,0.5299999713897705	CHEMBL2436889,FN,ACT,0.6700000166893005	CHEMBL1269505,TP,ACT,0.9900000095367432	CHEMBL3650618,TP,ACT,1.0	CHEMBL3650566,TP,ACT,1.0	CHEMBL3654140,TP,ACT,1.0	CHEMBL2312476,FN,ACT,0.9200000166893005	CHEMBL379196,TN,INACT,0.6000000238418579	CHEMBL1606857,TN,INACT,0.4099999964237213	CHEMBL1451039,TN,INACT,0.6499999761581421	CHEMBL3701387,TP,ACT,0.9599999785423279	CHEMBL1824190,FN,ACT,0.8199999928474426	CHEMBL2159725,TN,INACT,0.6700000166893005	CHEMBL3814082,TN,INACT,0.3700000047683716	CHEMBL1672070,TP,ACT,0.9300000071525574	CHEMBL3650489,TP,ACT,1.0	CHEMBL3650633,TP,ACT,1.0	CHEMBL510828,TP,ACT,0.9700000286102295	CHEMBL3650640,TP,ACT,1.0	CHEMBL3126327,TN,INACT,0.8700000047683716	CHEMBL448237,TP,ACT,0.9599999785423279	CHEMBL3650514,TP,ACT,1.0	CHEMBL3701296,TP,ACT,0.9900000095367432	CHEMBL3650643,TP,ACT,1.0	CHEMBL2312645,TN,INACT,0.7699999809265137	CHEMBL3780382,TN,INACT,0.5199999809265137	CHEMBL3704778,TP,ACT,0.9900000095367432	CHEMBL3426326,FN,ACT,0.6700000166893005	CHEMBL3704779,TP,ACT,0.9900000095367432	CHEMBL1331211,TN,INACT,0.49000000953674316	CHEMBL3287301,FN,ACT,0.8399999737739563	CHEMBL3654080,TP,ACT,1.0	CHEMBL3654023,TP,ACT,1.0	CHEMBL370837,TP,ACT,0.9700000286102295	CHEMBL1200938,FP,INACT,0.949999988079071	CHEMBL3126314,TN,INACT,0.8100000023841858	CHEMBL3654117,TP,ACT,1.0	CHEMBL3780331,TN,INACT,0.7400000095367432	CHEMBL267488,TN,INACT,0.9100000262260437	CHEMBL513800,TN,INACT,0.44999998807907104	CHEMBL3704803,TP,ACT,0.9800000190734863	CHEMBL3654010,TP,ACT,1.0	CHEMBL3650669,TP,ACT,1.0	CHEMBL3780614,TN,INACT,0.8100000023841858	CHEMBL3650546,TP,ACT,1.0	CHEMBL3701353,TP,ACT,0.9900000095367432	CHEMBL386955,TP,ACT,1.0	CHEMBL514298,TP,ACT,0.9700000286102295	CHEMBL2316901,TN,INACT,0.75	CHEMBL3650647,TP,ACT,1.0	CHEMBL3417405,TN,INACT,0.8600000143051147	CHEMBL2030858,TP,ACT,0.9900000095367432	CHEMBL2030855,TP,ACT,1.0	CHEMBL1098322,FP,INACT,0.9800000190734863	CHEMBL3814238,TN,INACT,0.47999998927116394	CHEMBL498873,TP,ACT,0.9900000095367432	CHEMBL502211,TP,ACT,1.0	CHEMBL2312482,FN,ACT,0.699999988079071	CHEMBL3654036,TP,ACT,1.0	CHEMBL3654059,TP,ACT,1.0	CHEMBL3701378,TP,ACT,0.9900000095367432	CHEMBL2314507,TN,INACT,0.5699999928474426	CHEMBL1910882,TN,INACT,0.550000011920929	CHEMBL3650632,TP,ACT,1.0	CHEMBL3704804,TP,ACT,0.9800000190734863	CHEMBL3260035,TN,INACT,0.75	CHEMBL3654092,TP,ACT,1.0	CHEMBL243203,TN,INACT,0.4300000071525574	CHEMBL1271440,TP,ACT,0.9399999976158142	CHEMBL3342195,TP,ACT,0.9900000095367432	CHEMBL3126337,TN,INACT,0.8999999761581421	CHEMBL2436890,FN,ACT,0.9200000166893005	CHEMBL1940671,TP,ACT,1.0	CHEMBL3426319,FN,ACT,0.75	CHEMBL1459125,TN,INACT,0.7699999809265137	CHEMBL590043,TN,INACT,0.9200000166893005	CHEMBL2170836,TP,ACT,1.0	CHEMBL502066,TP,ACT,1.0	CHEMBL2170843,TP,ACT,1.0	CHEMBL3701362,TP,ACT,0.9900000095367432	CHEMBL3703623,TP,ACT,0.9900000095367432	CHEMBL3653986,TP,ACT,1.0	CHEMBL3650533,TP,ACT,1.0	CHEMBL3329966,TN,INACT,0.5299999713897705	CHEMBL3650581,TP,ACT,1.0	CHEMBL508385,TP,ACT,1.0	CHEMBL2171010,TP,ACT,1.0	CHEMBL3314286,TN,INACT,0.7599999904632568	CHEMBL3701406,TP,ACT,1.0	CHEMBL3342198,TP,ACT,0.9900000095367432	CHEMBL1824807,FP,INACT,0.9599999785423279	CHEMBL3426327,FN,ACT,0.800000011920929	CHEMBL2314520,TN,INACT,0.6000000238418579	CHEMBL3650487,TP,ACT,1.0	CHEMBL3701315,TP,ACT,0.9800000190734863	CHEMBL427653,TP,ACT,1.0	CHEMBL2314371,TN,INACT,0.6000000238418579	CHEMBL214839,TN,INACT,0.7200000286102295	CHEMBL2159733,TN,INACT,0.7599999904632568	CHEMBL214378,TN,INACT,0.6200000047683716	CHEMBL2031032,TP,ACT,1.0	CHEMBL3085438,TN,INACT,0.75	CHEMBL36834,TN,INACT,0.6499999761581421	CHEMBL3650522,TP,ACT,1.0	CHEMBL453466,TP,ACT,0.9800000190734863	CHEMBL3358689,TN,INACT,0.7799999713897705	CHEMBL1689140,TP,ACT,1.0	CHEMBL395653,TN,INACT,0.8299999833106995	CHEMBL573989,TN,INACT,0.5299999713897705	CHEMBL3650621,TP,ACT,1.0	CHEMBL2326746,TP,ACT,1.0	CHEMBL3701364,TP,ACT,0.9900000095367432	CHEMBL439495,TP,ACT,0.9700000286102295	CHEMBL3650505,TP,ACT,1.0	CHEMBL598832,TN,INACT,0.7300000190734863	CHEMBL1813625,TN,INACT,0.6399999856948853	CHEMBL337604,TN,INACT,0.9200000166893005	CHEMBL3629837,FN,ACT,0.8199999928474426	CHEMBL2030854,TP,ACT,1.0	CHEMBL1490685,TN,INACT,0.6800000071525574	CHEMBL3650641,TP,ACT,1.0	CHEMBL1417070,TN,INACT,0.8899999856948853	CHEMBL1271231,TP,ACT,0.9800000190734863	CHEMBL3654037,TP,ACT,1.0	CHEMBL1417701,TN,INACT,0.800000011920929	CHEMBL385101,TP,ACT,0.9900000095367432	CHEMBL2314193,FN,ACT,0.699999988079071	CHEMBL121772,TN,INACT,0.7599999904632568	CHEMBL110458,FP,INACT,1.0	CHEMBL268903,FP,INACT,0.9800000190734863	CHEMBL3701360,TP,ACT,1.0	CHEMBL397906,FN,ACT,0.5299999713897705	CHEMBL2063880,TN,INACT,0.9100000262260437	CHEMBL371861,TP,ACT,0.9900000095367432	CHEMBL2171003,TP,ACT,1.0	CHEMBL2436888,FN,ACT,0.7099999785423279	CHEMBL3701292,TP,ACT,1.0	

