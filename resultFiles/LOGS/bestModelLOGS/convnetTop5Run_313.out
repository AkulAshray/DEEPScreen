ImageNetInceptionV2 CHEMBL1255149 adam 0.001 15 0 0 0.6 False True
Number of active compounds :	165
Number of inactive compounds :	165
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL1255149_adam_0.001_15_0_0_0.6_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL1255149_adam_0.001_15_0.6/
---------------------------------
Training samples: 211
Validation samples: 66
--
Training Step: 1  | time: 338.324s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/211
[A[ATraining Step: 2  | total loss: [1m[32m0.65416[0m[0m | time: 967.265s
[2K
| Adam | epoch: 001 | loss: 0.65416 - acc: 0.4219 -- iter: 064/211
[A[ATraining Step: 3  | total loss: [1m[32m0.49324[0m[0m | time: 1231.081s
[2K
| Adam | epoch: 001 | loss: 0.49324 - acc: 0.7415 -- iter: 096/211
[A[ATraining Step: 4  | total loss: [1m[32m0.38770[0m[0m | time: 1499.867s
[2K
| Adam | epoch: 001 | loss: 0.38770 - acc: 0.8416 -- iter: 128/211
[A[ATraining Step: 5  | total loss: [1m[32m0.80251[0m[0m | time: 1682.988s
[2K
| Adam | epoch: 001 | loss: 0.80251 - acc: 0.7349 -- iter: 160/211
[A[ATraining Step: 6  | total loss: [1m[32m0.70147[0m[0m | time: 1920.822s
[2K
| Adam | epoch: 001 | loss: 0.70147 - acc: 0.6643 -- iter: 192/211
[A[ATraining Step: 7  | total loss: [1m[32m0.43451[0m[0m | time: 1946.338s
[2K
| Adam | epoch: 001 | loss: 0.43451 - acc: 0.8095 | val_loss: 1.19983 - val_acc: 0.5000 -- iter: 211/211
--
Training Step: 8  | total loss: [1m[32m0.39557[0m[0m | time: 23.941s
[2K
| Adam | epoch: 002 | loss: 0.39557 - acc: 0.7982 -- iter: 032/211
[A[ATraining Step: 9  | total loss: [1m[32m0.28280[0m[0m | time: 138.220s
[2K
| Adam | epoch: 002 | loss: 0.28280 - acc: 0.8772 -- iter: 064/211
[A[ATraining Step: 10  | total loss: [1m[32m0.37009[0m[0m | time: 245.312s
[2K
| Adam | epoch: 002 | loss: 0.37009 - acc: 0.8292 -- iter: 096/211
[A[ATraining Step: 11  | total loss: [1m[32m0.46673[0m[0m | time: 393.169s
[2K
| Adam | epoch: 002 | loss: 0.46673 - acc: 0.8509 -- iter: 128/211
[A[ATraining Step: 12  | total loss: [1m[32m0.35985[0m[0m | time: 427.029s
[2K
| Adam | epoch: 002 | loss: 0.35985 - acc: 0.8758 -- iter: 160/211
[A[ATraining Step: 13  | total loss: [1m[32m0.31308[0m[0m | time: 509.616s
[2K
| Adam | epoch: 002 | loss: 0.31308 - acc: 0.9022 -- iter: 192/211
[A[ATraining Step: 14  | total loss: [1m[32m0.27549[0m[0m | time: 592.075s
[2K
| Adam | epoch: 002 | loss: 0.27549 - acc: 0.9039 | val_loss: 0.71374 - val_acc: 0.5000 -- iter: 211/211
--
Training Step: 15  | total loss: [1m[32m0.29861[0m[0m | time: 8.705s
[2K
| Adam | epoch: 003 | loss: 0.29861 - acc: 0.8926 -- iter: 032/211
[A[ATraining Step: 16  | total loss: [1m[32m0.40226[0m[0m | time: 20.428s
[2K
| Adam | epoch: 003 | loss: 0.40226 - acc: 0.8342 -- iter: 064/211
[A[ATraining Step: 17  | total loss: [1m[32m0.31632[0m[0m | time: 90.865s
[2K
| Adam | epoch: 003 | loss: 0.31632 - acc: 0.8749 -- iter: 096/211
[A[ATraining Step: 18  | total loss: [1m[32m0.29189[0m[0m | time: 132.142s
[2K
| Adam | epoch: 003 | loss: 0.29189 - acc: 0.8858 -- iter: 128/211
[A[ATraining Step: 19  | total loss: [1m[32m0.25760[0m[0m | time: 264.097s
[2K
| Adam | epoch: 003 | loss: 0.25760 - acc: 0.9134 -- iter: 160/211
[A[ATraining Step: 20  | total loss: [1m[32m0.20840[0m[0m | time: 292.015s
[2K
| Adam | epoch: 003 | loss: 0.20840 - acc: 0.9312 -- iter: 192/211
[A[ATraining Step: 21  | total loss: [1m[32m0.24180[0m[0m | time: 359.692s
[2K
| Adam | epoch: 003 | loss: 0.24180 - acc: 0.9235 | val_loss: 2.15623 - val_acc: 0.5000 -- iter: 211/211
--
Training Step: 22  | total loss: [1m[32m0.21882[0m[0m | time: 14.457s
[2K
| Adam | epoch: 004 | loss: 0.21882 - acc: 0.9371 -- iter: 032/211
[A[ATraining Step: 23  | total loss: [1m[32m0.17922[0m[0m | time: 26.270s
[2K
| Adam | epoch: 004 | loss: 0.17922 - acc: 0.9553 -- iter: 064/211
[A[ATraining Step: 24  | total loss: [1m[32m0.13955[0m[0m | time: 38.044s
[2K
| Adam | epoch: 004 | loss: 0.13955 - acc: 0.9679 -- iter: 096/211
[A[ATraining Step: 25  | total loss: [1m[32m0.11030[0m[0m | time: 56.791s
[2K
| Adam | epoch: 004 | loss: 0.11030 - acc: 0.9766 -- iter: 128/211
[A[ATraining Step: 26  | total loss: [1m[32m0.09546[0m[0m | time: 133.639s
[2K
| Adam | epoch: 004 | loss: 0.09546 - acc: 0.9828 -- iter: 160/211
[A[ATraining Step: 27  | total loss: [1m[32m0.08413[0m[0m | time: 151.826s
[2K
| Adam | epoch: 004 | loss: 0.08413 - acc: 0.9792 -- iter: 192/211
[A[ATraining Step: 28  | total loss: [1m[32m0.07755[0m[0m | time: 204.914s
[2K
| Adam | epoch: 004 | loss: 0.07755 - acc: 0.9844 | val_loss: 3.94096 - val_acc: 0.5000 -- iter: 211/211
--
Training Step: 29  | total loss: [1m[32m0.06484[0m[0m | time: 10.951s
[2K
| Adam | epoch: 005 | loss: 0.06484 - acc: 0.9882 -- iter: 032/211
[A[ATraining Step: 30  | total loss: [1m[32m0.09005[0m[0m | time: 28.970s
[2K
| Adam | epoch: 005 | loss: 0.09005 - acc: 0.9688 -- iter: 064/211
[A[ATraining Step: 31  | total loss: [1m[32m0.07041[0m[0m | time: 40.521s
[2K
| Adam | epoch: 005 | loss: 0.07041 - acc: 0.9760 -- iter: 096/211
[A[ATraining Step: 32  | total loss: [1m[32m0.36313[0m[0m | time: 52.167s
[2K
| Adam | epoch: 005 | loss: 0.36313 - acc: 0.9459 -- iter: 128/211
[A[ATraining Step: 33  | total loss: [1m[32m0.31142[0m[0m | time: 70.252s
[2K
| Adam | epoch: 005 | loss: 0.31142 - acc: 0.9462 -- iter: 160/211
[A[ATraining Step: 34  | total loss: [1m[32m0.29635[0m[0m | time: 87.218s
[2K
| Adam | epoch: 005 | loss: 0.29635 - acc: 0.9443 -- iter: 192/211
[A[ATraining Step: 35  | total loss: [1m[32m0.24641[0m[0m | time: 112.561s
[2K
| Adam | epoch: 005 | loss: 0.24641 - acc: 0.9494 | val_loss: 6.11260 - val_acc: 0.5000 -- iter: 211/211
--
Training Step: 36  | total loss: [1m[32m0.20359[0m[0m | time: 15.208s
[2K
| Adam | epoch: 006 | loss: 0.20359 - acc: 0.9534 -- iter: 032/211
[A[ATraining Step: 37  | total loss: [1m[32m0.17614[0m[0m | time: 32.368s
[2K
| Adam | epoch: 006 | loss: 0.17614 - acc: 0.9565 -- iter: 064/211
[A[ATraining Step: 38  | total loss: [1m[32m0.14623[0m[0m | time: 49.628s
[2K
| Adam | epoch: 006 | loss: 0.14623 - acc: 0.9650 -- iter: 096/211
[A[ATraining Step: 39  | total loss: [1m[32m0.14049[0m[0m | time: 61.197s
[2K
| Adam | epoch: 006 | loss: 0.14049 - acc: 0.9597 -- iter: 128/211
[A[ATraining Step: 40  | total loss: [1m[32m0.22451[0m[0m | time: 72.392s
[2K
| Adam | epoch: 006 | loss: 0.22451 - acc: 0.9574 -- iter: 160/211
[A[ATraining Step: 41  | total loss: [1m[32m0.19011[0m[0m | time: 89.119s
[2K
| Adam | epoch: 006 | loss: 0.19011 - acc: 0.9652 -- iter: 192/211
[A[ATraining Step: 42  | total loss: [1m[32m0.17158[0m[0m | time: 114.135s
[2K
| Adam | epoch: 006 | loss: 0.17158 - acc: 0.9602 | val_loss: 2.11408 - val_acc: 0.5000 -- iter: 211/211
--
Training Step: 43  | total loss: [1m[32m0.15029[0m[0m | time: 13.636s
[2K
| Adam | epoch: 007 | loss: 0.15029 - acc: 0.9617 -- iter: 032/211
[A[ATraining Step: 44  | total loss: [1m[32m0.13021[0m[0m | time: 30.669s
[2K
| Adam | epoch: 007 | loss: 0.13021 - acc: 0.9684 -- iter: 064/211
[A[ATraining Step: 45  | total loss: [1m[32m0.14530[0m[0m | time: 47.899s
[2K
| Adam | epoch: 007 | loss: 0.14530 - acc: 0.9578 -- iter: 096/211
[A[ATraining Step: 46  | total loss: [1m[32m0.12638[0m[0m | time: 65.487s
[2K
| Adam | epoch: 007 | loss: 0.12638 - acc: 0.9648 -- iter: 128/211
[A[ATraining Step: 47  | total loss: [1m[32m0.11396[0m[0m | time: 76.807s
[2K
| Adam | epoch: 007 | loss: 0.11396 - acc: 0.9706 -- iter: 160/211
[A[ATraining Step: 48  | total loss: [1m[32m0.14065[0m[0m | time: 87.691s
[2K
| Adam | epoch: 007 | loss: 0.14065 - acc: 0.9669 -- iter: 192/211
[A[ATraining Step: 49  | total loss: [1m[32m0.11952[0m[0m | time: 111.771s
[2K
| Adam | epoch: 007 | loss: 0.11952 - acc: 0.9721 | val_loss: 1.81783 - val_acc: 0.5000 -- iter: 211/211
--
Training Step: 50  | total loss: [1m[32m0.11721[0m[0m | time: 12.921s
[2K
| Adam | epoch: 008 | loss: 0.11721 - acc: 0.9667 -- iter: 032/211
[A[ATraining Step: 51  | total loss: [1m[32m0.10277[0m[0m | time: 26.823s
[2K
| Adam | epoch: 008 | loss: 0.10277 - acc: 0.9718 -- iter: 064/211
[A[ATraining Step: 52  | total loss: [1m[32m0.12237[0m[0m | time: 44.556s
[2K
| Adam | epoch: 008 | loss: 0.12237 - acc: 0.9713 -- iter: 096/211
[A[ATraining Step: 53  | total loss: [1m[32m0.14621[0m[0m | time: 61.692s
[2K
| Adam | epoch: 008 | loss: 0.14621 - acc: 0.9617 -- iter: 128/211
[A[ATraining Step: 54  | total loss: [1m[32m0.12929[0m[0m | time: 78.980s
[2K
| Adam | epoch: 008 | loss: 0.12929 - acc: 0.9673 -- iter: 160/211
[A[ATraining Step: 55  | total loss: [1m[32m0.11707[0m[0m | time: 89.904s
[2K
| Adam | epoch: 008 | loss: 0.11707 - acc: 0.9720 -- iter: 192/211
[A[ATraining Step: 56  | total loss: [1m[32m0.13450[0m[0m | time: 108.601s
[2K
| Adam | epoch: 008 | loss: 0.13450 - acc: 0.9685 | val_loss: 0.47561 - val_acc: 0.8636 -- iter: 211/211
--
Training Step: 57  | total loss: [1m[32m0.11748[0m[0m | time: 12.331s
[2K
| Adam | epoch: 009 | loss: 0.11748 - acc: 0.9729 -- iter: 032/211
[A[ATraining Step: 58  | total loss: [1m[32m0.11978[0m[0m | time: 26.199s
[2K
| Adam | epoch: 009 | loss: 0.11978 - acc: 0.9680 -- iter: 064/211
[A[ATraining Step: 59  | total loss: [1m[32m0.11573[0m[0m | time: 43.473s
[2K
| Adam | epoch: 009 | loss: 0.11573 - acc: 0.9681 -- iter: 096/211
[A[ATraining Step: 60  | total loss: [1m[32m0.11466[0m[0m | time: 60.147s
[2K
| Adam | epoch: 009 | loss: 0.11466 - acc: 0.9682 -- iter: 128/211
[A[ATraining Step: 61  | total loss: [1m[32m0.11073[0m[0m | time: 77.108s
[2K
| Adam | epoch: 009 | loss: 0.11073 - acc: 0.9683 -- iter: 160/211
[A[ATraining Step: 62  | total loss: [1m[32m0.10133[0m[0m | time: 94.900s
[2K
| Adam | epoch: 009 | loss: 0.10133 - acc: 0.9724 -- iter: 192/211
[A[ATraining Step: 63  | total loss: [1m[32m0.11046[0m[0m | time: 113.434s
[2K
| Adam | epoch: 009 | loss: 0.11046 - acc: 0.9679 | val_loss: 0.40206 - val_acc: 0.8485 -- iter: 211/211
--
Training Step: 64  | total loss: [1m[32m0.47669[0m[0m | time: 8.992s
[2K
| Adam | epoch: 010 | loss: 0.47669 - acc: 0.8996 -- iter: 032/211
[A[ATraining Step: 65  | total loss: [1m[32m0.42298[0m[0m | time: 21.304s
[2K
| Adam | epoch: 010 | loss: 0.42298 - acc: 0.9120 -- iter: 064/211
[A[ATraining Step: 66  | total loss: [1m[32m0.38127[0m[0m | time: 34.064s
[2K
| Adam | epoch: 010 | loss: 0.38127 - acc: 0.9227 -- iter: 096/211
[A[ATraining Step: 67  | total loss: [1m[32m0.34598[0m[0m | time: 48.329s
[2K
| Adam | epoch: 010 | loss: 0.34598 - acc: 0.9320 -- iter: 128/211
[A[ATraining Step: 68  | total loss: [1m[32m0.31247[0m[0m | time: 66.323s
[2K
| Adam | epoch: 010 | loss: 0.31247 - acc: 0.9400 -- iter: 160/211
[A[ATraining Step: 69  | total loss: [1m[32m0.28382[0m[0m | time: 85.508s
[2K
| Adam | epoch: 010 | loss: 0.28382 - acc: 0.9470 -- iter: 192/211
[A[ATraining Step: 70  | total loss: [1m[32m0.27144[0m[0m | time: 111.901s
[2K
| Adam | epoch: 010 | loss: 0.27144 - acc: 0.9495 | val_loss: 0.48040 - val_acc: 0.8636 -- iter: 211/211
--
Training Step: 71  | total loss: [1m[32m0.24996[0m[0m | time: 8.341s
[2K
| Adam | epoch: 011 | loss: 0.24996 - acc: 0.9517 -- iter: 032/211
[A[ATraining Step: 72  | total loss: [1m[32m0.24718[0m[0m | time: 17.066s
[2K
| Adam | epoch: 011 | loss: 0.24718 - acc: 0.9512 -- iter: 064/211
[A[ATraining Step: 73  | total loss: [1m[32m0.22189[0m[0m | time: 26.377s
[2K
| Adam | epoch: 011 | loss: 0.22189 - acc: 0.9566 -- iter: 096/211
[A[ATraining Step: 74  | total loss: [1m[32m0.20256[0m[0m | time: 35.993s
[2K
| Adam | epoch: 011 | loss: 0.20256 - acc: 0.9614 -- iter: 128/211
[A[ATraining Step: 75  | total loss: [1m[32m0.18529[0m[0m | time: 51.083s
[2K
| Adam | epoch: 011 | loss: 0.18529 - acc: 0.9656 -- iter: 160/211
[A[ATraining Step: 76  | total loss: [1m[32m0.17369[0m[0m | time: 68.681s
[2K
| Adam | epoch: 011 | loss: 0.17369 - acc: 0.9693 -- iter: 192/211
[A[ATraining Step: 77  | total loss: [1m[32m0.16693[0m[0m | time: 93.494s
[2K
| Adam | epoch: 011 | loss: 0.16693 - acc: 0.9692 | val_loss: 0.40566 - val_acc: 0.8333 -- iter: 211/211
--
Training Step: 78  | total loss: [1m[32m0.15336[0m[0m | time: 19.016s
[2K
| Adam | epoch: 012 | loss: 0.15336 - acc: 0.9724 -- iter: 032/211
[A[ATraining Step: 79  | total loss: [1m[32m0.14644[0m[0m | time: 30.281s
[2K
| Adam | epoch: 012 | loss: 0.14644 - acc: 0.9721 -- iter: 064/211
[A[ATraining Step: 80  | total loss: [1m[32m0.29175[0m[0m | time: 40.981s
[2K
| Adam | epoch: 012 | loss: 0.29175 - acc: 0.9426 -- iter: 096/211
[A[ATraining Step: 81  | total loss: [1m[32m0.26426[0m[0m | time: 53.951s
[2K
| Adam | epoch: 012 | loss: 0.26426 - acc: 0.9484 -- iter: 128/211
[A[ATraining Step: 82  | total loss: [1m[32m0.24284[0m[0m | time: 66.936s
[2K
| Adam | epoch: 012 | loss: 0.24284 - acc: 0.9536 -- iter: 160/211
[A[ATraining Step: 83  | total loss: [1m[32m0.22318[0m[0m | time: 82.423s
[2K
| Adam | epoch: 012 | loss: 0.22318 - acc: 0.9582 -- iter: 192/211
[A[ATraining Step: 84  | total loss: [1m[32m0.20642[0m[0m | time: 106.708s
[2K
| Adam | epoch: 012 | loss: 0.20642 - acc: 0.9624 | val_loss: 1.23105 - val_acc: 0.5303 -- iter: 211/211
--
Training Step: 85  | total loss: [1m[32m0.19226[0m[0m | time: 17.202s
[2K
| Adam | epoch: 013 | loss: 0.19226 - acc: 0.9662 -- iter: 032/211
[A[ATraining Step: 86  | total loss: [1m[32m0.18072[0m[0m | time: 34.697s
[2K
| Adam | epoch: 013 | loss: 0.18072 - acc: 0.9695 -- iter: 064/211
[A[ATraining Step: 87  | total loss: [1m[32m0.16961[0m[0m | time: 46.772s
[2K
| Adam | epoch: 013 | loss: 0.16961 - acc: 0.9726 -- iter: 096/211
[A[ATraining Step: 88  | total loss: [1m[32m0.17848[0m[0m | time: 56.569s
[2K
| Adam | epoch: 013 | loss: 0.17848 - acc: 0.9648 -- iter: 128/211
[A[ATraining Step: 89  | total loss: [1m[32m0.16490[0m[0m | time: 68.739s
[2K
| Adam | epoch: 013 | loss: 0.16490 - acc: 0.9683 -- iter: 160/211
[A[ATraining Step: 90  | total loss: [1m[32m0.15126[0m[0m | time: 83.957s
[2K
| Adam | epoch: 013 | loss: 0.15126 - acc: 0.9715 -- iter: 192/211
[A[ATraining Step: 91  | total loss: [1m[32m0.14054[0m[0m | time: 109.130s
[2K
| Adam | epoch: 013 | loss: 0.14054 - acc: 0.9743 | val_loss: 1.91320 - val_acc: 0.6061 -- iter: 211/211
--
Training Step: 92  | total loss: [1m[32m0.13440[0m[0m | time: 17.083s
[2K
| Adam | epoch: 014 | loss: 0.13440 - acc: 0.9738 -- iter: 032/211
[A[ATraining Step: 93  | total loss: [1m[32m0.12430[0m[0m | time: 34.807s
[2K
| Adam | epoch: 014 | loss: 0.12430 - acc: 0.9764 -- iter: 064/211
[A[ATraining Step: 94  | total loss: [1m[32m0.11879[0m[0m | time: 51.835s
[2K
| Adam | epoch: 014 | loss: 0.11879 - acc: 0.9788 -- iter: 096/211
[A[ATraining Step: 95  | total loss: [1m[32m0.10920[0m[0m | time: 59.520s
[2K
| Adam | epoch: 014 | loss: 0.10920 - acc: 0.9809 -- iter: 128/211
[A[ATraining Step: 96  | total loss: [1m[32m0.13674[0m[0m | time: 67.755s
[2K
| Adam | epoch: 014 | loss: 0.13674 - acc: 0.9775 -- iter: 160/211
[A[ATraining Step: 97  | total loss: [1m[32m0.12515[0m[0m | time: 82.055s
[2K
| Adam | epoch: 014 | loss: 0.12515 - acc: 0.9798 -- iter: 192/211
[A[ATraining Step: 98  | total loss: [1m[32m0.11765[0m[0m | time: 108.068s
[2K
| Adam | epoch: 014 | loss: 0.11765 - acc: 0.9818 | val_loss: 0.46774 - val_acc: 0.8636 -- iter: 211/211
--
Training Step: 99  | total loss: [1m[32m0.10687[0m[0m | time: 17.542s
[2K
| Adam | epoch: 015 | loss: 0.10687 - acc: 0.9836 -- iter: 032/211
[A[ATraining Step: 100  | total loss: [1m[32m0.09880[0m[0m | time: 34.821s
[2K
| Adam | epoch: 015 | loss: 0.09880 - acc: 0.9853 -- iter: 064/211
[A[ATraining Step: 101  | total loss: [1m[32m0.09129[0m[0m | time: 52.835s
[2K
| Adam | epoch: 015 | loss: 0.09129 - acc: 0.9867 -- iter: 096/211
[A[ATraining Step: 102  | total loss: [1m[32m0.09528[0m[0m | time: 65.414s
[2K
| Adam | epoch: 015 | loss: 0.09528 - acc: 0.9849 -- iter: 128/211
[A[ATraining Step: 103  | total loss: [1m[32m0.08788[0m[0m | time: 73.808s
[2K
| Adam | epoch: 015 | loss: 0.08788 - acc: 0.9864 -- iter: 160/211
[A[ATraining Step: 104  | total loss: [1m[32m0.16102[0m[0m | time: 85.170s
[2K
| Adam | epoch: 015 | loss: 0.16102 - acc: 0.9773 -- iter: 192/211
[A[ATraining Step: 105  | total loss: [1m[32m0.14530[0m[0m | time: 108.536s
[2K
| Adam | epoch: 015 | loss: 0.14530 - acc: 0.9795 | val_loss: 0.49555 - val_acc: 0.8182 -- iter: 211/211
--
Validation AUC:0.9338842975206612
Validation AUPRC:0.9392783105291916
Test AUC:0.9712962962962963
Test AUPRC:0.9644011124425408
BestTestF1Score	0.89	0.8	0.89	0.83	0.97	29	6	30	1	0.05
BestTestMCCScore	0.89	0.8	0.89	0.83	0.97	29	6	30	1	0.05
BestTestAccuracyScore	0.89	0.8	0.89	0.83	0.97	29	6	30	1	0.05
BestValidationF1Score	0.91	0.82	0.91	0.86	0.97	32	5	28	1	0.05
BestValidationMCC	0.91	0.82	0.91	0.86	0.97	32	5	28	1	0.05
BestValidationAccuracy	0.91	0.82	0.91	0.86	0.97	32	5	28	1	0.05
TestPredictions (Threshold:0.05)
CHEMBL232759,TN,INACT,0.0	CHEMBL372391,TN,INACT,0.029999999329447746	CHEMBL250353,TN,INACT,0.019999999552965164	CHEMBL209064,TN,INACT,0.019999999552965164	CHEMBL2022785,TP,ACT,1.0	CHEMBL3797299,TN,INACT,0.019999999552965164	CHEMBL2023206,TP,ACT,1.0	CHEMBL196333,TN,INACT,0.009999999776482582	CHEMBL3237377,TP,ACT,0.9700000286102295	CHEMBL1950309,TP,ACT,0.30000001192092896	CHEMBL1684297,TP,ACT,0.9700000286102295	CHEMBL3086042,TP,ACT,0.47999998927116394	CHEMBL385711,TN,INACT,0.019999999552965164	CHEMBL1683098,TN,INACT,0.0	CHEMBL426649,TN,INACT,0.019999999552965164	CHEMBL1938495,FN,ACT,0.03999999910593033	CHEMBL1938519,TP,ACT,0.4399999976158142	CHEMBL3086037,TP,ACT,0.8199999928474426	CHEMBL175442,TN,INACT,0.0	CHEMBL1683122,TN,INACT,0.019999999552965164	CHEMBL250283,TN,INACT,0.0	CHEMBL1259118,TP,ACT,0.9900000095367432	CHEMBL2031593,TP,ACT,1.0	CHEMBL389804,TN,INACT,0.009999999776482582	CHEMBL1950788,TP,ACT,0.3499999940395355	CHEMBL3735843,FP,INACT,0.9100000262260437	CHEMBL509753,FP,INACT,0.6700000166893005	CHEMBL398661,TN,INACT,0.03999999910593033	CHEMBL1683130,TN,INACT,0.0	CHEMBL30532,FP,INACT,0.3400000035762787	CHEMBL148285,TN,INACT,0.0	CHEMBL230985,TN,INACT,0.0	CHEMBL1684282,TP,ACT,0.9900000095367432	CHEMBL2022789,TP,ACT,1.0	CHEMBL560729,TN,INACT,0.029999999329447746	CHEMBL1683091,FP,INACT,0.07000000029802322	CHEMBL2031596,TP,ACT,0.9900000095367432	CHEMBL1683136,TN,INACT,0.019999999552965164	CHEMBL3086045,TP,ACT,0.9900000095367432	CHEMBL283043,FP,INACT,0.07999999821186066	CHEMBL2023213,TP,ACT,0.9900000095367432	CHEMBL2031597,TP,ACT,1.0	CHEMBL3086029,TP,ACT,0.8899999856948853	CHEMBL248615,TN,INACT,0.019999999552965164	CHEMBL435882,FP,INACT,0.12999999523162842	CHEMBL1951469,TP,ACT,0.75	CHEMBL400954,TN,INACT,0.019999999552965164	CHEMBL1684303,TP,ACT,0.9900000095367432	CHEMBL3085780,TP,ACT,0.8199999928474426	CHEMBL190217,TN,INACT,0.0	CHEMBL1683094,TN,INACT,0.019999999552965164	CHEMBL2024200,TP,ACT,0.6200000047683716	CHEMBL1259176,TN,INACT,0.009999999776482582	CHEMBL3330052,TN,INACT,0.029999999329447746	CHEMBL146995,TN,INACT,0.0	CHEMBL1683119,TN,INACT,0.009999999776482582	CHEMBL360488,TN,INACT,0.009999999776482582	CHEMBL536088,TN,INACT,0.019999999552965164	CHEMBL1950919,TP,ACT,0.05999999865889549	CHEMBL2024193,TP,ACT,0.9900000095367432	CHEMBL2031598,TP,ACT,1.0	CHEMBL2023215,TP,ACT,0.9900000095367432	CHEMBL397242,TN,INACT,0.019999999552965164	CHEMBL2022791,TP,ACT,0.699999988079071	CHEMBL3086047,TP,ACT,0.5600000023841858	CHEMBL1684295,TP,ACT,0.9700000286102295	

