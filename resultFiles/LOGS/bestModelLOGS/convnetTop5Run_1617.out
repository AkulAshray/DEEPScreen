CNNModel CHEMBL2216739 adam 0.0005 15 256 0 0.6 False True
Number of active compounds :	168
Number of inactive compounds :	168
---------------------------------
Run id: CNNModel_CHEMBL2216739_adam_0.0005_15_256_0_0.6_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL2216739_adam_0.0005_15_256_0.6_True/
---------------------------------
Training samples: 214
Validation samples: 68
--
Training Step: 1  | time: 0.757s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/214
[A[ATraining Step: 2  | total loss: [1m[32m0.62358[0m[0m | time: 1.355s
[2K
| Adam | epoch: 001 | loss: 0.62358 - acc: 0.4500 -- iter: 064/214
[A[ATraining Step: 3  | total loss: [1m[32m0.68070[0m[0m | time: 1.961s
[2K
| Adam | epoch: 001 | loss: 0.68070 - acc: 0.4909 -- iter: 096/214
[A[ATraining Step: 4  | total loss: [1m[32m0.68569[0m[0m | time: 2.607s
[2K
| Adam | epoch: 001 | loss: 0.68569 - acc: 0.6384 -- iter: 128/214
[A[ATraining Step: 5  | total loss: [1m[32m0.69113[0m[0m | time: 3.217s
[2K
| Adam | epoch: 001 | loss: 0.69113 - acc: 0.5426 -- iter: 160/214
[A[ATraining Step: 6  | total loss: [1m[32m0.70329[0m[0m | time: 3.830s
[2K
| Adam | epoch: 001 | loss: 0.70329 - acc: 0.4348 -- iter: 192/214
[A[ATraining Step: 7  | total loss: [1m[32m0.69959[0m[0m | time: 5.287s
[2K
| Adam | epoch: 001 | loss: 0.69959 - acc: 0.4552 | val_loss: 0.69191 - val_acc: 0.5294 -- iter: 214/214
--
Training Step: 8  | total loss: [1m[32m0.69539[0m[0m | time: 0.444s
[2K
| Adam | epoch: 002 | loss: 0.69539 - acc: 0.5060 -- iter: 032/214
[A[ATraining Step: 9  | total loss: [1m[32m0.69320[0m[0m | time: 1.066s
[2K
| Adam | epoch: 002 | loss: 0.69320 - acc: 0.5269 -- iter: 064/214
[A[ATraining Step: 10  | total loss: [1m[32m0.69608[0m[0m | time: 1.688s
[2K
| Adam | epoch: 002 | loss: 0.69608 - acc: 0.4353 -- iter: 096/214
[A[ATraining Step: 11  | total loss: [1m[32m0.69357[0m[0m | time: 2.303s
[2K
| Adam | epoch: 002 | loss: 0.69357 - acc: 0.5104 -- iter: 128/214
[A[ATraining Step: 12  | total loss: [1m[32m0.69400[0m[0m | time: 2.934s
[2K
| Adam | epoch: 002 | loss: 0.69400 - acc: 0.4776 -- iter: 160/214
[A[ATraining Step: 13  | total loss: [1m[32m0.69387[0m[0m | time: 3.584s
[2K
| Adam | epoch: 002 | loss: 0.69387 - acc: 0.4604 -- iter: 192/214
[A[ATraining Step: 14  | total loss: [1m[32m0.69326[0m[0m | time: 5.211s
[2K
| Adam | epoch: 002 | loss: 0.69326 - acc: 0.5150 | val_loss: 0.69291 - val_acc: 0.5294 -- iter: 214/214
--
Training Step: 15  | total loss: [1m[32m0.69307[0m[0m | time: 0.453s
[2K
| Adam | epoch: 003 | loss: 0.69307 - acc: 0.5336 -- iter: 032/214
[A[ATraining Step: 16  | total loss: [1m[32m0.69265[0m[0m | time: 0.888s
[2K
| Adam | epoch: 003 | loss: 0.69265 - acc: 0.5892 -- iter: 064/214
[A[ATraining Step: 17  | total loss: [1m[32m0.69225[0m[0m | time: 1.488s
[2K
| Adam | epoch: 003 | loss: 0.69225 - acc: 0.6225 -- iter: 096/214
[A[ATraining Step: 18  | total loss: [1m[32m0.69222[0m[0m | time: 2.095s
[2K
| Adam | epoch: 003 | loss: 0.69222 - acc: 0.6126 -- iter: 128/214
[A[ATraining Step: 19  | total loss: [1m[32m0.69262[0m[0m | time: 2.720s
[2K
| Adam | epoch: 003 | loss: 0.69262 - acc: 0.5646 -- iter: 160/214
[A[ATraining Step: 20  | total loss: [1m[32m0.69280[0m[0m | time: 3.326s
[2K
| Adam | epoch: 003 | loss: 0.69280 - acc: 0.5439 -- iter: 192/214
[A[ATraining Step: 21  | total loss: [1m[32m0.69303[0m[0m | time: 4.929s
[2K
| Adam | epoch: 003 | loss: 0.69303 - acc: 0.5205 | val_loss: 0.69249 - val_acc: 0.5294 -- iter: 214/214
--
Training Step: 22  | total loss: [1m[32m0.69453[0m[0m | time: 0.632s
[2K
| Adam | epoch: 004 | loss: 0.69453 - acc: 0.4488 -- iter: 032/214
[A[ATraining Step: 23  | total loss: [1m[32m0.69376[0m[0m | time: 1.079s
[2K
| Adam | epoch: 004 | loss: 0.69376 - acc: 0.4818 -- iter: 064/214
[A[ATraining Step: 24  | total loss: [1m[32m0.69409[0m[0m | time: 1.551s
[2K
| Adam | epoch: 004 | loss: 0.69409 - acc: 0.4613 -- iter: 096/214
[A[ATraining Step: 25  | total loss: [1m[32m0.69434[0m[0m | time: 2.155s
[2K
| Adam | epoch: 004 | loss: 0.69434 - acc: 0.4471 -- iter: 128/214
[A[ATraining Step: 26  | total loss: [1m[32m0.69442[0m[0m | time: 2.750s
[2K
| Adam | epoch: 004 | loss: 0.69442 - acc: 0.4363 -- iter: 160/214
[A[ATraining Step: 27  | total loss: [1m[32m0.69394[0m[0m | time: 3.372s
[2K
| Adam | epoch: 004 | loss: 0.69394 - acc: 0.4607 -- iter: 192/214
[A[ATraining Step: 28  | total loss: [1m[32m0.69363[0m[0m | time: 4.981s
[2K
| Adam | epoch: 004 | loss: 0.69363 - acc: 0.4783 | val_loss: 0.69278 - val_acc: 0.5294 -- iter: 214/214
--
Training Step: 29  | total loss: [1m[32m0.69353[0m[0m | time: 0.622s
[2K
| Adam | epoch: 005 | loss: 0.69353 - acc: 0.4836 -- iter: 032/214
[A[ATraining Step: 30  | total loss: [1m[32m0.69322[0m[0m | time: 1.227s
[2K
| Adam | epoch: 005 | loss: 0.69322 - acc: 0.5097 -- iter: 064/214
[A[ATraining Step: 31  | total loss: [1m[32m0.69319[0m[0m | time: 1.661s
[2K
| Adam | epoch: 005 | loss: 0.69319 - acc: 0.5075 -- iter: 096/214
[A[ATraining Step: 32  | total loss: [1m[32m0.69286[0m[0m | time: 2.092s
[2K
| Adam | epoch: 005 | loss: 0.69286 - acc: 0.5365 -- iter: 128/214
[A[ATraining Step: 33  | total loss: [1m[32m0.69257[0m[0m | time: 2.693s
[2K
| Adam | epoch: 005 | loss: 0.69257 - acc: 0.5584 -- iter: 160/214
[A[ATraining Step: 34  | total loss: [1m[32m0.69259[0m[0m | time: 3.303s
[2K
| Adam | epoch: 005 | loss: 0.69259 - acc: 0.5526 -- iter: 192/214
[A[ATraining Step: 35  | total loss: [1m[32m0.69343[0m[0m | time: 4.910s
[2K
| Adam | epoch: 005 | loss: 0.69343 - acc: 0.4892 | val_loss: 0.69259 - val_acc: 0.5294 -- iter: 214/214
--
Training Step: 36  | total loss: [1m[32m0.69332[0m[0m | time: 0.619s
[2K
| Adam | epoch: 006 | loss: 0.69332 - acc: 0.4914 -- iter: 032/214
[A[ATraining Step: 37  | total loss: [1m[32m0.69325[0m[0m | time: 1.215s
[2K
| Adam | epoch: 006 | loss: 0.69325 - acc: 0.4932 -- iter: 064/214
[A[ATraining Step: 38  | total loss: [1m[32m0.69324[0m[0m | time: 1.824s
[2K
| Adam | epoch: 006 | loss: 0.69324 - acc: 0.4884 -- iter: 096/214
[A[ATraining Step: 39  | total loss: [1m[32m0.69281[0m[0m | time: 2.243s
[2K
| Adam | epoch: 006 | loss: 0.69281 - acc: 0.5205 -- iter: 128/214
[A[ATraining Step: 40  | total loss: [1m[32m0.69295[0m[0m | time: 2.794s
[2K
| Adam | epoch: 006 | loss: 0.69295 - acc: 0.5082 -- iter: 160/214
[A[ATraining Step: 41  | total loss: [1m[32m0.69306[0m[0m | time: 3.402s
[2K
| Adam | epoch: 006 | loss: 0.69306 - acc: 0.4983 -- iter: 192/214
[A[ATraining Step: 42  | total loss: [1m[32m0.69324[0m[0m | time: 5.022s
[2K
| Adam | epoch: 006 | loss: 0.69324 - acc: 0.4817 | val_loss: 0.69239 - val_acc: 0.5294 -- iter: 214/214
--
Training Step: 43  | total loss: [1m[32m0.69269[0m[0m | time: 0.611s
[2K
| Adam | epoch: 007 | loss: 0.69269 - acc: 0.5236 -- iter: 032/214
[A[ATraining Step: 44  | total loss: [1m[32m0.69309[0m[0m | time: 1.273s
[2K
| Adam | epoch: 007 | loss: 0.69309 - acc: 0.4924 -- iter: 064/214
[A[ATraining Step: 45  | total loss: [1m[32m0.69309[0m[0m | time: 1.878s
[2K
| Adam | epoch: 007 | loss: 0.69309 - acc: 0.4884 -- iter: 096/214
[A[ATraining Step: 46  | total loss: [1m[32m0.69258[0m[0m | time: 2.479s
[2K
| Adam | epoch: 007 | loss: 0.69258 - acc: 0.5216 -- iter: 128/214
[A[ATraining Step: 47  | total loss: [1m[32m0.69279[0m[0m | time: 2.900s
[2K
| Adam | epoch: 007 | loss: 0.69279 - acc: 0.5027 -- iter: 160/214
[A[ATraining Step: 48  | total loss: [1m[32m0.69271[0m[0m | time: 3.334s
[2K
| Adam | epoch: 007 | loss: 0.69271 - acc: 0.5096 -- iter: 192/214
[A[ATraining Step: 49  | total loss: [1m[32m0.69251[0m[0m | time: 4.973s
[2K
| Adam | epoch: 007 | loss: 0.69251 - acc: 0.5153 | val_loss: 0.69136 - val_acc: 0.5294 -- iter: 214/214
--
Training Step: 50  | total loss: [1m[32m0.69239[0m[0m | time: 0.611s
[2K
| Adam | epoch: 008 | loss: 0.69239 - acc: 0.5129 -- iter: 032/214
[A[ATraining Step: 51  | total loss: [1m[32m0.69234[0m[0m | time: 1.265s
[2K
| Adam | epoch: 008 | loss: 0.69234 - acc: 0.5062 -- iter: 064/214
[A[ATraining Step: 52  | total loss: [1m[32m0.69220[0m[0m | time: 1.858s
[2K
| Adam | epoch: 008 | loss: 0.69220 - acc: 0.5005 -- iter: 096/214
[A[ATraining Step: 53  | total loss: [1m[32m0.69181[0m[0m | time: 2.530s
[2K
| Adam | epoch: 008 | loss: 0.69181 - acc: 0.5143 -- iter: 128/214
[A[ATraining Step: 54  | total loss: [1m[32m0.69179[0m[0m | time: 3.156s
[2K
| Adam | epoch: 008 | loss: 0.69179 - acc: 0.5077 -- iter: 160/214
[A[ATraining Step: 55  | total loss: [1m[32m0.69153[0m[0m | time: 3.593s
[2K
| Adam | epoch: 008 | loss: 0.69153 - acc: 0.5021 -- iter: 192/214
[A[ATraining Step: 56  | total loss: [1m[32m0.69145[0m[0m | time: 5.086s
[2K
| Adam | epoch: 008 | loss: 0.69145 - acc: 0.4890 | val_loss: 0.68759 - val_acc: 0.8088 -- iter: 214/214
--
Training Step: 57  | total loss: [1m[32m0.69124[0m[0m | time: 0.622s
[2K
| Adam | epoch: 009 | loss: 0.69124 - acc: 0.5220 -- iter: 032/214
[A[ATraining Step: 58  | total loss: [1m[32m0.69040[0m[0m | time: 1.340s
[2K
| Adam | epoch: 009 | loss: 0.69040 - acc: 0.5744 -- iter: 064/214
[A[ATraining Step: 59  | total loss: [1m[32m0.68953[0m[0m | time: 1.963s
[2K
| Adam | epoch: 009 | loss: 0.68953 - acc: 0.5686 -- iter: 096/214
[A[ATraining Step: 60  | total loss: [1m[32m0.68945[0m[0m | time: 2.573s
[2K
| Adam | epoch: 009 | loss: 0.68945 - acc: 0.5513 -- iter: 128/214
[A[ATraining Step: 61  | total loss: [1m[32m0.68889[0m[0m | time: 3.177s
[2K
| Adam | epoch: 009 | loss: 0.68889 - acc: 0.5527 -- iter: 160/214
[A[ATraining Step: 62  | total loss: [1m[32m0.68775[0m[0m | time: 3.813s
[2K
| Adam | epoch: 009 | loss: 0.68775 - acc: 0.5781 -- iter: 192/214
[A[ATraining Step: 63  | total loss: [1m[32m0.68690[0m[0m | time: 5.237s
[2K
| Adam | epoch: 009 | loss: 0.68690 - acc: 0.6078 | val_loss: 0.66788 - val_acc: 0.6471 -- iter: 214/214
--
Training Step: 64  | total loss: [1m[32m0.68512[0m[0m | time: 0.436s
[2K
| Adam | epoch: 010 | loss: 0.68512 - acc: 0.6455 -- iter: 032/214
[A[ATraining Step: 65  | total loss: [1m[32m0.68169[0m[0m | time: 1.049s
[2K
| Adam | epoch: 010 | loss: 0.68169 - acc: 0.6443 -- iter: 064/214
[A[ATraining Step: 66  | total loss: [1m[32m0.67841[0m[0m | time: 1.654s
[2K
| Adam | epoch: 010 | loss: 0.67841 - acc: 0.6344 -- iter: 096/214
[A[ATraining Step: 67  | total loss: [1m[32m0.68058[0m[0m | time: 2.255s
[2K
| Adam | epoch: 010 | loss: 0.68058 - acc: 0.6108 -- iter: 128/214
[A[ATraining Step: 68  | total loss: [1m[32m0.67601[0m[0m | time: 2.873s
[2K
| Adam | epoch: 010 | loss: 0.67601 - acc: 0.6125 -- iter: 160/214
[A[ATraining Step: 69  | total loss: [1m[32m0.66924[0m[0m | time: 3.467s
[2K
| Adam | epoch: 010 | loss: 0.66924 - acc: 0.6468 -- iter: 192/214
[A[ATraining Step: 70  | total loss: [1m[32m0.66640[0m[0m | time: 5.077s
[2K
| Adam | epoch: 010 | loss: 0.66640 - acc: 0.6479 | val_loss: 0.60831 - val_acc: 0.7647 -- iter: 214/214
--
Training Step: 71  | total loss: [1m[32m0.66106[0m[0m | time: 0.429s
[2K
| Adam | epoch: 011 | loss: 0.66106 - acc: 0.6666 -- iter: 032/214
[A[ATraining Step: 72  | total loss: [1m[32m0.64975[0m[0m | time: 0.880s
[2K
| Adam | epoch: 011 | loss: 0.64975 - acc: 0.6939 -- iter: 064/214
[A[ATraining Step: 73  | total loss: [1m[32m0.63378[0m[0m | time: 1.497s
[2K
| Adam | epoch: 011 | loss: 0.63378 - acc: 0.7128 -- iter: 096/214
[A[ATraining Step: 74  | total loss: [1m[32m0.62199[0m[0m | time: 2.129s
[2K
| Adam | epoch: 011 | loss: 0.62199 - acc: 0.7203 -- iter: 128/214
[A[ATraining Step: 75  | total loss: [1m[32m0.60307[0m[0m | time: 2.740s
[2K
| Adam | epoch: 011 | loss: 0.60307 - acc: 0.7371 -- iter: 160/214
[A[ATraining Step: 76  | total loss: [1m[32m0.60227[0m[0m | time: 3.356s
[2K
| Adam | epoch: 011 | loss: 0.60227 - acc: 0.7418 -- iter: 192/214
[A[ATraining Step: 77  | total loss: [1m[32m0.58704[0m[0m | time: 5.125s
[2K
| Adam | epoch: 011 | loss: 0.58704 - acc: 0.7427 | val_loss: 0.56929 - val_acc: 0.7206 -- iter: 214/214
--
Training Step: 78  | total loss: [1m[32m0.56573[0m[0m | time: 0.625s
[2K
| Adam | epoch: 012 | loss: 0.56573 - acc: 0.7598 -- iter: 032/214
[A[ATraining Step: 79  | total loss: [1m[32m0.56309[0m[0m | time: 1.068s
[2K
| Adam | epoch: 012 | loss: 0.56309 - acc: 0.7555 -- iter: 064/214
[A[ATraining Step: 80  | total loss: [1m[32m0.53428[0m[0m | time: 1.506s
[2K
| Adam | epoch: 012 | loss: 0.53428 - acc: 0.7759 -- iter: 096/214
[A[ATraining Step: 81  | total loss: [1m[32m0.52060[0m[0m | time: 2.124s
[2K
| Adam | epoch: 012 | loss: 0.52060 - acc: 0.7802 -- iter: 128/214
[A[ATraining Step: 82  | total loss: [1m[32m0.54825[0m[0m | time: 2.736s
[2K
| Adam | epoch: 012 | loss: 0.54825 - acc: 0.7678 -- iter: 160/214
[A[ATraining Step: 83  | total loss: [1m[32m0.51655[0m[0m | time: 3.347s
[2K
| Adam | epoch: 012 | loss: 0.51655 - acc: 0.7879 -- iter: 192/214
[A[ATraining Step: 84  | total loss: [1m[32m0.50913[0m[0m | time: 4.972s
[2K
| Adam | epoch: 012 | loss: 0.50913 - acc: 0.7966 | val_loss: 0.47200 - val_acc: 0.8235 -- iter: 214/214
--
Training Step: 85  | total loss: [1m[32m0.48894[0m[0m | time: 0.602s
[2K
| Adam | epoch: 013 | loss: 0.48894 - acc: 0.8076 -- iter: 032/214
[A[ATraining Step: 86  | total loss: [1m[32m0.47729[0m[0m | time: 1.197s
[2K
| Adam | epoch: 013 | loss: 0.47729 - acc: 0.8112 -- iter: 064/214
[A[ATraining Step: 87  | total loss: [1m[32m0.48203[0m[0m | time: 1.637s
[2K
| Adam | epoch: 013 | loss: 0.48203 - acc: 0.8082 -- iter: 096/214
[A[ATraining Step: 88  | total loss: [1m[32m0.47387[0m[0m | time: 2.088s
[2K
| Adam | epoch: 013 | loss: 0.47387 - acc: 0.8092 -- iter: 128/214
[A[ATraining Step: 89  | total loss: [1m[32m0.45509[0m[0m | time: 2.803s
[2K
| Adam | epoch: 013 | loss: 0.45509 - acc: 0.8237 -- iter: 160/214
[A[ATraining Step: 90  | total loss: [1m[32m0.45450[0m[0m | time: 3.413s
[2K
| Adam | epoch: 013 | loss: 0.45450 - acc: 0.8195 -- iter: 192/214
[A[ATraining Step: 91  | total loss: [1m[32m0.44668[0m[0m | time: 5.028s
[2K
| Adam | epoch: 013 | loss: 0.44668 - acc: 0.8188 | val_loss: 0.45946 - val_acc: 0.8529 -- iter: 214/214
--
Training Step: 92  | total loss: [1m[32m0.44914[0m[0m | time: 0.620s
[2K
| Adam | epoch: 014 | loss: 0.44914 - acc: 0.8150 -- iter: 032/214
[A[ATraining Step: 93  | total loss: [1m[32m0.44438[0m[0m | time: 1.230s
[2K
| Adam | epoch: 014 | loss: 0.44438 - acc: 0.8210 -- iter: 064/214
[A[ATraining Step: 94  | total loss: [1m[32m0.43143[0m[0m | time: 1.843s
[2K
| Adam | epoch: 014 | loss: 0.43143 - acc: 0.8233 -- iter: 096/214
[A[ATraining Step: 95  | total loss: [1m[32m0.41491[0m[0m | time: 2.279s
[2K
| Adam | epoch: 014 | loss: 0.41491 - acc: 0.8285 -- iter: 128/214
[A[ATraining Step: 96  | total loss: [1m[32m0.43511[0m[0m | time: 2.707s
[2K
| Adam | epoch: 014 | loss: 0.43511 - acc: 0.8138 -- iter: 160/214
[A[ATraining Step: 97  | total loss: [1m[32m0.43612[0m[0m | time: 3.303s
[2K
| Adam | epoch: 014 | loss: 0.43612 - acc: 0.8051 -- iter: 192/214
[A[ATraining Step: 98  | total loss: [1m[32m0.42856[0m[0m | time: 4.950s
[2K
| Adam | epoch: 014 | loss: 0.42856 - acc: 0.8028 | val_loss: 0.48566 - val_acc: 0.8235 -- iter: 214/214
--
Training Step: 99  | total loss: [1m[32m0.43053[0m[0m | time: 0.604s
[2K
| Adam | epoch: 015 | loss: 0.43053 - acc: 0.7944 -- iter: 032/214
[A[ATraining Step: 100  | total loss: [1m[32m0.41853[0m[0m | time: 1.214s
[2K
| Adam | epoch: 015 | loss: 0.41853 - acc: 0.8087 -- iter: 064/214
[A[ATraining Step: 101  | total loss: [1m[32m0.42927[0m[0m | time: 1.831s
[2K
| Adam | epoch: 015 | loss: 0.42927 - acc: 0.7997 -- iter: 096/214
[A[ATraining Step: 102  | total loss: [1m[32m0.43072[0m[0m | time: 2.451s
[2K
| Adam | epoch: 015 | loss: 0.43072 - acc: 0.8010 -- iter: 128/214
[A[ATraining Step: 103  | total loss: [1m[32m0.41763[0m[0m | time: 2.884s
[2K
| Adam | epoch: 015 | loss: 0.41763 - acc: 0.8146 -- iter: 160/214
[A[ATraining Step: 104  | total loss: [1m[32m0.40476[0m[0m | time: 3.314s
[2K
| Adam | epoch: 015 | loss: 0.40476 - acc: 0.8286 -- iter: 192/214
[A[ATraining Step: 105  | total loss: [1m[32m0.39493[0m[0m | time: 4.954s
[2K
| Adam | epoch: 015 | loss: 0.39493 - acc: 0.8412 | val_loss: 0.54452 - val_acc: 0.7206 -- iter: 214/214
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.8654513888888888
Validation AUPRC:0.8544442270690534
Test AUC:0.8796536796536796
Test AUPRC:0.8705408894904386
BestTestF1Score	0.85	0.68	0.84	0.83	0.86	30	6	27	5	0.81
BestTestMCCScore	0.85	0.68	0.84	0.83	0.86	30	6	27	5	0.81
BestTestAccuracyScore	0.85	0.68	0.84	0.83	0.86	30	6	27	5	0.81
BestValidationF1Score	0.86	0.73	0.87	0.87	0.84	27	4	32	5	0.81
BestValidationMCC	0.86	0.73	0.87	0.87	0.84	27	4	32	5	0.81
BestValidationAccuracy	0.86	0.73	0.87	0.87	0.84	27	4	32	5	0.81
TestPredictions (Threshold:0.81)
CHEMBL3431835,TP,ACT,0.9100000262260437	CHEMBL3431506,TP,ACT,0.9200000166893005	CHEMBL3431824,FP,INACT,0.9100000262260437	CHEMBL3431571,TN,INACT,0.5099999904632568	CHEMBL3431798,TN,INACT,0.11999999731779099	CHEMBL3431605,TP,ACT,0.8999999761581421	CHEMBL75880,FP,INACT,0.8199999928474426	CHEMBL3431535,TP,ACT,0.9100000262260437	CHEMBL3431574,TP,ACT,0.9100000262260437	CHEMBL3431580,TP,ACT,0.8299999833106995	CHEMBL3431470,TP,ACT,0.9100000262260437	CHEMBL3431582,TN,INACT,0.10000000149011612	CHEMBL3431461,TP,ACT,0.9200000166893005	CHEMBL3431687,FN,ACT,0.7599999904632568	CHEMBL3431584,TN,INACT,0.2199999988079071	CHEMBL3431610,TP,ACT,0.9100000262260437	CHEMBL3431760,TN,INACT,0.30000001192092896	CHEMBL3431471,TP,ACT,0.9100000262260437	CHEMBL3431638,TP,ACT,0.8999999761581421	CHEMBL3431578,TN,INACT,0.07999999821186066	CHEMBL3431688,TN,INACT,0.07999999821186066	CHEMBL1149,TN,INACT,0.75	CHEMBL3431491,TN,INACT,0.12999999523162842	CHEMBL3431559,TN,INACT,0.10999999940395355	CHEMBL3431680,TP,ACT,0.8999999761581421	CHEMBL2216771,TP,ACT,0.8100000023841858	CHEMBL3431724,FN,ACT,0.800000011920929	CHEMBL3431682,TP,ACT,0.9200000166893005	CHEMBL3431721,TP,ACT,0.8799999952316284	CHEMBL3431518,TP,ACT,0.8700000047683716	CHEMBL3431604,TP,ACT,0.8999999761581421	CHEMBL3431810,TN,INACT,0.15000000596046448	CHEMBL3431601,FN,ACT,0.09000000357627869	CHEMBL3431844,TP,ACT,0.9100000262260437	CHEMBL3431625,FP,INACT,0.9100000262260437	CHEMBL3431548,TP,ACT,0.9200000166893005	CHEMBL3431757,TN,INACT,0.10999999940395355	CHEMBL3431526,TN,INACT,0.4399999976158142	CHEMBL3431683,TN,INACT,0.1599999964237213	CHEMBL3431622,TP,ACT,0.8799999952316284	CHEMBL3431521,FP,INACT,0.8999999761581421	CHEMBL3431878,TN,INACT,0.3499999940395355	CHEMBL3431818,TP,ACT,0.8399999737739563	CHEMBL3431533,FN,ACT,0.7599999904632568	CHEMBL3431916,FP,INACT,0.8899999856948853	CHEMBL3431866,TN,INACT,0.07000000029802322	CHEMBL3431796,FP,INACT,0.9100000262260437	CHEMBL3431624,TP,ACT,0.8999999761581421	CHEMBL3431620,TN,INACT,0.7400000095367432	CHEMBL3431609,TN,INACT,0.47999998927116394	CHEMBL3431822,TP,ACT,0.8999999761581421	CHEMBL3431671,TP,ACT,0.9200000166893005	CHEMBL3431492,TN,INACT,0.5299999713897705	CHEMBL3431883,TP,ACT,0.8999999761581421	CHEMBL3431543,FN,ACT,0.5099999904632568	CHEMBL3431805,TN,INACT,0.23000000417232513	CHEMBL3431603,TP,ACT,0.8399999737739563	CHEMBL3431661,TP,ACT,0.8899999856948853	CHEMBL3431572,TN,INACT,0.07999999821186066	CHEMBL3431830,TN,INACT,0.6600000262260437	CHEMBL3431565,TN,INACT,0.10999999940395355	CHEMBL3431920,TP,ACT,0.8899999856948853	CHEMBL3431581,TN,INACT,0.05999999865889549	CHEMBL3431697,TN,INACT,0.20999999344348907	CHEMBL3431646,TP,ACT,0.9100000262260437	CHEMBL3431764,TP,ACT,0.9100000262260437	CHEMBL3431865,TN,INACT,0.7699999809265137	CHEMBL3431643,TN,INACT,0.10999999940395355	

