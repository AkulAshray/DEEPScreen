ImageNetInceptionV2 CHEMBL5017 adam 0.0001 15 0 0 0.6 False True
Number of active compounds :	231
Number of inactive compounds :	231
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL5017_adam_0.0001_15_0_0_0.6_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL5017_adam_0.0001_15_0.6/
---------------------------------
Training samples: 286
Validation samples: 90
--
Training Step: 1  | time: 369.365s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/286
[A[ATraining Step: 2  | total loss: [1m[32m0.58337[0m[0m | time: 585.236s
[2K
| Adam | epoch: 001 | loss: 0.58337 - acc: 0.5625 -- iter: 064/286
[A[ATraining Step: 3  | total loss: [1m[32m0.56193[0m[0m | time: 743.542s
[2K
| Adam | epoch: 001 | loss: 0.56193 - acc: 0.7159 -- iter: 096/286
[A[ATraining Step: 4  | total loss: [1m[32m0.56941[0m[0m | time: 828.010s
[2K
| Adam | epoch: 001 | loss: 0.56941 - acc: 0.7180 -- iter: 128/286
[A[ATraining Step: 5  | total loss: [1m[32m0.58510[0m[0m | time: 868.535s
[2K
| Adam | epoch: 001 | loss: 0.58510 - acc: 0.6969 -- iter: 160/286
[A[ATraining Step: 6  | total loss: [1m[32m0.63371[0m[0m | time: 881.328s
[2K
| Adam | epoch: 001 | loss: 0.63371 - acc: 0.6909 -- iter: 192/286
[A[ATraining Step: 7  | total loss: [1m[32m0.53496[0m[0m | time: 894.133s
[2K
| Adam | epoch: 001 | loss: 0.53496 - acc: 0.7451 -- iter: 224/286
[A[ATraining Step: 8  | total loss: [1m[32m0.46945[0m[0m | time: 946.208s
[2K
| Adam | epoch: 001 | loss: 0.46945 - acc: 0.8006 -- iter: 256/286
[A[ATraining Step: 9  | total loss: [1m[32m0.39067[0m[0m | time: 1005.131s
[2K
| Adam | epoch: 001 | loss: 0.39067 - acc: 0.8565 | val_loss: 2.83138 - val_acc: 0.4222 -- iter: 286/286
--
Training Step: 10  | total loss: [1m[32m0.44888[0m[0m | time: 16.489s
[2K
| Adam | epoch: 002 | loss: 0.44888 - acc: 0.8283 -- iter: 032/286
[A[ATraining Step: 11  | total loss: [1m[32m0.41455[0m[0m | time: 69.321s
[2K
| Adam | epoch: 002 | loss: 0.41455 - acc: 0.8622 -- iter: 064/286
[A[ATraining Step: 12  | total loss: [1m[32m0.40428[0m[0m | time: 124.296s
[2K
| Adam | epoch: 002 | loss: 0.40428 - acc: 0.8258 -- iter: 096/286
[A[ATraining Step: 13  | total loss: [1m[32m0.44282[0m[0m | time: 142.798s
[2K
| Adam | epoch: 002 | loss: 0.44282 - acc: 0.8067 -- iter: 128/286
[A[ATraining Step: 14  | total loss: [1m[32m0.38254[0m[0m | time: 212.623s
[2K
| Adam | epoch: 002 | loss: 0.38254 - acc: 0.8730 -- iter: 160/286
[A[ATraining Step: 15  | total loss: [1m[32m0.36437[0m[0m | time: 243.940s
[2K
| Adam | epoch: 002 | loss: 0.36437 - acc: 0.8860 -- iter: 192/286
[A[ATraining Step: 16  | total loss: [1m[32m0.36933[0m[0m | time: 267.266s
[2K
| Adam | epoch: 002 | loss: 0.36933 - acc: 0.8702 -- iter: 224/286
[A[ATraining Step: 17  | total loss: [1m[32m0.40068[0m[0m | time: 293.856s
[2K
| Adam | epoch: 002 | loss: 0.40068 - acc: 0.8607 -- iter: 256/286
[A[ATraining Step: 18  | total loss: [1m[32m0.36116[0m[0m | time: 323.379s
[2K
| Adam | epoch: 002 | loss: 0.36116 - acc: 0.8764 | val_loss: 3.45951 - val_acc: 0.4222 -- iter: 286/286
--
Training Step: 19  | total loss: [1m[32m0.31628[0m[0m | time: 82.646s
[2K
| Adam | epoch: 003 | loss: 0.31628 - acc: 0.8864 -- iter: 032/286
[A[ATraining Step: 20  | total loss: [1m[32m0.28130[0m[0m | time: 148.119s
[2K
| Adam | epoch: 003 | loss: 0.28130 - acc: 0.9015 -- iter: 064/286
[A[ATraining Step: 21  | total loss: [1m[32m0.23627[0m[0m | time: 165.076s
[2K
| Adam | epoch: 003 | loss: 0.23627 - acc: 0.9320 -- iter: 096/286
[A[ATraining Step: 22  | total loss: [1m[32m0.21180[0m[0m | time: 180.263s
[2K
| Adam | epoch: 003 | loss: 0.21180 - acc: 0.9524 -- iter: 128/286
[A[ATraining Step: 23  | total loss: [1m[32m0.18757[0m[0m | time: 193.225s
[2K
| Adam | epoch: 003 | loss: 0.18757 - acc: 0.9662 -- iter: 160/286
[A[ATraining Step: 24  | total loss: [1m[32m0.17371[0m[0m | time: 205.876s
[2K
| Adam | epoch: 003 | loss: 0.17371 - acc: 0.9757 -- iter: 192/286
[A[ATraining Step: 25  | total loss: [1m[32m0.18790[0m[0m | time: 221.982s
[2K
| Adam | epoch: 003 | loss: 0.18790 - acc: 0.9653 -- iter: 224/286
[A[ATraining Step: 26  | total loss: [1m[32m0.18860[0m[0m | time: 256.340s
[2K
| Adam | epoch: 003 | loss: 0.18860 - acc: 0.9745 -- iter: 256/286
[A[ATraining Step: 27  | total loss: [1m[32m0.15770[0m[0m | time: 284.561s
[2K
| Adam | epoch: 003 | loss: 0.15770 - acc: 0.9811 | val_loss: 1.44734 - val_acc: 0.4333 -- iter: 286/286
--
Training Step: 28  | total loss: [1m[32m0.15321[0m[0m | time: 17.405s
[2K
| Adam | epoch: 004 | loss: 0.15321 - acc: 0.9780 -- iter: 032/286
[A[ATraining Step: 29  | total loss: [1m[32m0.12964[0m[0m | time: 125.585s
[2K
| Adam | epoch: 004 | loss: 0.12964 - acc: 0.9833 -- iter: 064/286
[A[ATraining Step: 30  | total loss: [1m[32m0.10401[0m[0m | time: 142.493s
[2K
| Adam | epoch: 004 | loss: 0.10401 - acc: 0.9873 -- iter: 096/286
[A[ATraining Step: 31  | total loss: [1m[32m0.08384[0m[0m | time: 166.234s
[2K
| Adam | epoch: 004 | loss: 0.08384 - acc: 0.9902 -- iter: 128/286
[A[ATraining Step: 32  | total loss: [1m[32m0.06990[0m[0m | time: 198.469s
[2K
| Adam | epoch: 004 | loss: 0.06990 - acc: 0.9924 -- iter: 160/286
[A[ATraining Step: 33  | total loss: [1m[32m0.05660[0m[0m | time: 212.810s
[2K
| Adam | epoch: 004 | loss: 0.05660 - acc: 0.9941 -- iter: 192/286
[A[ATraining Step: 34  | total loss: [1m[32m0.05226[0m[0m | time: 227.495s
[2K
| Adam | epoch: 004 | loss: 0.05226 - acc: 0.9953 -- iter: 224/286
[A[ATraining Step: 35  | total loss: [1m[32m0.04532[0m[0m | time: 245.057s
[2K
| Adam | epoch: 004 | loss: 0.04532 - acc: 0.9963 -- iter: 256/286
[A[ATraining Step: 36  | total loss: [1m[32m0.04142[0m[0m | time: 274.299s
[2K
| Adam | epoch: 004 | loss: 0.04142 - acc: 0.9971 | val_loss: 0.66848 - val_acc: 0.6778 -- iter: 286/286
--
Training Step: 37  | total loss: [1m[32m0.03750[0m[0m | time: 18.213s
[2K
| Adam | epoch: 005 | loss: 0.03750 - acc: 0.9977 -- iter: 032/286
[A[ATraining Step: 38  | total loss: [1m[32m0.03124[0m[0m | time: 34.605s
[2K
| Adam | epoch: 005 | loss: 0.03124 - acc: 0.9981 -- iter: 064/286
[A[ATraining Step: 39  | total loss: [1m[32m0.02642[0m[0m | time: 51.158s
[2K
| Adam | epoch: 005 | loss: 0.02642 - acc: 0.9985 -- iter: 096/286
[A[ATraining Step: 40  | total loss: [1m[32m0.02291[0m[0m | time: 68.654s
[2K
| Adam | epoch: 005 | loss: 0.02291 - acc: 0.9988 -- iter: 128/286
[A[ATraining Step: 41  | total loss: [1m[32m0.01956[0m[0m | time: 84.837s
[2K
| Adam | epoch: 005 | loss: 0.01956 - acc: 0.9990 -- iter: 160/286
[A[ATraining Step: 42  | total loss: [1m[32m0.01703[0m[0m | time: 97.938s
[2K
| Adam | epoch: 005 | loss: 0.01703 - acc: 0.9992 -- iter: 192/286
[A[ATraining Step: 43  | total loss: [1m[32m0.01457[0m[0m | time: 110.888s
[2K
| Adam | epoch: 005 | loss: 0.01457 - acc: 0.9993 -- iter: 224/286
[A[ATraining Step: 44  | total loss: [1m[32m0.01803[0m[0m | time: 130.610s
[2K
| Adam | epoch: 005 | loss: 0.01803 - acc: 0.9994 -- iter: 256/286
[A[ATraining Step: 45  | total loss: [1m[32m0.03185[0m[0m | time: 158.099s
[2K
| Adam | epoch: 005 | loss: 0.03185 - acc: 0.9942 | val_loss: 0.55331 - val_acc: 0.7889 -- iter: 286/286
--
Training Step: 46  | total loss: [1m[32m0.03553[0m[0m | time: 12.861s
[2K
| Adam | epoch: 006 | loss: 0.03553 - acc: 0.9900 -- iter: 032/286
[A[ATraining Step: 47  | total loss: [1m[32m0.03015[0m[0m | time: 25.714s
[2K
| Adam | epoch: 006 | loss: 0.03015 - acc: 0.9916 -- iter: 064/286
[A[ATraining Step: 48  | total loss: [1m[32m0.04894[0m[0m | time: 55.488s
[2K
| Adam | epoch: 006 | loss: 0.04894 - acc: 0.9879 -- iter: 096/286
[A[ATraining Step: 49  | total loss: [1m[32m0.04151[0m[0m | time: 85.830s
[2K
| Adam | epoch: 006 | loss: 0.04151 - acc: 0.9898 -- iter: 128/286
[A[ATraining Step: 50  | total loss: [1m[32m0.03551[0m[0m | time: 102.532s
[2K
| Adam | epoch: 006 | loss: 0.03551 - acc: 0.9914 -- iter: 160/286
[A[ATraining Step: 51  | total loss: [1m[32m0.03085[0m[0m | time: 118.893s
[2K
| Adam | epoch: 006 | loss: 0.03085 - acc: 0.9927 -- iter: 192/286
[A[ATraining Step: 52  | total loss: [1m[32m0.02697[0m[0m | time: 133.290s
[2K
| Adam | epoch: 006 | loss: 0.02697 - acc: 0.9938 -- iter: 224/286
[A[ATraining Step: 53  | total loss: [1m[32m0.02467[0m[0m | time: 146.835s
[2K
| Adam | epoch: 006 | loss: 0.02467 - acc: 0.9947 -- iter: 256/286
[A[ATraining Step: 54  | total loss: [1m[32m0.02296[0m[0m | time: 171.816s
[2K
| Adam | epoch: 006 | loss: 0.02296 - acc: 0.9955 | val_loss: 0.43000 - val_acc: 0.8556 -- iter: 286/286
--
Training Step: 55  | total loss: [1m[32m0.05951[0m[0m | time: 15.581s
[2K
| Adam | epoch: 007 | loss: 0.05951 - acc: 0.9917 -- iter: 032/286
[A[ATraining Step: 56  | total loss: [1m[32m0.05204[0m[0m | time: 28.993s
[2K
| Adam | epoch: 007 | loss: 0.05204 - acc: 0.9928 -- iter: 064/286
[A[ATraining Step: 57  | total loss: [1m[32m0.04621[0m[0m | time: 42.329s
[2K
| Adam | epoch: 007 | loss: 0.04621 - acc: 0.9938 -- iter: 096/286
[A[ATraining Step: 58  | total loss: [1m[32m0.05826[0m[0m | time: 58.769s
[2K
| Adam | epoch: 007 | loss: 0.05826 - acc: 0.9904 -- iter: 128/286
[A[ATraining Step: 59  | total loss: [1m[32m0.05193[0m[0m | time: 76.119s
[2K
| Adam | epoch: 007 | loss: 0.05193 - acc: 0.9917 -- iter: 160/286
[A[ATraining Step: 60  | total loss: [1m[32m0.04866[0m[0m | time: 93.772s
[2K
| Adam | epoch: 007 | loss: 0.04866 - acc: 0.9928 -- iter: 192/286
[A[ATraining Step: 61  | total loss: [1m[32m0.04305[0m[0m | time: 112.850s
[2K
| Adam | epoch: 007 | loss: 0.04305 - acc: 0.9937 -- iter: 224/286
[A[ATraining Step: 62  | total loss: [1m[32m0.03850[0m[0m | time: 130.353s
[2K
| Adam | epoch: 007 | loss: 0.03850 - acc: 0.9945 -- iter: 256/286
[A[ATraining Step: 63  | total loss: [1m[32m0.03455[0m[0m | time: 149.825s
[2K
| Adam | epoch: 007 | loss: 0.03455 - acc: 0.9952 | val_loss: 0.66786 - val_acc: 0.7778 -- iter: 286/286
--
Training Step: 64  | total loss: [1m[32m0.03090[0m[0m | time: 26.224s
[2K
| Adam | epoch: 008 | loss: 0.03090 - acc: 0.9958 -- iter: 032/286
[A[ATraining Step: 65  | total loss: [1m[32m0.02828[0m[0m | time: 44.171s
[2K
| Adam | epoch: 008 | loss: 0.02828 - acc: 0.9963 -- iter: 064/286
[A[ATraining Step: 66  | total loss: [1m[32m0.02744[0m[0m | time: 58.676s
[2K
| Adam | epoch: 008 | loss: 0.02744 - acc: 0.9968 -- iter: 096/286
[A[ATraining Step: 67  | total loss: [1m[32m0.08898[0m[0m | time: 71.676s
[2K
| Adam | epoch: 008 | loss: 0.08898 - acc: 0.9934 -- iter: 128/286
[A[ATraining Step: 68  | total loss: [1m[32m0.12251[0m[0m | time: 137.314s
[2K
| Adam | epoch: 008 | loss: 0.12251 - acc: 0.9868 -- iter: 160/286
[A[ATraining Step: 69  | total loss: [1m[32m0.10897[0m[0m | time: 152.305s
[2K
| Adam | epoch: 008 | loss: 0.10897 - acc: 0.9883 -- iter: 192/286
[A[ATraining Step: 70  | total loss: [1m[32m0.09766[0m[0m | time: 171.867s
[2K
| Adam | epoch: 008 | loss: 0.09766 - acc: 0.9897 -- iter: 224/286
[A[ATraining Step: 71  | total loss: [1m[32m0.08745[0m[0m | time: 189.328s
[2K
| Adam | epoch: 008 | loss: 0.08745 - acc: 0.9909 -- iter: 256/286
[A[ATraining Step: 72  | total loss: [1m[32m0.08067[0m[0m | time: 209.754s
[2K
| Adam | epoch: 008 | loss: 0.08067 - acc: 0.9919 | val_loss: 0.41433 - val_acc: 0.8556 -- iter: 286/286
--
Training Step: 73  | total loss: [1m[32m0.07725[0m[0m | time: 17.348s
[2K
| Adam | epoch: 009 | loss: 0.07725 - acc: 0.9928 -- iter: 032/286
[A[ATraining Step: 74  | total loss: [1m[32m0.06974[0m[0m | time: 34.019s
[2K
| Adam | epoch: 009 | loss: 0.06974 - acc: 0.9936 -- iter: 064/286
[A[ATraining Step: 75  | total loss: [1m[32m0.06398[0m[0m | time: 50.104s
[2K
| Adam | epoch: 009 | loss: 0.06398 - acc: 0.9943 -- iter: 096/286
[A[ATraining Step: 76  | total loss: [1m[32m0.05825[0m[0m | time: 62.979s
[2K
| Adam | epoch: 009 | loss: 0.05825 - acc: 0.9949 -- iter: 128/286
[A[ATraining Step: 77  | total loss: [1m[32m0.05402[0m[0m | time: 77.047s
[2K
| Adam | epoch: 009 | loss: 0.05402 - acc: 0.9954 -- iter: 160/286
[A[ATraining Step: 78  | total loss: [1m[32m0.04871[0m[0m | time: 94.247s
[2K
| Adam | epoch: 009 | loss: 0.04871 - acc: 0.9959 -- iter: 192/286
[A[ATraining Step: 79  | total loss: [1m[32m0.04457[0m[0m | time: 111.081s
[2K
| Adam | epoch: 009 | loss: 0.04457 - acc: 0.9963 -- iter: 224/286
[A[ATraining Step: 80  | total loss: [1m[32m0.04091[0m[0m | time: 127.062s
[2K
| Adam | epoch: 009 | loss: 0.04091 - acc: 0.9967 -- iter: 256/286
[A[ATraining Step: 81  | total loss: [1m[32m0.03738[0m[0m | time: 149.223s
[2K
| Adam | epoch: 009 | loss: 0.03738 - acc: 0.9970 | val_loss: 1.05724 - val_acc: 0.6556 -- iter: 286/286
--
Training Step: 82  | total loss: [1m[32m0.03412[0m[0m | time: 17.538s
[2K
| Adam | epoch: 010 | loss: 0.03412 - acc: 0.9973 -- iter: 032/286
[A[ATraining Step: 83  | total loss: [1m[32m0.03856[0m[0m | time: 34.750s
[2K
| Adam | epoch: 010 | loss: 0.03856 - acc: 0.9945 -- iter: 064/286
[A[ATraining Step: 84  | total loss: [1m[32m0.03510[0m[0m | time: 51.535s
[2K
| Adam | epoch: 010 | loss: 0.03510 - acc: 0.9950 -- iter: 096/286
[A[ATraining Step: 85  | total loss: [1m[32m0.03183[0m[0m | time: 67.696s
[2K
| Adam | epoch: 010 | loss: 0.03183 - acc: 0.9955 -- iter: 128/286
[A[ATraining Step: 86  | total loss: [1m[32m0.02888[0m[0m | time: 79.903s
[2K
| Adam | epoch: 010 | loss: 0.02888 - acc: 0.9960 -- iter: 160/286
[A[ATraining Step: 87  | total loss: [1m[32m0.04269[0m[0m | time: 92.824s
[2K
| Adam | epoch: 010 | loss: 0.04269 - acc: 0.9933 -- iter: 192/286
[A[ATraining Step: 88  | total loss: [1m[32m0.05890[0m[0m | time: 109.423s
[2K
| Adam | epoch: 010 | loss: 0.05890 - acc: 0.9908 -- iter: 224/286
[A[ATraining Step: 89  | total loss: [1m[32m0.05324[0m[0m | time: 125.993s
[2K
| Adam | epoch: 010 | loss: 0.05324 - acc: 0.9917 -- iter: 256/286
[A[ATraining Step: 90  | total loss: [1m[32m0.04855[0m[0m | time: 152.923s
[2K
| Adam | epoch: 010 | loss: 0.04855 - acc: 0.9926 | val_loss: 2.58081 - val_acc: 0.5778 -- iter: 286/286
--
Training Step: 91  | total loss: [1m[32m0.04424[0m[0m | time: 13.828s
[2K
| Adam | epoch: 011 | loss: 0.04424 - acc: 0.9933 -- iter: 032/286
[A[ATraining Step: 92  | total loss: [1m[32m0.04251[0m[0m | time: 27.976s
[2K
| Adam | epoch: 011 | loss: 0.04251 - acc: 0.9940 -- iter: 064/286
[A[ATraining Step: 93  | total loss: [1m[32m0.03924[0m[0m | time: 44.282s
[2K
| Adam | epoch: 011 | loss: 0.03924 - acc: 0.9946 -- iter: 096/286
[A[ATraining Step: 94  | total loss: [1m[32m0.03891[0m[0m | time: 62.606s
[2K
| Adam | epoch: 011 | loss: 0.03891 - acc: 0.9951 -- iter: 128/286
[A[ATraining Step: 95  | total loss: [1m[32m0.05503[0m[0m | time: 77.490s
[2K
| Adam | epoch: 011 | loss: 0.05503 - acc: 0.9925 -- iter: 160/286
[A[ATraining Step: 96  | total loss: [1m[32m0.05047[0m[0m | time: 89.712s
[2K
| Adam | epoch: 011 | loss: 0.05047 - acc: 0.9932 -- iter: 192/286
[A[ATraining Step: 97  | total loss: [1m[32m0.04611[0m[0m | time: 98.569s
[2K
| Adam | epoch: 011 | loss: 0.04611 - acc: 0.9939 -- iter: 224/286
[A[ATraining Step: 98  | total loss: [1m[32m0.06528[0m[0m | time: 107.433s
[2K
| Adam | epoch: 011 | loss: 0.06528 - acc: 0.9914 -- iter: 256/286
[A[ATraining Step: 99  | total loss: [1m[32m0.05950[0m[0m | time: 129.803s
[2K
| Adam | epoch: 011 | loss: 0.05950 - acc: 0.9923 | val_loss: 2.99807 - val_acc: 0.4444 -- iter: 286/286
--
Training Step: 100  | total loss: [1m[32m0.05724[0m[0m | time: 16.646s
[2K
| Adam | epoch: 012 | loss: 0.05724 - acc: 0.9930 -- iter: 032/286
[A[ATraining Step: 101  | total loss: [1m[32m0.05341[0m[0m | time: 29.947s
[2K
| Adam | epoch: 012 | loss: 0.05341 - acc: 0.9937 -- iter: 064/286
[A[ATraining Step: 102  | total loss: [1m[32m0.05094[0m[0m | time: 43.122s
[2K
| Adam | epoch: 012 | loss: 0.05094 - acc: 0.9944 -- iter: 096/286
[A[ATraining Step: 103  | total loss: [1m[32m0.06450[0m[0m | time: 60.572s
[2K
| Adam | epoch: 012 | loss: 0.06450 - acc: 0.9918 -- iter: 128/286
[A[ATraining Step: 104  | total loss: [1m[32m0.05941[0m[0m | time: 77.553s
[2K
| Adam | epoch: 012 | loss: 0.05941 - acc: 0.9926 -- iter: 160/286
[A[ATraining Step: 105  | total loss: [1m[32m0.05420[0m[0m | time: 93.406s
[2K
| Adam | epoch: 012 | loss: 0.05420 - acc: 0.9934 -- iter: 192/286
[A[ATraining Step: 106  | total loss: [1m[32m0.04967[0m[0m | time: 110.147s
[2K
| Adam | epoch: 012 | loss: 0.04967 - acc: 0.9940 -- iter: 224/286
[A[ATraining Step: 107  | total loss: [1m[32m0.04620[0m[0m | time: 122.771s
[2K
| Adam | epoch: 012 | loss: 0.04620 - acc: 0.9946 -- iter: 256/286
[A[ATraining Step: 108  | total loss: [1m[32m0.05689[0m[0m | time: 145.189s
[2K
| Adam | epoch: 012 | loss: 0.05689 - acc: 0.9920 | val_loss: 0.59231 - val_acc: 0.7556 -- iter: 286/286
--
Training Step: 109  | total loss: [1m[32m0.05176[0m[0m | time: 15.007s
[2K
| Adam | epoch: 013 | loss: 0.05176 - acc: 0.9928 -- iter: 032/286
[A[ATraining Step: 110  | total loss: [1m[32m0.04685[0m[0m | time: 28.949s
[2K
| Adam | epoch: 013 | loss: 0.04685 - acc: 0.9935 -- iter: 064/286
[A[ATraining Step: 111  | total loss: [1m[32m0.04239[0m[0m | time: 41.203s
[2K
| Adam | epoch: 013 | loss: 0.04239 - acc: 0.9942 -- iter: 096/286
[A[ATraining Step: 112  | total loss: [1m[32m0.05117[0m[0m | time: 57.026s
[2K
| Adam | epoch: 013 | loss: 0.05117 - acc: 0.9916 -- iter: 128/286
[A[ATraining Step: 113  | total loss: [1m[32m0.04691[0m[0m | time: 74.227s
[2K
| Adam | epoch: 013 | loss: 0.04691 - acc: 0.9925 -- iter: 160/286
[A[ATraining Step: 114  | total loss: [1m[32m0.04318[0m[0m | time: 90.917s
[2K
| Adam | epoch: 013 | loss: 0.04318 - acc: 0.9932 -- iter: 192/286
[A[ATraining Step: 115  | total loss: [1m[32m0.03974[0m[0m | time: 107.592s
[2K
| Adam | epoch: 013 | loss: 0.03974 - acc: 0.9939 -- iter: 224/286
[A[ATraining Step: 116  | total loss: [1m[32m0.03647[0m[0m | time: 124.154s
[2K
| Adam | epoch: 013 | loss: 0.03647 - acc: 0.9945 -- iter: 256/286
[A[ATraining Step: 117  | total loss: [1m[32m0.04054[0m[0m | time: 144.083s
[2K
| Adam | epoch: 013 | loss: 0.04054 - acc: 0.9919 | val_loss: 0.51507 - val_acc: 0.8556 -- iter: 286/286
--
Training Step: 118  | total loss: [1m[32m0.05886[0m[0m | time: 17.036s
[2K
| Adam | epoch: 014 | loss: 0.05886 - acc: 0.9896 -- iter: 032/286
[A[ATraining Step: 119  | total loss: [1m[32m0.05346[0m[0m | time: 33.707s
[2K
| Adam | epoch: 014 | loss: 0.05346 - acc: 0.9907 -- iter: 064/286
[A[ATraining Step: 120  | total loss: [1m[32m0.04902[0m[0m | time: 50.008s
[2K
| Adam | epoch: 014 | loss: 0.04902 - acc: 0.9916 -- iter: 096/286
[A[ATraining Step: 121  | total loss: [1m[32m0.04494[0m[0m | time: 65.133s
[2K
| Adam | epoch: 014 | loss: 0.04494 - acc: 0.9924 -- iter: 128/286
[A[ATraining Step: 122  | total loss: [1m[32m0.06566[0m[0m | time: 77.469s
[2K
| Adam | epoch: 014 | loss: 0.06566 - acc: 0.9901 -- iter: 160/286
[A[ATraining Step: 123  | total loss: [1m[32m0.05946[0m[0m | time: 91.050s
[2K
| Adam | epoch: 014 | loss: 0.05946 - acc: 0.9911 -- iter: 192/286
[A[ATraining Step: 124  | total loss: [1m[32m0.05398[0m[0m | time: 106.691s
[2K
| Adam | epoch: 014 | loss: 0.05398 - acc: 0.9920 -- iter: 224/286
[A[ATraining Step: 125  | total loss: [1m[32m0.04913[0m[0m | time: 123.776s
[2K
| Adam | epoch: 014 | loss: 0.04913 - acc: 0.9928 -- iter: 256/286
[A[ATraining Step: 126  | total loss: [1m[32m0.04651[0m[0m | time: 150.824s
[2K
| Adam | epoch: 014 | loss: 0.04651 - acc: 0.9935 | val_loss: 0.88988 - val_acc: 0.7444 -- iter: 286/286
--
Training Step: 127  | total loss: [1m[32m0.04324[0m[0m | time: 16.759s
[2K
| Adam | epoch: 015 | loss: 0.04324 - acc: 0.9941 -- iter: 032/286
[A[ATraining Step: 128  | total loss: [1m[32m0.24375[0m[0m | time: 32.819s
[2K
| Adam | epoch: 015 | loss: 0.24375 - acc: 0.9697 -- iter: 064/286
[A[ATraining Step: 129  | total loss: [1m[32m0.22094[0m[0m | time: 48.784s
[2K
| Adam | epoch: 015 | loss: 0.22094 - acc: 0.9727 -- iter: 096/286
[A[ATraining Step: 130  | total loss: [1m[32m0.19930[0m[0m | time: 65.157s
[2K
| Adam | epoch: 015 | loss: 0.19930 - acc: 0.9755 -- iter: 128/286
[A[ATraining Step: 131  | total loss: [1m[32m0.18002[0m[0m | time: 78.702s
[2K
| Adam | epoch: 015 | loss: 0.18002 - acc: 0.9779 -- iter: 160/286
[A[ATraining Step: 132  | total loss: [1m[32m0.16610[0m[0m | time: 91.280s
[2K
| Adam | epoch: 015 | loss: 0.16610 - acc: 0.9801 -- iter: 192/286
[A[ATraining Step: 133  | total loss: [1m[32m0.15183[0m[0m | time: 108.431s
[2K
| Adam | epoch: 015 | loss: 0.15183 - acc: 0.9821 -- iter: 224/286
[A[ATraining Step: 134  | total loss: [1m[32m0.13769[0m[0m | time: 124.881s
[2K
| Adam | epoch: 015 | loss: 0.13769 - acc: 0.9839 -- iter: 256/286
[A[ATraining Step: 135  | total loss: [1m[32m0.12540[0m[0m | time: 149.787s
[2K
| Adam | epoch: 015 | loss: 0.12540 - acc: 0.9855 | val_loss: 0.37714 - val_acc: 0.8667 -- iter: 286/286
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.94585020242915
Validation AUPRC:0.9452851531452995
Test AUC:0.9094507669470558
Test AUPRC:0.9212502801806047
BestTestF1Score	0.86	0.71	0.86	0.89	0.83	39	5	38	8	0.38
BestTestMCCScore	0.86	0.71	0.86	0.89	0.83	39	5	38	8	0.38
BestTestAccuracyScore	0.86	0.71	0.86	0.89	0.83	39	5	38	8	0.38
BestValidationF1Score	0.88	0.79	0.9	0.89	0.87	33	4	48	5	0.38
BestValidationMCC	0.88	0.79	0.9	0.89	0.87	33	4	48	5	0.38
BestValidationAccuracy	0.88	0.79	0.9	0.89	0.87	33	4	48	5	0.38
TestPredictions (Threshold:0.38)
CHEMBL99591,TP,ACT,0.9700000286102295	CHEMBL209121,TN,INACT,0.0	CHEMBL267094,TN,INACT,0.0	CHEMBL107680,TN,INACT,0.019999999552965164	CHEMBL99578,TP,ACT,0.9700000286102295	CHEMBL462650,TN,INACT,0.3199999928474426	CHEMBL806,FN,ACT,0.05000000074505806	CHEMBL3291084,TP,ACT,0.8299999833106995	CHEMBL233552,TN,INACT,0.009999999776482582	CHEMBL328089,TN,INACT,0.0	CHEMBL164425,TP,ACT,0.4099999964237213	CHEMBL9666,TN,INACT,0.0	CHEMBL109248,TN,INACT,0.009999999776482582	CHEMBL279520,TN,INACT,0.0	CHEMBL1258672,TP,ACT,0.9700000286102295	CHEMBL3417001,TP,ACT,0.6899999976158142	CHEMBL707,TP,ACT,0.6899999976158142	CHEMBL202122,TP,ACT,0.8700000047683716	CHEMBL545363,TN,INACT,0.0	CHEMBL1732,FN,ACT,0.009999999776482582	CHEMBL470432,TP,ACT,0.949999988079071	CHEMBL2370509,TN,INACT,0.0	CHEMBL450595,TP,ACT,0.9900000095367432	CHEMBL471661,TP,ACT,0.9300000071525574	CHEMBL112770,TN,INACT,0.009999999776482582	CHEMBL358119,TP,ACT,0.9800000190734863	CHEMBL36558,TP,ACT,0.7099999785423279	CHEMBL128360,TN,INACT,0.18000000715255737	CHEMBL323175,TN,INACT,0.0	CHEMBL471234,TP,ACT,0.8899999856948853	CHEMBL330003,TN,INACT,0.10999999940395355	CHEMBL44615,TN,INACT,0.0	CHEMBL3218877,TP,ACT,0.9399999976158142	CHEMBL295155,TP,ACT,0.4000000059604645	CHEMBL99723,TP,ACT,0.9599999785423279	CHEMBL1255837,FN,ACT,0.0	CHEMBL415879,TN,INACT,0.0	CHEMBL293232,TN,INACT,0.10000000149011612	CHEMBL168632,FP,INACT,0.5699999928474426	CHEMBL2181187,TP,ACT,0.5600000023841858	CHEMBL118553,TN,INACT,0.009999999776482582	CHEMBL513254,TP,ACT,0.8500000238418579	CHEMBL3417005,TP,ACT,0.699999988079071	CHEMBL472481,TP,ACT,0.9599999785423279	CHEMBL3093185,TP,ACT,0.6600000262260437	CHEMBL56994,TP,ACT,0.9300000071525574	CHEMBL408493,TN,INACT,0.009999999776482582	CHEMBL195893,FP,INACT,0.49000000953674316	CHEMBL594801,TN,INACT,0.0	CHEMBL48031,TN,INACT,0.0	CHEMBL351183,TN,INACT,0.17000000178813934	CHEMBL3417000,TP,ACT,0.9900000095367432	CHEMBL112877,TN,INACT,0.0	CHEMBL28992,FN,ACT,0.23000000417232513	CHEMBL1258223,TP,ACT,0.9100000262260437	CHEMBL59597,TN,INACT,0.1899999976158142	CHEMBL20748,FN,ACT,0.2800000011920929	CHEMBL442,FN,ACT,0.009999999776482582	CHEMBL516340,TP,ACT,0.9100000262260437	CHEMBL45305,FP,INACT,0.7599999904632568	CHEMBL95968,TP,ACT,0.5699999928474426	CHEMBL76360,TN,INACT,0.009999999776482582	CHEMBL2181190,TP,ACT,0.9300000071525574	CHEMBL99141,TP,ACT,0.9399999976158142	CHEMBL245319,TN,INACT,0.019999999552965164	CHEMBL421187,TP,ACT,0.800000011920929	CHEMBL469582,TP,ACT,0.9800000190734863	CHEMBL2181172,TP,ACT,0.949999988079071	CHEMBL460470,TN,INACT,0.009999999776482582	CHEMBL1729,TP,ACT,0.8799999952316284	CHEMBL273410,TN,INACT,0.0	CHEMBL2391353,FP,INACT,0.949999988079071	CHEMBL82976,FN,ACT,0.10999999940395355	CHEMBL294649,TN,INACT,0.009999999776482582	CHEMBL42359,TN,INACT,0.029999999329447746	CHEMBL47404,TN,INACT,0.0	CHEMBL2181169,TP,ACT,0.8700000047683716	CHEMBL89445,TN,INACT,0.019999999552965164	CHEMBL297599,FP,INACT,0.7099999785423279	CHEMBL100199,TP,ACT,0.9700000286102295	CHEMBL162095,TN,INACT,0.2199999988079071	CHEMBL279225,TN,INACT,0.0	CHEMBL44262,TN,INACT,0.0	CHEMBL34263,TP,ACT,0.9700000286102295	CHEMBL2181167,TP,ACT,0.7699999809265137	CHEMBL2181189,TP,ACT,0.8700000047683716	CHEMBL1550957,FN,ACT,0.0	CHEMBL165175,TN,INACT,0.1899999976158142	CHEMBL3417006,TP,ACT,0.8299999833106995	CHEMBL2042401,TN,INACT,0.0	

