CNNModel CHEMBL2272 adam 0.0005 15 256 0 0.6 False True
Number of active compounds :	143
Number of inactive compounds :	143
---------------------------------
Run id: CNNModel_CHEMBL2272_adam_0.0005_15_256_0_0.6_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL2272_adam_0.0005_15_256_0.6_True/
---------------------------------
Training samples: 180
Validation samples: 57
--
Training Step: 1  | time: 0.765s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/180
[A[ATraining Step: 2  | total loss: [1m[32m0.62410[0m[0m | time: 1.356s
[2K
| Adam | epoch: 001 | loss: 0.62410 - acc: 0.3094 -- iter: 064/180
[A[ATraining Step: 3  | total loss: [1m[32m0.68132[0m[0m | time: 1.975s
[2K
| Adam | epoch: 001 | loss: 0.68132 - acc: 0.4142 -- iter: 096/180
[A[ATraining Step: 4  | total loss: [1m[32m0.68922[0m[0m | time: 2.579s
[2K
| Adam | epoch: 001 | loss: 0.68922 - acc: 0.5489 -- iter: 128/180
[A[ATraining Step: 5  | total loss: [1m[32m0.68747[0m[0m | time: 3.201s
[2K
| Adam | epoch: 001 | loss: 0.68747 - acc: 0.7097 -- iter: 160/180
[A[ATraining Step: 6  | total loss: [1m[32m0.69449[0m[0m | time: 4.613s
[2K
| Adam | epoch: 001 | loss: 0.69449 - acc: 0.5146 | val_loss: 0.69371 - val_acc: 0.4912 -- iter: 180/180
--
Training Step: 7  | total loss: [1m[32m0.69665[0m[0m | time: 0.387s
[2K
| Adam | epoch: 002 | loss: 0.69665 - acc: 0.4459 -- iter: 032/180
[A[ATraining Step: 8  | total loss: [1m[32m0.69726[0m[0m | time: 1.025s
[2K
| Adam | epoch: 002 | loss: 0.69726 - acc: 0.4201 -- iter: 064/180
[A[ATraining Step: 9  | total loss: [1m[32m0.69637[0m[0m | time: 1.644s
[2K
| Adam | epoch: 002 | loss: 0.69637 - acc: 0.4293 -- iter: 096/180
[A[ATraining Step: 10  | total loss: [1m[32m0.69480[0m[0m | time: 2.261s
[2K
| Adam | epoch: 002 | loss: 0.69480 - acc: 0.4646 -- iter: 128/180
[A[ATraining Step: 11  | total loss: [1m[32m0.69421[0m[0m | time: 2.870s
[2K
| Adam | epoch: 002 | loss: 0.69421 - acc: 0.4666 -- iter: 160/180
[A[ATraining Step: 12  | total loss: [1m[32m0.69397[0m[0m | time: 4.490s
[2K
| Adam | epoch: 002 | loss: 0.69397 - acc: 0.4535 | val_loss: 0.69315 - val_acc: 0.4912 -- iter: 180/180
--
Training Step: 13  | total loss: [1m[32m0.69312[0m[0m | time: 0.416s
[2K
| Adam | epoch: 003 | loss: 0.69312 - acc: 0.5270 -- iter: 032/180
[A[ATraining Step: 14  | total loss: [1m[32m0.69296[0m[0m | time: 0.826s
[2K
| Adam | epoch: 003 | loss: 0.69296 - acc: 0.5364 -- iter: 064/180
[A[ATraining Step: 15  | total loss: [1m[32m0.69278[0m[0m | time: 1.446s
[2K
| Adam | epoch: 003 | loss: 0.69278 - acc: 0.5417 -- iter: 096/180
[A[ATraining Step: 16  | total loss: [1m[32m0.69275[0m[0m | time: 2.047s
[2K
| Adam | epoch: 003 | loss: 0.69275 - acc: 0.5378 -- iter: 128/180
[A[ATraining Step: 17  | total loss: [1m[32m0.69286[0m[0m | time: 2.685s
[2K
| Adam | epoch: 003 | loss: 0.69286 - acc: 0.5242 -- iter: 160/180
[A[ATraining Step: 18  | total loss: [1m[32m0.69289[0m[0m | time: 4.388s
[2K
| Adam | epoch: 003 | loss: 0.69289 - acc: 0.5158 | val_loss: 0.69294 - val_acc: 0.4912 -- iter: 180/180
--
Training Step: 19  | total loss: [1m[32m0.69227[0m[0m | time: 0.948s
[2K
| Adam | epoch: 004 | loss: 0.69227 - acc: 0.5522 -- iter: 032/180
[A[ATraining Step: 20  | total loss: [1m[32m0.69310[0m[0m | time: 1.570s
[2K
| Adam | epoch: 004 | loss: 0.69310 - acc: 0.4953 -- iter: 064/180
[A[ATraining Step: 21  | total loss: [1m[32m0.69219[0m[0m | time: 2.286s
[2K
| Adam | epoch: 004 | loss: 0.69219 - acc: 0.5433 -- iter: 096/180
[A[ATraining Step: 22  | total loss: [1m[32m0.69141[0m[0m | time: 3.378s
[2K
| Adam | epoch: 004 | loss: 0.69141 - acc: 0.5753 -- iter: 128/180
[A[ATraining Step: 23  | total loss: [1m[32m0.69171[0m[0m | time: 4.219s
[2K
| Adam | epoch: 004 | loss: 0.69171 - acc: 0.5534 -- iter: 160/180
[A[ATraining Step: 24  | total loss: [1m[32m0.69195[0m[0m | time: 6.368s
[2K
| Adam | epoch: 004 | loss: 0.69195 - acc: 0.5384 | val_loss: 0.69159 - val_acc: 0.4912 -- iter: 180/180
--
Training Step: 25  | total loss: [1m[32m0.69110[0m[0m | time: 0.937s
[2K
| Adam | epoch: 005 | loss: 0.69110 - acc: 0.5620 -- iter: 032/180
[A[ATraining Step: 26  | total loss: [1m[32m0.69119[0m[0m | time: 2.051s
[2K
| Adam | epoch: 005 | loss: 0.69119 - acc: 0.5456 -- iter: 064/180
[A[ATraining Step: 27  | total loss: [1m[32m0.69133[0m[0m | time: 2.762s
[2K
| Adam | epoch: 005 | loss: 0.69133 - acc: 0.5258 -- iter: 096/180
[A[ATraining Step: 28  | total loss: [1m[32m0.69071[0m[0m | time: 3.351s
[2K
| Adam | epoch: 005 | loss: 0.69071 - acc: 0.5319 -- iter: 128/180
[A[ATraining Step: 29  | total loss: [1m[32m0.68994[0m[0m | time: 4.273s
[2K
| Adam | epoch: 005 | loss: 0.68994 - acc: 0.5363 -- iter: 160/180
[A[ATraining Step: 30  | total loss: [1m[32m0.68969[0m[0m | time: 6.414s
[2K
| Adam | epoch: 005 | loss: 0.68969 - acc: 0.5203 | val_loss: 0.68554 - val_acc: 0.4912 -- iter: 180/180
--
Training Step: 31  | total loss: [1m[32m0.68885[0m[0m | time: 1.018s
[2K
| Adam | epoch: 006 | loss: 0.68885 - acc: 0.5228 -- iter: 032/180
[A[ATraining Step: 32  | total loss: [1m[32m0.68685[0m[0m | time: 2.112s
[2K
| Adam | epoch: 006 | loss: 0.68685 - acc: 0.5528 -- iter: 064/180
[A[ATraining Step: 33  | total loss: [1m[32m0.68696[0m[0m | time: 3.002s
[2K
| Adam | epoch: 006 | loss: 0.68696 - acc: 0.5275 -- iter: 096/180
[A[ATraining Step: 34  | total loss: [1m[32m0.68506[0m[0m | time: 3.674s
[2K
| Adam | epoch: 006 | loss: 0.68506 - acc: 0.5283 -- iter: 128/180
[A[ATraining Step: 35  | total loss: [1m[32m0.68763[0m[0m | time: 4.359s
[2K
| Adam | epoch: 006 | loss: 0.68763 - acc: 0.4701 -- iter: 160/180
[A[ATraining Step: 36  | total loss: [1m[32m0.68733[0m[0m | time: 6.409s
[2K
| Adam | epoch: 006 | loss: 0.68733 - acc: 0.5376 | val_loss: 0.68990 - val_acc: 0.5088 -- iter: 180/180
--
Training Step: 37  | total loss: [1m[32m0.68885[0m[0m | time: 0.863s
[2K
| Adam | epoch: 007 | loss: 0.68885 - acc: 0.5113 -- iter: 032/180
[A[ATraining Step: 38  | total loss: [1m[32m0.69111[0m[0m | time: 1.573s
[2K
| Adam | epoch: 007 | loss: 0.69111 - acc: 0.4785 -- iter: 064/180
[A[ATraining Step: 39  | total loss: [1m[32m0.69007[0m[0m | time: 2.358s
[2K
| Adam | epoch: 007 | loss: 0.69007 - acc: 0.4826 -- iter: 096/180
[A[ATraining Step: 40  | total loss: [1m[32m0.68740[0m[0m | time: 3.170s
[2K
| Adam | epoch: 007 | loss: 0.68740 - acc: 0.5269 -- iter: 128/180
[A[ATraining Step: 41  | total loss: [1m[32m0.68258[0m[0m | time: 3.623s
[2K
| Adam | epoch: 007 | loss: 0.68258 - acc: 0.6081 -- iter: 160/180
[A[ATraining Step: 42  | total loss: [1m[32m0.67672[0m[0m | time: 5.112s
[2K
| Adam | epoch: 007 | loss: 0.67672 - acc: 0.6516 | val_loss: 0.64312 - val_acc: 0.5789 -- iter: 180/180
--
Training Step: 43  | total loss: [1m[32m0.67016[0m[0m | time: 1.010s
[2K
| Adam | epoch: 008 | loss: 0.67016 - acc: 0.6513 -- iter: 032/180
[A[ATraining Step: 44  | total loss: [1m[32m0.66637[0m[0m | time: 1.962s
[2K
| Adam | epoch: 008 | loss: 0.66637 - acc: 0.6251 -- iter: 064/180
[A[ATraining Step: 45  | total loss: [1m[32m0.65834[0m[0m | time: 3.104s
[2K
| Adam | epoch: 008 | loss: 0.65834 - acc: 0.6145 -- iter: 096/180
[A[ATraining Step: 46  | total loss: [1m[32m0.65208[0m[0m | time: 4.033s
[2K
| Adam | epoch: 008 | loss: 0.65208 - acc: 0.6215 -- iter: 128/180
[A[ATraining Step: 47  | total loss: [1m[32m0.63875[0m[0m | time: 4.979s
[2K
| Adam | epoch: 008 | loss: 0.63875 - acc: 0.6783 -- iter: 160/180
[A[ATraining Step: 48  | total loss: [1m[32m0.62922[0m[0m | time: 6.671s
[2K
| Adam | epoch: 008 | loss: 0.62922 - acc: 0.7049 | val_loss: 0.49236 - val_acc: 0.8947 -- iter: 180/180
--
Training Step: 49  | total loss: [1m[32m0.62324[0m[0m | time: 0.683s
[2K
| Adam | epoch: 009 | loss: 0.62324 - acc: 0.7120 -- iter: 032/180
[A[ATraining Step: 50  | total loss: [1m[32m0.61322[0m[0m | time: 1.769s
[2K
| Adam | epoch: 009 | loss: 0.61322 - acc: 0.7179 -- iter: 064/180
[A[ATraining Step: 51  | total loss: [1m[32m0.59453[0m[0m | time: 2.664s
[2K
| Adam | epoch: 009 | loss: 0.59453 - acc: 0.7514 -- iter: 096/180
[A[ATraining Step: 52  | total loss: [1m[32m0.57956[0m[0m | time: 3.692s
[2K
| Adam | epoch: 009 | loss: 0.57956 - acc: 0.7606 -- iter: 128/180
[A[ATraining Step: 53  | total loss: [1m[32m0.54994[0m[0m | time: 4.779s
[2K
| Adam | epoch: 009 | loss: 0.54994 - acc: 0.7774 -- iter: 160/180
[A[ATraining Step: 54  | total loss: [1m[32m0.53943[0m[0m | time: 6.834s
[2K
| Adam | epoch: 009 | loss: 0.53943 - acc: 0.7689 | val_loss: 0.29507 - val_acc: 0.9649 -- iter: 180/180
--
Training Step: 55  | total loss: [1m[32m0.50883[0m[0m | time: 0.729s
[2K
| Adam | epoch: 010 | loss: 0.50883 - acc: 0.7841 -- iter: 032/180
[A[ATraining Step: 56  | total loss: [1m[32m0.46578[0m[0m | time: 1.400s
[2K
| Adam | epoch: 010 | loss: 0.46578 - acc: 0.8144 -- iter: 064/180
[A[ATraining Step: 57  | total loss: [1m[32m0.43242[0m[0m | time: 2.255s
[2K
| Adam | epoch: 010 | loss: 0.43242 - acc: 0.8332 -- iter: 096/180
[A[ATraining Step: 58  | total loss: [1m[32m0.41348[0m[0m | time: 3.323s
[2K
| Adam | epoch: 010 | loss: 0.41348 - acc: 0.8474 -- iter: 128/180
[A[ATraining Step: 59  | total loss: [1m[32m0.41174[0m[0m | time: 4.443s
[2K
| Adam | epoch: 010 | loss: 0.41174 - acc: 0.8469 -- iter: 160/180
[A[ATraining Step: 60  | total loss: [1m[32m0.39549[0m[0m | time: 6.580s
[2K
| Adam | epoch: 010 | loss: 0.39549 - acc: 0.8507 | val_loss: 0.16320 - val_acc: 0.9474 -- iter: 180/180
--
Training Step: 61  | total loss: [1m[32m0.35954[0m[0m | time: 1.156s
[2K
| Adam | epoch: 011 | loss: 0.35954 - acc: 0.8661 -- iter: 032/180
[A[ATraining Step: 62  | total loss: [1m[32m0.33652[0m[0m | time: 1.754s
[2K
| Adam | epoch: 011 | loss: 0.33652 - acc: 0.8752 -- iter: 064/180
[A[ATraining Step: 63  | total loss: [1m[32m0.33398[0m[0m | time: 2.311s
[2K
| Adam | epoch: 011 | loss: 0.33398 - acc: 0.8784 -- iter: 096/180
[A[ATraining Step: 64  | total loss: [1m[32m0.33202[0m[0m | time: 3.319s
[2K
| Adam | epoch: 011 | loss: 0.33202 - acc: 0.8811 -- iter: 128/180
[A[ATraining Step: 65  | total loss: [1m[32m0.30731[0m[0m | time: 4.317s
[2K
| Adam | epoch: 011 | loss: 0.30731 - acc: 0.8880 -- iter: 160/180
[A[ATraining Step: 66  | total loss: [1m[32m0.27505[0m[0m | time: 6.500s
[2K
| Adam | epoch: 011 | loss: 0.27505 - acc: 0.9017 | val_loss: 0.14772 - val_acc: 0.9474 -- iter: 180/180
--
Training Step: 67  | total loss: [1m[32m0.25465[0m[0m | time: 0.634s
[2K
| Adam | epoch: 012 | loss: 0.25465 - acc: 0.9097 -- iter: 032/180
[A[ATraining Step: 68  | total loss: [1m[32m0.25633[0m[0m | time: 1.244s
[2K
| Adam | epoch: 012 | loss: 0.25633 - acc: 0.9093 -- iter: 064/180
[A[ATraining Step: 69  | total loss: [1m[32m0.23791[0m[0m | time: 1.642s
[2K
| Adam | epoch: 012 | loss: 0.23791 - acc: 0.9162 -- iter: 096/180
[A[ATraining Step: 70  | total loss: [1m[32m0.27285[0m[0m | time: 2.024s
[2K
| Adam | epoch: 012 | loss: 0.27285 - acc: 0.9086 -- iter: 128/180
[A[ATraining Step: 71  | total loss: [1m[32m0.29514[0m[0m | time: 2.653s
[2K
| Adam | epoch: 012 | loss: 0.29514 - acc: 0.9019 -- iter: 160/180
[A[ATraining Step: 72  | total loss: [1m[32m0.27207[0m[0m | time: 4.295s
[2K
| Adam | epoch: 012 | loss: 0.27207 - acc: 0.9130 | val_loss: 0.15089 - val_acc: 0.9474 -- iter: 180/180
--
Training Step: 73  | total loss: [1m[32m0.27808[0m[0m | time: 0.609s
[2K
| Adam | epoch: 013 | loss: 0.27808 - acc: 0.9018 -- iter: 032/180
[A[ATraining Step: 74  | total loss: [1m[32m0.27463[0m[0m | time: 1.259s
[2K
| Adam | epoch: 013 | loss: 0.27463 - acc: 0.9057 -- iter: 064/180
[A[ATraining Step: 75  | total loss: [1m[32m0.27113[0m[0m | time: 1.886s
[2K
| Adam | epoch: 013 | loss: 0.27113 - acc: 0.9092 -- iter: 096/180
[A[ATraining Step: 76  | total loss: [1m[32m0.24598[0m[0m | time: 2.284s
[2K
| Adam | epoch: 013 | loss: 0.24598 - acc: 0.9189 -- iter: 128/180
[A[ATraining Step: 77  | total loss: [1m[32m0.22715[0m[0m | time: 2.671s
[2K
| Adam | epoch: 013 | loss: 0.22715 - acc: 0.9275 -- iter: 160/180
[A[ATraining Step: 78  | total loss: [1m[32m0.21021[0m[0m | time: 4.291s
[2K
| Adam | epoch: 013 | loss: 0.21021 - acc: 0.9351 | val_loss: 0.12223 - val_acc: 0.9474 -- iter: 180/180
--
Training Step: 79  | total loss: [1m[32m0.19774[0m[0m | time: 0.621s
[2K
| Adam | epoch: 014 | loss: 0.19774 - acc: 0.9386 -- iter: 032/180
[A[ATraining Step: 80  | total loss: [1m[32m0.19060[0m[0m | time: 1.236s
[2K
| Adam | epoch: 014 | loss: 0.19060 - acc: 0.9416 -- iter: 064/180
[A[ATraining Step: 81  | total loss: [1m[32m0.18178[0m[0m | time: 1.938s
[2K
| Adam | epoch: 014 | loss: 0.18178 - acc: 0.9444 -- iter: 096/180
[A[ATraining Step: 82  | total loss: [1m[32m0.18955[0m[0m | time: 3.101s
[2K
| Adam | epoch: 014 | loss: 0.18955 - acc: 0.9437 -- iter: 128/180
[A[ATraining Step: 83  | total loss: [1m[32m0.17649[0m[0m | time: 3.771s
[2K
| Adam | epoch: 014 | loss: 0.17649 - acc: 0.9493 -- iter: 160/180
[A[ATraining Step: 84  | total loss: [1m[32m0.16382[0m[0m | time: 5.448s
[2K
| Adam | epoch: 014 | loss: 0.16382 - acc: 0.9544 | val_loss: 0.12575 - val_acc: 0.9474 -- iter: 180/180
--
Training Step: 85  | total loss: [1m[32m0.15183[0m[0m | time: 1.032s
[2K
| Adam | epoch: 015 | loss: 0.15183 - acc: 0.9590 -- iter: 032/180
[A[ATraining Step: 86  | total loss: [1m[32m0.15424[0m[0m | time: 2.134s
[2K
| Adam | epoch: 015 | loss: 0.15424 - acc: 0.9599 -- iter: 064/180
[A[ATraining Step: 87  | total loss: [1m[32m0.19845[0m[0m | time: 3.136s
[2K
| Adam | epoch: 015 | loss: 0.19845 - acc: 0.9514 -- iter: 096/180
[A[ATraining Step: 88  | total loss: [1m[32m0.18416[0m[0m | time: 4.081s
[2K
| Adam | epoch: 015 | loss: 0.18416 - acc: 0.9532 -- iter: 128/180
[A[ATraining Step: 89  | total loss: [1m[32m0.17553[0m[0m | time: 5.056s
[2K
| Adam | epoch: 015 | loss: 0.17553 - acc: 0.9547 -- iter: 160/180
[A[ATraining Step: 90  | total loss: [1m[32m0.16713[0m[0m | time: 6.671s
[2K
| Adam | epoch: 015 | loss: 0.16713 - acc: 0.9561 | val_loss: 0.13329 - val_acc: 0.9474 -- iter: 180/180
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.9913793103448276
Validation AUPRC:0.9928571428571428
Test AUC:0.9974747474747475
Test AUPRC:0.9967948717948718
BestTestF1Score	0.96	0.93	0.96	0.92	1.0	24	2	31	0	0.3
BestTestMCCScore	0.96	0.93	0.96	0.92	1.0	24	2	31	0	0.3
BestTestAccuracyScore	0.96	0.93	0.96	0.92	1.0	24	2	31	0	0.3
BestValidationF1Score	0.98	0.97	0.98	1.0	0.96	27	0	29	1	0.3
BestValidationMCC	0.98	0.97	0.98	1.0	0.96	27	0	29	1	0.3
BestValidationAccuracy	0.98	0.97	0.98	1.0	0.96	27	0	29	1	0.3
TestPredictions (Threshold:0.3)
CHEMBL3823863,TN,INACT,0.019999999552965164	CHEMBL184089,TN,INACT,0.009999999776482582	CHEMBL3822479,TN,INACT,0.029999999329447746	CHEMBL303154,TN,INACT,0.009999999776482582	CHEMBL3650338,TP,ACT,0.9200000166893005	CHEMBL192,TN,INACT,0.009999999776482582	CHEMBL184334,TN,INACT,0.009999999776482582	CHEMBL1779440,TN,INACT,0.0	CHEMBL3647481,TP,ACT,0.8899999856948853	CHEMBL3650262,TP,ACT,0.9399999976158142	CHEMBL2387436,TN,INACT,0.019999999552965164	CHEMBL1091451,TN,INACT,0.029999999329447746	CHEMBL3393195,TN,INACT,0.019999999552965164	CHEMBL1779433,TN,INACT,0.0	CHEMBL348356,TN,INACT,0.0	CHEMBL3360412,TN,INACT,0.009999999776482582	CHEMBL3650252,TP,ACT,0.9900000095367432	CHEMBL3650272,TP,ACT,0.9800000190734863	CHEMBL22016,TN,INACT,0.27000001072883606	CHEMBL3650323,TP,ACT,0.949999988079071	CHEMBL3403364,TN,INACT,0.009999999776482582	CHEMBL255130,TN,INACT,0.009999999776482582	CHEMBL3650312,FP,INACT,0.4699999988079071	CHEMBL3647484,TP,ACT,0.9200000166893005	CHEMBL3650260,TP,ACT,0.9300000071525574	CHEMBL1094023,TN,INACT,0.11999999731779099	CHEMBL3647479,TP,ACT,0.8999999761581421	CHEMBL360434,TN,INACT,0.009999999776482582	CHEMBL3823049,TN,INACT,0.009999999776482582	CHEMBL3650276,TP,ACT,0.9100000262260437	CHEMBL3647491,TP,ACT,0.9399999976158142	CHEMBL2326946,TN,INACT,0.03999999910593033	CHEMBL3647451,TP,ACT,0.9800000190734863	CHEMBL3647458,TP,ACT,0.8600000143051147	CHEMBL3650302,TP,ACT,0.9399999976158142	CHEMBL3617184,TN,INACT,0.03999999910593033	CHEMBL3771211,TN,INACT,0.0	CHEMBL184960,TN,INACT,0.009999999776482582	CHEMBL3647469,TP,ACT,0.9900000095367432	CHEMBL3650313,TP,ACT,0.9700000286102295	CHEMBL3647457,TP,ACT,0.9800000190734863	CHEMBL3650246,TP,ACT,0.949999988079071	CHEMBL184470,TN,INACT,0.009999999776482582	CHEMBL2041596,TN,INACT,0.12999999523162842	CHEMBL187201,FP,INACT,0.8500000238418579	CHEMBL3650255,TP,ACT,0.8700000047683716	CHEMBL3647493,TP,ACT,0.3700000047683716	CHEMBL3617178,TN,INACT,0.019999999552965164	CHEMBL3770470,TN,INACT,0.0	CHEMBL3650278,TP,ACT,0.949999988079071	CHEMBL3647474,TP,ACT,0.9599999785423279	CHEMBL3647453,TP,ACT,0.9700000286102295	CHEMBL365634,TN,INACT,0.009999999776482582	CHEMBL3650258,TP,ACT,0.9800000190734863	CHEMBL1090032,TN,INACT,0.11999999731779099	CHEMBL2158066,TN,INACT,0.019999999552965164	CHEMBL1779431,TN,INACT,0.0	

