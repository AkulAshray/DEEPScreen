CNNModel CHEMBL1075284 adam 0.0005 15 256 0 0.8 False True
Number of active compounds :	147
Number of inactive compounds :	147
---------------------------------
Run id: CNNModel_CHEMBL1075284_adam_0.0005_15_256_0_0.8_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL1075284_adam_0.0005_15_256_0.8_True/
---------------------------------
Training samples: 188
Validation samples: 59
--
Training Step: 1  | time: 0.752s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/188
[A[ATraining Step: 2  | total loss: [1m[32m0.62365[0m[0m | time: 1.365s
[2K
| Adam | epoch: 001 | loss: 0.62365 - acc: 0.4781 -- iter: 064/188
[A[ATraining Step: 3  | total loss: [1m[32m0.68224[0m[0m | time: 1.975s
[2K
| Adam | epoch: 001 | loss: 0.68224 - acc: 0.4193 -- iter: 096/188
[A[ATraining Step: 4  | total loss: [1m[32m0.69025[0m[0m | time: 2.579s
[2K
| Adam | epoch: 001 | loss: 0.69025 - acc: 0.5501 -- iter: 128/188
[A[ATraining Step: 5  | total loss: [1m[32m0.69221[0m[0m | time: 3.177s
[2K
| Adam | epoch: 001 | loss: 0.69221 - acc: 0.5371 -- iter: 160/188
[A[ATraining Step: 6  | total loss: [1m[32m0.69209[0m[0m | time: 4.735s
[2K
| Adam | epoch: 001 | loss: 0.69209 - acc: 0.5735 | val_loss: 0.69239 - val_acc: 0.5254 -- iter: 188/188
--
Training Step: 7  | total loss: [1m[32m0.69264[0m[0m | time: 0.543s
[2K
| Adam | epoch: 002 | loss: 0.69264 - acc: 0.5294 -- iter: 032/188
[A[ATraining Step: 8  | total loss: [1m[32m0.69311[0m[0m | time: 1.144s
[2K
| Adam | epoch: 002 | loss: 0.69311 - acc: 0.5129 -- iter: 064/188
[A[ATraining Step: 9  | total loss: [1m[32m0.69354[0m[0m | time: 1.746s
[2K
| Adam | epoch: 002 | loss: 0.69354 - acc: 0.4895 -- iter: 096/188
[A[ATraining Step: 10  | total loss: [1m[32m0.69401[0m[0m | time: 2.353s
[2K
| Adam | epoch: 002 | loss: 0.69401 - acc: 0.4791 -- iter: 128/188
[A[ATraining Step: 11  | total loss: [1m[32m0.69379[0m[0m | time: 2.986s
[2K
| Adam | epoch: 002 | loss: 0.69379 - acc: 0.4742 -- iter: 160/188
[A[ATraining Step: 12  | total loss: [1m[32m0.69296[0m[0m | time: 4.632s
[2K
| Adam | epoch: 002 | loss: 0.69296 - acc: 0.5139 | val_loss: 0.69254 - val_acc: 0.5254 -- iter: 188/188
--
Training Step: 13  | total loss: [1m[32m0.69340[0m[0m | time: 0.542s
[2K
| Adam | epoch: 003 | loss: 0.69340 - acc: 0.4812 -- iter: 032/188
[A[ATraining Step: 14  | total loss: [1m[32m0.69290[0m[0m | time: 1.078s
[2K
| Adam | epoch: 003 | loss: 0.69290 - acc: 0.5035 -- iter: 064/188
[A[ATraining Step: 15  | total loss: [1m[32m0.69252[0m[0m | time: 1.725s
[2K
| Adam | epoch: 003 | loss: 0.69252 - acc: 0.5161 -- iter: 096/188
[A[ATraining Step: 16  | total loss: [1m[32m0.69190[0m[0m | time: 2.330s
[2K
| Adam | epoch: 003 | loss: 0.69190 - acc: 0.5335 -- iter: 128/188
[A[ATraining Step: 17  | total loss: [1m[32m0.69239[0m[0m | time: 2.956s
[2K
| Adam | epoch: 003 | loss: 0.69239 - acc: 0.5214 -- iter: 160/188
[A[ATraining Step: 18  | total loss: [1m[32m0.69133[0m[0m | time: 4.565s
[2K
| Adam | epoch: 003 | loss: 0.69133 - acc: 0.5248 | val_loss: 0.69006 - val_acc: 0.5254 -- iter: 188/188
--
Training Step: 19  | total loss: [1m[32m0.69399[0m[0m | time: 0.626s
[2K
| Adam | epoch: 004 | loss: 0.69399 - acc: 0.4853 -- iter: 032/188
[A[ATraining Step: 20  | total loss: [1m[32m0.69265[0m[0m | time: 1.155s
[2K
| Adam | epoch: 004 | loss: 0.69265 - acc: 0.5001 -- iter: 064/188
[A[ATraining Step: 21  | total loss: [1m[32m0.69100[0m[0m | time: 1.700s
[2K
| Adam | epoch: 004 | loss: 0.69100 - acc: 0.5222 -- iter: 096/188
[A[ATraining Step: 22  | total loss: [1m[32m0.68938[0m[0m | time: 2.307s
[2K
| Adam | epoch: 004 | loss: 0.68938 - acc: 0.5370 -- iter: 128/188
[A[ATraining Step: 23  | total loss: [1m[32m0.68917[0m[0m | time: 2.912s
[2K
| Adam | epoch: 004 | loss: 0.68917 - acc: 0.5262 -- iter: 160/188
[A[ATraining Step: 24  | total loss: [1m[32m0.68661[0m[0m | time: 4.507s
[2K
| Adam | epoch: 004 | loss: 0.68661 - acc: 0.5364 | val_loss: 0.68863 - val_acc: 0.5254 -- iter: 188/188
--
Training Step: 25  | total loss: [1m[32m0.67946[0m[0m | time: 0.618s
[2K
| Adam | epoch: 005 | loss: 0.67946 - acc: 0.5691 -- iter: 032/188
[A[ATraining Step: 26  | total loss: [1m[32m0.67980[0m[0m | time: 1.235s
[2K
| Adam | epoch: 005 | loss: 0.67980 - acc: 0.5591 -- iter: 064/188
[A[ATraining Step: 27  | total loss: [1m[32m0.69874[0m[0m | time: 1.749s
[2K
| Adam | epoch: 005 | loss: 0.69874 - acc: 0.5118 -- iter: 096/188
[A[ATraining Step: 28  | total loss: [1m[32m0.69190[0m[0m | time: 2.279s
[2K
| Adam | epoch: 005 | loss: 0.69190 - acc: 0.5267 -- iter: 128/188
[A[ATraining Step: 29  | total loss: [1m[32m0.68652[0m[0m | time: 2.883s
[2K
| Adam | epoch: 005 | loss: 0.68652 - acc: 0.5376 -- iter: 160/188
[A[ATraining Step: 30  | total loss: [1m[32m0.68983[0m[0m | time: 4.493s
[2K
| Adam | epoch: 005 | loss: 0.68983 - acc: 0.4917 | val_loss: 0.68537 - val_acc: 0.7627 -- iter: 188/188
--
Training Step: 31  | total loss: [1m[32m0.68767[0m[0m | time: 0.621s
[2K
| Adam | epoch: 006 | loss: 0.68767 - acc: 0.4864 -- iter: 032/188
[A[ATraining Step: 32  | total loss: [1m[32m0.68570[0m[0m | time: 1.226s
[2K
| Adam | epoch: 006 | loss: 0.68570 - acc: 0.5387 -- iter: 064/188
[A[ATraining Step: 33  | total loss: [1m[32m0.68440[0m[0m | time: 1.844s
[2K
| Adam | epoch: 006 | loss: 0.68440 - acc: 0.5988 -- iter: 096/188
[A[ATraining Step: 34  | total loss: [1m[32m0.68218[0m[0m | time: 2.376s
[2K
| Adam | epoch: 006 | loss: 0.68218 - acc: 0.6714 -- iter: 128/188
[A[ATraining Step: 35  | total loss: [1m[32m0.68161[0m[0m | time: 2.916s
[2K
| Adam | epoch: 006 | loss: 0.68161 - acc: 0.6654 -- iter: 160/188
[A[ATraining Step: 36  | total loss: [1m[32m0.68075[0m[0m | time: 4.526s
[2K
| Adam | epoch: 006 | loss: 0.68075 - acc: 0.6462 | val_loss: 0.65647 - val_acc: 0.5763 -- iter: 188/188
--
Training Step: 37  | total loss: [1m[32m0.67569[0m[0m | time: 0.604s
[2K
| Adam | epoch: 007 | loss: 0.67569 - acc: 0.6732 -- iter: 032/188
[A[ATraining Step: 38  | total loss: [1m[32m0.67048[0m[0m | time: 1.215s
[2K
| Adam | epoch: 007 | loss: 0.67048 - acc: 0.6515 -- iter: 064/188
[A[ATraining Step: 39  | total loss: [1m[32m0.66476[0m[0m | time: 1.821s
[2K
| Adam | epoch: 007 | loss: 0.66476 - acc: 0.6644 -- iter: 096/188
[A[ATraining Step: 40  | total loss: [1m[32m0.65447[0m[0m | time: 2.459s
[2K
| Adam | epoch: 007 | loss: 0.65447 - acc: 0.6863 -- iter: 128/188
[A[ATraining Step: 41  | total loss: [1m[32m0.64715[0m[0m | time: 2.986s
[2K
| Adam | epoch: 007 | loss: 0.64715 - acc: 0.6464 -- iter: 160/188
[A[ATraining Step: 42  | total loss: [1m[32m0.64447[0m[0m | time: 4.535s
[2K
| Adam | epoch: 007 | loss: 0.64447 - acc: 0.6779 | val_loss: 0.63498 - val_acc: 0.6271 -- iter: 188/188
--
Training Step: 43  | total loss: [1m[32m0.63546[0m[0m | time: 0.606s
[2K
| Adam | epoch: 008 | loss: 0.63546 - acc: 0.7158 -- iter: 032/188
[A[ATraining Step: 44  | total loss: [1m[32m0.62767[0m[0m | time: 1.208s
[2K
| Adam | epoch: 008 | loss: 0.62767 - acc: 0.7325 -- iter: 064/188
[A[ATraining Step: 45  | total loss: [1m[32m0.60276[0m[0m | time: 1.813s
[2K
| Adam | epoch: 008 | loss: 0.60276 - acc: 0.7727 -- iter: 096/188
[A[ATraining Step: 46  | total loss: [1m[32m0.58807[0m[0m | time: 2.417s
[2K
| Adam | epoch: 008 | loss: 0.58807 - acc: 0.7793 -- iter: 128/188
[A[ATraining Step: 47  | total loss: [1m[32m0.56598[0m[0m | time: 3.018s
[2K
| Adam | epoch: 008 | loss: 0.56598 - acc: 0.7847 -- iter: 160/188
[A[ATraining Step: 48  | total loss: [1m[32m0.56045[0m[0m | time: 4.557s
[2K
| Adam | epoch: 008 | loss: 0.56045 - acc: 0.7591 | val_loss: 0.48866 - val_acc: 0.7458 -- iter: 188/188
--
Training Step: 49  | total loss: [1m[32m0.53546[0m[0m | time: 0.547s
[2K
| Adam | epoch: 009 | loss: 0.53546 - acc: 0.7802 -- iter: 032/188
[A[ATraining Step: 50  | total loss: [1m[32m0.52249[0m[0m | time: 1.153s
[2K
| Adam | epoch: 009 | loss: 0.52249 - acc: 0.7755 -- iter: 064/188
[A[ATraining Step: 51  | total loss: [1m[32m0.49162[0m[0m | time: 1.764s
[2K
| Adam | epoch: 009 | loss: 0.49162 - acc: 0.7954 -- iter: 096/188
[A[ATraining Step: 52  | total loss: [1m[32m0.46550[0m[0m | time: 2.375s
[2K
| Adam | epoch: 009 | loss: 0.46550 - acc: 0.7980 -- iter: 128/188
[A[ATraining Step: 53  | total loss: [1m[32m0.46198[0m[0m | time: 2.976s
[2K
| Adam | epoch: 009 | loss: 0.46198 - acc: 0.7909 -- iter: 160/188
[A[ATraining Step: 54  | total loss: [1m[32m0.43601[0m[0m | time: 4.593s
[2K
| Adam | epoch: 009 | loss: 0.43601 - acc: 0.7986 | val_loss: 0.56406 - val_acc: 0.7458 -- iter: 188/188
--
Training Step: 55  | total loss: [1m[32m0.40755[0m[0m | time: 0.537s
[2K
| Adam | epoch: 010 | loss: 0.40755 - acc: 0.8184 -- iter: 032/188
[A[ATraining Step: 56  | total loss: [1m[32m0.39660[0m[0m | time: 1.076s
[2K
| Adam | epoch: 010 | loss: 0.39660 - acc: 0.8289 -- iter: 064/188
[A[ATraining Step: 57  | total loss: [1m[32m0.39254[0m[0m | time: 1.696s
[2K
| Adam | epoch: 010 | loss: 0.39254 - acc: 0.8378 -- iter: 096/188
[A[ATraining Step: 58  | total loss: [1m[32m0.36547[0m[0m | time: 2.300s
[2K
| Adam | epoch: 010 | loss: 0.36547 - acc: 0.8514 -- iter: 128/188
[A[ATraining Step: 59  | total loss: [1m[32m0.35289[0m[0m | time: 2.910s
[2K
| Adam | epoch: 010 | loss: 0.35289 - acc: 0.8629 -- iter: 160/188
[A[ATraining Step: 60  | total loss: [1m[32m0.33177[0m[0m | time: 4.529s
[2K
| Adam | epoch: 010 | loss: 0.33177 - acc: 0.8769 | val_loss: 0.38566 - val_acc: 0.8983 -- iter: 188/188
--
Training Step: 61  | total loss: [1m[32m0.32636[0m[0m | time: 0.604s
[2K
| Adam | epoch: 011 | loss: 0.32636 - acc: 0.8726 -- iter: 032/188
[A[ATraining Step: 62  | total loss: [1m[32m0.31217[0m[0m | time: 1.131s
[2K
| Adam | epoch: 011 | loss: 0.31217 - acc: 0.8809 -- iter: 064/188
[A[ATraining Step: 63  | total loss: [1m[32m0.29297[0m[0m | time: 1.658s
[2K
| Adam | epoch: 011 | loss: 0.29297 - acc: 0.8825 -- iter: 096/188
[A[ATraining Step: 64  | total loss: [1m[32m0.28126[0m[0m | time: 2.289s
[2K
| Adam | epoch: 011 | loss: 0.28126 - acc: 0.8838 -- iter: 128/188
[A[ATraining Step: 65  | total loss: [1m[32m0.26723[0m[0m | time: 2.901s
[2K
| Adam | epoch: 011 | loss: 0.26723 - acc: 0.8942 -- iter: 160/188
[A[ATraining Step: 66  | total loss: [1m[32m0.24846[0m[0m | time: 4.502s
[2K
| Adam | epoch: 011 | loss: 0.24846 - acc: 0.9033 | val_loss: 0.39180 - val_acc: 0.8305 -- iter: 188/188
--
Training Step: 67  | total loss: [1m[32m0.23244[0m[0m | time: 0.618s
[2K
| Adam | epoch: 012 | loss: 0.23244 - acc: 0.9112 -- iter: 032/188
[A[ATraining Step: 68  | total loss: [1m[32m0.24134[0m[0m | time: 1.218s
[2K
| Adam | epoch: 012 | loss: 0.24134 - acc: 0.9069 -- iter: 064/188
[A[ATraining Step: 69  | total loss: [1m[32m0.22887[0m[0m | time: 1.739s
[2K
| Adam | epoch: 012 | loss: 0.22887 - acc: 0.9105 -- iter: 096/188
[A[ATraining Step: 70  | total loss: [1m[32m0.22538[0m[0m | time: 2.277s
[2K
| Adam | epoch: 012 | loss: 0.22538 - acc: 0.9125 -- iter: 128/188
[A[ATraining Step: 71  | total loss: [1m[32m0.21832[0m[0m | time: 2.889s
[2K
| Adam | epoch: 012 | loss: 0.21832 - acc: 0.9144 -- iter: 160/188
[A[ATraining Step: 72  | total loss: [1m[32m0.20866[0m[0m | time: 4.491s
[2K
| Adam | epoch: 012 | loss: 0.20866 - acc: 0.9170 | val_loss: 0.39842 - val_acc: 0.8644 -- iter: 188/188
--
Training Step: 73  | total loss: [1m[32m0.19907[0m[0m | time: 0.605s
[2K
| Adam | epoch: 013 | loss: 0.19907 - acc: 0.9193 -- iter: 032/188
[A[ATraining Step: 74  | total loss: [1m[32m0.18720[0m[0m | time: 1.214s
[2K
| Adam | epoch: 013 | loss: 0.18720 - acc: 0.9247 -- iter: 064/188
[A[ATraining Step: 75  | total loss: [1m[32m0.19017[0m[0m | time: 1.816s
[2K
| Adam | epoch: 013 | loss: 0.19017 - acc: 0.9261 -- iter: 096/188
[A[ATraining Step: 76  | total loss: [1m[32m0.18372[0m[0m | time: 2.351s
[2K
| Adam | epoch: 013 | loss: 0.18372 - acc: 0.9340 -- iter: 128/188
[A[ATraining Step: 77  | total loss: [1m[32m0.21630[0m[0m | time: 2.903s
[2K
| Adam | epoch: 013 | loss: 0.21630 - acc: 0.9296 -- iter: 160/188
[A[ATraining Step: 78  | total loss: [1m[32m0.22859[0m[0m | time: 4.538s
[2K
| Adam | epoch: 013 | loss: 0.22859 - acc: 0.9258 | val_loss: 0.49698 - val_acc: 0.8644 -- iter: 188/188
--
Training Step: 79  | total loss: [1m[32m0.20930[0m[0m | time: 0.634s
[2K
| Adam | epoch: 014 | loss: 0.20930 - acc: 0.9335 -- iter: 032/188
[A[ATraining Step: 80  | total loss: [1m[32m0.22655[0m[0m | time: 1.270s
[2K
| Adam | epoch: 014 | loss: 0.22655 - acc: 0.9275 -- iter: 064/188
[A[ATraining Step: 81  | total loss: [1m[32m0.23455[0m[0m | time: 1.862s
[2K
| Adam | epoch: 014 | loss: 0.23455 - acc: 0.9222 -- iter: 096/188
[A[ATraining Step: 82  | total loss: [1m[32m0.21979[0m[0m | time: 2.459s
[2K
| Adam | epoch: 014 | loss: 0.21979 - acc: 0.9237 -- iter: 128/188
[A[ATraining Step: 83  | total loss: [1m[32m0.20968[0m[0m | time: 2.983s
[2K
| Adam | epoch: 014 | loss: 0.20968 - acc: 0.9251 -- iter: 160/188
[A[ATraining Step: 84  | total loss: [1m[32m0.20345[0m[0m | time: 4.509s
[2K
| Adam | epoch: 014 | loss: 0.20345 - acc: 0.9254 | val_loss: 0.46931 - val_acc: 0.9153 -- iter: 188/188
--
Training Step: 85  | total loss: [1m[32m0.20471[0m[0m | time: 0.603s
[2K
| Adam | epoch: 015 | loss: 0.20471 - acc: 0.9258 -- iter: 032/188
[A[ATraining Step: 86  | total loss: [1m[32m0.23219[0m[0m | time: 1.218s
[2K
| Adam | epoch: 015 | loss: 0.23219 - acc: 0.9176 -- iter: 064/188
[A[ATraining Step: 87  | total loss: [1m[32m0.22767[0m[0m | time: 1.831s
[2K
| Adam | epoch: 015 | loss: 0.22767 - acc: 0.9195 -- iter: 096/188
[A[ATraining Step: 88  | total loss: [1m[32m0.22047[0m[0m | time: 2.431s
[2K
| Adam | epoch: 015 | loss: 0.22047 - acc: 0.9213 -- iter: 128/188
[A[ATraining Step: 89  | total loss: [1m[32m0.21636[0m[0m | time: 3.035s
[2K
| Adam | epoch: 015 | loss: 0.21636 - acc: 0.9261 -- iter: 160/188
[A[ATraining Step: 90  | total loss: [1m[32m0.21014[0m[0m | time: 4.569s
[2K
| Adam | epoch: 015 | loss: 0.21014 - acc: 0.9272 | val_loss: 0.43789 - val_acc: 0.8644 -- iter: 188/188
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.9619815668202765
Validation AUPRC:0.9744779463147941
Test AUC:1.0
Test AUPRC:1.0
BestTestF1Score	0.98	0.97	0.98	1.0	0.97	29	0	29	1	0.95
BestTestMCCScore	0.98	0.97	0.98	1.0	0.97	29	0	29	1	0.95
BestTestAccuracyScore	0.98	0.97	0.98	1.0	0.97	29	0	29	1	0.95
BestValidationF1Score	0.94	0.9	0.95	1.0	0.89	25	0	31	3	0.95
BestValidationMCC	0.94	0.9	0.95	1.0	0.89	25	0	31	3	0.95
BestValidationAccuracy	0.94	0.9	0.95	1.0	0.89	25	0	31	3	0.95
TestPredictions (Threshold:0.95)
CHEMBL371024,TN,INACT,0.009999999776482582	CHEMBL3613349,TP,ACT,0.9800000190734863	CHEMBL2063282,TP,ACT,0.9599999785423279	CHEMBL3287881,TP,ACT,0.9800000190734863	CHEMBL137466,TN,INACT,0.8700000047683716	CHEMBL1939631,TP,ACT,0.9700000286102295	CHEMBL218383,TN,INACT,0.0	CHEMBL3233876,TP,ACT,0.9700000286102295	CHEMBL22525,TN,INACT,0.0	CHEMBL440392,TN,INACT,0.0	CHEMBL3287888,TP,ACT,0.9700000286102295	CHEMBL2441212,TP,ACT,0.9700000286102295	CHEMBL2165819,TP,ACT,0.9599999785423279	CHEMBL416183,TN,INACT,0.7099999785423279	CHEMBL430405,TP,ACT,0.9700000286102295	CHEMBL22424,TN,INACT,0.20000000298023224	CHEMBL3287884,TP,ACT,0.9700000286102295	CHEMBL2409568,TP,ACT,0.9700000286102295	CHEMBL422929,TN,INACT,0.0	CHEMBL172055,TN,INACT,0.6499999761581421	CHEMBL3613344,TP,ACT,0.9800000190734863	CHEMBL22725,TN,INACT,0.0	CHEMBL3287898,TP,ACT,0.9700000286102295	CHEMBL3342771,TP,ACT,0.9599999785423279	CHEMBL3233879,TP,ACT,0.9700000286102295	CHEMBL3287892,TP,ACT,0.9700000286102295	CHEMBL1089874,TP,ACT,0.9599999785423279	CHEMBL179528,TN,INACT,0.3700000047683716	CHEMBL2408623,TP,ACT,0.9700000286102295	CHEMBL3233886,TP,ACT,0.9700000286102295	CHEMBL2441213,FN,ACT,0.949999988079071	CHEMBL415036,TN,INACT,0.009999999776482582	CHEMBL3613340,TP,ACT,0.9800000190734863	CHEMBL2408629,TP,ACT,0.9599999785423279	CHEMBL335747,TN,INACT,0.029999999329447746	CHEMBL2036730,TP,ACT,0.9700000286102295	CHEMBL8805,TN,INACT,0.009999999776482582	CHEMBL197916,TN,INACT,0.09000000357627869	CHEMBL406139,TN,INACT,0.0	CHEMBL3287883,TP,ACT,0.9700000286102295	CHEMBL76780,TN,INACT,0.8100000023841858	CHEMBL3287889,TP,ACT,0.9700000286102295	CHEMBL335910,TN,INACT,0.0	CHEMBL8806,TN,INACT,0.0	CHEMBL200429,TN,INACT,0.0	CHEMBL3287877,TP,ACT,0.9700000286102295	CHEMBL22400,TN,INACT,0.019999999552965164	CHEMBL3233890,TP,ACT,0.9700000286102295	CHEMBL8951,TN,INACT,0.0	CHEMBL3287865,TP,ACT,0.9700000286102295	CHEMBL77054,TN,INACT,0.05000000074505806	CHEMBL22409,TN,INACT,0.44999998807907104	CHEMBL368497,TN,INACT,0.4000000059604645	CHEMBL2335549,TN,INACT,0.05000000074505806	CHEMBL2178950,TN,INACT,0.8100000023841858	CHEMBL3287886,TP,ACT,0.9800000190734863	CHEMBL2178940,TN,INACT,0.9300000071525574	CHEMBL408322,TN,INACT,0.019999999552965164	CHEMBL1089498,TP,ACT,0.9599999785423279	

