ImageNetInceptionV2 CHEMBL2955 adam 0.0005 15 0 0 0.8 False True
Number of active compounds :	133
Number of inactive compounds :	133
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL2955_adam_0.0005_15_0_0_0.8_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL2955_adam_0.0005_15_0.8/
---------------------------------
Training samples: 162
Validation samples: 51
--
Training Step: 1  | time: 67.730s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/162
[A[ATraining Step: 2  | total loss: [1m[32m0.64163[0m[0m | time: 81.578s
[2K
| Adam | epoch: 001 | loss: 0.64163 - acc: 0.4781 -- iter: 064/162
[A[ATraining Step: 3  | total loss: [1m[32m0.64876[0m[0m | time: 98.011s
[2K
| Adam | epoch: 001 | loss: 0.64876 - acc: 0.6750 -- iter: 096/162
[A[ATraining Step: 4  | total loss: [1m[32m0.55064[0m[0m | time: 116.701s
[2K
| Adam | epoch: 001 | loss: 0.55064 - acc: 0.7781 -- iter: 128/162
[A[ATraining Step: 5  | total loss: [1m[32m0.48423[0m[0m | time: 130.262s
[2K
| Adam | epoch: 001 | loss: 0.48423 - acc: 0.8019 -- iter: 160/162
[A[ATraining Step: 6  | total loss: [1m[32m0.40952[0m[0m | time: 146.949s
[2K
| Adam | epoch: 001 | loss: 0.40952 - acc: 0.8489 | val_loss: 2.20897 - val_acc: 0.5098 -- iter: 162/162
--
Training Step: 7  | total loss: [1m[32m0.17625[0m[0m | time: 2.083s
[2K
| Adam | epoch: 002 | loss: 0.17625 - acc: 0.9396 -- iter: 032/162
[A[ATraining Step: 8  | total loss: [1m[32m0.08200[0m[0m | time: 30.148s
[2K
| Adam | epoch: 002 | loss: 0.08200 - acc: 0.9736 -- iter: 064/162
[A[ATraining Step: 9  | total loss: [1m[32m0.33430[0m[0m | time: 46.656s
[2K
| Adam | epoch: 002 | loss: 0.33430 - acc: 0.8552 -- iter: 096/162
[A[ATraining Step: 10  | total loss: [1m[32m0.37664[0m[0m | time: 65.371s
[2K
| Adam | epoch: 002 | loss: 0.37664 - acc: 0.8339 -- iter: 128/162
[A[ATraining Step: 11  | total loss: [1m[32m0.29454[0m[0m | time: 83.480s
[2K
| Adam | epoch: 002 | loss: 0.29454 - acc: 0.8533 -- iter: 160/162
[A[ATraining Step: 12  | total loss: [1m[32m0.26881[0m[0m | time: 104.078s
[2K
| Adam | epoch: 002 | loss: 0.26881 - acc: 0.8772 | val_loss: 2.51922 - val_acc: 0.5098 -- iter: 162/162
--
Training Step: 13  | total loss: [1m[32m0.22897[0m[0m | time: 2.087s
[2K
| Adam | epoch: 003 | loss: 0.22897 - acc: 0.8896 -- iter: 032/162
[A[ATraining Step: 14  | total loss: [1m[32m0.53138[0m[0m | time: 4.299s
[2K
| Adam | epoch: 003 | loss: 0.53138 - acc: 0.5257 -- iter: 064/162
[A[ATraining Step: 15  | total loss: [1m[32m0.66663[0m[0m | time: 55.021s
[2K
| Adam | epoch: 003 | loss: 0.66663 - acc: 0.5156 -- iter: 096/162
[A[ATraining Step: 16  | total loss: [1m[32m0.55217[0m[0m | time: 67.469s
[2K
| Adam | epoch: 003 | loss: 0.55217 - acc: 0.6270 -- iter: 128/162
[A[ATraining Step: 17  | total loss: [1m[32m0.39574[0m[0m | time: 79.893s
[2K
| Adam | epoch: 003 | loss: 0.39574 - acc: 0.7500 -- iter: 160/162
[A[ATraining Step: 18  | total loss: [1m[32m0.31577[0m[0m | time: 95.872s
[2K
| Adam | epoch: 003 | loss: 0.31577 - acc: 0.8041 | val_loss: 4.29664 - val_acc: 0.5098 -- iter: 162/162
--
Training Step: 19  | total loss: [1m[32m0.26741[0m[0m | time: 12.718s
[2K
| Adam | epoch: 004 | loss: 0.26741 - acc: 0.8486 -- iter: 032/162
[A[ATraining Step: 20  | total loss: [1m[32m0.24750[0m[0m | time: 14.569s
[2K
| Adam | epoch: 004 | loss: 0.24750 - acc: 0.8571 -- iter: 064/162
[A[ATraining Step: 21  | total loss: [1m[32m0.17271[0m[0m | time: 16.601s
[2K
| Adam | epoch: 004 | loss: 0.17271 - acc: 0.9014 -- iter: 096/162
[A[ATraining Step: 22  | total loss: [1m[32m0.12611[0m[0m | time: 29.107s
[2K
| Adam | epoch: 004 | loss: 0.12611 - acc: 0.9310 -- iter: 128/162
[A[ATraining Step: 23  | total loss: [1m[32m0.12870[0m[0m | time: 41.561s
[2K
| Adam | epoch: 004 | loss: 0.12870 - acc: 0.9329 -- iter: 160/162
[A[ATraining Step: 24  | total loss: [1m[32m0.20375[0m[0m | time: 57.572s
[2K
| Adam | epoch: 004 | loss: 0.20375 - acc: 0.9078 | val_loss: 5.26806 - val_acc: 0.5098 -- iter: 162/162
--
Training Step: 25  | total loss: [1m[32m0.20740[0m[0m | time: 12.606s
[2K
| Adam | epoch: 005 | loss: 0.20740 - acc: 0.9159 -- iter: 032/162
[A[ATraining Step: 26  | total loss: [1m[32m0.16347[0m[0m | time: 25.359s
[2K
| Adam | epoch: 005 | loss: 0.16347 - acc: 0.9299 -- iter: 064/162
[A[ATraining Step: 27  | total loss: [1m[32m0.15396[0m[0m | time: 27.333s
[2K
| Adam | epoch: 005 | loss: 0.15396 - acc: 0.9399 -- iter: 096/162
[A[ATraining Step: 28  | total loss: [1m[32m0.11788[0m[0m | time: 29.322s
[2K
| Adam | epoch: 005 | loss: 0.11788 - acc: 0.9549 -- iter: 128/162
[A[ATraining Step: 29  | total loss: [1m[32m0.09276[0m[0m | time: 42.343s
[2K
| Adam | epoch: 005 | loss: 0.09276 - acc: 0.9659 -- iter: 160/162
[A[ATraining Step: 30  | total loss: [1m[32m0.07337[0m[0m | time: 58.337s
[2K
| Adam | epoch: 005 | loss: 0.07337 - acc: 0.9740 | val_loss: 4.32708 - val_acc: 0.5098 -- iter: 162/162
--
Training Step: 31  | total loss: [1m[32m0.12894[0m[0m | time: 12.302s
[2K
| Adam | epoch: 006 | loss: 0.12894 - acc: 0.9728 -- iter: 032/162
[A[ATraining Step: 32  | total loss: [1m[32m0.10661[0m[0m | time: 25.493s
[2K
| Adam | epoch: 006 | loss: 0.10661 - acc: 0.9789 -- iter: 064/162
[A[ATraining Step: 33  | total loss: [1m[32m0.08641[0m[0m | time: 38.453s
[2K
| Adam | epoch: 006 | loss: 0.08641 - acc: 0.9835 -- iter: 096/162
[A[ATraining Step: 34  | total loss: [1m[32m0.09328[0m[0m | time: 40.643s
[2K
| Adam | epoch: 006 | loss: 0.09328 - acc: 0.9804 -- iter: 128/162
[A[ATraining Step: 35  | total loss: [1m[32m0.26652[0m[0m | time: 42.948s
[2K
| Adam | epoch: 006 | loss: 0.26652 - acc: 0.8798 -- iter: 160/162
[A[ATraining Step: 36  | total loss: [1m[32m0.37967[0m[0m | time: 59.996s
[2K
| Adam | epoch: 006 | loss: 0.37967 - acc: 0.8021 | val_loss: 2.98632 - val_acc: 0.5098 -- iter: 162/162
--
Training Step: 37  | total loss: [1m[32m0.30556[0m[0m | time: 8.478s
[2K
| Adam | epoch: 007 | loss: 0.30556 - acc: 0.8417 -- iter: 032/162
[A[ATraining Step: 38  | total loss: [1m[32m0.44045[0m[0m | time: 16.855s
[2K
| Adam | epoch: 007 | loss: 0.44045 - acc: 0.8421 -- iter: 064/162
[A[ATraining Step: 39  | total loss: [1m[32m0.35989[0m[0m | time: 48.109s
[2K
| Adam | epoch: 007 | loss: 0.35989 - acc: 0.8723 -- iter: 096/162
[A[ATraining Step: 40  | total loss: [1m[32m0.29778[0m[0m | time: 60.471s
[2K
| Adam | epoch: 007 | loss: 0.29778 - acc: 0.8963 -- iter: 128/162
[A[ATraining Step: 41  | total loss: [1m[32m0.26473[0m[0m | time: 62.437s
[2K
| Adam | epoch: 007 | loss: 0.26473 - acc: 0.9038 -- iter: 160/162
[A[ATraining Step: 42  | total loss: [1m[32m0.54366[0m[0m | time: 67.850s
[2K
| Adam | epoch: 007 | loss: 0.54366 - acc: 0.8312 | val_loss: 2.49092 - val_acc: 0.5098 -- iter: 162/162
--
Training Step: 43  | total loss: [1m[32m0.66007[0m[0m | time: 12.144s
[2K
| Adam | epoch: 008 | loss: 0.66007 - acc: 0.7727 -- iter: 032/162
[A[ATraining Step: 44  | total loss: [1m[32m0.57774[0m[0m | time: 51.765s
[2K
| Adam | epoch: 008 | loss: 0.57774 - acc: 0.8012 -- iter: 064/162
[A[ATraining Step: 45  | total loss: [1m[32m0.51059[0m[0m | time: 63.880s
[2K
| Adam | epoch: 008 | loss: 0.51059 - acc: 0.8244 -- iter: 096/162
[A[ATraining Step: 46  | total loss: [1m[32m0.43563[0m[0m | time: 74.988s
[2K
| Adam | epoch: 008 | loss: 0.43563 - acc: 0.8536 -- iter: 128/162
[A[ATraining Step: 47  | total loss: [1m[32m0.38044[0m[0m | time: 86.386s
[2K
| Adam | epoch: 008 | loss: 0.38044 - acc: 0.8725 -- iter: 160/162
[A[ATraining Step: 48  | total loss: [1m[32m0.33333[0m[0m | time: 91.934s
[2K
| Adam | epoch: 008 | loss: 0.33333 - acc: 0.8880 | val_loss: 0.53036 - val_acc: 0.7059 -- iter: 162/162
--
Training Step: 49  | total loss: [1m[32m0.30081[0m[0m | time: 1.997s
[2K
| Adam | epoch: 009 | loss: 0.30081 - acc: 0.9056 -- iter: 032/162
[A[ATraining Step: 50  | total loss: [1m[32m0.25496[0m[0m | time: 32.537s
[2K
| Adam | epoch: 009 | loss: 0.25496 - acc: 0.9203 -- iter: 064/162
[A[ATraining Step: 51  | total loss: [1m[32m0.22161[0m[0m | time: 45.207s
[2K
| Adam | epoch: 009 | loss: 0.22161 - acc: 0.9324 -- iter: 096/162
[A[ATraining Step: 52  | total loss: [1m[32m0.23496[0m[0m | time: 57.821s
[2K
| Adam | epoch: 009 | loss: 0.23496 - acc: 0.9332 -- iter: 128/162
[A[ATraining Step: 53  | total loss: [1m[32m0.20445[0m[0m | time: 70.001s
[2K
| Adam | epoch: 009 | loss: 0.20445 - acc: 0.9431 -- iter: 160/162
[A[ATraining Step: 54  | total loss: [1m[32m0.18157[0m[0m | time: 85.788s
[2K
| Adam | epoch: 009 | loss: 0.18157 - acc: 0.9513 | val_loss: 0.49746 - val_acc: 0.8431 -- iter: 162/162
--
Training Step: 55  | total loss: [1m[32m0.17124[0m[0m | time: 1.979s
[2K
| Adam | epoch: 010 | loss: 0.17124 - acc: 0.9538 -- iter: 032/162
[A[ATraining Step: 56  | total loss: [1m[32m0.26697[0m[0m | time: 4.396s
[2K
| Adam | epoch: 010 | loss: 0.26697 - acc: 0.8900 -- iter: 064/162
[A[ATraining Step: 57  | total loss: [1m[32m0.42578[0m[0m | time: 16.390s
[2K
| Adam | epoch: 010 | loss: 0.42578 - acc: 0.8360 -- iter: 096/162
[A[ATraining Step: 58  | total loss: [1m[32m0.37223[0m[0m | time: 30.966s
[2K
| Adam | epoch: 010 | loss: 0.37223 - acc: 0.8584 -- iter: 128/162
[A[ATraining Step: 59  | total loss: [1m[32m0.32608[0m[0m | time: 43.134s
[2K
| Adam | epoch: 010 | loss: 0.32608 - acc: 0.8774 -- iter: 160/162
[A[ATraining Step: 60  | total loss: [1m[32m0.28650[0m[0m | time: 199.931s
[2K
| Adam | epoch: 010 | loss: 0.28650 - acc: 0.8936 | val_loss: 1.98078 - val_acc: 0.4902 -- iter: 162/162
--
Training Step: 61  | total loss: [1m[32m0.25100[0m[0m | time: 32.070s
[2K
| Adam | epoch: 011 | loss: 0.25100 - acc: 0.9075 -- iter: 032/162
[A[ATraining Step: 62  | total loss: [1m[32m0.22355[0m[0m | time: 35.504s
[2K
| Adam | epoch: 011 | loss: 0.22355 - acc: 0.9154 -- iter: 064/162
[A[ATraining Step: 63  | total loss: [1m[32m0.35512[0m[0m | time: 38.349s
[2K
| Adam | epoch: 011 | loss: 0.35512 - acc: 0.7993 -- iter: 096/162
[A[ATraining Step: 64  | total loss: [1m[32m0.43992[0m[0m | time: 133.039s
[2K
| Adam | epoch: 011 | loss: 0.43992 - acc: 0.7619 -- iter: 128/162
[A[ATraining Step: 65  | total loss: [1m[32m0.38664[0m[0m | time: 254.949s
[2K
| Adam | epoch: 011 | loss: 0.38664 - acc: 0.7913 -- iter: 160/162
[A[ATraining Step: 66  | total loss: [1m[32m0.36347[0m[0m | time: 290.386s
[2K
| Adam | epoch: 011 | loss: 0.36347 - acc: 0.8129 | val_loss: 1.87548 - val_acc: 0.5098 -- iter: 162/162
--
Training Step: 67  | total loss: [1m[32m0.32183[0m[0m | time: 22.765s
[2K
| Adam | epoch: 012 | loss: 0.32183 - acc: 0.8353 -- iter: 032/162
[A[ATraining Step: 68  | total loss: [1m[32m0.28519[0m[0m | time: 40.786s
[2K
| Adam | epoch: 012 | loss: 0.28519 - acc: 0.8548 -- iter: 064/162
[A[ATraining Step: 69  | total loss: [1m[32m0.25281[0m[0m | time: 44.085s
[2K
| Adam | epoch: 012 | loss: 0.25281 - acc: 0.8718 -- iter: 096/162
[A[ATraining Step: 70  | total loss: [1m[32m0.30701[0m[0m | time: 47.219s
[2K
| Adam | epoch: 012 | loss: 0.30701 - acc: 0.8289 -- iter: 128/162
[A[ATraining Step: 71  | total loss: [1m[32m0.34677[0m[0m | time: 105.118s
[2K
| Adam | epoch: 012 | loss: 0.34677 - acc: 0.7914 -- iter: 160/162
[A[ATraining Step: 72  | total loss: [1m[32m0.30884[0m[0m | time: 130.977s
[2K
| Adam | epoch: 012 | loss: 0.30884 - acc: 0.8149 | val_loss: 0.26447 - val_acc: 0.9020 -- iter: 162/162
--
Training Step: 73  | total loss: [1m[32m0.28993[0m[0m | time: 12.215s
[2K
| Adam | epoch: 013 | loss: 0.28993 - acc: 0.8285 -- iter: 032/162
[A[ATraining Step: 74  | total loss: [1m[32m0.26426[0m[0m | time: 24.813s
[2K
| Adam | epoch: 013 | loss: 0.26426 - acc: 0.8473 -- iter: 064/162
[A[ATraining Step: 75  | total loss: [1m[32m0.23836[0m[0m | time: 37.890s
[2K
| Adam | epoch: 013 | loss: 0.23836 - acc: 0.8639 -- iter: 096/162
[A[ATraining Step: 76  | total loss: [1m[32m0.22815[0m[0m | time: 40.633s
[2K
| Adam | epoch: 013 | loss: 0.22815 - acc: 0.8751 -- iter: 128/162
[A[ATraining Step: 77  | total loss: [1m[32m0.41135[0m[0m | time: 43.997s
[2K
| Adam | epoch: 013 | loss: 0.41135 - acc: 0.8354 -- iter: 160/162
[A[ATraining Step: 78  | total loss: [1m[32m0.50093[0m[0m | time: 74.857s
[2K
| Adam | epoch: 013 | loss: 0.50093 - acc: 0.8003 | val_loss: 0.57405 - val_acc: 0.7843 -- iter: 162/162
--
Training Step: 79  | total loss: [1m[32m0.45014[0m[0m | time: 22.019s
[2K
| Adam | epoch: 014 | loss: 0.45014 - acc: 0.8210 -- iter: 032/162
[A[ATraining Step: 80  | total loss: [1m[32m0.50414[0m[0m | time: 46.469s
[2K
| Adam | epoch: 014 | loss: 0.50414 - acc: 0.8233 -- iter: 064/162
[A[ATraining Step: 81  | total loss: [1m[32m0.45445[0m[0m | time: 66.665s
[2K
| Adam | epoch: 014 | loss: 0.45445 - acc: 0.8412 -- iter: 096/162
[A[ATraining Step: 82  | total loss: [1m[32m0.41189[0m[0m | time: 86.393s
[2K
| Adam | epoch: 014 | loss: 0.41189 - acc: 0.8570 -- iter: 128/162
[A[ATraining Step: 83  | total loss: [1m[32m0.37998[0m[0m | time: 89.494s
[2K
| Adam | epoch: 014 | loss: 0.37998 - acc: 0.8682 -- iter: 160/162
[A[ATraining Step: 84  | total loss: [1m[32m0.34234[0m[0m | time: 98.377s
[2K
| Adam | epoch: 014 | loss: 0.34234 - acc: 0.8814 | val_loss: 0.69768 - val_acc: 0.7255 -- iter: 162/162
--
Training Step: 85  | total loss: [1m[32m0.30840[0m[0m | time: 82.429s
[2K
| Adam | epoch: 015 | loss: 0.30840 - acc: 0.8933 -- iter: 032/162
[A[ATraining Step: 86  | total loss: [1m[32m0.28405[0m[0m | time: 195.136s
[2K
| Adam | epoch: 015 | loss: 0.28405 - acc: 0.9039 -- iter: 064/162
[A[ATraining Step: 87  | total loss: [1m[32m0.26107[0m[0m | time: 265.552s
[2K
| Adam | epoch: 015 | loss: 0.26107 - acc: 0.9135 -- iter: 096/162
[A[ATraining Step: 88  | total loss: [1m[32m0.23822[0m[0m | time: 319.659s
[2K
| Adam | epoch: 015 | loss: 0.23822 - acc: 0.9222 -- iter: 128/162
[A[ATraining Step: 89  | total loss: [1m[32m0.21777[0m[0m | time: 337.363s
[2K
| Adam | epoch: 015 | loss: 0.21777 - acc: 0.9300 -- iter: 160/162
[A[ATraining Step: 90  | total loss: [1m[32m0.20102[0m[0m | time: 343.205s
[2K
| Adam | epoch: 015 | loss: 0.20102 - acc: 0.9370 | val_loss: 0.29501 - val_acc: 0.9020 -- iter: 162/162
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.9892307692307692
Validation AUPRC:0.9903618794998104
Test AUC:0.9691358024691358
Test AUPRC:0.9715388137169892
BestTestF1Score	0.88	0.78	0.88	0.82	0.96	23	5	22	1	0.05
BestTestMCCScore	0.88	0.82	0.9	1.0	0.79	19	0	27	5	0.28
BestTestAccuracyScore	0.88	0.82	0.9	1.0	0.79	19	0	27	5	0.28
BestValidationF1Score	0.95	0.89	0.94	0.9	1.0	26	3	22	0	0.05
BestValidationMCC	0.94	0.89	0.94	1.0	0.88	23	0	25	3	0.28
BestValidationAccuracy	0.94	0.89	0.94	1.0	0.88	23	0	25	3	0.28
TestPredictions (Threshold:0.28)
CHEMBL1765671,TN,INACT,0.0	CHEMBL431172,TN,INACT,0.14000000059604645	CHEMBL117973,TP,ACT,0.9800000190734863	CHEMBL107680,TN,INACT,0.03999999910593033	CHEMBL279520,TN,INACT,0.0	CHEMBL119256,TP,ACT,0.9800000190734863	CHEMBL1765667,TN,INACT,0.0	CHEMBL241082,TN,INACT,0.15000000596046448	CHEMBL114478,TN,INACT,0.0	CHEMBL257547,TN,INACT,0.20000000298023224	CHEMBL1779732,TP,ACT,0.6100000143051147	CHEMBL3401375,TP,ACT,0.7099999785423279	CHEMBL95727,TN,INACT,0.05999999865889549	CHEMBL355851,TN,INACT,0.0	CHEMBL91283,FN,ACT,0.17000000178813934	CHEMBL302829,TN,INACT,0.009999999776482582	CHEMBL3401378,TP,ACT,0.7699999809265137	CHEMBL119110,TP,ACT,0.7200000286102295	CHEMBL332645,TN,INACT,0.0	CHEMBL334213,TP,ACT,0.9200000166893005	CHEMBL60435,TN,INACT,0.0	CHEMBL3401386,FN,ACT,0.25	CHEMBL115713,TP,ACT,0.9100000262260437	CHEMBL3401379,TP,ACT,0.7300000190734863	CHEMBL3401377,TP,ACT,0.8799999952316284	CHEMBL325198,TP,ACT,0.9700000286102295	CHEMBL1078642,TN,INACT,0.009999999776482582	CHEMBL11629,TN,INACT,0.029999999329447746	CHEMBL304888,TN,INACT,0.009999999776482582	CHEMBL302196,TN,INACT,0.0	CHEMBL461087,TN,INACT,0.0	CHEMBL3633665,TN,INACT,0.0	CHEMBL114976,TP,ACT,0.9300000071525574	CHEMBL595265,TN,INACT,0.0	CHEMBL332472,TP,ACT,0.3799999952316284	CHEMBL3759551,TP,ACT,0.5400000214576721	CHEMBL40986,TN,INACT,0.0	CHEMBL325050,TP,ACT,0.949999988079071	CHEMBL3618189,FN,ACT,0.07999999821186066	CHEMBL100624,TN,INACT,0.20000000298023224	CHEMBL1779916,FN,ACT,0.009999999776482582	CHEMBL1180343,TN,INACT,0.029999999329447746	CHEMBL3758803,FN,ACT,0.07000000029802322	CHEMBL129198,TN,INACT,0.029999999329447746	CHEMBL115970,TP,ACT,0.949999988079071	CHEMBL2042401,TN,INACT,0.0	CHEMBL115505,TP,ACT,0.800000011920929	CHEMBL3758775,TP,ACT,0.7200000286102295	CHEMBL117007,TP,ACT,0.9399999976158142	CHEMBL168223,TN,INACT,0.0	CHEMBL306645,TN,INACT,0.0	

