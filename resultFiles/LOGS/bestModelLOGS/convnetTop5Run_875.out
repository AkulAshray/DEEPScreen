ImageNetInceptionV2 CHEMBL4036 adam 0.0005 15 0 0 0.8 False True
Number of active compounds :	160
Number of inactive compounds :	160
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL4036_adam_0.0005_15_0_0_0.8_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL4036_adam_0.0005_15_0.8/
---------------------------------
Training samples: 197
Validation samples: 62
--
Training Step: 1  | time: 50.941s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/197
[A[ATraining Step: 2  | total loss: [1m[32m0.68605[0m[0m | time: 61.847s
[2K
| Adam | epoch: 001 | loss: 0.68605 - acc: 0.4781 -- iter: 064/197
[A[ATraining Step: 3  | total loss: [1m[32m0.67745[0m[0m | time: 74.417s
[2K
| Adam | epoch: 001 | loss: 0.67745 - acc: 0.5727 -- iter: 096/197
[A[ATraining Step: 4  | total loss: [1m[32m0.63299[0m[0m | time: 86.982s
[2K
| Adam | epoch: 001 | loss: 0.63299 - acc: 0.6822 -- iter: 128/197
[A[ATraining Step: 5  | total loss: [1m[32m0.65114[0m[0m | time: 96.585s
[2K
| Adam | epoch: 001 | loss: 0.65114 - acc: 0.6210 -- iter: 160/197
[A[ATraining Step: 6  | total loss: [1m[32m0.58503[0m[0m | time: 104.636s
[2K
| Adam | epoch: 001 | loss: 0.58503 - acc: 0.7240 -- iter: 192/197
[A[ATraining Step: 7  | total loss: [1m[32m0.54897[0m[0m | time: 115.733s
[2K
| Adam | epoch: 001 | loss: 0.54897 - acc: 0.7021 | val_loss: 1.71388 - val_acc: 0.4677 -- iter: 197/197
--
Training Step: 8  | total loss: [1m[32m0.44121[0m[0m | time: 2.109s
[2K
| Adam | epoch: 002 | loss: 0.44121 - acc: 0.8697 -- iter: 032/197
[A[ATraining Step: 9  | total loss: [1m[32m0.60375[0m[0m | time: 10.007s
[2K
| Adam | epoch: 002 | loss: 0.60375 - acc: 0.8328 -- iter: 064/197
[A[ATraining Step: 10  | total loss: [1m[32m0.45721[0m[0m | time: 18.771s
[2K
| Adam | epoch: 002 | loss: 0.45721 - acc: 0.8226 -- iter: 096/197
[A[ATraining Step: 11  | total loss: [1m[32m0.36180[0m[0m | time: 31.370s
[2K
| Adam | epoch: 002 | loss: 0.36180 - acc: 0.8770 -- iter: 128/197
[A[ATraining Step: 12  | total loss: [1m[32m0.43799[0m[0m | time: 43.830s
[2K
| Adam | epoch: 002 | loss: 0.43799 - acc: 0.8199 -- iter: 160/197
[A[ATraining Step: 13  | total loss: [1m[32m0.44005[0m[0m | time: 56.183s
[2K
| Adam | epoch: 002 | loss: 0.44005 - acc: 0.8569 -- iter: 192/197
[A[ATraining Step: 14  | total loss: [1m[32m0.38629[0m[0m | time: 71.823s
[2K
| Adam | epoch: 002 | loss: 0.38629 - acc: 0.8515 | val_loss: 0.88084 - val_acc: 0.4677 -- iter: 197/197
--
Training Step: 15  | total loss: [1m[32m0.34022[0m[0m | time: 3.846s
[2K
| Adam | epoch: 003 | loss: 0.34022 - acc: 0.8852 -- iter: 032/197
[A[ATraining Step: 16  | total loss: [1m[32m0.39004[0m[0m | time: 7.702s
[2K
| Adam | epoch: 003 | loss: 0.39004 - acc: 0.8532 -- iter: 064/197
[A[ATraining Step: 17  | total loss: [1m[32m0.26896[0m[0m | time: 23.993s
[2K
| Adam | epoch: 003 | loss: 0.26896 - acc: 0.9061 -- iter: 096/197
[A[ATraining Step: 18  | total loss: [1m[32m0.30126[0m[0m | time: 42.250s
[2K
| Adam | epoch: 003 | loss: 0.30126 - acc: 0.9061 -- iter: 128/197
[A[ATraining Step: 19  | total loss: [1m[32m0.29370[0m[0m | time: 63.119s
[2K
| Adam | epoch: 003 | loss: 0.29370 - acc: 0.8958 -- iter: 160/197
[A[ATraining Step: 20  | total loss: [1m[32m0.28620[0m[0m | time: 84.501s
[2K
| Adam | epoch: 003 | loss: 0.28620 - acc: 0.8991 -- iter: 192/197
[A[ATraining Step: 21  | total loss: [1m[32m0.26049[0m[0m | time: 111.392s
[2K
| Adam | epoch: 003 | loss: 0.26049 - acc: 0.9207 | val_loss: 1.22063 - val_acc: 0.5323 -- iter: 197/197
--
Training Step: 22  | total loss: [1m[32m0.25729[0m[0m | time: 17.719s
[2K
| Adam | epoch: 004 | loss: 0.25729 - acc: 0.9070 -- iter: 032/197
[A[ATraining Step: 23  | total loss: [1m[32m0.25762[0m[0m | time: 22.250s
[2K
| Adam | epoch: 004 | loss: 0.25762 - acc: 0.8886 -- iter: 064/197
[A[ATraining Step: 24  | total loss: [1m[32m0.42260[0m[0m | time: 27.300s
[2K
| Adam | epoch: 004 | loss: 0.42260 - acc: 0.8637 -- iter: 096/197
[A[ATraining Step: 25  | total loss: [1m[32m0.34072[0m[0m | time: 120.088s
[2K
| Adam | epoch: 004 | loss: 0.34072 - acc: 0.9009 -- iter: 128/197
[A[ATraining Step: 26  | total loss: [1m[32m0.29516[0m[0m | time: 211.511s
[2K
| Adam | epoch: 004 | loss: 0.29516 - acc: 0.9188 -- iter: 160/197
[A[ATraining Step: 27  | total loss: [1m[32m0.24166[0m[0m | time: 230.728s
[2K
| Adam | epoch: 004 | loss: 0.24166 - acc: 0.9317 -- iter: 192/197
[A[ATraining Step: 28  | total loss: [1m[32m0.31844[0m[0m | time: 252.073s
[2K
| Adam | epoch: 004 | loss: 0.31844 - acc: 0.9097 | val_loss: 2.97510 - val_acc: 0.5323 -- iter: 197/197
--
Training Step: 29  | total loss: [1m[32m0.26414[0m[0m | time: 20.655s
[2K
| Adam | epoch: 005 | loss: 0.26414 - acc: 0.9165 -- iter: 032/197
[A[ATraining Step: 30  | total loss: [1m[32m0.22532[0m[0m | time: 41.189s
[2K
| Adam | epoch: 005 | loss: 0.22532 - acc: 0.9288 -- iter: 064/197
[A[ATraining Step: 31  | total loss: [1m[32m0.18441[0m[0m | time: 47.195s
[2K
| Adam | epoch: 005 | loss: 0.18441 - acc: 0.9453 -- iter: 096/197
[A[ATraining Step: 32  | total loss: [1m[32m0.15603[0m[0m | time: 51.440s
[2K
| Adam | epoch: 005 | loss: 0.15603 - acc: 0.9576 -- iter: 128/197
[A[ATraining Step: 33  | total loss: [1m[32m0.13392[0m[0m | time: 61.228s
[2K
| Adam | epoch: 005 | loss: 0.13392 - acc: 0.9669 -- iter: 160/197
[A[ATraining Step: 34  | total loss: [1m[32m0.13864[0m[0m | time: 71.132s
[2K
| Adam | epoch: 005 | loss: 0.13864 - acc: 0.9606 -- iter: 192/197
[A[ATraining Step: 35  | total loss: [1m[32m0.11153[0m[0m | time: 95.847s
[2K
| Adam | epoch: 005 | loss: 0.11153 - acc: 0.9688 | val_loss: 3.94144 - val_acc: 0.5323 -- iter: 197/197
--
Training Step: 36  | total loss: [1m[32m0.09551[0m[0m | time: 20.678s
[2K
| Adam | epoch: 006 | loss: 0.09551 - acc: 0.9752 -- iter: 032/197
[A[ATraining Step: 37  | total loss: [1m[32m0.08103[0m[0m | time: 36.470s
[2K
| Adam | epoch: 006 | loss: 0.08103 - acc: 0.9802 -- iter: 064/197
[A[ATraining Step: 38  | total loss: [1m[32m0.07178[0m[0m | time: 49.215s
[2K
| Adam | epoch: 006 | loss: 0.07178 - acc: 0.9841 -- iter: 096/197
[A[ATraining Step: 39  | total loss: [1m[32m0.07949[0m[0m | time: 53.100s
[2K
| Adam | epoch: 006 | loss: 0.07949 - acc: 0.9751 -- iter: 128/197
[A[ATraining Step: 40  | total loss: [1m[32m0.28422[0m[0m | time: 56.775s
[2K
| Adam | epoch: 006 | loss: 0.28422 - acc: 0.9423 -- iter: 160/197
[A[ATraining Step: 41  | total loss: [1m[32m0.23228[0m[0m | time: 73.681s
[2K
| Adam | epoch: 006 | loss: 0.23228 - acc: 0.9529 -- iter: 192/197
[A[ATraining Step: 42  | total loss: [1m[32m0.20077[0m[0m | time: 90.491s
[2K
| Adam | epoch: 006 | loss: 0.20077 - acc: 0.9558 | val_loss: 5.46026 - val_acc: 0.5323 -- iter: 197/197
--
Training Step: 43  | total loss: [1m[32m0.21134[0m[0m | time: 13.038s
[2K
| Adam | epoch: 007 | loss: 0.21134 - acc: 0.9415 -- iter: 032/197
[A[ATraining Step: 44  | total loss: [1m[32m0.19747[0m[0m | time: 27.160s
[2K
| Adam | epoch: 007 | loss: 0.19747 - acc: 0.9462 -- iter: 064/197
[A[ATraining Step: 45  | total loss: [1m[32m0.18137[0m[0m | time: 44.974s
[2K
| Adam | epoch: 007 | loss: 0.18137 - acc: 0.9500 -- iter: 096/197
[A[ATraining Step: 46  | total loss: [1m[32m0.16693[0m[0m | time: 62.991s
[2K
| Adam | epoch: 007 | loss: 0.16693 - acc: 0.9532 -- iter: 128/197
[A[ATraining Step: 47  | total loss: [1m[32m0.14378[0m[0m | time: 68.587s
[2K
| Adam | epoch: 007 | loss: 0.14378 - acc: 0.9608 -- iter: 160/197
[A[ATraining Step: 48  | total loss: [1m[32m0.12104[0m[0m | time: 74.181s
[2K
| Adam | epoch: 007 | loss: 0.12104 - acc: 0.9671 -- iter: 192/197
[A[ATraining Step: 49  | total loss: [1m[32m0.10225[0m[0m | time: 97.058s
[2K
| Adam | epoch: 007 | loss: 0.10225 - acc: 0.9723 | val_loss: 2.44384 - val_acc: 0.6290 -- iter: 197/197
--
Training Step: 50  | total loss: [1m[32m0.08714[0m[0m | time: 14.844s
[2K
| Adam | epoch: 008 | loss: 0.08714 - acc: 0.9766 -- iter: 032/197
[A[ATraining Step: 51  | total loss: [1m[32m0.09759[0m[0m | time: 33.107s
[2K
| Adam | epoch: 008 | loss: 0.09759 - acc: 0.9659 -- iter: 064/197
[A[ATraining Step: 52  | total loss: [1m[32m0.10262[0m[0m | time: 51.187s
[2K
| Adam | epoch: 008 | loss: 0.10262 - acc: 0.9663 -- iter: 096/197
[A[ATraining Step: 53  | total loss: [1m[32m0.10756[0m[0m | time: 68.989s
[2K
| Adam | epoch: 008 | loss: 0.10756 - acc: 0.9621 -- iter: 128/197
[A[ATraining Step: 54  | total loss: [1m[32m0.09606[0m[0m | time: 85.847s
[2K
| Adam | epoch: 008 | loss: 0.09606 - acc: 0.9676 -- iter: 160/197
[A[ATraining Step: 55  | total loss: [1m[32m0.09423[0m[0m | time: 89.544s
[2K
| Adam | epoch: 008 | loss: 0.09423 - acc: 0.9677 -- iter: 192/197
[A[ATraining Step: 56  | total loss: [1m[32m0.08986[0m[0m | time: 98.477s
[2K
| Adam | epoch: 008 | loss: 0.08986 - acc: 0.9723 | val_loss: 2.12851 - val_acc: 0.5484 -- iter: 197/197
--
Training Step: 57  | total loss: [1m[32m0.07793[0m[0m | time: 16.071s
[2K
| Adam | epoch: 009 | loss: 0.07793 - acc: 0.9761 -- iter: 032/197
[A[ATraining Step: 58  | total loss: [1m[32m0.08936[0m[0m | time: 33.643s
[2K
| Adam | epoch: 009 | loss: 0.08936 - acc: 0.9708 -- iter: 064/197
[A[ATraining Step: 59  | total loss: [1m[32m0.08085[0m[0m | time: 51.149s
[2K
| Adam | epoch: 009 | loss: 0.08085 - acc: 0.9748 -- iter: 096/197
[A[ATraining Step: 60  | total loss: [1m[32m0.08242[0m[0m | time: 67.116s
[2K
| Adam | epoch: 009 | loss: 0.08242 - acc: 0.9698 -- iter: 128/197
[A[ATraining Step: 61  | total loss: [1m[32m0.07710[0m[0m | time: 80.131s
[2K
| Adam | epoch: 009 | loss: 0.07710 - acc: 0.9697 -- iter: 160/197
[A[ATraining Step: 62  | total loss: [1m[32m0.08474[0m[0m | time: 93.622s
[2K
| Adam | epoch: 009 | loss: 0.08474 - acc: 0.9696 -- iter: 192/197
[A[ATraining Step: 63  | total loss: [1m[32m0.07963[0m[0m | time: 105.220s
[2K
| Adam | epoch: 009 | loss: 0.07963 - acc: 0.9695 | val_loss: 3.45810 - val_acc: 0.5000 -- iter: 197/197
--
Training Step: 64  | total loss: [1m[32m0.54050[0m[0m | time: 3.616s
[2K
| Adam | epoch: 010 | loss: 0.54050 - acc: 0.9233 -- iter: 032/197
[A[ATraining Step: 65  | total loss: [1m[32m0.49702[0m[0m | time: 16.070s
[2K
| Adam | epoch: 010 | loss: 0.49702 - acc: 0.9081 -- iter: 064/197
[A[ATraining Step: 66  | total loss: [1m[32m0.44192[0m[0m | time: 25.303s
[2K
| Adam | epoch: 010 | loss: 0.44192 - acc: 0.9155 -- iter: 096/197
[A[ATraining Step: 67  | total loss: [1m[32m0.38986[0m[0m | time: 38.476s
[2K
| Adam | epoch: 010 | loss: 0.38986 - acc: 0.9256 -- iter: 128/197
[A[ATraining Step: 68  | total loss: [1m[32m0.35199[0m[0m | time: 51.364s
[2K
| Adam | epoch: 010 | loss: 0.35199 - acc: 0.9307 -- iter: 160/197
[A[ATraining Step: 69  | total loss: [1m[32m0.32304[0m[0m | time: 67.528s
[2K
| Adam | epoch: 010 | loss: 0.32304 - acc: 0.9315 -- iter: 192/197
[A[ATraining Step: 70  | total loss: [1m[32m0.28806[0m[0m | time: 83.986s
[2K
| Adam | epoch: 010 | loss: 0.28806 - acc: 0.9394 | val_loss: 0.62266 - val_acc: 0.6935 -- iter: 197/197
--
Training Step: 71  | total loss: [1m[32m0.25714[0m[0m | time: 2.072s
[2K
| Adam | epoch: 011 | loss: 0.25714 - acc: 0.9463 -- iter: 032/197
[A[ATraining Step: 72  | total loss: [1m[32m0.45290[0m[0m | time: 4.131s
[2K
| Adam | epoch: 011 | loss: 0.45290 - acc: 0.9299 -- iter: 064/197
[A[ATraining Step: 73  | total loss: [1m[32m0.40641[0m[0m | time: 12.067s
[2K
| Adam | epoch: 011 | loss: 0.40641 - acc: 0.9376 -- iter: 096/197
[A[ATraining Step: 74  | total loss: [1m[32m0.36471[0m[0m | time: 24.181s
[2K
| Adam | epoch: 011 | loss: 0.36471 - acc: 0.9445 -- iter: 128/197
[A[ATraining Step: 75  | total loss: [1m[32m0.33686[0m[0m | time: 37.121s
[2K
| Adam | epoch: 011 | loss: 0.33686 - acc: 0.9437 -- iter: 160/197
[A[ATraining Step: 76  | total loss: [1m[32m0.30592[0m[0m | time: 49.864s
[2K
| Adam | epoch: 011 | loss: 0.30592 - acc: 0.9498 -- iter: 192/197
[A[ATraining Step: 77  | total loss: [1m[32m0.27892[0m[0m | time: 67.468s
[2K
| Adam | epoch: 011 | loss: 0.27892 - acc: 0.9551 | val_loss: 0.63182 - val_acc: 0.6935 -- iter: 197/197
--
Training Step: 78  | total loss: [1m[32m0.25238[0m[0m | time: 8.176s
[2K
| Adam | epoch: 012 | loss: 0.25238 - acc: 0.9598 -- iter: 032/197
[A[ATraining Step: 79  | total loss: [1m[32m0.22951[0m[0m | time: 10.228s
[2K
| Adam | epoch: 012 | loss: 0.22951 - acc: 0.9639 -- iter: 064/197
[A[ATraining Step: 80  | total loss: [1m[32m0.20802[0m[0m | time: 12.242s
[2K
| Adam | epoch: 012 | loss: 0.20802 - acc: 0.9676 -- iter: 096/197
[A[ATraining Step: 81  | total loss: [1m[32m0.18949[0m[0m | time: 20.163s
[2K
| Adam | epoch: 012 | loss: 0.18949 - acc: 0.9709 -- iter: 128/197
[A[ATraining Step: 82  | total loss: [1m[32m0.18131[0m[0m | time: 28.209s
[2K
| Adam | epoch: 012 | loss: 0.18131 - acc: 0.9707 -- iter: 160/197
[A[ATraining Step: 83  | total loss: [1m[32m0.17596[0m[0m | time: 40.432s
[2K
| Adam | epoch: 012 | loss: 0.17596 - acc: 0.9705 -- iter: 192/197
[A[ATraining Step: 84  | total loss: [1m[32m0.16065[0m[0m | time: 58.704s
[2K
| Adam | epoch: 012 | loss: 0.16065 - acc: 0.9734 | val_loss: 0.62092 - val_acc: 0.7419 -- iter: 197/197
--
Training Step: 85  | total loss: [1m[32m0.14692[0m[0m | time: 12.613s
[2K
| Adam | epoch: 013 | loss: 0.14692 - acc: 0.9761 -- iter: 032/197
[A[ATraining Step: 86  | total loss: [1m[32m0.13510[0m[0m | time: 23.495s
[2K
| Adam | epoch: 013 | loss: 0.13510 - acc: 0.9785 -- iter: 064/197
[A[ATraining Step: 87  | total loss: [1m[32m0.12235[0m[0m | time: 25.508s
[2K
| Adam | epoch: 013 | loss: 0.12235 - acc: 0.9806 -- iter: 096/197
[A[ATraining Step: 88  | total loss: [1m[32m0.11026[0m[0m | time: 27.564s
[2K
| Adam | epoch: 013 | loss: 0.11026 - acc: 0.9826 -- iter: 128/197
[A[ATraining Step: 89  | total loss: [1m[32m0.09959[0m[0m | time: 35.637s
[2K
| Adam | epoch: 013 | loss: 0.09959 - acc: 0.9843 -- iter: 160/197
[A[ATraining Step: 90  | total loss: [1m[32m0.09083[0m[0m | time: 44.774s
[2K
| Adam | epoch: 013 | loss: 0.09083 - acc: 0.9859 -- iter: 192/197
[A[ATraining Step: 91  | total loss: [1m[32m0.08217[0m[0m | time: 61.669s
[2K
| Adam | epoch: 013 | loss: 0.08217 - acc: 0.9873 | val_loss: 0.82579 - val_acc: 0.6774 -- iter: 197/197
--
Training Step: 92  | total loss: [1m[32m0.07465[0m[0m | time: 12.514s
[2K
| Adam | epoch: 014 | loss: 0.07465 - acc: 0.9886 -- iter: 032/197
[A[ATraining Step: 93  | total loss: [1m[32m0.07088[0m[0m | time: 25.213s
[2K
| Adam | epoch: 014 | loss: 0.07088 - acc: 0.9866 -- iter: 064/197
[A[ATraining Step: 94  | total loss: [1m[32m0.06442[0m[0m | time: 36.236s
[2K
| Adam | epoch: 014 | loss: 0.06442 - acc: 0.9879 -- iter: 096/197
[A[ATraining Step: 95  | total loss: [1m[32m0.05845[0m[0m | time: 40.045s
[2K
| Adam | epoch: 014 | loss: 0.05845 - acc: 0.9891 -- iter: 128/197
[A[ATraining Step: 96  | total loss: [1m[32m0.05466[0m[0m | time: 42.233s
[2K
| Adam | epoch: 014 | loss: 0.05466 - acc: 0.9902 -- iter: 160/197
[A[ATraining Step: 97  | total loss: [1m[32m0.04966[0m[0m | time: 50.095s
[2K
| Adam | epoch: 014 | loss: 0.04966 - acc: 0.9912 -- iter: 192/197
[A[ATraining Step: 98  | total loss: [1m[32m0.04700[0m[0m | time: 61.000s
[2K
| Adam | epoch: 014 | loss: 0.04700 - acc: 0.9921 | val_loss: 0.82873 - val_acc: 0.7419 -- iter: 197/197
--
Training Step: 99  | total loss: [1m[32m0.04249[0m[0m | time: 13.165s
[2K
| Adam | epoch: 015 | loss: 0.04249 - acc: 0.9929 -- iter: 032/197
[A[ATraining Step: 100  | total loss: [1m[32m0.03882[0m[0m | time: 25.893s
[2K
| Adam | epoch: 015 | loss: 0.03882 - acc: 0.9936 -- iter: 064/197
[A[ATraining Step: 101  | total loss: [1m[32m0.03507[0m[0m | time: 39.041s
[2K
| Adam | epoch: 015 | loss: 0.03507 - acc: 0.9942 -- iter: 096/197
[A[ATraining Step: 102  | total loss: [1m[32m0.03169[0m[0m | time: 51.914s
[2K
| Adam | epoch: 015 | loss: 0.03169 - acc: 0.9948 -- iter: 128/197
[A[ATraining Step: 103  | total loss: [1m[32m0.02911[0m[0m | time: 55.614s
[2K
| Adam | epoch: 015 | loss: 0.02911 - acc: 0.9953 -- iter: 160/197
[A[ATraining Step: 104  | total loss: [1m[32m0.02908[0m[0m | time: 59.251s
[2K
| Adam | epoch: 015 | loss: 0.02908 - acc: 0.9958 -- iter: 192/197
[A[ATraining Step: 105  | total loss: [1m[32m0.02628[0m[0m | time: 74.259s
[2K
| Adam | epoch: 015 | loss: 0.02628 - acc: 0.9962 | val_loss: 0.89173 - val_acc: 0.7581 -- iter: 197/197
--
Validation AUC:0.845350052246604
Validation AUPRC:0.8692783137145749
Test AUC:0.8020833333333334
Test AUPRC:0.7299605379223274
BestTestF1Score	0.85	0.66	0.82	0.77	0.94	30	9	21	2	0.3
BestTestMCCScore	0.85	0.66	0.82	0.77	0.94	30	9	21	2	0.3
BestTestAccuracyScore	0.68	0.39	0.69	0.74	0.62	20	7	23	12	0.94
BestValidationF1Score	0.82	0.57	0.77	0.72	0.94	31	12	17	2	0.3
BestValidationMCC	0.82	0.57	0.77	0.72	0.94	31	12	17	2	0.3
BestValidationAccuracy	0.78	0.55	0.77	0.81	0.76	25	6	23	8	0.94
TestPredictions (Threshold:0.3)
CHEMBL115220,FN,ACT,0.019999999552965164	CHEMBL469770,TN,INACT,0.019999999552965164	CHEMBL26564,TP,ACT,0.9900000095367432	CHEMBL2164696,TN,INACT,0.0	CHEMBL3673435,TP,ACT,0.4399999976158142	CHEMBL599519,FP,INACT,1.0	CHEMBL427901,TP,ACT,0.9900000095367432	CHEMBL515051,TN,INACT,0.0	CHEMBL2426399,TP,ACT,1.0	CHEMBL186342,TP,ACT,1.0	CHEMBL23327,TP,ACT,1.0	CHEMBL240799,FN,ACT,0.009999999776482582	CHEMBL3774503,TP,ACT,1.0	CHEMBL1784660,FP,INACT,0.7300000190734863	CHEMBL1767294,FP,INACT,0.9900000095367432	CHEMBL3297832,TP,ACT,0.4000000059604645	CHEMBL188172,TP,ACT,0.949999988079071	CHEMBL2392223,TN,INACT,0.0	CHEMBL428690,TP,ACT,0.8399999737739563	CHEMBL2392392,TN,INACT,0.0	CHEMBL185498,TP,ACT,1.0	CHEMBL3774632,TP,ACT,1.0	CHEMBL363283,TP,ACT,1.0	CHEMBL3651157,TP,ACT,0.9100000262260437	CHEMBL103667,TP,ACT,1.0	CHEMBL23317,TP,ACT,1.0	CHEMBL560278,TN,INACT,0.019999999552965164	CHEMBL2392355,FP,INACT,0.9599999785423279	CHEMBL102622,FP,INACT,0.4099999964237213	CHEMBL187903,TP,ACT,0.9300000071525574	CHEMBL2392234,TN,INACT,0.009999999776482582	CHEMBL561136,TN,INACT,0.0	CHEMBL384304,TP,ACT,0.4099999964237213	CHEMBL186201,TP,ACT,1.0	CHEMBL504550,FP,INACT,1.0	CHEMBL1288069,TN,INACT,0.10999999940395355	CHEMBL445813,TP,ACT,0.9900000095367432	CHEMBL2392388,TN,INACT,0.07000000029802322	CHEMBL1784649,TN,INACT,0.09000000357627869	CHEMBL3651155,FP,INACT,1.0	CHEMBL2321948,TP,ACT,0.8299999833106995	CHEMBL563948,TN,INACT,0.019999999552965164	CHEMBL2392366,TN,INACT,0.23999999463558197	CHEMBL2392233,TN,INACT,0.009999999776482582	CHEMBL518732,TN,INACT,0.14000000059604645	CHEMBL3085252,TP,ACT,1.0	CHEMBL457191,TN,INACT,0.0	CHEMBL75680,TP,ACT,0.7099999785423279	CHEMBL521734,FP,INACT,1.0	CHEMBL434917,TP,ACT,0.9200000166893005	CHEMBL2321965,TP,ACT,0.949999988079071	CHEMBL522760,TN,INACT,0.20999999344348907	CHEMBL1734241,FP,INACT,1.0	CHEMBL2392385,TN,INACT,0.10000000149011612	CHEMBL3775467,TP,ACT,1.0	CHEMBL457180,TN,INACT,0.009999999776482582	CHEMBL3774434,TP,ACT,0.30000001192092896	CHEMBL258805,TP,ACT,0.949999988079071	CHEMBL2321878,TP,ACT,0.9800000190734863	CHEMBL3775223,TP,ACT,1.0	CHEMBL102136,TN,INACT,0.27000001072883606	CHEMBL498248,TN,INACT,0.0	

