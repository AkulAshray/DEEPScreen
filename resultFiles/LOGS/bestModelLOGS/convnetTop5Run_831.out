CNNModel CHEMBL4077 adam 0.0005 15 32 0 0.6 False True
Number of active compounds :	273
Number of inactive compounds :	273
---------------------------------
Run id: CNNModel_CHEMBL4077_adam_0.0005_15_32_0_0.6_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL4077_adam_0.0005_15_32_0.6_True/
---------------------------------
Training samples: 315
Validation samples: 99
--
Training Step: 1  | time: 1.294s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/315
[A[ATraining Step: 2  | total loss: [1m[32m0.62404[0m[0m | time: 2.142s
[2K
| Adam | epoch: 001 | loss: 0.62404 - acc: 0.3094 -- iter: 064/315
[A[ATraining Step: 3  | total loss: [1m[32m0.68429[0m[0m | time: 3.031s
[2K
| Adam | epoch: 001 | loss: 0.68429 - acc: 0.3119 -- iter: 096/315
[A[ATraining Step: 4  | total loss: [1m[32m0.69022[0m[0m | time: 3.982s
[2K
| Adam | epoch: 001 | loss: 0.69022 - acc: 0.4999 -- iter: 128/315
[A[ATraining Step: 5  | total loss: [1m[32m0.69155[0m[0m | time: 4.936s
[2K
| Adam | epoch: 001 | loss: 0.69155 - acc: 0.5432 -- iter: 160/315
[A[ATraining Step: 6  | total loss: [1m[32m0.69288[0m[0m | time: 5.956s
[2K
| Adam | epoch: 001 | loss: 0.69288 - acc: 0.4953 -- iter: 192/315
[A[ATraining Step: 7  | total loss: [1m[32m0.69290[0m[0m | time: 7.297s
[2K
| Adam | epoch: 001 | loss: 0.69290 - acc: 0.4981 -- iter: 224/315
[A[ATraining Step: 8  | total loss: [1m[32m0.69300[0m[0m | time: 8.527s
[2K
| Adam | epoch: 001 | loss: 0.69300 - acc: 0.4992 -- iter: 256/315
[A[ATraining Step: 9  | total loss: [1m[32m0.69472[0m[0m | time: 9.534s
[2K
| Adam | epoch: 001 | loss: 0.69472 - acc: 0.4500 -- iter: 288/315
[A[ATraining Step: 10  | total loss: [1m[32m0.69211[0m[0m | time: 11.260s
[2K
| Adam | epoch: 001 | loss: 0.69211 - acc: 0.5531 | val_loss: 0.69246 - val_acc: 0.5253 -- iter: 315/315
--
Training Step: 11  | total loss: [1m[32m0.69437[0m[0m | time: 1.017s
[2K
| Adam | epoch: 002 | loss: 0.69437 - acc: 0.4666 -- iter: 032/315
[A[ATraining Step: 12  | total loss: [1m[32m0.69293[0m[0m | time: 1.971s
[2K
| Adam | epoch: 002 | loss: 0.69293 - acc: 0.5233 -- iter: 064/315
[A[ATraining Step: 13  | total loss: [1m[32m0.69363[0m[0m | time: 2.874s
[2K
| Adam | epoch: 002 | loss: 0.69363 - acc: 0.4865 -- iter: 096/315
[A[ATraining Step: 14  | total loss: [1m[32m0.69347[0m[0m | time: 3.849s
[2K
| Adam | epoch: 002 | loss: 0.69347 - acc: 0.4920 -- iter: 128/315
[A[ATraining Step: 15  | total loss: [1m[32m0.69206[0m[0m | time: 4.736s
[2K
| Adam | epoch: 002 | loss: 0.69206 - acc: 0.5563 -- iter: 160/315
[A[ATraining Step: 16  | total loss: [1m[32m0.69267[0m[0m | time: 5.368s
[2K
| Adam | epoch: 002 | loss: 0.69267 - acc: 0.5352 -- iter: 192/315
[A[ATraining Step: 17  | total loss: [1m[32m0.69365[0m[0m | time: 5.989s
[2K
| Adam | epoch: 002 | loss: 0.69365 - acc: 0.5000 -- iter: 224/315
[A[ATraining Step: 18  | total loss: [1m[32m0.69312[0m[0m | time: 6.612s
[2K
| Adam | epoch: 002 | loss: 0.69312 - acc: 0.5108 -- iter: 256/315
[A[ATraining Step: 19  | total loss: [1m[32m0.69459[0m[0m | time: 7.225s
[2K
| Adam | epoch: 002 | loss: 0.69459 - acc: 0.4656 -- iter: 288/315
[A[ATraining Step: 20  | total loss: [1m[32m0.69480[0m[0m | time: 8.840s
[2K
| Adam | epoch: 002 | loss: 0.69480 - acc: 0.4565 | val_loss: 0.69224 - val_acc: 0.5253 -- iter: 315/315
--
Training Step: 21  | total loss: [1m[32m0.69330[0m[0m | time: 0.547s
[2K
| Adam | epoch: 003 | loss: 0.69330 - acc: 0.5088 -- iter: 032/315
[A[ATraining Step: 22  | total loss: [1m[32m0.69262[0m[0m | time: 1.066s
[2K
| Adam | epoch: 003 | loss: 0.69262 - acc: 0.5339 -- iter: 064/315
[A[ATraining Step: 23  | total loss: [1m[32m0.69273[0m[0m | time: 1.687s
[2K
| Adam | epoch: 003 | loss: 0.69273 - acc: 0.5295 -- iter: 096/315
[A[ATraining Step: 24  | total loss: [1m[32m0.69238[0m[0m | time: 2.338s
[2K
| Adam | epoch: 003 | loss: 0.69238 - acc: 0.5300 -- iter: 128/315
[A[ATraining Step: 25  | total loss: [1m[32m0.69215[0m[0m | time: 2.983s
[2K
| Adam | epoch: 003 | loss: 0.69215 - acc: 0.5303 -- iter: 160/315
[A[ATraining Step: 26  | total loss: [1m[32m0.69046[0m[0m | time: 3.618s
[2K
| Adam | epoch: 003 | loss: 0.69046 - acc: 0.5637 -- iter: 192/315
[A[ATraining Step: 27  | total loss: [1m[32m0.69314[0m[0m | time: 4.231s
[2K
| Adam | epoch: 003 | loss: 0.69314 - acc: 0.5071 -- iter: 224/315
[A[ATraining Step: 28  | total loss: [1m[32m0.69488[0m[0m | time: 4.867s
[2K
| Adam | epoch: 003 | loss: 0.69488 - acc: 0.4741 -- iter: 256/315
[A[ATraining Step: 29  | total loss: [1m[32m0.69262[0m[0m | time: 5.514s
[2K
| Adam | epoch: 003 | loss: 0.69262 - acc: 0.5108 -- iter: 288/315
[A[ATraining Step: 30  | total loss: [1m[32m0.69210[0m[0m | time: 7.171s
[2K
| Adam | epoch: 003 | loss: 0.69210 - acc: 0.5156 | val_loss: 0.68975 - val_acc: 0.5253 -- iter: 315/315
--
Training Step: 31  | total loss: [1m[32m0.69123[0m[0m | time: 0.646s
[2K
| Adam | epoch: 004 | loss: 0.69123 - acc: 0.5265 -- iter: 032/315
[A[ATraining Step: 32  | total loss: [1m[32m0.69276[0m[0m | time: 1.206s
[2K
| Adam | epoch: 004 | loss: 0.69276 - acc: 0.4994 -- iter: 064/315
[A[ATraining Step: 33  | total loss: [1m[32m0.69195[0m[0m | time: 1.790s
[2K
| Adam | epoch: 004 | loss: 0.69195 - acc: 0.5036 -- iter: 096/315
[A[ATraining Step: 34  | total loss: [1m[32m0.69342[0m[0m | time: 2.470s
[2K
| Adam | epoch: 004 | loss: 0.69342 - acc: 0.4830 -- iter: 128/315
[A[ATraining Step: 35  | total loss: [1m[32m0.69123[0m[0m | time: 3.141s
[2K
| Adam | epoch: 004 | loss: 0.69123 - acc: 0.5127 -- iter: 160/315
[A[ATraining Step: 36  | total loss: [1m[32m0.69091[0m[0m | time: 4.614s
[2K
| Adam | epoch: 004 | loss: 0.69091 - acc: 0.5101 -- iter: 192/315
[A[ATraining Step: 37  | total loss: [1m[32m0.68800[0m[0m | time: 5.924s
[2K
| Adam | epoch: 004 | loss: 0.68800 - acc: 0.5393 -- iter: 224/315
[A[ATraining Step: 38  | total loss: [1m[32m0.69241[0m[0m | time: 7.137s
[2K
| Adam | epoch: 004 | loss: 0.69241 - acc: 0.4950 -- iter: 256/315
[A[ATraining Step: 39  | total loss: [1m[32m0.69403[0m[0m | time: 8.903s
[2K
| Adam | epoch: 004 | loss: 0.69403 - acc: 0.4720 -- iter: 288/315
[A[ATraining Step: 40  | total loss: [1m[32m0.69412[0m[0m | time: 13.153s
[2K
| Adam | epoch: 004 | loss: 0.69412 - acc: 0.4655 | val_loss: 0.68613 - val_acc: 0.5253 -- iter: 315/315
--
Training Step: 41  | total loss: [1m[32m0.69096[0m[0m | time: 1.258s
[2K
| Adam | epoch: 005 | loss: 0.69096 - acc: 0.5063 -- iter: 032/315
[A[ATraining Step: 42  | total loss: [1m[32m0.68968[0m[0m | time: 2.361s
[2K
| Adam | epoch: 005 | loss: 0.68968 - acc: 0.5164 -- iter: 064/315
[A[ATraining Step: 43  | total loss: [1m[32m0.68930[0m[0m | time: 3.323s
[2K
| Adam | epoch: 005 | loss: 0.68930 - acc: 0.5135 -- iter: 096/315
[A[ATraining Step: 44  | total loss: [1m[32m0.69021[0m[0m | time: 4.620s
[2K
| Adam | epoch: 005 | loss: 0.69021 - acc: 0.4951 -- iter: 128/315
[A[ATraining Step: 45  | total loss: [1m[32m0.68999[0m[0m | time: 5.932s
[2K
| Adam | epoch: 005 | loss: 0.68999 - acc: 0.4865 -- iter: 160/315
[A[ATraining Step: 46  | total loss: [1m[32m0.68700[0m[0m | time: 7.161s
[2K
| Adam | epoch: 005 | loss: 0.68700 - acc: 0.5044 -- iter: 192/315
[A[ATraining Step: 47  | total loss: [1m[32m0.68320[0m[0m | time: 10.984s
[2K
| Adam | epoch: 005 | loss: 0.68320 - acc: 0.5293 -- iter: 224/315
[A[ATraining Step: 48  | total loss: [1m[32m0.67902[0m[0m | time: 13.695s
[2K
| Adam | epoch: 005 | loss: 0.67902 - acc: 0.5446 -- iter: 256/315
[A[ATraining Step: 49  | total loss: [1m[32m0.67650[0m[0m | time: 14.742s
[2K
| Adam | epoch: 005 | loss: 0.67650 - acc: 0.5425 -- iter: 288/315
[A[ATraining Step: 50  | total loss: [1m[32m0.68057[0m[0m | time: 16.887s
[2K
| Adam | epoch: 005 | loss: 0.68057 - acc: 0.5311 | val_loss: 0.65783 - val_acc: 0.5253 -- iter: 315/315
--
Training Step: 51  | total loss: [1m[32m0.68905[0m[0m | time: 1.062s
[2K
| Adam | epoch: 006 | loss: 0.68905 - acc: 0.5073 -- iter: 032/315
[A[ATraining Step: 52  | total loss: [1m[32m0.68624[0m[0m | time: 2.193s
[2K
| Adam | epoch: 006 | loss: 0.68624 - acc: 0.4968 -- iter: 064/315
[A[ATraining Step: 53  | total loss: [1m[32m0.68361[0m[0m | time: 3.375s
[2K
| Adam | epoch: 006 | loss: 0.68361 - acc: 0.5019 -- iter: 096/315
[A[ATraining Step: 54  | total loss: [1m[32m0.68323[0m[0m | time: 4.210s
[2K
| Adam | epoch: 006 | loss: 0.68323 - acc: 0.5379 -- iter: 128/315
[A[ATraining Step: 55  | total loss: [1m[32m0.68280[0m[0m | time: 5.003s
[2K
| Adam | epoch: 006 | loss: 0.68280 - acc: 0.5722 -- iter: 160/315
[A[ATraining Step: 56  | total loss: [1m[32m0.68304[0m[0m | time: 6.011s
[2K
| Adam | epoch: 006 | loss: 0.68304 - acc: 0.5959 -- iter: 192/315
[A[ATraining Step: 57  | total loss: [1m[32m0.68090[0m[0m | time: 7.001s
[2K
| Adam | epoch: 006 | loss: 0.68090 - acc: 0.6259 -- iter: 224/315
[A[ATraining Step: 58  | total loss: [1m[32m0.67975[0m[0m | time: 8.136s
[2K
| Adam | epoch: 006 | loss: 0.67975 - acc: 0.6300 -- iter: 256/315
[A[ATraining Step: 59  | total loss: [1m[32m0.67574[0m[0m | time: 9.052s
[2K
| Adam | epoch: 006 | loss: 0.67574 - acc: 0.6335 -- iter: 288/315
[A[ATraining Step: 60  | total loss: [1m[32m0.66969[0m[0m | time: 11.349s
[2K
| Adam | epoch: 006 | loss: 0.66969 - acc: 0.6283 | val_loss: 0.62952 - val_acc: 0.5253 -- iter: 315/315
--
Training Step: 61  | total loss: [1m[32m0.65830[0m[0m | time: 1.069s
[2K
| Adam | epoch: 007 | loss: 0.65830 - acc: 0.6401 -- iter: 032/315
[A[ATraining Step: 62  | total loss: [1m[32m0.66802[0m[0m | time: 2.096s
[2K
| Adam | epoch: 007 | loss: 0.66802 - acc: 0.6060 -- iter: 064/315
[A[ATraining Step: 63  | total loss: [1m[32m0.65178[0m[0m | time: 3.073s
[2K
| Adam | epoch: 007 | loss: 0.65178 - acc: 0.6084 -- iter: 096/315
[A[ATraining Step: 64  | total loss: [1m[32m0.63957[0m[0m | time: 4.241s
[2K
| Adam | epoch: 007 | loss: 0.63957 - acc: 0.6378 -- iter: 128/315
[A[ATraining Step: 65  | total loss: [1m[32m0.64048[0m[0m | time: 5.130s
[2K
| Adam | epoch: 007 | loss: 0.64048 - acc: 0.6324 -- iter: 160/315
[A[ATraining Step: 66  | total loss: [1m[32m0.63489[0m[0m | time: 5.768s
[2K
| Adam | epoch: 007 | loss: 0.63489 - acc: 0.6411 -- iter: 192/315
[A[ATraining Step: 67  | total loss: [1m[32m0.62573[0m[0m | time: 6.428s
[2K
| Adam | epoch: 007 | loss: 0.62573 - acc: 0.6530 -- iter: 224/315
[A[ATraining Step: 68  | total loss: [1m[32m0.61603[0m[0m | time: 7.105s
[2K
| Adam | epoch: 007 | loss: 0.61603 - acc: 0.6608 -- iter: 256/315
[A[ATraining Step: 69  | total loss: [1m[32m0.61681[0m[0m | time: 7.765s
[2K
| Adam | epoch: 007 | loss: 0.61681 - acc: 0.6603 -- iter: 288/315
[A[ATraining Step: 70  | total loss: [1m[32m0.59956[0m[0m | time: 9.442s
[2K
| Adam | epoch: 007 | loss: 0.59956 - acc: 0.6706 | val_loss: 0.46271 - val_acc: 0.7677 -- iter: 315/315
--
Training Step: 71  | total loss: [1m[32m0.58266[0m[0m | time: 0.657s
[2K
| Adam | epoch: 008 | loss: 0.58266 - acc: 0.6903 -- iter: 032/315
[A[ATraining Step: 72  | total loss: [1m[32m0.56316[0m[0m | time: 1.334s
[2K
| Adam | epoch: 008 | loss: 0.56316 - acc: 0.7041 -- iter: 064/315
[A[ATraining Step: 73  | total loss: [1m[32m0.56109[0m[0m | time: 2.018s
[2K
| Adam | epoch: 008 | loss: 0.56109 - acc: 0.7022 -- iter: 096/315
[A[ATraining Step: 74  | total loss: [1m[32m0.53384[0m[0m | time: 2.691s
[2K
| Adam | epoch: 008 | loss: 0.53384 - acc: 0.7212 -- iter: 128/315
[A[ATraining Step: 75  | total loss: [1m[32m0.52597[0m[0m | time: 3.338s
[2K
| Adam | epoch: 008 | loss: 0.52597 - acc: 0.7277 -- iter: 160/315
[A[ATraining Step: 76  | total loss: [1m[32m0.51872[0m[0m | time: 3.927s
[2K
| Adam | epoch: 008 | loss: 0.51872 - acc: 0.7335 -- iter: 192/315
[A[ATraining Step: 77  | total loss: [1m[32m0.50337[0m[0m | time: 4.497s
[2K
| Adam | epoch: 008 | loss: 0.50337 - acc: 0.7499 -- iter: 224/315
[A[ATraining Step: 78  | total loss: [1m[32m0.55279[0m[0m | time: 5.179s
[2K
| Adam | epoch: 008 | loss: 0.55279 - acc: 0.7373 -- iter: 256/315
[A[ATraining Step: 79  | total loss: [1m[32m0.55897[0m[0m | time: 5.820s
[2K
| Adam | epoch: 008 | loss: 0.55897 - acc: 0.7289 -- iter: 288/315
[A[ATraining Step: 80  | total loss: [1m[32m0.53449[0m[0m | time: 7.513s
[2K
| Adam | epoch: 008 | loss: 0.53449 - acc: 0.7471 | val_loss: 0.52740 - val_acc: 0.7071 -- iter: 315/315
--
Training Step: 81  | total loss: [1m[32m0.50686[0m[0m | time: 0.660s
[2K
| Adam | epoch: 009 | loss: 0.50686 - acc: 0.7632 -- iter: 032/315
[A[ATraining Step: 82  | total loss: [1m[32m0.51006[0m[0m | time: 1.329s
[2K
| Adam | epoch: 009 | loss: 0.51006 - acc: 0.7587 -- iter: 064/315
[A[ATraining Step: 83  | total loss: [1m[32m0.50522[0m[0m | time: 1.972s
[2K
| Adam | epoch: 009 | loss: 0.50522 - acc: 0.7610 -- iter: 096/315
[A[ATraining Step: 84  | total loss: [1m[32m0.49951[0m[0m | time: 3.237s
[2K
| Adam | epoch: 009 | loss: 0.49951 - acc: 0.7599 -- iter: 128/315
[A[ATraining Step: 85  | total loss: [1m[32m0.49753[0m[0m | time: 4.586s
[2K
| Adam | epoch: 009 | loss: 0.49753 - acc: 0.7620 -- iter: 160/315
[A[ATraining Step: 86  | total loss: [1m[32m0.47712[0m[0m | time: 5.897s
[2K
| Adam | epoch: 009 | loss: 0.47712 - acc: 0.7733 -- iter: 192/315
[A[ATraining Step: 87  | total loss: [1m[32m0.45366[0m[0m | time: 7.996s
[2K
| Adam | epoch: 009 | loss: 0.45366 - acc: 0.7866 -- iter: 224/315
[A[ATraining Step: 88  | total loss: [1m[32m0.44488[0m[0m | time: 11.640s
[2K
| Adam | epoch: 009 | loss: 0.44488 - acc: 0.7968 -- iter: 256/315
[A[ATraining Step: 89  | total loss: [1m[32m0.44737[0m[0m | time: 16.706s
[2K
| Adam | epoch: 009 | loss: 0.44737 - acc: 0.7986 -- iter: 288/315
[A[ATraining Step: 90  | total loss: [1m[32m0.43259[0m[0m | time: 18.795s
[2K
| Adam | epoch: 009 | loss: 0.43259 - acc: 0.8063 | val_loss: 0.50019 - val_acc: 0.7980 -- iter: 315/315
--
Training Step: 91  | total loss: [1m[32m0.42048[0m[0m | time: 1.053s
[2K
| Adam | epoch: 010 | loss: 0.42048 - acc: 0.8131 -- iter: 032/315
[A[ATraining Step: 92  | total loss: [1m[32m0.42665[0m[0m | time: 2.420s
[2K
| Adam | epoch: 010 | loss: 0.42665 - acc: 0.8100 -- iter: 064/315
[A[ATraining Step: 93  | total loss: [1m[32m0.41486[0m[0m | time: 3.638s
[2K
| Adam | epoch: 010 | loss: 0.41486 - acc: 0.8133 -- iter: 096/315
[A[ATraining Step: 94  | total loss: [1m[32m0.39692[0m[0m | time: 4.701s
[2K
| Adam | epoch: 010 | loss: 0.39692 - acc: 0.8226 -- iter: 128/315
[A[ATraining Step: 95  | total loss: [1m[32m0.39086[0m[0m | time: 5.541s
[2K
| Adam | epoch: 010 | loss: 0.39086 - acc: 0.8279 -- iter: 160/315
[A[ATraining Step: 96  | total loss: [1m[32m0.41490[0m[0m | time: 6.703s
[2K
| Adam | epoch: 010 | loss: 0.41490 - acc: 0.8138 -- iter: 192/315
[A[ATraining Step: 97  | total loss: [1m[32m0.43542[0m[0m | time: 7.878s
[2K
| Adam | epoch: 010 | loss: 0.43542 - acc: 0.7981 -- iter: 224/315
[A[ATraining Step: 98  | total loss: [1m[32m0.43742[0m[0m | time: 8.934s
[2K
| Adam | epoch: 010 | loss: 0.43742 - acc: 0.7964 -- iter: 256/315
[A[ATraining Step: 99  | total loss: [1m[32m0.43102[0m[0m | time: 9.947s
[2K
| Adam | epoch: 010 | loss: 0.43102 - acc: 0.8019 -- iter: 288/315
[A[ATraining Step: 100  | total loss: [1m[32m0.40551[0m[0m | time: 12.254s
[2K
| Adam | epoch: 010 | loss: 0.40551 - acc: 0.8180 | val_loss: 0.64506 - val_acc: 0.7677 -- iter: 315/315
--
Training Step: 101  | total loss: [1m[32m0.42058[0m[0m | time: 1.186s
[2K
| Adam | epoch: 011 | loss: 0.42058 - acc: 0.8144 -- iter: 032/315
[A[ATraining Step: 102  | total loss: [1m[32m0.42808[0m[0m | time: 2.320s
[2K
| Adam | epoch: 011 | loss: 0.42808 - acc: 0.8110 -- iter: 064/315
[A[ATraining Step: 103  | total loss: [1m[32m0.43902[0m[0m | time: 3.123s
[2K
| Adam | epoch: 011 | loss: 0.43902 - acc: 0.8081 -- iter: 096/315
[A[ATraining Step: 104  | total loss: [1m[32m0.43706[0m[0m | time: 4.133s
[2K
| Adam | epoch: 011 | loss: 0.43706 - acc: 0.8179 -- iter: 128/315
[A[ATraining Step: 105  | total loss: [1m[32m0.41756[0m[0m | time: 5.193s
[2K
| Adam | epoch: 011 | loss: 0.41756 - acc: 0.8298 -- iter: 160/315
[A[ATraining Step: 106  | total loss: [1m[32m0.41053[0m[0m | time: 6.276s
[2K
| Adam | epoch: 011 | loss: 0.41053 - acc: 0.8312 -- iter: 192/315
[A[ATraining Step: 107  | total loss: [1m[32m0.41163[0m[0m | time: 7.410s
[2K
| Adam | epoch: 011 | loss: 0.41163 - acc: 0.8262 -- iter: 224/315
[A[ATraining Step: 108  | total loss: [1m[32m0.41582[0m[0m | time: 8.640s
[2K
| Adam | epoch: 011 | loss: 0.41582 - acc: 0.8249 -- iter: 256/315
[A[ATraining Step: 109  | total loss: [1m[32m0.39568[0m[0m | time: 9.623s
[2K
| Adam | epoch: 011 | loss: 0.39568 - acc: 0.8361 -- iter: 288/315
[A[ATraining Step: 110  | total loss: [1m[32m0.37429[0m[0m | time: 11.459s
[2K
| Adam | epoch: 011 | loss: 0.37429 - acc: 0.8451 | val_loss: 0.50907 - val_acc: 0.8081 -- iter: 315/315
--
Training Step: 111  | total loss: [1m[32m0.37470[0m[0m | time: 1.155s
[2K
| Adam | epoch: 012 | loss: 0.37470 - acc: 0.8421 -- iter: 032/315
[A[ATraining Step: 112  | total loss: [1m[32m0.37640[0m[0m | time: 2.160s
[2K
| Adam | epoch: 012 | loss: 0.37640 - acc: 0.8485 -- iter: 064/315
[A[ATraining Step: 113  | total loss: [1m[32m0.38052[0m[0m | time: 3.301s
[2K
| Adam | epoch: 012 | loss: 0.38052 - acc: 0.8511 -- iter: 096/315
[A[ATraining Step: 114  | total loss: [1m[32m0.37034[0m[0m | time: 4.379s
[2K
| Adam | epoch: 012 | loss: 0.37034 - acc: 0.8473 -- iter: 128/315
[A[ATraining Step: 115  | total loss: [1m[32m0.35816[0m[0m | time: 5.079s
[2K
| Adam | epoch: 012 | loss: 0.35816 - acc: 0.8532 -- iter: 160/315
[A[ATraining Step: 116  | total loss: [1m[32m0.36765[0m[0m | time: 5.747s
[2K
| Adam | epoch: 012 | loss: 0.36765 - acc: 0.8397 -- iter: 192/315
[A[ATraining Step: 117  | total loss: [1m[32m0.35870[0m[0m | time: 6.420s
[2K
| Adam | epoch: 012 | loss: 0.35870 - acc: 0.8433 -- iter: 224/315
[A[ATraining Step: 118  | total loss: [1m[32m0.35160[0m[0m | time: 7.066s
[2K
| Adam | epoch: 012 | loss: 0.35160 - acc: 0.8402 -- iter: 256/315
[A[ATraining Step: 119  | total loss: [1m[32m0.34279[0m[0m | time: 7.737s
[2K
| Adam | epoch: 012 | loss: 0.34279 - acc: 0.8468 -- iter: 288/315
[A[ATraining Step: 120  | total loss: [1m[32m0.33007[0m[0m | time: 9.308s
[2K
| Adam | epoch: 012 | loss: 0.33007 - acc: 0.8527 | val_loss: 0.44994 - val_acc: 0.8485 -- iter: 315/315
--
Training Step: 121  | total loss: [1m[32m0.31854[0m[0m | time: 0.602s
[2K
| Adam | epoch: 013 | loss: 0.31854 - acc: 0.8564 -- iter: 032/315
[A[ATraining Step: 122  | total loss: [1m[32m0.32150[0m[0m | time: 1.249s
[2K
| Adam | epoch: 013 | loss: 0.32150 - acc: 0.8596 -- iter: 064/315
[A[ATraining Step: 123  | total loss: [1m[32m0.32346[0m[0m | time: 1.894s
[2K
| Adam | epoch: 013 | loss: 0.32346 - acc: 0.8580 -- iter: 096/315
[A[ATraining Step: 124  | total loss: [1m[32m0.30370[0m[0m | time: 2.581s
[2K
| Adam | epoch: 013 | loss: 0.30370 - acc: 0.8660 -- iter: 128/315
[A[ATraining Step: 125  | total loss: [1m[32m0.29014[0m[0m | time: 3.282s
[2K
| Adam | epoch: 013 | loss: 0.29014 - acc: 0.8762 -- iter: 160/315
[A[ATraining Step: 126  | total loss: [1m[32m0.28578[0m[0m | time: 3.960s
[2K
| Adam | epoch: 013 | loss: 0.28578 - acc: 0.8792 -- iter: 192/315
[A[ATraining Step: 127  | total loss: [1m[32m0.27510[0m[0m | time: 4.637s
[2K
| Adam | epoch: 013 | loss: 0.27510 - acc: 0.8819 -- iter: 224/315
[A[ATraining Step: 128  | total loss: [1m[32m0.26504[0m[0m | time: 5.280s
[2K
| Adam | epoch: 013 | loss: 0.26504 - acc: 0.8875 -- iter: 256/315
[A[ATraining Step: 129  | total loss: [1m[32m0.26003[0m[0m | time: 5.941s
[2K
| Adam | epoch: 013 | loss: 0.26003 - acc: 0.8925 -- iter: 288/315
[A[ATraining Step: 130  | total loss: [1m[32m0.24903[0m[0m | time: 7.596s
[2K
| Adam | epoch: 013 | loss: 0.24903 - acc: 0.9001 | val_loss: 0.36148 - val_acc: 0.8788 -- iter: 315/315
--
Training Step: 131  | total loss: [1m[32m0.25816[0m[0m | time: 0.580s
[2K
| Adam | epoch: 014 | loss: 0.25816 - acc: 0.8976 -- iter: 032/315
[A[ATraining Step: 132  | total loss: [1m[32m0.24546[0m[0m | time: 1.144s
[2K
| Adam | epoch: 014 | loss: 0.24546 - acc: 0.9041 -- iter: 064/315
[A[ATraining Step: 133  | total loss: [1m[32m0.25674[0m[0m | time: 1.826s
[2K
| Adam | epoch: 014 | loss: 0.25674 - acc: 0.9063 -- iter: 096/315
[A[ATraining Step: 134  | total loss: [1m[32m0.24795[0m[0m | time: 2.509s
[2K
| Adam | epoch: 014 | loss: 0.24795 - acc: 0.9094 -- iter: 128/315
[A[ATraining Step: 135  | total loss: [1m[32m0.24632[0m[0m | time: 3.557s
[2K
| Adam | epoch: 014 | loss: 0.24632 - acc: 0.9091 -- iter: 160/315
[A[ATraining Step: 136  | total loss: [1m[32m0.24774[0m[0m | time: 4.814s
[2K
| Adam | epoch: 014 | loss: 0.24774 - acc: 0.9088 -- iter: 192/315
[A[ATraining Step: 137  | total loss: [1m[32m0.24817[0m[0m | time: 5.768s
[2K
| Adam | epoch: 014 | loss: 0.24817 - acc: 0.8992 -- iter: 224/315
[A[ATraining Step: 138  | total loss: [1m[32m0.24426[0m[0m | time: 6.801s
[2K
| Adam | epoch: 014 | loss: 0.24426 - acc: 0.9030 -- iter: 256/315
[A[ATraining Step: 139  | total loss: [1m[32m0.23746[0m[0m | time: 7.797s
[2K
| Adam | epoch: 014 | loss: 0.23746 - acc: 0.9096 -- iter: 288/315
[A[ATraining Step: 140  | total loss: [1m[32m0.23459[0m[0m | time: 9.901s
[2K
| Adam | epoch: 014 | loss: 0.23459 - acc: 0.9124 | val_loss: 0.38748 - val_acc: 0.8687 -- iter: 315/315
--
Training Step: 141  | total loss: [1m[32m0.22198[0m[0m | time: 1.383s
[2K
| Adam | epoch: 015 | loss: 0.22198 - acc: 0.9180 -- iter: 032/315
[A[ATraining Step: 142  | total loss: [1m[32m0.22278[0m[0m | time: 2.661s
[2K
| Adam | epoch: 015 | loss: 0.22278 - acc: 0.9169 -- iter: 064/315
[A[ATraining Step: 143  | total loss: [1m[32m0.20498[0m[0m | time: 5.535s
[2K
| Adam | epoch: 015 | loss: 0.20498 - acc: 0.9252 -- iter: 096/315
[A[ATraining Step: 144  | total loss: [1m[32m0.21988[0m[0m | time: 6.325s
[2K
| Adam | epoch: 015 | loss: 0.21988 - acc: 0.9289 -- iter: 128/315
[A[ATraining Step: 145  | total loss: [1m[32m0.20790[0m[0m | time: 7.378s
[2K
| Adam | epoch: 015 | loss: 0.20790 - acc: 0.9329 -- iter: 160/315
[A[ATraining Step: 146  | total loss: [1m[32m0.19511[0m[0m | time: 8.378s
[2K
| Adam | epoch: 015 | loss: 0.19511 - acc: 0.9365 -- iter: 192/315
[A[ATraining Step: 147  | total loss: [1m[32m0.18665[0m[0m | time: 9.397s
[2K
| Adam | epoch: 015 | loss: 0.18665 - acc: 0.9397 -- iter: 224/315
[A[ATraining Step: 148  | total loss: [1m[32m0.21272[0m[0m | time: 10.492s
[2K
| Adam | epoch: 015 | loss: 0.21272 - acc: 0.9301 -- iter: 256/315
[A[ATraining Step: 149  | total loss: [1m[32m0.21870[0m[0m | time: 11.692s
[2K
| Adam | epoch: 015 | loss: 0.21870 - acc: 0.9277 -- iter: 288/315
[A[ATraining Step: 150  | total loss: [1m[32m0.20819[0m[0m | time: 13.876s
[2K
| Adam | epoch: 015 | loss: 0.20819 - acc: 0.9287 | val_loss: 0.29442 - val_acc: 0.9192 -- iter: 315/315
--
Validation AUC:0.9574468085106382
Validation AUPRC:0.9325403518607934
Test AUC:0.9351020408163265
Test AUPRC:0.932572995115333
BestTestF1Score	0.92	0.84	0.92	0.9	0.94	46	5	45	3	0.64
BestTestMCCScore	0.92	0.84	0.92	0.9	0.94	46	5	45	3	0.64
BestTestAccuracyScore	0.92	0.84	0.92	0.9	0.94	46	5	45	3	0.64
BestValidationF1Score	0.94	0.87	0.93	0.88	1.0	52	7	40	0	0.64
BestValidationMCC	0.94	0.87	0.93	0.88	1.0	52	7	40	0	0.64
BestValidationAccuracy	0.94	0.87	0.93	0.88	1.0	52	7	40	0	0.64
TestPredictions (Threshold:0.64)
CHEMBL112770,TN,INACT,0.009999999776482582	CHEMBL94369,TP,ACT,1.0	CHEMBL236020,TP,ACT,0.9900000095367432	CHEMBL236847,TP,ACT,0.8899999856948853	CHEMBL393558,TP,ACT,0.9900000095367432	CHEMBL110064,TN,INACT,0.019999999552965164	CHEMBL75475,FN,ACT,0.05999999865889549	CHEMBL257547,FP,INACT,0.75	CHEMBL237920,TP,ACT,1.0	CHEMBL96531,TP,ACT,0.9900000095367432	CHEMBL593443,TN,INACT,0.019999999552965164	CHEMBL1688107,TP,ACT,1.0	CHEMBL241100,TN,INACT,0.07999999821186066	CHEMBL151906,TP,ACT,0.9900000095367432	CHEMBL396303,TP,ACT,0.8899999856948853	CHEMBL392094,TP,ACT,0.9300000071525574	CHEMBL44615,TN,INACT,0.5699999928474426	CHEMBL424214,TN,INACT,0.550000011920929	CHEMBL336033,FP,INACT,0.9300000071525574	CHEMBL3422429,TP,ACT,0.9800000190734863	CHEMBL1834392,TP,ACT,1.0	CHEMBL317969,TP,ACT,0.9900000095367432	CHEMBL107574,TN,INACT,0.07000000029802322	CHEMBL148967,TN,INACT,0.28999999165534973	CHEMBL432201,TP,ACT,1.0	CHEMBL392316,TP,ACT,0.8600000143051147	CHEMBL104223,TN,INACT,0.2199999988079071	CHEMBL237069,TP,ACT,0.9900000095367432	CHEMBL394149,TP,ACT,0.9800000190734863	CHEMBL43788,TN,INACT,0.019999999552965164	CHEMBL297473,TN,INACT,0.07000000029802322	CHEMBL237528,FN,ACT,0.15000000596046448	CHEMBL3422431,TP,ACT,1.0	CHEMBL3577981,TP,ACT,0.9800000190734863	CHEMBL48120,TN,INACT,0.029999999329447746	CHEMBL323175,TN,INACT,0.019999999552965164	CHEMBL240021,TN,INACT,0.019999999552965164	CHEMBL77962,TN,INACT,0.019999999552965164	CHEMBL19808,TN,INACT,0.05999999865889549	CHEMBL2096742,TP,ACT,0.9900000095367432	CHEMBL173059,TN,INACT,0.33000001311302185	CHEMBL2369962,TP,ACT,1.0	CHEMBL237502,TP,ACT,0.9900000095367432	CHEMBL237694,TP,ACT,0.8100000023841858	CHEMBL338768,FN,ACT,0.009999999776482582	CHEMBL236649,TP,ACT,0.9399999976158142	CHEMBL21508,TN,INACT,0.05999999865889549	CHEMBL48031,TN,INACT,0.029999999329447746	CHEMBL298203,TN,INACT,0.10999999940395355	CHEMBL404866,FP,INACT,1.0	CHEMBL241082,TN,INACT,0.009999999776482582	CHEMBL392096,TP,ACT,0.9700000286102295	CHEMBL236629,TN,INACT,0.05000000074505806	CHEMBL147238,TN,INACT,0.05999999865889549	CHEMBL11629,FP,INACT,0.9200000166893005	CHEMBL113472,TN,INACT,0.029999999329447746	CHEMBL149592,TN,INACT,0.14000000059604645	CHEMBL237937,TP,ACT,0.8899999856948853	CHEMBL177546,TN,INACT,0.3700000047683716	CHEMBL80807,TN,INACT,0.019999999552965164	CHEMBL10211,TN,INACT,0.05999999865889549	CHEMBL325935,TN,INACT,0.019999999552965164	CHEMBL237299,TP,ACT,0.9200000166893005	CHEMBL114478,TN,INACT,0.019999999552965164	CHEMBL107680,TN,INACT,0.019999999552965164	CHEMBL235815,TP,ACT,0.8799999952316284	CHEMBL393552,TP,ACT,0.9700000286102295	CHEMBL3287328,TP,ACT,0.8999999761581421	CHEMBL3735690,TP,ACT,1.0	CHEMBL397550,TP,ACT,0.8600000143051147	CHEMBL319036,FP,INACT,0.9599999785423279	CHEMBL3799094,TP,ACT,0.9800000190734863	CHEMBL2323796,TP,ACT,0.9800000190734863	CHEMBL319924,TN,INACT,0.5699999928474426	CHEMBL111023,TN,INACT,0.019999999552965164	CHEMBL2323794,TP,ACT,1.0	CHEMBL319005,TN,INACT,0.03999999910593033	CHEMBL236803,TP,ACT,1.0	CHEMBL78929,TN,INACT,0.03999999910593033	CHEMBL237291,TP,ACT,0.9900000095367432	CHEMBL2312376,TN,INACT,0.12999999523162842	CHEMBL2369973,TP,ACT,1.0	CHEMBL2323786,TP,ACT,1.0	CHEMBL235627,TP,ACT,0.949999988079071	CHEMBL63290,TN,INACT,0.10999999940395355	CHEMBL267094,TN,INACT,0.3199999928474426	CHEMBL238159,TP,ACT,1.0	CHEMBL461087,TN,INACT,0.019999999552965164	CHEMBL48024,TN,INACT,0.44999998807907104	CHEMBL238341,TP,ACT,0.9300000071525574	CHEMBL9666,TN,INACT,0.3199999928474426	CHEMBL413982,TP,ACT,0.9599999785423279	CHEMBL1834393,TP,ACT,1.0	CHEMBL104848,TN,INACT,0.07000000029802322	CHEMBL50056,TP,ACT,0.9900000095367432	CHEMBL3577976,TP,ACT,1.0	CHEMBL320569,TN,INACT,0.009999999776482582	CHEMBL170335,TN,INACT,0.28999999165534973	CHEMBL42359,TN,INACT,0.019999999552965164	

