CNNModel CHEMBL269 RMSprop 0.0005 15 256 0 0.6 False True
Number of active compounds :	1774
Number of inactive compounds :	1774
---------------------------------
Run id: CNNModel_CHEMBL269_RMSprop_0.0005_15_256_0_0.6_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL269_RMSprop_0.0005_15_256_0.6_True/
---------------------------------
Training samples: 2226
Validation samples: 696
--
Training Step: 1  | time: 0.960s
[2K
| RMSProp | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 0032/2226
[A[ATraining Step: 2  | total loss: [1m[32m0.62401[0m[0m | time: 1.669s
[2K
| RMSProp | epoch: 001 | loss: 0.62401 - acc: 0.4781 -- iter: 0064/2226
[A[ATraining Step: 3  | total loss: [1m[32m0.68093[0m[0m | time: 2.420s
[2K
| RMSProp | epoch: 001 | loss: 0.68093 - acc: 0.3938 -- iter: 0096/2226
[A[ATraining Step: 4  | total loss: [1m[32m0.69015[0m[0m | time: 3.147s
[2K
| RMSProp | epoch: 001 | loss: 0.69015 - acc: 0.4734 -- iter: 0128/2226
[A[ATraining Step: 5  | total loss: [1m[32m0.69238[0m[0m | time: 3.879s
[2K
| RMSProp | epoch: 001 | loss: 0.69238 - acc: 0.4702 -- iter: 0160/2226
[A[ATraining Step: 6  | total loss: [1m[32m0.69259[0m[0m | time: 4.593s
[2K
| RMSProp | epoch: 001 | loss: 0.69259 - acc: 0.5295 -- iter: 0192/2226
[A[ATraining Step: 7  | total loss: [1m[32m0.69288[0m[0m | time: 5.297s
[2K
| RMSProp | epoch: 001 | loss: 0.69288 - acc: 0.5118 -- iter: 0224/2226
[A[ATraining Step: 8  | total loss: [1m[32m0.69306[0m[0m | time: 6.089s
[2K
| RMSProp | epoch: 001 | loss: 0.69306 - acc: 0.4876 -- iter: 0256/2226
[A[ATraining Step: 9  | total loss: [1m[32m0.69312[0m[0m | time: 6.827s
[2K
| RMSProp | epoch: 001 | loss: 0.69312 - acc: 0.5272 -- iter: 0288/2226
[A[ATraining Step: 10  | total loss: [1m[32m0.69311[0m[0m | time: 7.648s
[2K
| RMSProp | epoch: 001 | loss: 0.69311 - acc: 0.4824 -- iter: 0320/2226
[A[ATraining Step: 11  | total loss: [1m[32m0.69314[0m[0m | time: 8.376s
[2K
| RMSProp | epoch: 001 | loss: 0.69314 - acc: 0.4759 -- iter: 0352/2226
[A[ATraining Step: 12  | total loss: [1m[32m0.69320[0m[0m | time: 9.097s
[2K
| RMSProp | epoch: 001 | loss: 0.69320 - acc: 0.4586 -- iter: 0384/2226
[A[ATraining Step: 13  | total loss: [1m[32m0.69300[0m[0m | time: 9.811s
[2K
| RMSProp | epoch: 001 | loss: 0.69300 - acc: 0.5165 -- iter: 0416/2226
[A[ATraining Step: 14  | total loss: [1m[32m0.69315[0m[0m | time: 10.595s
[2K
| RMSProp | epoch: 001 | loss: 0.69315 - acc: 0.4714 -- iter: 0448/2226
[A[ATraining Step: 15  | total loss: [1m[32m0.69322[0m[0m | time: 11.316s
[2K
| RMSProp | epoch: 001 | loss: 0.69322 - acc: 0.4948 -- iter: 0480/2226
[A[ATraining Step: 16  | total loss: [1m[32m0.69323[0m[0m | time: 12.029s
[2K
| RMSProp | epoch: 001 | loss: 0.69323 - acc: 0.5085 -- iter: 0512/2226
[A[ATraining Step: 17  | total loss: [1m[32m0.69313[0m[0m | time: 12.825s
[2K
| RMSProp | epoch: 001 | loss: 0.69313 - acc: 0.5392 -- iter: 0544/2226
[A[ATraining Step: 18  | total loss: [1m[32m0.69303[0m[0m | time: 13.593s
[2K
| RMSProp | epoch: 001 | loss: 0.69303 - acc: 0.5581 -- iter: 0576/2226
[A[ATraining Step: 19  | total loss: [1m[32m0.69320[0m[0m | time: 14.600s
[2K
| RMSProp | epoch: 001 | loss: 0.69320 - acc: 0.5075 -- iter: 0608/2226
[A[ATraining Step: 20  | total loss: [1m[32m0.69293[0m[0m | time: 15.855s
[2K
| RMSProp | epoch: 001 | loss: 0.69293 - acc: 0.5854 -- iter: 0640/2226
[A[ATraining Step: 21  | total loss: [1m[32m0.69300[0m[0m | time: 20.268s
[2K
| RMSProp | epoch: 001 | loss: 0.69300 - acc: 0.5686 -- iter: 0672/2226
[A[ATraining Step: 22  | total loss: [1m[32m0.69299[0m[0m | time: 23.795s
[2K
| RMSProp | epoch: 001 | loss: 0.69299 - acc: 0.5668 -- iter: 0704/2226
[A[ATraining Step: 23  | total loss: [1m[32m0.69313[0m[0m | time: 31.018s
[2K
| RMSProp | epoch: 001 | loss: 0.69313 - acc: 0.5474 -- iter: 0736/2226
[A[ATraining Step: 24  | total loss: [1m[32m0.69338[0m[0m | time: 35.559s
[2K
| RMSProp | epoch: 001 | loss: 0.69338 - acc: 0.4813 -- iter: 0768/2226
[A[ATraining Step: 25  | total loss: [1m[32m0.69324[0m[0m | time: 39.863s
[2K
| RMSProp | epoch: 001 | loss: 0.69324 - acc: 0.5035 -- iter: 0800/2226
[A[ATraining Step: 26  | total loss: [1m[32m0.69316[0m[0m | time: 40.727s
[2K
| RMSProp | epoch: 001 | loss: 0.69316 - acc: 0.4943 -- iter: 0832/2226
[A[ATraining Step: 27  | total loss: [1m[32m0.69300[0m[0m | time: 41.648s
[2K
| RMSProp | epoch: 001 | loss: 0.69300 - acc: 0.5359 -- iter: 0864/2226
[A[ATraining Step: 28  | total loss: [1m[32m0.69319[0m[0m | time: 42.738s
[2K
| RMSProp | epoch: 001 | loss: 0.69319 - acc: 0.4957 -- iter: 0896/2226
[A[ATraining Step: 29  | total loss: [1m[32m0.69306[0m[0m | time: 43.767s
[2K
| RMSProp | epoch: 001 | loss: 0.69306 - acc: 0.5119 -- iter: 0928/2226
[A[ATraining Step: 30  | total loss: [1m[32m0.69317[0m[0m | time: 44.773s
[2K
| RMSProp | epoch: 001 | loss: 0.69317 - acc: 0.4943 -- iter: 0960/2226
[A[ATraining Step: 31  | total loss: [1m[32m0.69310[0m[0m | time: 45.832s
[2K
| RMSProp | epoch: 001 | loss: 0.69310 - acc: 0.5245 -- iter: 0992/2226
[A[ATraining Step: 32  | total loss: [1m[32m0.69297[0m[0m | time: 46.976s
[2K
| RMSProp | epoch: 001 | loss: 0.69297 - acc: 0.5541 -- iter: 1024/2226
[A[ATraining Step: 33  | total loss: [1m[32m0.69296[0m[0m | time: 48.073s
[2K
| RMSProp | epoch: 001 | loss: 0.69296 - acc: 0.5491 -- iter: 1056/2226
[A[ATraining Step: 34  | total loss: [1m[32m0.69300[0m[0m | time: 49.355s
[2K
| RMSProp | epoch: 001 | loss: 0.69300 - acc: 0.5520 -- iter: 1088/2226
[A[ATraining Step: 35  | total loss: [1m[32m0.69300[0m[0m | time: 50.708s
[2K
| RMSProp | epoch: 001 | loss: 0.69300 - acc: 0.5476 -- iter: 1120/2226
[A[ATraining Step: 36  | total loss: [1m[32m0.69310[0m[0m | time: 54.184s
[2K
| RMSProp | epoch: 001 | loss: 0.69310 - acc: 0.5315 -- iter: 1152/2226
[A[ATraining Step: 37  | total loss: [1m[32m0.69302[0m[0m | time: 59.721s
[2K
| RMSProp | epoch: 001 | loss: 0.69302 - acc: 0.5377 -- iter: 1184/2226
[A[ATraining Step: 38  | total loss: [1m[32m0.69306[0m[0m | time: 64.071s
[2K
| RMSProp | epoch: 001 | loss: 0.69306 - acc: 0.5242 -- iter: 1216/2226
[A[ATraining Step: 39  | total loss: [1m[32m0.69328[0m[0m | time: 69.094s
[2K
| RMSProp | epoch: 001 | loss: 0.69328 - acc: 0.4837 -- iter: 1248/2226
[A[ATraining Step: 40  | total loss: [1m[32m0.69324[0m[0m | time: 73.932s
[2K
| RMSProp | epoch: 001 | loss: 0.69324 - acc: 0.4926 -- iter: 1280/2226
[A[ATraining Step: 41  | total loss: [1m[32m0.69314[0m[0m | time: 78.353s
[2K
| RMSProp | epoch: 001 | loss: 0.69314 - acc: 0.5054 -- iter: 1312/2226
[A[ATraining Step: 42  | total loss: [1m[32m0.69318[0m[0m | time: 79.368s
[2K
| RMSProp | epoch: 001 | loss: 0.69318 - acc: 0.4988 -- iter: 1344/2226
[A[ATraining Step: 43  | total loss: [1m[32m0.69331[0m[0m | time: 80.391s
[2K
| RMSProp | epoch: 001 | loss: 0.69331 - acc: 0.4770 -- iter: 1376/2226
[A[ATraining Step: 44  | total loss: [1m[32m0.69330[0m[0m | time: 81.445s
[2K
| RMSProp | epoch: 001 | loss: 0.69330 - acc: 0.4647 -- iter: 1408/2226
[A[ATraining Step: 45  | total loss: [1m[32m0.69322[0m[0m | time: 82.496s
[2K
| RMSProp | epoch: 001 | loss: 0.69322 - acc: 0.4920 -- iter: 1440/2226
[A[ATraining Step: 46  | total loss: [1m[32m0.69309[0m[0m | time: 83.485s
[2K
| RMSProp | epoch: 001 | loss: 0.69309 - acc: 0.5298 -- iter: 1472/2226
[A[ATraining Step: 47  | total loss: [1m[32m0.69302[0m[0m | time: 84.567s
[2K
| RMSProp | epoch: 001 | loss: 0.69302 - acc: 0.5505 -- iter: 1504/2226
[A[ATraining Step: 48  | total loss: [1m[32m0.69284[0m[0m | time: 85.676s
[2K
| RMSProp | epoch: 001 | loss: 0.69284 - acc: 0.5675 -- iter: 1536/2226
[A[ATraining Step: 49  | total loss: [1m[32m0.69274[0m[0m | time: 86.802s
[2K
| RMSProp | epoch: 001 | loss: 0.69274 - acc: 0.5716 -- iter: 1568/2226
[A[ATraining Step: 50  | total loss: [1m[32m0.69272[0m[0m | time: 88.001s
[2K
| RMSProp | epoch: 001 | loss: 0.69272 - acc: 0.5702 -- iter: 1600/2226
[A[ATraining Step: 51  | total loss: [1m[32m0.69280[0m[0m | time: 90.444s
[2K
| RMSProp | epoch: 001 | loss: 0.69280 - acc: 0.5547 -- iter: 1632/2226
[A[ATraining Step: 52  | total loss: [1m[32m0.69310[0m[0m | time: 93.827s
[2K
| RMSProp | epoch: 001 | loss: 0.69310 - acc: 0.5231 -- iter: 1664/2226
[A[ATraining Step: 53  | total loss: [1m[32m0.69298[0m[0m | time: 98.445s
[2K
| RMSProp | epoch: 001 | loss: 0.69298 - acc: 0.5381 -- iter: 1696/2226
[A[ATraining Step: 54  | total loss: [1m[32m0.69307[0m[0m | time: 102.471s
[2K
| RMSProp | epoch: 001 | loss: 0.69307 - acc: 0.5235 -- iter: 1728/2226
[A[ATraining Step: 55  | total loss: [1m[32m0.69308[0m[0m | time: 106.439s
[2K
| RMSProp | epoch: 001 | loss: 0.69308 - acc: 0.5201 -- iter: 1760/2226
[A[ATraining Step: 56  | total loss: [1m[32m0.69302[0m[0m | time: 109.188s
[2K
| RMSProp | epoch: 001 | loss: 0.69302 - acc: 0.5261 -- iter: 1792/2226
[A[ATraining Step: 57  | total loss: [1m[32m0.69306[0m[0m | time: 110.322s
[2K
| RMSProp | epoch: 001 | loss: 0.69306 - acc: 0.5225 -- iter: 1824/2226
[A[ATraining Step: 58  | total loss: [1m[32m0.69313[0m[0m | time: 111.464s
[2K
| RMSProp | epoch: 001 | loss: 0.69313 - acc: 0.5152 -- iter: 1856/2226
[A[ATraining Step: 59  | total loss: [1m[32m0.69305[0m[0m | time: 112.580s
[2K
| RMSProp | epoch: 001 | loss: 0.69305 - acc: 0.5257 -- iter: 1888/2226
[A[ATraining Step: 60  | total loss: [1m[32m0.69331[0m[0m | time: 113.661s
[2K
| RMSProp | epoch: 001 | loss: 0.69331 - acc: 0.4975 -- iter: 1920/2226
[A[ATraining Step: 61  | total loss: [1m[32m0.69320[0m[0m | time: 114.809s
[2K
| RMSProp | epoch: 001 | loss: 0.69320 - acc: 0.5141 -- iter: 1952/2226
[A[ATraining Step: 62  | total loss: [1m[32m0.69329[0m[0m | time: 115.886s
[2K
| RMSProp | epoch: 001 | loss: 0.69329 - acc: 0.5003 -- iter: 1984/2226
[A[ATraining Step: 63  | total loss: [1m[32m0.69329[0m[0m | time: 117.012s
[2K
| RMSProp | epoch: 001 | loss: 0.69329 - acc: 0.5002 -- iter: 2016/2226
[A[ATraining Step: 64  | total loss: [1m[32m0.69319[0m[0m | time: 118.045s
[2K
| RMSProp | epoch: 001 | loss: 0.69319 - acc: 0.5080 -- iter: 2048/2226
[A[ATraining Step: 65  | total loss: [1m[32m0.69305[0m[0m | time: 119.242s
[2K
| RMSProp | epoch: 001 | loss: 0.69305 - acc: 0.5224 -- iter: 2080/2226
[A[ATraining Step: 66  | total loss: [1m[32m0.69312[0m[0m | time: 123.157s
[2K
| RMSProp | epoch: 001 | loss: 0.69312 - acc: 0.5159 -- iter: 2112/2226
[A[ATraining Step: 67  | total loss: [1m[32m0.69313[0m[0m | time: 127.949s
[2K
| RMSProp | epoch: 001 | loss: 0.69313 - acc: 0.5140 -- iter: 2144/2226
[A[ATraining Step: 68  | total loss: [1m[32m0.69328[0m[0m | time: 132.545s
[2K
| RMSProp | epoch: 001 | loss: 0.69328 - acc: 0.5012 -- iter: 2176/2226
[A[ATraining Step: 69  | total loss: [1m[32m0.69342[0m[0m | time: 136.031s
[2K
| RMSProp | epoch: 001 | loss: 0.69342 - acc: 0.4828 -- iter: 2208/2226
[A[ATraining Step: 70  | total loss: [1m[32m0.69338[0m[0m | time: 144.172s
[2K
| RMSProp | epoch: 001 | loss: 0.69338 - acc: 0.4884 | val_loss: 0.69315 - val_acc: 0.5014 -- iter: 2226/2226
--
Training Step: 71  | total loss: [1m[32m0.69343[0m[0m | time: 0.625s
[2K
| RMSProp | epoch: 002 | loss: 0.69343 - acc: 0.4771 -- iter: 0032/2226
[A[ATraining Step: 72  | total loss: [1m[32m0.69346[0m[0m | time: 1.722s
[2K
| RMSProp | epoch: 002 | loss: 0.69346 - acc: 0.4672 -- iter: 0064/2226
[A[ATraining Step: 73  | total loss: [1m[32m0.69337[0m[0m | time: 2.786s
[2K
| RMSProp | epoch: 002 | loss: 0.69337 - acc: 0.4951 -- iter: 0096/2226
[A[ATraining Step: 74  | total loss: [1m[32m0.69331[0m[0m | time: 3.821s
[2K
| RMSProp | epoch: 002 | loss: 0.69331 - acc: 0.4991 -- iter: 0128/2226
[A[ATraining Step: 75  | total loss: [1m[32m0.69319[0m[0m | time: 4.929s
[2K
| RMSProp | epoch: 002 | loss: 0.69319 - acc: 0.5127 -- iter: 0160/2226
[A[ATraining Step: 76  | total loss: [1m[32m0.69323[0m[0m | time: 6.075s
[2K
| RMSProp | epoch: 002 | loss: 0.69323 - acc: 0.5080 -- iter: 0192/2226
[A[ATraining Step: 77  | total loss: [1m[32m0.69335[0m[0m | time: 8.229s
[2K
| RMSProp | epoch: 002 | loss: 0.69335 - acc: 0.4906 -- iter: 0224/2226
[A[ATraining Step: 78  | total loss: [1m[32m0.69331[0m[0m | time: 13.869s
[2K
| RMSProp | epoch: 002 | loss: 0.69331 - acc: 0.4949 -- iter: 0256/2226
[A[ATraining Step: 79  | total loss: [1m[32m0.69322[0m[0m | time: 19.205s
[2K
| RMSProp | epoch: 002 | loss: 0.69322 - acc: 0.5051 -- iter: 0288/2226
[A[ATraining Step: 80  | total loss: [1m[32m0.69321[0m[0m | time: 24.440s
[2K
| RMSProp | epoch: 002 | loss: 0.69321 - acc: 0.5046 -- iter: 0320/2226
[A[ATraining Step: 81  | total loss: [1m[32m0.69325[0m[0m | time: 25.359s
[2K
| RMSProp | epoch: 002 | loss: 0.69325 - acc: 0.4978 -- iter: 0352/2226
[A[ATraining Step: 82  | total loss: [1m[32m0.69323[0m[0m | time: 26.439s
[2K
| RMSProp | epoch: 002 | loss: 0.69323 - acc: 0.4980 -- iter: 0384/2226
[A[ATraining Step: 83  | total loss: [1m[32m0.69321[0m[0m | time: 27.487s
[2K
| RMSProp | epoch: 002 | loss: 0.69321 - acc: 0.4982 -- iter: 0416/2226
[A[ATraining Step: 84  | total loss: [1m[32m0.69319[0m[0m | time: 28.487s
[2K
| RMSProp | epoch: 002 | loss: 0.69319 - acc: 0.5015 -- iter: 0448/2226
[A[ATraining Step: 85  | total loss: [1m[32m0.69314[0m[0m | time: 29.606s
[2K
| RMSProp | epoch: 002 | loss: 0.69314 - acc: 0.5076 -- iter: 0480/2226
[A[ATraining Step: 86  | total loss: [1m[32m0.69313[0m[0m | time: 30.787s
[2K
| RMSProp | epoch: 002 | loss: 0.69313 - acc: 0.5069 -- iter: 0512/2226
[A[ATraining Step: 87  | total loss: [1m[32m0.69311[0m[0m | time: 31.898s
[2K
| RMSProp | epoch: 002 | loss: 0.69311 - acc: 0.5093 -- iter: 0544/2226
[A[ATraining Step: 88  | total loss: [1m[32m0.69313[0m[0m | time: 32.978s
[2K
| RMSProp | epoch: 002 | loss: 0.69313 - acc: 0.5084 -- iter: 0576/2226
[A[ATraining Step: 89  | total loss: [1m[32m0.69326[0m[0m | time: 34.055s
[2K
| RMSProp | epoch: 002 | loss: 0.69326 - acc: 0.4919 -- iter: 0608/2226
[A[ATraining Step: 90  | total loss: [1m[32m0.69325[0m[0m | time: 35.283s
[2K
| RMSProp | epoch: 002 | loss: 0.69325 - acc: 0.4958 -- iter: 0640/2226
[A[ATraining Step: 91  | total loss: [1m[32m0.69322[0m[0m | time: 39.971s
[2K
| RMSProp | epoch: 002 | loss: 0.69322 - acc: 0.4963 -- iter: 0672/2226
[A[ATraining Step: 92  | total loss: [1m[32m0.69322[0m[0m | time: 41.659s
[2K
| RMSProp | epoch: 002 | loss: 0.69322 - acc: 0.4998 -- iter: 0704/2226
[A[ATraining Step: 93  | total loss: [1m[32m0.69318[0m[0m | time: 46.494s
[2K
| RMSProp | epoch: 002 | loss: 0.69318 - acc: 0.5092 -- iter: 0736/2226
[A[ATraining Step: 94  | total loss: [1m[32m0.69317[0m[0m | time: 51.478s
[2K
| RMSProp | epoch: 002 | loss: 0.69317 - acc: 0.5082 -- iter: 0768/2226
[A[ATraining Step: 95  | total loss: [1m[32m0.69324[0m[0m | time: 55.467s
[2K
| RMSProp | epoch: 002 | loss: 0.69324 - acc: 0.4980 -- iter: 0800/2226
[A[ATraining Step: 96  | total loss: [1m[32m0.69326[0m[0m | time: 57.810s
[2K
| RMSProp | epoch: 002 | loss: 0.69326 - acc: 0.4889 -- iter: 0832/2226
[A[ATraining Step: 97  | total loss: [1m[32m0.69324[0m[0m | time: 58.860s
[2K
| RMSProp | epoch: 002 | loss: 0.69324 - acc: 0.4869 -- iter: 0864/2226
[A[ATraining Step: 98  | total loss: [1m[32m0.69322[0m[0m | time: 59.932s
[2K
| RMSProp | epoch: 002 | loss: 0.69322 - acc: 0.4944 -- iter: 0896/2226
[A[ATraining Step: 99  | total loss: [1m[32m0.69322[0m[0m | time: 60.968s
[2K
| RMSProp | epoch: 002 | loss: 0.69322 - acc: 0.4918 -- iter: 0928/2226
[A[ATraining Step: 100  | total loss: [1m[32m0.69320[0m[0m | time: 62.162s
[2K
| RMSProp | epoch: 002 | loss: 0.69320 - acc: 0.4895 -- iter: 0960/2226
[A[ATraining Step: 101  | total loss: [1m[32m0.69320[0m[0m | time: 63.156s
[2K
| RMSProp | epoch: 002 | loss: 0.69320 - acc: 0.4906 -- iter: 0992/2226
[A[ATraining Step: 102  | total loss: [1m[32m0.69324[0m[0m | time: 64.288s
[2K
| RMSProp | epoch: 002 | loss: 0.69324 - acc: 0.4853 -- iter: 1024/2226
[A[ATraining Step: 103  | total loss: [1m[32m0.69324[0m[0m | time: 65.515s
[2K
| RMSProp | epoch: 002 | loss: 0.69324 - acc: 0.4836 -- iter: 1056/2226
[A[ATraining Step: 104  | total loss: [1m[32m0.69324[0m[0m | time: 66.548s
[2K
| RMSProp | epoch: 002 | loss: 0.69324 - acc: 0.4821 -- iter: 1088/2226
[A[ATraining Step: 105  | total loss: [1m[32m0.69330[0m[0m | time: 67.622s
[2K
| RMSProp | epoch: 002 | loss: 0.69330 - acc: 0.4745 -- iter: 1120/2226
[A[ATraining Step: 106  | total loss: [1m[32m0.69328[0m[0m | time: 68.730s
[2K
| RMSProp | epoch: 002 | loss: 0.69328 - acc: 0.4865 -- iter: 1152/2226
[A[ATraining Step: 107  | total loss: [1m[32m0.69322[0m[0m | time: 69.720s
[2K
| RMSProp | epoch: 002 | loss: 0.69322 - acc: 0.4909 -- iter: 1184/2226
[A[ATraining Step: 108  | total loss: [1m[32m0.69323[0m[0m | time: 70.662s
[2K
| RMSProp | epoch: 002 | loss: 0.69323 - acc: 0.4887 -- iter: 1216/2226
[A[ATraining Step: 109  | total loss: [1m[32m0.69314[0m[0m | time: 71.770s
[2K
| RMSProp | epoch: 002 | loss: 0.69314 - acc: 0.4992 -- iter: 1248/2226
[A[ATraining Step: 110  | total loss: [1m[32m0.69325[0m[0m | time: 72.820s
[2K
| RMSProp | epoch: 002 | loss: 0.69325 - acc: 0.4899 -- iter: 1280/2226
[A[ATraining Step: 111  | total loss: [1m[32m0.69332[0m[0m | time: 73.872s
[2K
| RMSProp | epoch: 002 | loss: 0.69332 - acc: 0.4816 -- iter: 1312/2226
[A[ATraining Step: 112  | total loss: [1m[32m0.69330[0m[0m | time: 75.107s
[2K
| RMSProp | epoch: 002 | loss: 0.69330 - acc: 0.4897 -- iter: 1344/2226
[A[ATraining Step: 113  | total loss: [1m[32m0.69322[0m[0m | time: 76.152s
[2K
| RMSProp | epoch: 002 | loss: 0.69322 - acc: 0.5063 -- iter: 1376/2226
[A[ATraining Step: 114  | total loss: [1m[32m0.69325[0m[0m | time: 77.056s
[2K
| RMSProp | epoch: 002 | loss: 0.69325 - acc: 0.5026 -- iter: 1408/2226
[A[ATraining Step: 115  | total loss: [1m[32m0.69316[0m[0m | time: 77.998s
[2K
| RMSProp | epoch: 002 | loss: 0.69316 - acc: 0.5086 -- iter: 1440/2226
[A[ATraining Step: 116  | total loss: [1m[32m0.69315[0m[0m | time: 79.126s
[2K
| RMSProp | epoch: 002 | loss: 0.69315 - acc: 0.5077 -- iter: 1472/2226
[A[ATraining Step: 117  | total loss: [1m[32m0.69318[0m[0m | time: 80.362s
[2K
| RMSProp | epoch: 002 | loss: 0.69318 - acc: 0.5038 -- iter: 1504/2226
[A[ATraining Step: 118  | total loss: [1m[32m0.69323[0m[0m | time: 81.612s
[2K
| RMSProp | epoch: 002 | loss: 0.69323 - acc: 0.5003 -- iter: 1536/2226
[A[ATraining Step: 119  | total loss: [1m[32m0.69330[0m[0m | time: 82.799s
[2K
| RMSProp | epoch: 002 | loss: 0.69330 - acc: 0.4940 -- iter: 1568/2226
[A[ATraining Step: 120  | total loss: [1m[32m0.69330[0m[0m | time: 83.809s
[2K
| RMSProp | epoch: 002 | loss: 0.69330 - acc: 0.4884 -- iter: 1600/2226
[A[ATraining Step: 121  | total loss: [1m[32m0.69328[0m[0m | time: 84.653s
[2K
| RMSProp | epoch: 002 | loss: 0.69328 - acc: 0.4958 -- iter: 1632/2226
[A[ATraining Step: 122  | total loss: [1m[32m0.69310[0m[0m | time: 85.494s
[2K
| RMSProp | epoch: 002 | loss: 0.69310 - acc: 0.5056 -- iter: 1664/2226
[A[ATraining Step: 123  | total loss: [1m[32m0.69301[0m[0m | time: 86.281s
[2K
| RMSProp | epoch: 002 | loss: 0.69301 - acc: 0.5081 -- iter: 1696/2226
[A[ATraining Step: 124  | total loss: [1m[32m0.69312[0m[0m | time: 87.085s
[2K
| RMSProp | epoch: 002 | loss: 0.69312 - acc: 0.5042 -- iter: 1728/2226
[A[ATraining Step: 125  | total loss: [1m[32m0.69321[0m[0m | time: 87.979s
[2K
| RMSProp | epoch: 002 | loss: 0.69321 - acc: 0.5007 -- iter: 1760/2226
[A[ATraining Step: 126  | total loss: [1m[32m0.69344[0m[0m | time: 88.767s
[2K
| RMSProp | epoch: 002 | loss: 0.69344 - acc: 0.4850 -- iter: 1792/2226
[A[ATraining Step: 127  | total loss: [1m[32m0.69340[0m[0m | time: 89.482s
[2K
| RMSProp | epoch: 002 | loss: 0.69340 - acc: 0.4927 -- iter: 1824/2226
[A[ATraining Step: 128  | total loss: [1m[32m0.69347[0m[0m | time: 90.266s
[2K
| RMSProp | epoch: 002 | loss: 0.69347 - acc: 0.4809 -- iter: 1856/2226
[A[ATraining Step: 129  | total loss: [1m[32m0.69343[0m[0m | time: 91.126s
[2K
| RMSProp | epoch: 002 | loss: 0.69343 - acc: 0.4891 -- iter: 1888/2226
[A[ATraining Step: 130  | total loss: [1m[32m0.69331[0m[0m | time: 91.978s
[2K
| RMSProp | epoch: 002 | loss: 0.69331 - acc: 0.5027 -- iter: 1920/2226
[A[ATraining Step: 131  | total loss: [1m[32m0.69355[0m[0m | time: 92.772s
[2K
| RMSProp | epoch: 002 | loss: 0.69355 - acc: 0.4899 -- iter: 1952/2226
[A[ATraining Step: 132  | total loss: [1m[32m0.69348[0m[0m | time: 93.619s
[2K
| RMSProp | epoch: 002 | loss: 0.69348 - acc: 0.4972 -- iter: 1984/2226
[A[ATraining Step: 133  | total loss: [1m[32m0.69337[0m[0m | time: 94.476s
[2K
| RMSProp | epoch: 002 | loss: 0.69337 - acc: 0.5068 -- iter: 2016/2226
[A[ATraining Step: 134  | total loss: [1m[32m0.69334[0m[0m | time: 95.307s
[2K
| RMSProp | epoch: 002 | loss: 0.69334 - acc: 0.5062 -- iter: 2048/2226
[A[ATraining Step: 135  | total loss: [1m[32m0.69363[0m[0m | time: 96.044s
[2K
| RMSProp | epoch: 002 | loss: 0.69363 - acc: 0.4899 -- iter: 2080/2226
[A[ATraining Step: 136  | total loss: [1m[32m0.69361[0m[0m | time: 96.906s
[2K
| RMSProp | epoch: 002 | loss: 0.69361 - acc: 0.4815 -- iter: 2112/2226
[A[ATraining Step: 137  | total loss: [1m[32m0.69354[0m[0m | time: 97.777s
[2K
| RMSProp | epoch: 002 | loss: 0.69354 - acc: 0.4928 -- iter: 2144/2226
[A[ATraining Step: 138  | total loss: [1m[32m0.69332[0m[0m | time: 98.563s
[2K
| RMSProp | epoch: 002 | loss: 0.69332 - acc: 0.5122 -- iter: 2176/2226
[A[ATraining Step: 139  | total loss: [1m[32m0.69308[0m[0m | time: 99.370s
[2K
| RMSProp | epoch: 002 | loss: 0.69308 - acc: 0.5204 -- iter: 2208/2226
[A[ATraining Step: 140  | total loss: [1m[32m0.69332[0m[0m | time: 103.465s
[2K
| RMSProp | epoch: 002 | loss: 0.69332 - acc: 0.5121 | val_loss: 0.69324 - val_acc: 0.5014 -- iter: 2226/2226
--
Training Step: 141  | total loss: [1m[32m0.69316[0m[0m | time: 0.600s
[2K
| RMSProp | epoch: 003 | loss: 0.69316 - acc: 0.5171 -- iter: 0032/2226
[A[ATraining Step: 142  | total loss: [1m[32m0.69318[0m[0m | time: 1.401s
[2K
| RMSProp | epoch: 003 | loss: 0.69318 - acc: 0.5154 -- iter: 0064/2226
[A[ATraining Step: 143  | total loss: [1m[32m0.69319[0m[0m | time: 4.046s
[2K
| RMSProp | epoch: 003 | loss: 0.69319 - acc: 0.5139 -- iter: 0096/2226
[A[ATraining Step: 144  | total loss: [1m[32m0.69294[0m[0m | time: 7.662s
[2K
| RMSProp | epoch: 003 | loss: 0.69294 - acc: 0.5219 -- iter: 0128/2226
[A[ATraining Step: 145  | total loss: [1m[32m0.69264[0m[0m | time: 14.434s
[2K
| RMSProp | epoch: 003 | loss: 0.69264 - acc: 0.5291 -- iter: 0160/2226
[A[ATraining Step: 146  | total loss: [1m[32m0.69289[0m[0m | time: 17.118s
[2K
| RMSProp | epoch: 003 | loss: 0.69289 - acc: 0.5230 -- iter: 0192/2226
[A[ATraining Step: 147  | total loss: [1m[32m0.69333[0m[0m | time: 22.895s
[2K
| RMSProp | epoch: 003 | loss: 0.69333 - acc: 0.5114 -- iter: 0224/2226
[A[ATraining Step: 148  | total loss: [1m[32m0.69348[0m[0m | time: 25.890s
[2K
| RMSProp | epoch: 003 | loss: 0.69348 - acc: 0.5040 -- iter: 0256/2226
[A[ATraining Step: 149  | total loss: [1m[32m0.69329[0m[0m | time: 29.284s
[2K
| RMSProp | epoch: 003 | loss: 0.69329 - acc: 0.5129 -- iter: 0288/2226
[A[ATraining Step: 150  | total loss: [1m[32m0.69312[0m[0m | time: 34.358s
[2K
| RMSProp | epoch: 003 | loss: 0.69312 - acc: 0.5179 -- iter: 0320/2226
[A[ATraining Step: 151  | total loss: [1m[32m0.69342[0m[0m | time: 38.654s
[2K
| RMSProp | epoch: 003 | loss: 0.69342 - acc: 0.5067 -- iter: 0352/2226
[A[ATraining Step: 152  | total loss: [1m[32m0.69332[0m[0m | time: 39.829s
[2K
| RMSProp | epoch: 003 | loss: 0.69332 - acc: 0.5092 -- iter: 0384/2226
[A[ATraining Step: 153  | total loss: [1m[32m0.69359[0m[0m | time: 40.734s
[2K
| RMSProp | epoch: 003 | loss: 0.69359 - acc: 0.4958 -- iter: 0416/2226
[A[ATraining Step: 154  | total loss: [1m[32m0.69354[0m[0m | time: 41.784s
[2K
| RMSProp | epoch: 003 | loss: 0.69354 - acc: 0.4962 -- iter: 0448/2226
[A[ATraining Step: 155  | total loss: [1m[32m0.69367[0m[0m | time: 43.001s
[2K
| RMSProp | epoch: 003 | loss: 0.69367 - acc: 0.4809 -- iter: 0480/2226
[A[ATraining Step: 156  | total loss: [1m[32m0.69351[0m[0m | time: 44.073s
[2K
| RMSProp | epoch: 003 | loss: 0.69351 - acc: 0.5016 -- iter: 0512/2226
[A[ATraining Step: 157  | total loss: [1m[32m0.69334[0m[0m | time: 45.240s
[2K
| RMSProp | epoch: 003 | loss: 0.69334 - acc: 0.5046 -- iter: 0544/2226
[A[ATraining Step: 158  | total loss: [1m[32m0.69388[0m[0m | time: 46.417s
[2K
| RMSProp | epoch: 003 | loss: 0.69388 - acc: 0.4947 -- iter: 0576/2226
[A[ATraining Step: 159  | total loss: [1m[32m0.69380[0m[0m | time: 47.453s
[2K
| RMSProp | epoch: 003 | loss: 0.69380 - acc: 0.4890 -- iter: 0608/2226
[A[ATraining Step: 160  | total loss: [1m[32m0.69371[0m[0m | time: 48.538s
[2K
| RMSProp | epoch: 003 | loss: 0.69371 - acc: 0.4964 -- iter: 0640/2226
[A[ATraining Step: 161  | total loss: [1m[32m0.69345[0m[0m | time: 52.449s
[2K
| RMSProp | epoch: 003 | loss: 0.69345 - acc: 0.5092 -- iter: 0672/2226
[A[ATraining Step: 162  | total loss: [1m[32m0.69360[0m[0m | time: 57.998s
[2K
| RMSProp | epoch: 003 | loss: 0.69360 - acc: 0.5052 -- iter: 0704/2226
[A[ATraining Step: 163  | total loss: [1m[32m0.69346[0m[0m | time: 61.142s
[2K
| RMSProp | epoch: 003 | loss: 0.69346 - acc: 0.5078 -- iter: 0736/2226
[A[ATraining Step: 164  | total loss: [1m[32m0.69368[0m[0m | time: 66.086s
[2K
| RMSProp | epoch: 003 | loss: 0.69368 - acc: 0.4976 -- iter: 0768/2226
[A[ATraining Step: 165  | total loss: [1m[32m0.69363[0m[0m | time: 71.221s
[2K
| RMSProp | epoch: 003 | loss: 0.69363 - acc: 0.5010 -- iter: 0800/2226
[A[ATraining Step: 166  | total loss: [1m[32m0.69359[0m[0m | time: 74.322s
[2K
| RMSProp | epoch: 003 | loss: 0.69359 - acc: 0.4978 -- iter: 0832/2226
[A[ATraining Step: 167  | total loss: [1m[32m0.69344[0m[0m | time: 78.569s
[2K
| RMSProp | epoch: 003 | loss: 0.69344 - acc: 0.5074 -- iter: 0864/2226
[A[ATraining Step: 168  | total loss: [1m[32m0.69350[0m[0m | time: 81.403s
[2K
| RMSProp | epoch: 003 | loss: 0.69350 - acc: 0.5035 -- iter: 0896/2226
[A[ATraining Step: 169  | total loss: [1m[32m0.69332[0m[0m | time: 82.481s
[2K
| RMSProp | epoch: 003 | loss: 0.69332 - acc: 0.5125 -- iter: 0928/2226
[A[ATraining Step: 170  | total loss: [1m[32m0.69313[0m[0m | time: 83.588s
[2K
| RMSProp | epoch: 003 | loss: 0.69313 - acc: 0.5175 -- iter: 0960/2226
[A[ATraining Step: 171  | total loss: [1m[32m0.69343[0m[0m | time: 84.662s
[2K
| RMSProp | epoch: 003 | loss: 0.69343 - acc: 0.5095 -- iter: 0992/2226
[A[ATraining Step: 172  | total loss: [1m[32m0.69373[0m[0m | time: 85.637s
[2K
| RMSProp | epoch: 003 | loss: 0.69373 - acc: 0.4929 -- iter: 1024/2226
[A[ATraining Step: 173  | total loss: [1m[32m0.69365[0m[0m | time: 86.787s
[2K
| RMSProp | epoch: 003 | loss: 0.69365 - acc: 0.4999 -- iter: 1056/2226
[A[ATraining Step: 174  | total loss: [1m[32m0.69354[0m[0m | time: 87.980s
[2K
| RMSProp | epoch: 003 | loss: 0.69354 - acc: 0.5062 -- iter: 1088/2226
[A[ATraining Step: 175  | total loss: [1m[32m0.69325[0m[0m | time: 89.198s
[2K
| RMSProp | epoch: 003 | loss: 0.69325 - acc: 0.5180 -- iter: 1120/2226
[A[ATraining Step: 176  | total loss: [1m[32m0.69314[0m[0m | time: 90.274s
[2K
| RMSProp | epoch: 003 | loss: 0.69314 - acc: 0.5194 -- iter: 1152/2226
[A[ATraining Step: 177  | total loss: [1m[32m0.69330[0m[0m | time: 91.807s
[2K
| RMSProp | epoch: 003 | loss: 0.69330 - acc: 0.5143 -- iter: 1184/2226
[A[ATraining Step: 178  | total loss: [1m[32m0.69319[0m[0m | time: 95.290s
[2K
| RMSProp | epoch: 003 | loss: 0.69319 - acc: 0.5160 -- iter: 1216/2226
[A[ATraining Step: 179  | total loss: [1m[32m0.69284[0m[0m | time: 99.452s
[2K
| RMSProp | epoch: 003 | loss: 0.69284 - acc: 0.5238 -- iter: 1248/2226
[A[ATraining Step: 180  | total loss: [1m[32m0.69236[0m[0m | time: 104.466s
[2K
| RMSProp | epoch: 003 | loss: 0.69236 - acc: 0.5308 -- iter: 1280/2226
[A[ATraining Step: 181  | total loss: [1m[32m0.69096[0m[0m | time: 107.961s
[2K
| RMSProp | epoch: 003 | loss: 0.69096 - acc: 0.5402 -- iter: 1312/2226
[A[ATraining Step: 182  | total loss: [1m[32m0.69894[0m[0m | time: 112.083s
[2K
| RMSProp | epoch: 003 | loss: 0.69894 - acc: 0.5268 -- iter: 1344/2226
[A[ATraining Step: 183  | total loss: [1m[32m0.69819[0m[0m | time: 115.138s
[2K
| RMSProp | epoch: 003 | loss: 0.69819 - acc: 0.5272 -- iter: 1376/2226
[A[ATraining Step: 184  | total loss: [1m[32m0.69860[0m[0m | time: 118.338s
[2K
| RMSProp | epoch: 003 | loss: 0.69860 - acc: 0.5120 -- iter: 1408/2226
[A[ATraining Step: 185  | total loss: [1m[32m0.69749[0m[0m | time: 121.153s
[2K
| RMSProp | epoch: 003 | loss: 0.69749 - acc: 0.5264 -- iter: 1440/2226
[A[ATraining Step: 186  | total loss: [1m[32m0.69796[0m[0m | time: 123.237s
[2K
| RMSProp | epoch: 003 | loss: 0.69796 - acc: 0.5113 -- iter: 1472/2226
[A[ATraining Step: 187  | total loss: [1m[32m0.69738[0m[0m | time: 124.164s
[2K
| RMSProp | epoch: 003 | loss: 0.69738 - acc: 0.5133 -- iter: 1504/2226
[A[ATraining Step: 188  | total loss: [1m[32m0.69699[0m[0m | time: 125.276s
[2K
| RMSProp | epoch: 003 | loss: 0.69699 - acc: 0.5120 -- iter: 1536/2226
[A[ATraining Step: 189  | total loss: [1m[32m0.69698[0m[0m | time: 126.438s
[2K
| RMSProp | epoch: 003 | loss: 0.69698 - acc: 0.5014 -- iter: 1568/2226
[A[ATraining Step: 190  | total loss: [1m[32m0.69658[0m[0m | time: 127.607s
[2K
| RMSProp | epoch: 003 | loss: 0.69658 - acc: 0.5013 -- iter: 1600/2226
[A[ATraining Step: 191  | total loss: [1m[32m0.69623[0m[0m | time: 128.847s
[2K
| RMSProp | epoch: 003 | loss: 0.69623 - acc: 0.5011 -- iter: 1632/2226
[A[ATraining Step: 192  | total loss: [1m[32m0.69635[0m[0m | time: 130.022s
[2K
| RMSProp | epoch: 003 | loss: 0.69635 - acc: 0.4791 -- iter: 1664/2226
[A[ATraining Step: 193  | total loss: [1m[32m0.69597[0m[0m | time: 131.453s
[2K
| RMSProp | epoch: 003 | loss: 0.69597 - acc: 0.4906 -- iter: 1696/2226
[A[ATraining Step: 194  | total loss: [1m[32m0.69555[0m[0m | time: 132.855s
[2K
| RMSProp | epoch: 003 | loss: 0.69555 - acc: 0.4947 -- iter: 1728/2226
[A[ATraining Step: 195  | total loss: [1m[32m0.69578[0m[0m | time: 133.860s
[2K
| RMSProp | epoch: 003 | loss: 0.69578 - acc: 0.4889 -- iter: 1760/2226
[A[ATraining Step: 196  | total loss: [1m[32m0.69551[0m[0m | time: 135.127s
[2K
| RMSProp | epoch: 003 | loss: 0.69551 - acc: 0.4901 -- iter: 1792/2226
[A[ATraining Step: 197  | total loss: [1m[32m0.69526[0m[0m | time: 140.772s
[2K
| RMSProp | epoch: 003 | loss: 0.69526 - acc: 0.4910 -- iter: 1824/2226
[A[ATraining Step: 198  | total loss: [1m[32m0.69507[0m[0m | time: 144.523s
[2K
| RMSProp | epoch: 003 | loss: 0.69507 - acc: 0.4888 -- iter: 1856/2226
[A[ATraining Step: 199  | total loss: [1m[32m0.69484[0m[0m | time: 148.602s
[2K
| RMSProp | epoch: 003 | loss: 0.69484 - acc: 0.4993 -- iter: 1888/2226
[A[ATraining Step: 200  | total loss: [1m[32m0.69452[0m[0m | time: 165.981s
[2K
| RMSProp | epoch: 003 | loss: 0.69452 - acc: 0.5056 | val_loss: 0.69308 - val_acc: 0.4986 -- iter: 1920/2226
--
Training Step: 201  | total loss: [1m[32m0.69518[0m[0m | time: 166.999s
[2K
| RMSProp | epoch: 003 | loss: 0.69518 - acc: 0.4926 -- iter: 1952/2226
[A[ATraining Step: 202  | total loss: [1m[32m0.69502[0m[0m | time: 167.968s
[2K
| RMSProp | epoch: 003 | loss: 0.69502 - acc: 0.4746 -- iter: 1984/2226
[A[ATraining Step: 203  | total loss: [1m[32m0.69469[0m[0m | time: 169.195s
[2K
| RMSProp | epoch: 003 | loss: 0.69469 - acc: 0.4865 -- iter: 2016/2226
[A[ATraining Step: 204  | total loss: [1m[32m0.69423[0m[0m | time: 170.389s
[2K
| RMSProp | epoch: 003 | loss: 0.69423 - acc: 0.5003 -- iter: 2048/2226
[A[ATraining Step: 205  | total loss: [1m[32m0.69389[0m[0m | time: 171.461s
[2K
| RMSProp | epoch: 003 | loss: 0.69389 - acc: 0.5065 -- iter: 2080/2226
[A[ATraining Step: 206  | total loss: [1m[32m0.69465[0m[0m | time: 172.528s
[2K
| RMSProp | epoch: 003 | loss: 0.69465 - acc: 0.4903 -- iter: 2112/2226
[A[ATraining Step: 207  | total loss: [1m[32m0.69470[0m[0m | time: 173.874s
[2K
| RMSProp | epoch: 003 | loss: 0.69470 - acc: 0.4819 -- iter: 2144/2226
[A[ATraining Step: 208  | total loss: [1m[32m0.69452[0m[0m | time: 179.109s
[2K
| RMSProp | epoch: 003 | loss: 0.69452 - acc: 0.4868 -- iter: 2176/2226
[A[ATraining Step: 209  | total loss: [1m[32m0.69434[0m[0m | time: 182.569s
[2K
| RMSProp | epoch: 003 | loss: 0.69434 - acc: 0.4912 -- iter: 2208/2226
[A[ATraining Step: 210  | total loss: [1m[32m0.69434[0m[0m | time: 198.783s
[2K
| RMSProp | epoch: 003 | loss: 0.69434 - acc: 0.4859 | val_loss: 0.69302 - val_acc: 0.5014 -- iter: 2226/2226
--
Training Step: 211  | total loss: [1m[32m0.69422[0m[0m | time: 1.134s
[2K
| RMSProp | epoch: 004 | loss: 0.69422 - acc: 0.4873 -- iter: 0032/2226
[A[ATraining Step: 212  | total loss: [1m[32m0.69402[0m[0m | time: 1.865s
[2K
| RMSProp | epoch: 004 | loss: 0.69402 - acc: 0.4979 -- iter: 0064/2226
[A[ATraining Step: 213  | total loss: [1m[32m0.69379[0m[0m | time: 2.518s
[2K
| RMSProp | epoch: 004 | loss: 0.69379 - acc: 0.5037 -- iter: 0096/2226
[A[ATraining Step: 214  | total loss: [1m[32m0.69356[0m[0m | time: 3.778s
[2K
| RMSProp | epoch: 004 | loss: 0.69356 - acc: 0.5089 -- iter: 0128/2226
[A[ATraining Step: 215  | total loss: [1m[32m0.69323[0m[0m | time: 4.990s
[2K
| RMSProp | epoch: 004 | loss: 0.69323 - acc: 0.5174 -- iter: 0160/2226
[A[ATraining Step: 216  | total loss: [1m[32m0.69279[0m[0m | time: 6.084s
[2K
| RMSProp | epoch: 004 | loss: 0.69279 - acc: 0.5250 -- iter: 0192/2226
[A[ATraining Step: 217  | total loss: [1m[32m0.69338[0m[0m | time: 7.290s
[2K
| RMSProp | epoch: 004 | loss: 0.69338 - acc: 0.5163 -- iter: 0224/2226
[A[ATraining Step: 218  | total loss: [1m[32m0.69313[0m[0m | time: 8.544s
[2K
| RMSProp | epoch: 004 | loss: 0.69313 - acc: 0.5209 -- iter: 0256/2226
[A[ATraining Step: 219  | total loss: [1m[32m0.69303[0m[0m | time: 16.152s
[2K
| RMSProp | epoch: 004 | loss: 0.69303 - acc: 0.5219 -- iter: 0288/2226
[A[ATraining Step: 220  | total loss: [1m[32m0.69305[0m[0m | time: 19.122s
[2K
| RMSProp | epoch: 004 | loss: 0.69305 - acc: 0.5197 -- iter: 0320/2226
[A[ATraining Step: 221  | total loss: [1m[32m0.69240[0m[0m | time: 20.394s
[2K
| RMSProp | epoch: 004 | loss: 0.69240 - acc: 0.5334 -- iter: 0352/2226
[A[ATraining Step: 222  | total loss: [1m[32m0.69253[0m[0m | time: 21.685s
[2K
| RMSProp | epoch: 004 | loss: 0.69253 - acc: 0.5300 -- iter: 0384/2226
[A[ATraining Step: 223  | total loss: [1m[32m0.69246[0m[0m | time: 22.753s
[2K
| RMSProp | epoch: 004 | loss: 0.69246 - acc: 0.5302 -- iter: 0416/2226
[A[ATraining Step: 224  | total loss: [1m[32m0.69168[0m[0m | time: 23.952s
[2K
| RMSProp | epoch: 004 | loss: 0.69168 - acc: 0.5396 -- iter: 0448/2226
[A[ATraining Step: 225  | total loss: [1m[32m0.69116[0m[0m | time: 25.149s
[2K
| RMSProp | epoch: 004 | loss: 0.69116 - acc: 0.5419 -- iter: 0480/2226
[A[ATraining Step: 226  | total loss: [1m[32m0.69018[0m[0m | time: 26.323s
[2K
| RMSProp | epoch: 004 | loss: 0.69018 - acc: 0.5471 -- iter: 0512/2226
[A[ATraining Step: 227  | total loss: [1m[32m0.69411[0m[0m | time: 27.526s
[2K
| RMSProp | epoch: 004 | loss: 0.69411 - acc: 0.5362 -- iter: 0544/2226
[A[ATraining Step: 228  | total loss: [1m[32m0.69483[0m[0m | time: 28.659s
[2K
| RMSProp | epoch: 004 | loss: 0.69483 - acc: 0.5200 -- iter: 0576/2226
[A[ATraining Step: 229  | total loss: [1m[32m0.69453[0m[0m | time: 29.844s
[2K
| RMSProp | epoch: 004 | loss: 0.69453 - acc: 0.5243 -- iter: 0608/2226
[A[ATraining Step: 230  | total loss: [1m[32m0.69443[0m[0m | time: 31.460s
[2K
| RMSProp | epoch: 004 | loss: 0.69443 - acc: 0.5219 -- iter: 0640/2226
[A[ATraining Step: 231  | total loss: [1m[32m0.69476[0m[0m | time: 35.039s
[2K
| RMSProp | epoch: 004 | loss: 0.69476 - acc: 0.5072 -- iter: 0672/2226
[A[ATraining Step: 232  | total loss: [1m[32m0.69460[0m[0m | time: 38.391s
[2K
| RMSProp | epoch: 004 | loss: 0.69460 - acc: 0.5065 -- iter: 0704/2226
[A[ATraining Step: 233  | total loss: [1m[32m0.69443[0m[0m | time: 44.073s
[2K
| RMSProp | epoch: 004 | loss: 0.69443 - acc: 0.5089 -- iter: 0736/2226
[A[ATraining Step: 234  | total loss: [1m[32m0.69423[0m[0m | time: 47.837s
[2K
| RMSProp | epoch: 004 | loss: 0.69423 - acc: 0.5112 -- iter: 0768/2226
[A[ATraining Step: 235  | total loss: [1m[32m0.69392[0m[0m | time: 50.846s
[2K
| RMSProp | epoch: 004 | loss: 0.69392 - acc: 0.5194 -- iter: 0800/2226
[A[ATraining Step: 236  | total loss: [1m[32m0.69337[0m[0m | time: 52.842s
[2K
| RMSProp | epoch: 004 | loss: 0.69337 - acc: 0.5300 -- iter: 0832/2226
[A[ATraining Step: 237  | total loss: [1m[32m0.69285[0m[0m | time: 54.013s
[2K
| RMSProp | epoch: 004 | loss: 0.69285 - acc: 0.5364 -- iter: 0864/2226
[A[ATraining Step: 238  | total loss: [1m[32m0.69502[0m[0m | time: 55.163s
[2K
| RMSProp | epoch: 004 | loss: 0.69502 - acc: 0.5077 -- iter: 0896/2226
[A[ATraining Step: 239  | total loss: [1m[32m0.69472[0m[0m | time: 56.307s
[2K
| RMSProp | epoch: 004 | loss: 0.69472 - acc: 0.5101 -- iter: 0928/2226
[A[ATraining Step: 240  | total loss: [1m[32m0.69472[0m[0m | time: 57.524s
[2K
| RMSProp | epoch: 004 | loss: 0.69472 - acc: 0.5059 -- iter: 0960/2226
[A[ATraining Step: 241  | total loss: [1m[32m0.69466[0m[0m | time: 58.658s
[2K
| RMSProp | epoch: 004 | loss: 0.69466 - acc: 0.5022 -- iter: 0992/2226
[A[ATraining Step: 242  | total loss: [1m[32m0.69458[0m[0m | time: 60.051s
[2K
| RMSProp | epoch: 004 | loss: 0.69458 - acc: 0.4989 -- iter: 1024/2226
[A[ATraining Step: 243  | total loss: [1m[32m0.69465[0m[0m | time: 61.204s
[2K
| RMSProp | epoch: 004 | loss: 0.69465 - acc: 0.4834 -- iter: 1056/2226
[A[ATraining Step: 244  | total loss: [1m[32m0.69425[0m[0m | time: 62.310s
[2K
| RMSProp | epoch: 004 | loss: 0.69425 - acc: 0.4913 -- iter: 1088/2226
[A[ATraining Step: 245  | total loss: [1m[32m0.69273[0m[0m | time: 63.365s
[2K
| RMSProp | epoch: 004 | loss: 0.69273 - acc: 0.5015 -- iter: 1120/2226
[A[ATraining Step: 246  | total loss: [1m[32m0.70550[0m[0m | time: 64.412s
[2K
| RMSProp | epoch: 004 | loss: 0.70550 - acc: 0.4889 -- iter: 1152/2226
[A[ATraining Step: 247  | total loss: [1m[32m0.70479[0m[0m | time: 65.544s
[2K
| RMSProp | epoch: 004 | loss: 0.70479 - acc: 0.4806 -- iter: 1184/2226
[A[ATraining Step: 248  | total loss: [1m[32m0.70370[0m[0m | time: 66.627s
[2K
| RMSProp | epoch: 004 | loss: 0.70370 - acc: 0.4794 -- iter: 1216/2226
[A[ATraining Step: 249  | total loss: [1m[32m0.70245[0m[0m | time: 67.818s
[2K
| RMSProp | epoch: 004 | loss: 0.70245 - acc: 0.4846 -- iter: 1248/2226
[A[ATraining Step: 250  | total loss: [1m[32m0.70219[0m[0m | time: 69.093s
[2K
| RMSProp | epoch: 004 | loss: 0.70219 - acc: 0.4674 -- iter: 1280/2226
[A[ATraining Step: 251  | total loss: [1m[32m0.70131[0m[0m | time: 70.340s
[2K
| RMSProp | epoch: 004 | loss: 0.70131 - acc: 0.4644 -- iter: 1312/2226
[A[ATraining Step: 252  | total loss: [1m[32m0.70046[0m[0m | time: 71.587s
[2K
| RMSProp | epoch: 004 | loss: 0.70046 - acc: 0.4680 -- iter: 1344/2226
[A[ATraining Step: 253  | total loss: [1m[32m0.69969[0m[0m | time: 72.794s
[2K
| RMSProp | epoch: 004 | loss: 0.69969 - acc: 0.4680 -- iter: 1376/2226
[A[ATraining Step: 254  | total loss: [1m[32m0.69888[0m[0m | time: 73.671s
[2K
| RMSProp | epoch: 004 | loss: 0.69888 - acc: 0.4900 -- iter: 1408/2226
[A[ATraining Step: 255  | total loss: [1m[32m0.69781[0m[0m | time: 74.532s
[2K
| RMSProp | epoch: 004 | loss: 0.69781 - acc: 0.5004 -- iter: 1440/2226
[A[ATraining Step: 256  | total loss: [1m[32m0.69741[0m[0m | time: 75.347s
[2K
| RMSProp | epoch: 004 | loss: 0.69741 - acc: 0.5003 -- iter: 1472/2226
[A[ATraining Step: 257  | total loss: [1m[32m0.69746[0m[0m | time: 76.160s
[2K
| RMSProp | epoch: 004 | loss: 0.69746 - acc: 0.4940 -- iter: 1504/2226
[A[ATraining Step: 258  | total loss: [1m[32m0.69700[0m[0m | time: 77.014s
[2K
| RMSProp | epoch: 004 | loss: 0.69700 - acc: 0.4946 -- iter: 1536/2226
[A[ATraining Step: 259  | total loss: [1m[32m0.69637[0m[0m | time: 77.834s
[2K
| RMSProp | epoch: 004 | loss: 0.69637 - acc: 0.5014 -- iter: 1568/2226
[A[ATraining Step: 260  | total loss: [1m[32m0.69605[0m[0m | time: 78.655s
[2K
| RMSProp | epoch: 004 | loss: 0.69605 - acc: 0.5013 -- iter: 1600/2226
[A[ATraining Step: 261  | total loss: [1m[32m0.69599[0m[0m | time: 79.415s
[2K
| RMSProp | epoch: 004 | loss: 0.69599 - acc: 0.4918 -- iter: 1632/2226
[A[ATraining Step: 262  | total loss: [1m[32m0.69547[0m[0m | time: 80.296s
[2K
| RMSProp | epoch: 004 | loss: 0.69547 - acc: 0.4989 -- iter: 1664/2226
[A[ATraining Step: 263  | total loss: [1m[32m0.69555[0m[0m | time: 81.441s
[2K
| RMSProp | epoch: 004 | loss: 0.69555 - acc: 0.4896 -- iter: 1696/2226
[A[ATraining Step: 264  | total loss: [1m[32m0.69523[0m[0m | time: 82.342s
[2K
| RMSProp | epoch: 004 | loss: 0.69523 - acc: 0.5031 -- iter: 1728/2226
[A[ATraining Step: 265  | total loss: [1m[32m0.69475[0m[0m | time: 83.078s
[2K
| RMSProp | epoch: 004 | loss: 0.69475 - acc: 0.5216 -- iter: 1760/2226
[A[ATraining Step: 266  | total loss: [1m[32m0.69538[0m[0m | time: 83.891s
[2K
| RMSProp | epoch: 004 | loss: 0.69538 - acc: 0.5007 -- iter: 1792/2226
[A[ATraining Step: 267  | total loss: [1m[32m0.69487[0m[0m | time: 84.718s
[2K
| RMSProp | epoch: 004 | loss: 0.69487 - acc: 0.5068 -- iter: 1824/2226
[A[ATraining Step: 268  | total loss: [1m[32m0.69421[0m[0m | time: 85.533s
[2K
| RMSProp | epoch: 004 | loss: 0.69421 - acc: 0.5155 -- iter: 1856/2226
[A[ATraining Step: 269  | total loss: [1m[32m0.69491[0m[0m | time: 86.382s
[2K
| RMSProp | epoch: 004 | loss: 0.69491 - acc: 0.5015 -- iter: 1888/2226
[A[ATraining Step: 270  | total loss: [1m[32m0.69460[0m[0m | time: 87.181s
[2K
| RMSProp | epoch: 004 | loss: 0.69460 - acc: 0.5045 -- iter: 1920/2226
[A[ATraining Step: 271  | total loss: [1m[32m0.69447[0m[0m | time: 88.019s
[2K
| RMSProp | epoch: 004 | loss: 0.69447 - acc: 0.5009 -- iter: 1952/2226
[A[ATraining Step: 272  | total loss: [1m[32m0.69422[0m[0m | time: 88.879s
[2K
| RMSProp | epoch: 004 | loss: 0.69422 - acc: 0.5102 -- iter: 1984/2226
[A[ATraining Step: 273  | total loss: [1m[32m0.69377[0m[0m | time: 89.721s
[2K
| RMSProp | epoch: 004 | loss: 0.69377 - acc: 0.5123 -- iter: 2016/2226
[A[ATraining Step: 274  | total loss: [1m[32m0.69438[0m[0m | time: 90.561s
[2K
| RMSProp | epoch: 004 | loss: 0.69438 - acc: 0.4986 -- iter: 2048/2226
[A[ATraining Step: 275  | total loss: [1m[32m0.69431[0m[0m | time: 91.387s
[2K
| RMSProp | epoch: 004 | loss: 0.69431 - acc: 0.4893 -- iter: 2080/2226
[A[ATraining Step: 276  | total loss: [1m[32m0.69403[0m[0m | time: 92.247s
[2K
| RMSProp | epoch: 004 | loss: 0.69403 - acc: 0.4904 -- iter: 2112/2226
[A[ATraining Step: 277  | total loss: [1m[32m0.69384[0m[0m | time: 93.420s
[2K
| RMSProp | epoch: 004 | loss: 0.69384 - acc: 0.4976 -- iter: 2144/2226
[A[ATraining Step: 278  | total loss: [1m[32m0.69393[0m[0m | time: 94.388s
[2K
| RMSProp | epoch: 004 | loss: 0.69393 - acc: 0.4853 -- iter: 2176/2226
[A[ATraining Step: 279  | total loss: [1m[32m0.69396[0m[0m | time: 95.463s
[2K
| RMSProp | epoch: 004 | loss: 0.69396 - acc: 0.4774 -- iter: 2208/2226
[A[ATraining Step: 280  | total loss: [1m[32m0.69350[0m[0m | time: 116.802s
[2K
| RMSProp | epoch: 004 | loss: 0.69350 - acc: 0.4922 | val_loss: 0.69183 - val_acc: 0.4986 -- iter: 2226/2226
--
Training Step: 281  | total loss: [1m[32m0.69279[0m[0m | time: 2.033s
[2K
| RMSProp | epoch: 005 | loss: 0.69279 - acc: 0.5023 -- iter: 0032/2226
[A[ATraining Step: 282  | total loss: [1m[32m0.69214[0m[0m | time: 5.506s
[2K
| RMSProp | epoch: 005 | loss: 0.69214 - acc: 0.5052 -- iter: 0064/2226
[A[ATraining Step: 283  | total loss: [1m[32m0.69239[0m[0m | time: 7.218s
[2K
| RMSProp | epoch: 005 | loss: 0.69239 - acc: 0.5047 -- iter: 0096/2226
[A[ATraining Step: 284  | total loss: [1m[32m0.69079[0m[0m | time: 8.942s
[2K
| RMSProp | epoch: 005 | loss: 0.69079 - acc: 0.5209 -- iter: 0128/2226
[A[ATraining Step: 285  | total loss: [1m[32m0.68571[0m[0m | time: 11.257s
[2K
| RMSProp | epoch: 005 | loss: 0.68571 - acc: 0.5355 -- iter: 0160/2226
[A[ATraining Step: 286  | total loss: [1m[32m0.69722[0m[0m | time: 12.297s
[2K
| RMSProp | epoch: 005 | loss: 0.69722 - acc: 0.5319 -- iter: 0192/2226
[A[ATraining Step: 287  | total loss: [1m[32m0.69795[0m[0m | time: 13.475s
[2K
| RMSProp | epoch: 005 | loss: 0.69795 - acc: 0.5225 -- iter: 0224/2226
[A[ATraining Step: 288  | total loss: [1m[32m0.69831[0m[0m | time: 14.611s
[2K
| RMSProp | epoch: 005 | loss: 0.69831 - acc: 0.5077 -- iter: 0256/2226
[A[ATraining Step: 289  | total loss: [1m[32m0.69709[0m[0m | time: 15.776s
[2K
| RMSProp | epoch: 005 | loss: 0.69709 - acc: 0.5163 -- iter: 0288/2226
[A[ATraining Step: 290  | total loss: [1m[32m0.69666[0m[0m | time: 16.936s
[2K
| RMSProp | epoch: 005 | loss: 0.69666 - acc: 0.5116 -- iter: 0320/2226
[A[ATraining Step: 291  | total loss: [1m[32m0.69605[0m[0m | time: 18.167s
[2K
| RMSProp | epoch: 005 | loss: 0.69605 - acc: 0.5136 -- iter: 0352/2226
[A[ATraining Step: 292  | total loss: [1m[32m0.69511[0m[0m | time: 19.387s
[2K
| RMSProp | epoch: 005 | loss: 0.69511 - acc: 0.5372 -- iter: 0384/2226
[A[ATraining Step: 293  | total loss: [1m[32m0.69446[0m[0m | time: 20.541s
[2K
| RMSProp | epoch: 005 | loss: 0.69446 - acc: 0.5272 -- iter: 0416/2226
[A[ATraining Step: 294  | total loss: [1m[32m0.69532[0m[0m | time: 21.750s
[2K
| RMSProp | epoch: 005 | loss: 0.69532 - acc: 0.5089 -- iter: 0448/2226
[A[ATraining Step: 295  | total loss: [1m[32m0.69451[0m[0m | time: 22.851s
[2K
| RMSProp | epoch: 005 | loss: 0.69451 - acc: 0.5299 -- iter: 0480/2226
[A[ATraining Step: 296  | total loss: [1m[32m0.69407[0m[0m | time: 26.928s
[2K
| RMSProp | epoch: 005 | loss: 0.69407 - acc: 0.5269 -- iter: 0512/2226
[A[ATraining Step: 297  | total loss: [1m[32m0.69320[0m[0m | time: 29.783s
[2K
| RMSProp | epoch: 005 | loss: 0.69320 - acc: 0.5492 -- iter: 0544/2226
[A[ATraining Step: 298  | total loss: [1m[32m0.69209[0m[0m | time: 34.216s
[2K
| RMSProp | epoch: 005 | loss: 0.69209 - acc: 0.5693 -- iter: 0576/2226
[A[ATraining Step: 299  | total loss: [1m[32m0.69407[0m[0m | time: 36.650s
[2K
| RMSProp | epoch: 005 | loss: 0.69407 - acc: 0.5530 -- iter: 0608/2226
[A[ATraining Step: 300  | total loss: [1m[32m0.69397[0m[0m | time: 38.638s
[2K
| RMSProp | epoch: 005 | loss: 0.69397 - acc: 0.5383 -- iter: 0640/2226
[A[ATraining Step: 301  | total loss: [1m[32m0.69259[0m[0m | time: 39.815s
[2K
| RMSProp | epoch: 005 | loss: 0.69259 - acc: 0.5470 -- iter: 0672/2226
[A[ATraining Step: 302  | total loss: [1m[32m0.69241[0m[0m | time: 41.029s
[2K
| RMSProp | epoch: 005 | loss: 0.69241 - acc: 0.5391 -- iter: 0704/2226
[A[ATraining Step: 303  | total loss: [1m[32m0.69307[0m[0m | time: 42.027s
[2K
| RMSProp | epoch: 005 | loss: 0.69307 - acc: 0.5227 -- iter: 0736/2226
[A[ATraining Step: 304  | total loss: [1m[32m0.69183[0m[0m | time: 43.142s
[2K
| RMSProp | epoch: 005 | loss: 0.69183 - acc: 0.5455 -- iter: 0768/2226
[A[ATraining Step: 305  | total loss: [1m[32m0.69149[0m[0m | time: 44.185s
[2K
| RMSProp | epoch: 005 | loss: 0.69149 - acc: 0.5347 -- iter: 0800/2226
[A[ATraining Step: 306  | total loss: [1m[32m0.69064[0m[0m | time: 45.342s
[2K
| RMSProp | epoch: 005 | loss: 0.69064 - acc: 0.5343 -- iter: 0832/2226
[A[ATraining Step: 307  | total loss: [1m[32m0.68897[0m[0m | time: 46.502s
[2K
| RMSProp | epoch: 005 | loss: 0.68897 - acc: 0.5496 -- iter: 0864/2226
[A[ATraining Step: 308  | total loss: [1m[32m0.68890[0m[0m | time: 47.692s
[2K
| RMSProp | epoch: 005 | loss: 0.68890 - acc: 0.5447 -- iter: 0896/2226
[A[ATraining Step: 309  | total loss: [1m[32m0.68940[0m[0m | time: 48.794s
[2K
| RMSProp | epoch: 005 | loss: 0.68940 - acc: 0.5433 -- iter: 0928/2226
[A[ATraining Step: 310  | total loss: [1m[32m0.68719[0m[0m | time: 50.237s
[2K
| RMSProp | epoch: 005 | loss: 0.68719 - acc: 0.5796 -- iter: 0960/2226
[A[ATraining Step: 311  | total loss: [1m[32m0.69013[0m[0m | time: 54.953s
[2K
| RMSProp | epoch: 005 | loss: 0.69013 - acc: 0.5654 -- iter: 0992/2226
[A[ATraining Step: 312  | total loss: [1m[32m0.68878[0m[0m | time: 57.777s
[2K
| RMSProp | epoch: 005 | loss: 0.68878 - acc: 0.5745 -- iter: 1024/2226
[A[ATraining Step: 313  | total loss: [1m[32m0.68767[0m[0m | time: 61.086s
[2K
| RMSProp | epoch: 005 | loss: 0.68767 - acc: 0.5827 -- iter: 1056/2226
[A[ATraining Step: 314  | total loss: [1m[32m0.68552[0m[0m | time: 63.778s
[2K
| RMSProp | epoch: 005 | loss: 0.68552 - acc: 0.5807 -- iter: 1088/2226
[A[ATraining Step: 315  | total loss: [1m[32m0.68416[0m[0m | time: 66.680s
[2K
| RMSProp | epoch: 005 | loss: 0.68416 - acc: 0.5820 -- iter: 1120/2226
[A[ATraining Step: 316  | total loss: [1m[32m0.68221[0m[0m | time: 69.692s
[2K
| RMSProp | epoch: 005 | loss: 0.68221 - acc: 0.5894 -- iter: 1152/2226
[A[ATraining Step: 317  | total loss: [1m[32m0.67974[0m[0m | time: 71.889s
[2K
| RMSProp | epoch: 005 | loss: 0.67974 - acc: 0.5930 -- iter: 1184/2226
[A[ATraining Step: 318  | total loss: [1m[32m0.67854[0m[0m | time: 73.391s
[2K
| RMSProp | epoch: 005 | loss: 0.67854 - acc: 0.5930 -- iter: 1216/2226
[A[ATraining Step: 319  | total loss: [1m[32m0.67779[0m[0m | time: 76.406s
[2K
| RMSProp | epoch: 005 | loss: 0.67779 - acc: 0.5900 -- iter: 1248/2226
[A[ATraining Step: 320  | total loss: [1m[32m0.67536[0m[0m | time: 79.025s
[2K
| RMSProp | epoch: 005 | loss: 0.67536 - acc: 0.5904 -- iter: 1280/2226
[A[ATraining Step: 321  | total loss: [1m[32m0.66992[0m[0m | time: 80.634s
[2K
| RMSProp | epoch: 005 | loss: 0.66992 - acc: 0.6032 -- iter: 1312/2226
[A[ATraining Step: 322  | total loss: [1m[32m0.66732[0m[0m | time: 81.828s
[2K
| RMSProp | epoch: 005 | loss: 0.66732 - acc: 0.6054 -- iter: 1344/2226
[A[ATraining Step: 323  | total loss: [1m[32m0.70722[0m[0m | time: 83.066s
[2K
| RMSProp | epoch: 005 | loss: 0.70722 - acc: 0.5792 -- iter: 1376/2226
[A[ATraining Step: 324  | total loss: [1m[32m0.70836[0m[0m | time: 84.260s
[2K
| RMSProp | epoch: 005 | loss: 0.70836 - acc: 0.5650 -- iter: 1408/2226
[A[ATraining Step: 325  | total loss: [1m[32m0.70694[0m[0m | time: 85.407s
[2K
| RMSProp | epoch: 005 | loss: 0.70694 - acc: 0.5585 -- iter: 1440/2226
[A[ATraining Step: 326  | total loss: [1m[32m0.70529[0m[0m | time: 86.669s
[2K
| RMSProp | epoch: 005 | loss: 0.70529 - acc: 0.5464 -- iter: 1472/2226
[A[ATraining Step: 327  | total loss: [1m[32m0.70207[0m[0m | time: 87.838s
[2K
| RMSProp | epoch: 005 | loss: 0.70207 - acc: 0.5480 -- iter: 1504/2226
[A[ATraining Step: 328  | total loss: [1m[32m0.70153[0m[0m | time: 89.125s
[2K
| RMSProp | epoch: 005 | loss: 0.70153 - acc: 0.5370 -- iter: 1536/2226
[A[ATraining Step: 329  | total loss: [1m[32m0.69568[0m[0m | time: 90.249s
[2K
| RMSProp | epoch: 005 | loss: 0.69568 - acc: 0.5552 -- iter: 1568/2226
[A[ATraining Step: 330  | total loss: [1m[32m0.69578[0m[0m | time: 91.495s
[2K
| RMSProp | epoch: 005 | loss: 0.69578 - acc: 0.5465 -- iter: 1600/2226
[A[ATraining Step: 331  | total loss: [1m[32m0.69593[0m[0m | time: 95.712s
[2K
| RMSProp | epoch: 005 | loss: 0.69593 - acc: 0.5419 -- iter: 1632/2226
[A[ATraining Step: 332  | total loss: [1m[32m0.69319[0m[0m | time: 99.942s
[2K
| RMSProp | epoch: 005 | loss: 0.69319 - acc: 0.5502 -- iter: 1664/2226
[A[ATraining Step: 333  | total loss: [1m[32m0.68620[0m[0m | time: 103.582s
[2K
| RMSProp | epoch: 005 | loss: 0.68620 - acc: 0.5639 -- iter: 1696/2226
[A[ATraining Step: 334  | total loss: [1m[32m0.68106[0m[0m | time: 105.848s
[2K
| RMSProp | epoch: 005 | loss: 0.68106 - acc: 0.5731 -- iter: 1728/2226
[A[ATraining Step: 335  | total loss: [1m[32m0.67785[0m[0m | time: 106.926s
[2K
| RMSProp | epoch: 005 | loss: 0.67785 - acc: 0.5877 -- iter: 1760/2226
[A[ATraining Step: 336  | total loss: [1m[32m0.67088[0m[0m | time: 108.046s
[2K
| RMSProp | epoch: 005 | loss: 0.67088 - acc: 0.6071 -- iter: 1792/2226
[A[ATraining Step: 337  | total loss: [1m[32m0.65774[0m[0m | time: 109.164s
[2K
| RMSProp | epoch: 005 | loss: 0.65774 - acc: 0.6276 -- iter: 1824/2226
[A[ATraining Step: 338  | total loss: [1m[32m0.65578[0m[0m | time: 110.307s
[2K
| RMSProp | epoch: 005 | loss: 0.65578 - acc: 0.6336 -- iter: 1856/2226
[A[ATraining Step: 339  | total loss: [1m[32m0.66790[0m[0m | time: 111.481s
[2K
| RMSProp | epoch: 005 | loss: 0.66790 - acc: 0.6140 -- iter: 1888/2226
[A[ATraining Step: 340  | total loss: [1m[32m0.66042[0m[0m | time: 112.669s
[2K
| RMSProp | epoch: 005 | loss: 0.66042 - acc: 0.6370 -- iter: 1920/2226
[A[ATraining Step: 341  | total loss: [1m[32m0.65553[0m[0m | time: 114.009s
[2K
| RMSProp | epoch: 005 | loss: 0.65553 - acc: 0.6420 -- iter: 1952/2226
[A[ATraining Step: 342  | total loss: [1m[32m0.65979[0m[0m | time: 115.090s
[2K
| RMSProp | epoch: 005 | loss: 0.65979 - acc: 0.6278 -- iter: 1984/2226
[A[ATraining Step: 343  | total loss: [1m[32m0.65408[0m[0m | time: 116.282s
[2K
| RMSProp | epoch: 005 | loss: 0.65408 - acc: 0.6400 -- iter: 2016/2226
[A[ATraining Step: 344  | total loss: [1m[32m0.64873[0m[0m | time: 120.742s
[2K
| RMSProp | epoch: 005 | loss: 0.64873 - acc: 0.6448 -- iter: 2048/2226
[A[ATraining Step: 345  | total loss: [1m[32m0.64872[0m[0m | time: 122.345s
[2K
| RMSProp | epoch: 005 | loss: 0.64872 - acc: 0.6522 -- iter: 2080/2226
[A[ATraining Step: 346  | total loss: [1m[32m0.64186[0m[0m | time: 127.177s
[2K
| RMSProp | epoch: 005 | loss: 0.64186 - acc: 0.6557 -- iter: 2112/2226
[A[ATraining Step: 347  | total loss: [1m[32m0.66531[0m[0m | time: 130.148s
[2K
| RMSProp | epoch: 005 | loss: 0.66531 - acc: 0.6401 -- iter: 2144/2226
[A[ATraining Step: 348  | total loss: [1m[32m0.65960[0m[0m | time: 133.919s
[2K
| RMSProp | epoch: 005 | loss: 0.65960 - acc: 0.6542 -- iter: 2176/2226
[A[ATraining Step: 349  | total loss: [1m[32m0.65797[0m[0m | time: 135.616s
[2K
| RMSProp | epoch: 005 | loss: 0.65797 - acc: 0.6576 -- iter: 2208/2226
[A[ATraining Step: 350  | total loss: [1m[32m0.65483[0m[0m | time: 141.610s
[2K
| RMSProp | epoch: 005 | loss: 0.65483 - acc: 0.6606 | val_loss: 0.57965 - val_acc: 0.7126 -- iter: 2226/2226
--
Training Step: 351  | total loss: [1m[32m0.64549[0m[0m | time: 1.102s
[2K
| RMSProp | epoch: 006 | loss: 0.64549 - acc: 0.6726 -- iter: 0032/2226
[A[ATraining Step: 352  | total loss: [1m[32m0.64258[0m[0m | time: 2.197s
[2K
| RMSProp | epoch: 006 | loss: 0.64258 - acc: 0.6772 -- iter: 0064/2226
[A[ATraining Step: 353  | total loss: [1m[32m0.63457[0m[0m | time: 3.376s
[2K
| RMSProp | epoch: 006 | loss: 0.63457 - acc: 0.6814 -- iter: 0096/2226
[A[ATraining Step: 354  | total loss: [1m[32m0.63632[0m[0m | time: 4.124s
[2K
| RMSProp | epoch: 006 | loss: 0.63632 - acc: 0.6726 -- iter: 0128/2226
[A[ATraining Step: 355  | total loss: [1m[32m0.65733[0m[0m | time: 4.974s
[2K
| RMSProp | epoch: 006 | loss: 0.65733 - acc: 0.6498 -- iter: 0160/2226
[A[ATraining Step: 356  | total loss: [1m[32m0.65560[0m[0m | time: 13.571s
[2K
| RMSProp | epoch: 006 | loss: 0.65560 - acc: 0.6459 -- iter: 0192/2226
[A[ATraining Step: 357  | total loss: [1m[32m0.64673[0m[0m | time: 16.710s
[2K
| RMSProp | epoch: 006 | loss: 0.64673 - acc: 0.6657 -- iter: 0224/2226
[A[ATraining Step: 358  | total loss: [1m[32m0.64448[0m[0m | time: 20.883s
[2K
| RMSProp | epoch: 006 | loss: 0.64448 - acc: 0.6773 -- iter: 0256/2226
[A[ATraining Step: 359  | total loss: [1m[32m0.63515[0m[0m | time: 22.363s
[2K
| RMSProp | epoch: 006 | loss: 0.63515 - acc: 0.6939 -- iter: 0288/2226
[A[ATraining Step: 360  | total loss: [1m[32m0.62254[0m[0m | time: 23.347s
[2K
| RMSProp | epoch: 006 | loss: 0.62254 - acc: 0.7058 -- iter: 0320/2226
[A[ATraining Step: 361  | total loss: [1m[32m0.61335[0m[0m | time: 24.567s
[2K
| RMSProp | epoch: 006 | loss: 0.61335 - acc: 0.7071 -- iter: 0352/2226
[A[ATraining Step: 362  | total loss: [1m[32m0.60634[0m[0m | time: 25.669s
[2K
| RMSProp | epoch: 006 | loss: 0.60634 - acc: 0.7051 -- iter: 0384/2226
[A[ATraining Step: 363  | total loss: [1m[32m0.60341[0m[0m | time: 26.869s
[2K
| RMSProp | epoch: 006 | loss: 0.60341 - acc: 0.7096 -- iter: 0416/2226
[A[ATraining Step: 364  | total loss: [1m[32m0.64119[0m[0m | time: 28.138s
[2K
| RMSProp | epoch: 006 | loss: 0.64119 - acc: 0.6793 -- iter: 0448/2226
[A[ATraining Step: 365  | total loss: [1m[32m0.64065[0m[0m | time: 29.386s
[2K
| RMSProp | epoch: 006 | loss: 0.64065 - acc: 0.6801 -- iter: 0480/2226
[A[ATraining Step: 366  | total loss: [1m[32m0.63327[0m[0m | time: 30.590s
[2K
| RMSProp | epoch: 006 | loss: 0.63327 - acc: 0.6840 -- iter: 0512/2226
[A[ATraining Step: 367  | total loss: [1m[32m0.62431[0m[0m | time: 31.868s
[2K
| RMSProp | epoch: 006 | loss: 0.62431 - acc: 0.6937 -- iter: 0544/2226
[A[ATraining Step: 368  | total loss: [1m[32m0.60988[0m[0m | time: 33.089s
[2K
| RMSProp | epoch: 006 | loss: 0.60988 - acc: 0.7149 -- iter: 0576/2226
[A[ATraining Step: 369  | total loss: [1m[32m0.60195[0m[0m | time: 37.599s
[2K
| RMSProp | epoch: 006 | loss: 0.60195 - acc: 0.7122 -- iter: 0608/2226
[A[ATraining Step: 370  | total loss: [1m[32m0.59873[0m[0m | time: 43.230s
[2K
| RMSProp | epoch: 006 | loss: 0.59873 - acc: 0.7160 -- iter: 0640/2226
[A[ATraining Step: 371  | total loss: [1m[32m0.64680[0m[0m | time: 46.571s
[2K
| RMSProp | epoch: 006 | loss: 0.64680 - acc: 0.6850 -- iter: 0672/2226
[A[ATraining Step: 372  | total loss: [1m[32m0.63682[0m[0m | time: 49.373s
[2K
| RMSProp | epoch: 006 | loss: 0.63682 - acc: 0.6853 -- iter: 0704/2226
[A[ATraining Step: 373  | total loss: [1m[32m0.62721[0m[0m | time: 52.443s
[2K
| RMSProp | epoch: 006 | loss: 0.62721 - acc: 0.7011 -- iter: 0736/2226
[A[ATraining Step: 374  | total loss: [1m[32m0.61529[0m[0m | time: 54.042s
[2K
| RMSProp | epoch: 006 | loss: 0.61529 - acc: 0.7091 -- iter: 0768/2226
[A[ATraining Step: 375  | total loss: [1m[32m0.61055[0m[0m | time: 55.172s
[2K
| RMSProp | epoch: 006 | loss: 0.61055 - acc: 0.7070 -- iter: 0800/2226
[A[ATraining Step: 376  | total loss: [1m[32m0.62426[0m[0m | time: 56.380s
[2K
| RMSProp | epoch: 006 | loss: 0.62426 - acc: 0.6894 -- iter: 0832/2226
[A[ATraining Step: 377  | total loss: [1m[32m0.61122[0m[0m | time: 57.576s
[2K
| RMSProp | epoch: 006 | loss: 0.61122 - acc: 0.6986 -- iter: 0864/2226
[A[ATraining Step: 378  | total loss: [1m[32m0.61215[0m[0m | time: 58.751s
[2K
| RMSProp | epoch: 006 | loss: 0.61215 - acc: 0.6912 -- iter: 0896/2226
[A[ATraining Step: 379  | total loss: [1m[32m0.61652[0m[0m | time: 59.925s
[2K
| RMSProp | epoch: 006 | loss: 0.61652 - acc: 0.6815 -- iter: 0928/2226
[A[ATraining Step: 380  | total loss: [1m[32m0.60934[0m[0m | time: 61.163s
[2K
| RMSProp | epoch: 006 | loss: 0.60934 - acc: 0.6883 -- iter: 0960/2226
[A[ATraining Step: 381  | total loss: [1m[32m0.60474[0m[0m | time: 62.280s
[2K
| RMSProp | epoch: 006 | loss: 0.60474 - acc: 0.6882 -- iter: 0992/2226
[A[ATraining Step: 382  | total loss: [1m[32m0.60596[0m[0m | time: 63.454s
[2K
| RMSProp | epoch: 006 | loss: 0.60596 - acc: 0.6944 -- iter: 1024/2226
[A[ATraining Step: 383  | total loss: [1m[32m0.61722[0m[0m | time: 64.744s
[2K
| RMSProp | epoch: 006 | loss: 0.61722 - acc: 0.6781 -- iter: 1056/2226
[A[ATraining Step: 384  | total loss: [1m[32m0.60977[0m[0m | time: 66.101s
[2K
| RMSProp | epoch: 006 | loss: 0.60977 - acc: 0.6947 -- iter: 1088/2226
[A[ATraining Step: 385  | total loss: [1m[32m0.61189[0m[0m | time: 67.515s
[2K
| RMSProp | epoch: 006 | loss: 0.61189 - acc: 0.6814 -- iter: 1120/2226
[A[ATraining Step: 386  | total loss: [1m[32m0.61817[0m[0m | time: 68.673s
[2K
| RMSProp | epoch: 006 | loss: 0.61817 - acc: 0.6696 -- iter: 1152/2226
[A[ATraining Step: 387  | total loss: [1m[32m0.60557[0m[0m | time: 69.497s
[2K
| RMSProp | epoch: 006 | loss: 0.60557 - acc: 0.6932 -- iter: 1184/2226
[A[ATraining Step: 388  | total loss: [1m[32m0.59329[0m[0m | time: 70.325s
[2K
| RMSProp | epoch: 006 | loss: 0.59329 - acc: 0.7083 -- iter: 1216/2226
[A[ATraining Step: 389  | total loss: [1m[32m0.58445[0m[0m | time: 71.200s
[2K
| RMSProp | epoch: 006 | loss: 0.58445 - acc: 0.7156 -- iter: 1248/2226
[A[ATraining Step: 390  | total loss: [1m[32m0.57602[0m[0m | time: 72.041s
[2K
| RMSProp | epoch: 006 | loss: 0.57602 - acc: 0.7253 -- iter: 1280/2226
[A[ATraining Step: 391  | total loss: [1m[32m0.57307[0m[0m | time: 72.858s
[2K
| RMSProp | epoch: 006 | loss: 0.57307 - acc: 0.7184 -- iter: 1312/2226
[A[ATraining Step: 392  | total loss: [1m[32m0.60744[0m[0m | time: 73.678s
[2K
| RMSProp | epoch: 006 | loss: 0.60744 - acc: 0.6903 -- iter: 1344/2226
[A[ATraining Step: 393  | total loss: [1m[32m0.60773[0m[0m | time: 74.515s
[2K
| RMSProp | epoch: 006 | loss: 0.60773 - acc: 0.6931 -- iter: 1376/2226
[A[ATraining Step: 394  | total loss: [1m[32m0.60565[0m[0m | time: 75.377s
[2K
| RMSProp | epoch: 006 | loss: 0.60565 - acc: 0.6957 -- iter: 1408/2226
[A[ATraining Step: 395  | total loss: [1m[32m0.59993[0m[0m | time: 76.196s
[2K
| RMSProp | epoch: 006 | loss: 0.59993 - acc: 0.7011 -- iter: 1440/2226
[A[ATraining Step: 396  | total loss: [1m[32m0.59350[0m[0m | time: 77.049s
[2K
| RMSProp | epoch: 006 | loss: 0.59350 - acc: 0.7091 -- iter: 1472/2226
[A[ATraining Step: 397  | total loss: [1m[32m0.58608[0m[0m | time: 77.871s
[2K
| RMSProp | epoch: 006 | loss: 0.58608 - acc: 0.7038 -- iter: 1504/2226
[A[ATraining Step: 398  | total loss: [1m[32m0.57221[0m[0m | time: 78.729s
[2K
| RMSProp | epoch: 006 | loss: 0.57221 - acc: 0.7210 -- iter: 1536/2226
[A[ATraining Step: 399  | total loss: [1m[32m0.56344[0m[0m | time: 79.571s
[2K
| RMSProp | epoch: 006 | loss: 0.56344 - acc: 0.7301 -- iter: 1568/2226
[A[ATraining Step: 400  | total loss: [1m[32m0.57932[0m[0m | time: 83.840s
[2K
| RMSProp | epoch: 006 | loss: 0.57932 - acc: 0.7196 | val_loss: 0.50955 - val_acc: 0.7658 -- iter: 1600/2226
--
Training Step: 401  | total loss: [1m[32m0.57895[0m[0m | time: 84.748s
[2K
| RMSProp | epoch: 006 | loss: 0.57895 - acc: 0.7070 -- iter: 1632/2226
[A[ATraining Step: 402  | total loss: [1m[32m0.57175[0m[0m | time: 85.599s
[2K
| RMSProp | epoch: 006 | loss: 0.57175 - acc: 0.7207 -- iter: 1664/2226
[A[ATraining Step: 403  | total loss: [1m[32m0.56502[0m[0m | time: 86.425s
[2K
| RMSProp | epoch: 006 | loss: 0.56502 - acc: 0.7267 -- iter: 1696/2226
[A[ATraining Step: 404  | total loss: [1m[32m0.56877[0m[0m | time: 87.287s
[2K
| RMSProp | epoch: 006 | loss: 0.56877 - acc: 0.7166 -- iter: 1728/2226
[A[ATraining Step: 405  | total loss: [1m[32m0.57423[0m[0m | time: 88.223s
[2K
| RMSProp | epoch: 006 | loss: 0.57423 - acc: 0.7105 -- iter: 1760/2226
[A[ATraining Step: 406  | total loss: [1m[32m0.56830[0m[0m | time: 89.344s
[2K
| RMSProp | epoch: 006 | loss: 0.56830 - acc: 0.7114 -- iter: 1792/2226
[A[ATraining Step: 407  | total loss: [1m[32m0.56112[0m[0m | time: 90.403s
[2K
| RMSProp | epoch: 006 | loss: 0.56112 - acc: 0.7121 -- iter: 1824/2226
[A[ATraining Step: 408  | total loss: [1m[32m0.55816[0m[0m | time: 91.465s
[2K
| RMSProp | epoch: 006 | loss: 0.55816 - acc: 0.7128 -- iter: 1856/2226
[A[ATraining Step: 409  | total loss: [1m[32m0.55565[0m[0m | time: 92.513s
[2K
| RMSProp | epoch: 006 | loss: 0.55565 - acc: 0.7165 -- iter: 1888/2226
[A[ATraining Step: 410  | total loss: [1m[32m0.54993[0m[0m | time: 93.581s
[2K
| RMSProp | epoch: 006 | loss: 0.54993 - acc: 0.7198 -- iter: 1920/2226
[A[ATraining Step: 411  | total loss: [1m[32m0.54036[0m[0m | time: 94.766s
[2K
| RMSProp | epoch: 006 | loss: 0.54036 - acc: 0.7354 -- iter: 1952/2226
[A[ATraining Step: 412  | total loss: [1m[32m0.52361[0m[0m | time: 95.783s
[2K
| RMSProp | epoch: 006 | loss: 0.52361 - acc: 0.7462 -- iter: 1984/2226
[A[ATraining Step: 413  | total loss: [1m[32m0.51359[0m[0m | time: 96.826s
[2K
| RMSProp | epoch: 006 | loss: 0.51359 - acc: 0.7528 -- iter: 2016/2226
[A[ATraining Step: 414  | total loss: [1m[32m0.50404[0m[0m | time: 97.892s
[2K
| RMSProp | epoch: 006 | loss: 0.50404 - acc: 0.7557 -- iter: 2048/2226
[A[ATraining Step: 415  | total loss: [1m[32m0.52589[0m[0m | time: 98.962s
[2K
| RMSProp | epoch: 006 | loss: 0.52589 - acc: 0.7364 -- iter: 2080/2226
[A[ATraining Step: 416  | total loss: [1m[32m0.52700[0m[0m | time: 100.155s
[2K
| RMSProp | epoch: 006 | loss: 0.52700 - acc: 0.7283 -- iter: 2112/2226
[A[ATraining Step: 417  | total loss: [1m[32m0.52180[0m[0m | time: 101.663s
[2K
| RMSProp | epoch: 006 | loss: 0.52180 - acc: 0.7368 -- iter: 2144/2226
[A[ATraining Step: 418  | total loss: [1m[32m0.52695[0m[0m | time: 106.769s
[2K
| RMSProp | epoch: 006 | loss: 0.52695 - acc: 0.7381 -- iter: 2176/2226
[A[ATraining Step: 419  | total loss: [1m[32m0.53720[0m[0m | time: 112.555s
[2K
| RMSProp | epoch: 006 | loss: 0.53720 - acc: 0.7205 -- iter: 2208/2226
[A[ATraining Step: 420  | total loss: [1m[32m0.54191[0m[0m | time: 127.295s
[2K
| RMSProp | epoch: 006 | loss: 0.54191 - acc: 0.7172 | val_loss: 0.43860 - val_acc: 0.8175 -- iter: 2226/2226
--
Training Step: 421  | total loss: [1m[32m0.52155[0m[0m | time: 1.173s
[2K
| RMSProp | epoch: 007 | loss: 0.52155 - acc: 0.7361 -- iter: 0032/2226
[A[ATraining Step: 422  | total loss: [1m[32m0.50926[0m[0m | time: 2.365s
[2K
| RMSProp | epoch: 007 | loss: 0.50926 - acc: 0.7500 -- iter: 0064/2226
[A[ATraining Step: 423  | total loss: [1m[32m0.51285[0m[0m | time: 3.426s
[2K
| RMSProp | epoch: 007 | loss: 0.51285 - acc: 0.7531 -- iter: 0096/2226
[A[ATraining Step: 424  | total loss: [1m[32m0.52647[0m[0m | time: 4.534s
[2K
| RMSProp | epoch: 007 | loss: 0.52647 - acc: 0.7466 -- iter: 0128/2226
[A[ATraining Step: 425  | total loss: [1m[32m0.53508[0m[0m | time: 5.144s
[2K
| RMSProp | epoch: 007 | loss: 0.53508 - acc: 0.7313 -- iter: 0160/2226
[A[ATraining Step: 426  | total loss: [1m[32m0.52232[0m[0m | time: 6.008s
[2K
| RMSProp | epoch: 007 | loss: 0.52232 - acc: 0.7470 -- iter: 0192/2226
[A[ATraining Step: 427  | total loss: [1m[32m0.50420[0m[0m | time: 7.681s
[2K
| RMSProp | epoch: 007 | loss: 0.50420 - acc: 0.7612 -- iter: 0224/2226
[A[ATraining Step: 428  | total loss: [1m[32m0.50861[0m[0m | time: 10.079s
[2K
| RMSProp | epoch: 007 | loss: 0.50861 - acc: 0.7632 -- iter: 0256/2226
[A[ATraining Step: 429  | total loss: [1m[32m0.55290[0m[0m | time: 13.572s
[2K
| RMSProp | epoch: 007 | loss: 0.55290 - acc: 0.7432 -- iter: 0288/2226
[A[ATraining Step: 430  | total loss: [1m[32m0.54516[0m[0m | time: 16.815s
[2K
| RMSProp | epoch: 007 | loss: 0.54516 - acc: 0.7501 -- iter: 0320/2226
[A[ATraining Step: 431  | total loss: [1m[32m0.52535[0m[0m | time: 19.983s
[2K
| RMSProp | epoch: 007 | loss: 0.52535 - acc: 0.7688 -- iter: 0352/2226
[A[ATraining Step: 432  | total loss: [1m[32m0.51014[0m[0m | time: 24.826s
[2K
| RMSProp | epoch: 007 | loss: 0.51014 - acc: 0.7795 -- iter: 0384/2226
[A[ATraining Step: 433  | total loss: [1m[32m0.51795[0m[0m | time: 28.446s
[2K
| RMSProp | epoch: 007 | loss: 0.51795 - acc: 0.7703 -- iter: 0416/2226
[A[ATraining Step: 434  | total loss: [1m[32m0.51889[0m[0m | time: 32.356s
[2K
| RMSProp | epoch: 007 | loss: 0.51889 - acc: 0.7651 -- iter: 0448/2226
[A[ATraining Step: 435  | total loss: [1m[32m0.51292[0m[0m | time: 35.637s
[2K
| RMSProp | epoch: 007 | loss: 0.51292 - acc: 0.7636 -- iter: 0480/2226
[A[ATraining Step: 436  | total loss: [1m[32m0.49843[0m[0m | time: 37.353s
[2K
| RMSProp | epoch: 007 | loss: 0.49843 - acc: 0.7747 -- iter: 0512/2226
[A[ATraining Step: 437  | total loss: [1m[32m0.50082[0m[0m | time: 38.446s
[2K
| RMSProp | epoch: 007 | loss: 0.50082 - acc: 0.7785 -- iter: 0544/2226
[A[ATraining Step: 438  | total loss: [1m[32m0.50027[0m[0m | time: 39.520s
[2K
| RMSProp | epoch: 007 | loss: 0.50027 - acc: 0.7788 -- iter: 0576/2226
[A[ATraining Step: 439  | total loss: [1m[32m0.50366[0m[0m | time: 40.746s
[2K
| RMSProp | epoch: 007 | loss: 0.50366 - acc: 0.7759 -- iter: 0608/2226
[A[ATraining Step: 440  | total loss: [1m[32m0.49618[0m[0m | time: 41.923s
[2K
| RMSProp | epoch: 007 | loss: 0.49618 - acc: 0.7764 -- iter: 0640/2226
[A[ATraining Step: 441  | total loss: [1m[32m0.51654[0m[0m | time: 43.105s
[2K
| RMSProp | epoch: 007 | loss: 0.51654 - acc: 0.7644 -- iter: 0672/2226
[A[ATraining Step: 442  | total loss: [1m[32m0.51502[0m[0m | time: 44.388s
[2K
| RMSProp | epoch: 007 | loss: 0.51502 - acc: 0.7630 -- iter: 0704/2226
[A[ATraining Step: 443  | total loss: [1m[32m0.50636[0m[0m | time: 45.594s
[2K
| RMSProp | epoch: 007 | loss: 0.50636 - acc: 0.7679 -- iter: 0736/2226
[A[ATraining Step: 444  | total loss: [1m[32m0.51453[0m[0m | time: 46.838s
[2K
| RMSProp | epoch: 007 | loss: 0.51453 - acc: 0.7599 -- iter: 0768/2226
[A[ATraining Step: 445  | total loss: [1m[32m0.51511[0m[0m | time: 50.198s
[2K
| RMSProp | epoch: 007 | loss: 0.51511 - acc: 0.7652 -- iter: 0800/2226
[A[ATraining Step: 446  | total loss: [1m[32m0.51106[0m[0m | time: 54.484s
[2K
| RMSProp | epoch: 007 | loss: 0.51106 - acc: 0.7636 -- iter: 0832/2226
[A[ATraining Step: 447  | total loss: [1m[32m0.51091[0m[0m | time: 59.006s
[2K
| RMSProp | epoch: 007 | loss: 0.51091 - acc: 0.7716 -- iter: 0864/2226
[A[ATraining Step: 448  | total loss: [1m[32m0.51671[0m[0m | time: 62.453s
[2K
| RMSProp | epoch: 007 | loss: 0.51671 - acc: 0.7695 -- iter: 0896/2226
[A[ATraining Step: 449  | total loss: [1m[32m0.50857[0m[0m | time: 64.734s
[2K
| RMSProp | epoch: 007 | loss: 0.50857 - acc: 0.7707 -- iter: 0928/2226
[A[ATraining Step: 450  | total loss: [1m[32m0.48823[0m[0m | time: 65.696s
[2K
| RMSProp | epoch: 007 | loss: 0.48823 - acc: 0.7811 -- iter: 0960/2226
[A[ATraining Step: 451  | total loss: [1m[32m0.49802[0m[0m | time: 66.831s
[2K
| RMSProp | epoch: 007 | loss: 0.49802 - acc: 0.7780 -- iter: 0992/2226
[A[ATraining Step: 452  | total loss: [1m[32m0.50954[0m[0m | time: 67.959s
[2K
| RMSProp | epoch: 007 | loss: 0.50954 - acc: 0.7689 -- iter: 1024/2226
[A[ATraining Step: 453  | total loss: [1m[32m0.52300[0m[0m | time: 69.010s
[2K
| RMSProp | epoch: 007 | loss: 0.52300 - acc: 0.7639 -- iter: 1056/2226
[A[ATraining Step: 454  | total loss: [1m[32m0.52540[0m[0m | time: 70.041s
[2K
| RMSProp | epoch: 007 | loss: 0.52540 - acc: 0.7563 -- iter: 1088/2226
[A[ATraining Step: 455  | total loss: [1m[32m0.52012[0m[0m | time: 71.200s
[2K
| RMSProp | epoch: 007 | loss: 0.52012 - acc: 0.7619 -- iter: 1120/2226
[A[ATraining Step: 456  | total loss: [1m[32m0.51815[0m[0m | time: 72.369s
[2K
| RMSProp | epoch: 007 | loss: 0.51815 - acc: 0.7607 -- iter: 1152/2226
[A[ATraining Step: 457  | total loss: [1m[32m0.50614[0m[0m | time: 73.490s
[2K
| RMSProp | epoch: 007 | loss: 0.50614 - acc: 0.7690 -- iter: 1184/2226
[A[ATraining Step: 458  | total loss: [1m[32m0.51374[0m[0m | time: 74.873s
[2K
| RMSProp | epoch: 007 | loss: 0.51374 - acc: 0.7546 -- iter: 1216/2226
[A[ATraining Step: 459  | total loss: [1m[32m0.50213[0m[0m | time: 76.594s
[2K
| RMSProp | epoch: 007 | loss: 0.50213 - acc: 0.7698 -- iter: 1248/2226
[A[ATraining Step: 460  | total loss: [1m[32m0.47714[0m[0m | time: 78.083s
[2K
| RMSProp | epoch: 007 | loss: 0.47714 - acc: 0.7834 -- iter: 1280/2226
[A[ATraining Step: 461  | total loss: [1m[32m0.47904[0m[0m | time: 79.923s
[2K
| RMSProp | epoch: 007 | loss: 0.47904 - acc: 0.7801 -- iter: 1312/2226
[A[ATraining Step: 462  | total loss: [1m[32m0.51324[0m[0m | time: 83.805s
[2K
| RMSProp | epoch: 007 | loss: 0.51324 - acc: 0.7552 -- iter: 1344/2226
[A[ATraining Step: 463  | total loss: [1m[32m0.50773[0m[0m | time: 86.389s
[2K
| RMSProp | epoch: 007 | loss: 0.50773 - acc: 0.7609 -- iter: 1376/2226
[A[ATraining Step: 464  | total loss: [1m[32m0.49648[0m[0m | time: 89.148s
[2K
| RMSProp | epoch: 007 | loss: 0.49648 - acc: 0.7755 -- iter: 1408/2226
[A[ATraining Step: 465  | total loss: [1m[32m0.51468[0m[0m | time: 92.475s
[2K
| RMSProp | epoch: 007 | loss: 0.51468 - acc: 0.7635 -- iter: 1440/2226
[A[ATraining Step: 466  | total loss: [1m[32m0.51387[0m[0m | time: 94.630s
[2K
| RMSProp | epoch: 007 | loss: 0.51387 - acc: 0.7622 -- iter: 1472/2226
[A[ATraining Step: 467  | total loss: [1m[32m0.51044[0m[0m | time: 97.805s
[2K
| RMSProp | epoch: 007 | loss: 0.51044 - acc: 0.7641 -- iter: 1504/2226
[A[ATraining Step: 468  | total loss: [1m[32m0.49860[0m[0m | time: 100.199s
[2K
| RMSProp | epoch: 007 | loss: 0.49860 - acc: 0.7721 -- iter: 1536/2226
[A[ATraining Step: 469  | total loss: [1m[32m0.48647[0m[0m | time: 102.401s
[2K
| RMSProp | epoch: 007 | loss: 0.48647 - acc: 0.7761 -- iter: 1568/2226
[A[ATraining Step: 470  | total loss: [1m[32m0.47765[0m[0m | time: 105.873s
[2K
| RMSProp | epoch: 007 | loss: 0.47765 - acc: 0.7797 -- iter: 1600/2226
[A[ATraining Step: 471  | total loss: [1m[32m0.48391[0m[0m | time: 108.627s
[2K
| RMSProp | epoch: 007 | loss: 0.48391 - acc: 0.7705 -- iter: 1632/2226
[A[ATraining Step: 472  | total loss: [1m[32m0.48786[0m[0m | time: 109.701s
[2K
| RMSProp | epoch: 007 | loss: 0.48786 - acc: 0.7622 -- iter: 1664/2226
[A[ATraining Step: 473  | total loss: [1m[32m0.49146[0m[0m | time: 110.876s
[2K
| RMSProp | epoch: 007 | loss: 0.49146 - acc: 0.7610 -- iter: 1696/2226
[A[ATraining Step: 474  | total loss: [1m[32m0.48674[0m[0m | time: 112.020s
[2K
| RMSProp | epoch: 007 | loss: 0.48674 - acc: 0.7693 -- iter: 1728/2226
[A[ATraining Step: 475  | total loss: [1m[32m0.47225[0m[0m | time: 113.197s
[2K
| RMSProp | epoch: 007 | loss: 0.47225 - acc: 0.7830 -- iter: 1760/2226
[A[ATraining Step: 476  | total loss: [1m[32m0.46780[0m[0m | time: 114.433s
[2K
| RMSProp | epoch: 007 | loss: 0.46780 - acc: 0.7859 -- iter: 1792/2226
[A[ATraining Step: 477  | total loss: [1m[32m0.47727[0m[0m | time: 115.706s
[2K
| RMSProp | epoch: 007 | loss: 0.47727 - acc: 0.7792 -- iter: 1824/2226
[A[ATraining Step: 478  | total loss: [1m[32m0.48528[0m[0m | time: 116.890s
[2K
| RMSProp | epoch: 007 | loss: 0.48528 - acc: 0.7732 -- iter: 1856/2226
[A[ATraining Step: 479  | total loss: [1m[32m0.48845[0m[0m | time: 118.055s
[2K
| RMSProp | epoch: 007 | loss: 0.48845 - acc: 0.7708 -- iter: 1888/2226
[A[ATraining Step: 480  | total loss: [1m[32m0.48842[0m[0m | time: 119.269s
[2K
| RMSProp | epoch: 007 | loss: 0.48842 - acc: 0.7719 -- iter: 1920/2226
[A[ATraining Step: 481  | total loss: [1m[32m0.48448[0m[0m | time: 122.590s
[2K
| RMSProp | epoch: 007 | loss: 0.48448 - acc: 0.7791 -- iter: 1952/2226
[A[ATraining Step: 482  | total loss: [1m[32m0.48216[0m[0m | time: 127.030s
[2K
| RMSProp | epoch: 007 | loss: 0.48216 - acc: 0.7824 -- iter: 1984/2226
[A[ATraining Step: 483  | total loss: [1m[32m0.47336[0m[0m | time: 128.180s
[2K
| RMSProp | epoch: 007 | loss: 0.47336 - acc: 0.7854 -- iter: 2016/2226
[A[ATraining Step: 484  | total loss: [1m[32m0.45262[0m[0m | time: 129.325s
[2K
| RMSProp | epoch: 007 | loss: 0.45262 - acc: 0.8006 -- iter: 2048/2226
[A[ATraining Step: 485  | total loss: [1m[32m0.43738[0m[0m | time: 130.511s
[2K
| RMSProp | epoch: 007 | loss: 0.43738 - acc: 0.8081 -- iter: 2080/2226
[A[ATraining Step: 486  | total loss: [1m[32m0.45098[0m[0m | time: 131.679s
[2K
| RMSProp | epoch: 007 | loss: 0.45098 - acc: 0.7991 -- iter: 2112/2226
[A[ATraining Step: 487  | total loss: [1m[32m0.45267[0m[0m | time: 132.833s
[2K
| RMSProp | epoch: 007 | loss: 0.45267 - acc: 0.8005 -- iter: 2144/2226
[A[ATraining Step: 488  | total loss: [1m[32m0.47448[0m[0m | time: 133.980s
[2K
| RMSProp | epoch: 007 | loss: 0.47448 - acc: 0.7860 -- iter: 2176/2226
[A[ATraining Step: 489  | total loss: [1m[32m0.47411[0m[0m | time: 135.113s
[2K
| RMSProp | epoch: 007 | loss: 0.47411 - acc: 0.7918 -- iter: 2208/2226
[A[ATraining Step: 490  | total loss: [1m[32m0.47414[0m[0m | time: 153.955s
[2K
| RMSProp | epoch: 007 | loss: 0.47414 - acc: 0.7845 | val_loss: 0.53426 - val_acc: 0.7141 -- iter: 2226/2226
--
Training Step: 491  | total loss: [1m[32m0.48681[0m[0m | time: 1.239s
[2K
| RMSProp | epoch: 008 | loss: 0.48681 - acc: 0.7748 -- iter: 0032/2226
[A[ATraining Step: 492  | total loss: [1m[32m0.47938[0m[0m | time: 2.417s
[2K
| RMSProp | epoch: 008 | loss: 0.47938 - acc: 0.7817 -- iter: 0064/2226
[A[ATraining Step: 493  | total loss: [1m[32m0.46649[0m[0m | time: 3.547s
[2K
| RMSProp | epoch: 008 | loss: 0.46649 - acc: 0.7910 -- iter: 0096/2226
[A[ATraining Step: 494  | total loss: [1m[32m0.46386[0m[0m | time: 4.909s
[2K
| RMSProp | epoch: 008 | loss: 0.46386 - acc: 0.7932 -- iter: 0128/2226
[A[ATraining Step: 495  | total loss: [1m[32m0.45724[0m[0m | time: 6.348s
[2K
| RMSProp | epoch: 008 | loss: 0.45724 - acc: 0.7920 -- iter: 0160/2226
[A[ATraining Step: 496  | total loss: [1m[32m0.45125[0m[0m | time: 6.970s
[2K
| RMSProp | epoch: 008 | loss: 0.45125 - acc: 0.8003 -- iter: 0192/2226
[A[ATraining Step: 497  | total loss: [1m[32m0.43545[0m[0m | time: 7.610s
[2K
| RMSProp | epoch: 008 | loss: 0.43545 - acc: 0.8091 -- iter: 0224/2226
[A[ATraining Step: 498  | total loss: [1m[32m0.41229[0m[0m | time: 8.728s
[2K
| RMSProp | epoch: 008 | loss: 0.41229 - acc: 0.8227 -- iter: 0256/2226
[A[ATraining Step: 499  | total loss: [1m[32m0.44393[0m[0m | time: 9.963s
[2K
| RMSProp | epoch: 008 | loss: 0.44393 - acc: 0.8092 -- iter: 0288/2226
[A[ATraining Step: 500  | total loss: [1m[32m0.46330[0m[0m | time: 11.170s
[2K
| RMSProp | epoch: 008 | loss: 0.46330 - acc: 0.8001 -- iter: 0320/2226
[A[ATraining Step: 501  | total loss: [1m[32m0.46051[0m[0m | time: 12.481s
[2K
| RMSProp | epoch: 008 | loss: 0.46051 - acc: 0.8014 -- iter: 0352/2226
[A[ATraining Step: 502  | total loss: [1m[32m0.46178[0m[0m | time: 13.552s
[2K
| RMSProp | epoch: 008 | loss: 0.46178 - acc: 0.7962 -- iter: 0384/2226
[A[ATraining Step: 503  | total loss: [1m[32m0.46744[0m[0m | time: 14.427s
[2K
| RMSProp | epoch: 008 | loss: 0.46744 - acc: 0.7885 -- iter: 0416/2226
[A[ATraining Step: 504  | total loss: [1m[32m0.45848[0m[0m | time: 15.256s
[2K
| RMSProp | epoch: 008 | loss: 0.45848 - acc: 0.7909 -- iter: 0448/2226
[A[ATraining Step: 505  | total loss: [1m[32m0.44597[0m[0m | time: 16.119s
[2K
| RMSProp | epoch: 008 | loss: 0.44597 - acc: 0.7993 -- iter: 0480/2226
[A[ATraining Step: 506  | total loss: [1m[32m0.45090[0m[0m | time: 16.920s
[2K
| RMSProp | epoch: 008 | loss: 0.45090 - acc: 0.7975 -- iter: 0512/2226
[A[ATraining Step: 507  | total loss: [1m[32m0.46345[0m[0m | time: 17.754s
[2K
| RMSProp | epoch: 008 | loss: 0.46345 - acc: 0.7865 -- iter: 0544/2226
[A[ATraining Step: 508  | total loss: [1m[32m0.46800[0m[0m | time: 18.624s
[2K
| RMSProp | epoch: 008 | loss: 0.46800 - acc: 0.7797 -- iter: 0576/2226
[A[ATraining Step: 509  | total loss: [1m[32m0.47703[0m[0m | time: 19.454s
[2K
| RMSProp | epoch: 008 | loss: 0.47703 - acc: 0.7736 -- iter: 0608/2226
[A[ATraining Step: 510  | total loss: [1m[32m0.48167[0m[0m | time: 20.332s
[2K
| RMSProp | epoch: 008 | loss: 0.48167 - acc: 0.7744 -- iter: 0640/2226
[A[ATraining Step: 511  | total loss: [1m[32m0.49138[0m[0m | time: 21.210s
[2K
| RMSProp | epoch: 008 | loss: 0.49138 - acc: 0.7626 -- iter: 0672/2226
[A[ATraining Step: 512  | total loss: [1m[32m0.50558[0m[0m | time: 22.053s
[2K
| RMSProp | epoch: 008 | loss: 0.50558 - acc: 0.7551 -- iter: 0704/2226
[A[ATraining Step: 513  | total loss: [1m[32m0.49253[0m[0m | time: 22.849s
[2K
| RMSProp | epoch: 008 | loss: 0.49253 - acc: 0.7733 -- iter: 0736/2226
[A[ATraining Step: 514  | total loss: [1m[32m0.50137[0m[0m | time: 23.701s
[2K
| RMSProp | epoch: 008 | loss: 0.50137 - acc: 0.7678 -- iter: 0768/2226
[A[ATraining Step: 515  | total loss: [1m[32m0.48689[0m[0m | time: 24.537s
[2K
| RMSProp | epoch: 008 | loss: 0.48689 - acc: 0.7786 -- iter: 0800/2226
[A[ATraining Step: 516  | total loss: [1m[32m0.47572[0m[0m | time: 25.348s
[2K
| RMSProp | epoch: 008 | loss: 0.47572 - acc: 0.7820 -- iter: 0832/2226
[A[ATraining Step: 517  | total loss: [1m[32m0.46519[0m[0m | time: 26.234s
[2K
| RMSProp | epoch: 008 | loss: 0.46519 - acc: 0.7819 -- iter: 0864/2226
[A[ATraining Step: 518  | total loss: [1m[32m0.45971[0m[0m | time: 27.074s
[2K
| RMSProp | epoch: 008 | loss: 0.45971 - acc: 0.7818 -- iter: 0896/2226
[A[ATraining Step: 519  | total loss: [1m[32m0.43237[0m[0m | time: 27.884s
[2K
| RMSProp | epoch: 008 | loss: 0.43237 - acc: 0.8005 -- iter: 0928/2226
[A[ATraining Step: 520  | total loss: [1m[32m0.45120[0m[0m | time: 28.746s
[2K
| RMSProp | epoch: 008 | loss: 0.45120 - acc: 0.7955 -- iter: 0960/2226
[A[ATraining Step: 521  | total loss: [1m[32m0.44640[0m[0m | time: 29.617s
[2K
| RMSProp | epoch: 008 | loss: 0.44640 - acc: 0.8003 -- iter: 0992/2226
[A[ATraining Step: 522  | total loss: [1m[32m0.44363[0m[0m | time: 30.446s
[2K
| RMSProp | epoch: 008 | loss: 0.44363 - acc: 0.8015 -- iter: 1024/2226
[A[ATraining Step: 523  | total loss: [1m[32m0.44994[0m[0m | time: 31.221s
[2K
| RMSProp | epoch: 008 | loss: 0.44994 - acc: 0.7932 -- iter: 1056/2226
[A[ATraining Step: 524  | total loss: [1m[32m0.45634[0m[0m | time: 32.050s
[2K
| RMSProp | epoch: 008 | loss: 0.45634 - acc: 0.7920 -- iter: 1088/2226
[A[ATraining Step: 525  | total loss: [1m[32m0.46674[0m[0m | time: 32.897s
[2K
| RMSProp | epoch: 008 | loss: 0.46674 - acc: 0.7816 -- iter: 1120/2226
[A[ATraining Step: 526  | total loss: [1m[32m0.48193[0m[0m | time: 33.763s
[2K
| RMSProp | epoch: 008 | loss: 0.48193 - acc: 0.7691 -- iter: 1152/2226
[A[ATraining Step: 527  | total loss: [1m[32m0.49602[0m[0m | time: 34.578s
[2K
| RMSProp | epoch: 008 | loss: 0.49602 - acc: 0.7609 -- iter: 1184/2226
[A[ATraining Step: 528  | total loss: [1m[32m0.48940[0m[0m | time: 35.422s
[2K
| RMSProp | epoch: 008 | loss: 0.48940 - acc: 0.7723 -- iter: 1216/2226
[A[ATraining Step: 529  | total loss: [1m[32m0.47668[0m[0m | time: 36.543s
[2K
| RMSProp | epoch: 008 | loss: 0.47668 - acc: 0.7763 -- iter: 1248/2226
[A[ATraining Step: 530  | total loss: [1m[32m0.45655[0m[0m | time: 38.559s
[2K
| RMSProp | epoch: 008 | loss: 0.45655 - acc: 0.7924 -- iter: 1280/2226
[A[ATraining Step: 531  | total loss: [1m[32m0.45487[0m[0m | time: 41.711s
[2K
| RMSProp | epoch: 008 | loss: 0.45487 - acc: 0.7913 -- iter: 1312/2226
[A[ATraining Step: 532  | total loss: [1m[32m0.44010[0m[0m | time: 45.841s
[2K
| RMSProp | epoch: 008 | loss: 0.44010 - acc: 0.8028 -- iter: 1344/2226
[A[ATraining Step: 533  | total loss: [1m[32m0.42983[0m[0m | time: 48.136s
[2K
| RMSProp | epoch: 008 | loss: 0.42983 - acc: 0.8132 -- iter: 1376/2226
[A[ATraining Step: 534  | total loss: [1m[32m0.45993[0m[0m | time: 51.999s
[2K
| RMSProp | epoch: 008 | loss: 0.45993 - acc: 0.7912 -- iter: 1408/2226
[A[ATraining Step: 535  | total loss: [1m[32m0.45484[0m[0m | time: 54.644s
[2K
| RMSProp | epoch: 008 | loss: 0.45484 - acc: 0.7965 -- iter: 1440/2226
[A[ATraining Step: 536  | total loss: [1m[32m0.48143[0m[0m | time: 56.131s
[2K
| RMSProp | epoch: 008 | loss: 0.48143 - acc: 0.7731 -- iter: 1472/2226
[A[ATraining Step: 537  | total loss: [1m[32m0.46888[0m[0m | time: 57.267s
[2K
| RMSProp | epoch: 008 | loss: 0.46888 - acc: 0.7864 -- iter: 1504/2226
[A[ATraining Step: 538  | total loss: [1m[32m0.47022[0m[0m | time: 58.495s
[2K
| RMSProp | epoch: 008 | loss: 0.47022 - acc: 0.7828 -- iter: 1536/2226
[A[ATraining Step: 539  | total loss: [1m[32m0.46573[0m[0m | time: 59.626s
[2K
| RMSProp | epoch: 008 | loss: 0.46573 - acc: 0.7826 -- iter: 1568/2226
[A[ATraining Step: 540  | total loss: [1m[32m0.46735[0m[0m | time: 60.838s
[2K
| RMSProp | epoch: 008 | loss: 0.46735 - acc: 0.7856 -- iter: 1600/2226
[A[ATraining Step: 541  | total loss: [1m[32m0.46652[0m[0m | time: 62.058s
[2K
| RMSProp | epoch: 008 | loss: 0.46652 - acc: 0.7820 -- iter: 1632/2226
[A[ATraining Step: 542  | total loss: [1m[32m0.50267[0m[0m | time: 63.310s
[2K
| RMSProp | epoch: 008 | loss: 0.50267 - acc: 0.7601 -- iter: 1664/2226
[A[ATraining Step: 543  | total loss: [1m[32m0.49010[0m[0m | time: 64.557s
[2K
| RMSProp | epoch: 008 | loss: 0.49010 - acc: 0.7716 -- iter: 1696/2226
[A[ATraining Step: 544  | total loss: [1m[32m0.47287[0m[0m | time: 65.725s
[2K
| RMSProp | epoch: 008 | loss: 0.47287 - acc: 0.7850 -- iter: 1728/2226
[A[ATraining Step: 545  | total loss: [1m[32m0.46752[0m[0m | time: 66.831s
[2K
| RMSProp | epoch: 008 | loss: 0.46752 - acc: 0.7815 -- iter: 1760/2226
[A[ATraining Step: 546  | total loss: [1m[32m0.45683[0m[0m | time: 67.945s
[2K
| RMSProp | epoch: 008 | loss: 0.45683 - acc: 0.7909 -- iter: 1792/2226
[A[ATraining Step: 547  | total loss: [1m[32m0.44800[0m[0m | time: 69.046s
[2K
| RMSProp | epoch: 008 | loss: 0.44800 - acc: 0.7962 -- iter: 1824/2226
[A[ATraining Step: 548  | total loss: [1m[32m0.45214[0m[0m | time: 70.151s
[2K
| RMSProp | epoch: 008 | loss: 0.45214 - acc: 0.7978 -- iter: 1856/2226
[A[ATraining Step: 549  | total loss: [1m[32m0.44780[0m[0m | time: 71.280s
[2K
| RMSProp | epoch: 008 | loss: 0.44780 - acc: 0.8024 -- iter: 1888/2226
[A[ATraining Step: 550  | total loss: [1m[32m0.44446[0m[0m | time: 72.369s
[2K
| RMSProp | epoch: 008 | loss: 0.44446 - acc: 0.8003 -- iter: 1920/2226
[A[ATraining Step: 551  | total loss: [1m[32m0.43821[0m[0m | time: 73.499s
[2K
| RMSProp | epoch: 008 | loss: 0.43821 - acc: 0.8046 -- iter: 1952/2226
[A[ATraining Step: 552  | total loss: [1m[32m0.44392[0m[0m | time: 74.498s
[2K
| RMSProp | epoch: 008 | loss: 0.44392 - acc: 0.8054 -- iter: 1984/2226
[A[ATraining Step: 553  | total loss: [1m[32m0.44458[0m[0m | time: 75.551s
[2K
| RMSProp | epoch: 008 | loss: 0.44458 - acc: 0.8030 -- iter: 2016/2226
[A[ATraining Step: 554  | total loss: [1m[32m0.42941[0m[0m | time: 76.655s
[2K
| RMSProp | epoch: 008 | loss: 0.42941 - acc: 0.8133 -- iter: 2048/2226
[A[ATraining Step: 555  | total loss: [1m[32m0.44689[0m[0m | time: 77.625s
[2K
| RMSProp | epoch: 008 | loss: 0.44689 - acc: 0.8070 -- iter: 2080/2226
[A[ATraining Step: 556  | total loss: [1m[32m0.48504[0m[0m | time: 78.894s
[2K
| RMSProp | epoch: 008 | loss: 0.48504 - acc: 0.7857 -- iter: 2112/2226
[A[ATraining Step: 557  | total loss: [1m[32m0.48091[0m[0m | time: 80.732s
[2K
| RMSProp | epoch: 008 | loss: 0.48091 - acc: 0.7821 -- iter: 2144/2226
[A[ATraining Step: 558  | total loss: [1m[32m0.47795[0m[0m | time: 84.253s
[2K
| RMSProp | epoch: 008 | loss: 0.47795 - acc: 0.7820 -- iter: 2176/2226
[A[ATraining Step: 559  | total loss: [1m[32m0.47418[0m[0m | time: 87.157s
[2K
| RMSProp | epoch: 008 | loss: 0.47418 - acc: 0.7851 -- iter: 2208/2226
[A[ATraining Step: 560  | total loss: [1m[32m0.46747[0m[0m | time: 101.682s
[2K
| RMSProp | epoch: 008 | loss: 0.46747 - acc: 0.7878 | val_loss: 0.40467 - val_acc: 0.8319 -- iter: 2226/2226
--
Training Step: 561  | total loss: [1m[32m0.45884[0m[0m | time: 1.175s
[2K
| RMSProp | epoch: 009 | loss: 0.45884 - acc: 0.7934 -- iter: 0032/2226
[A[ATraining Step: 562  | total loss: [1m[32m0.45847[0m[0m | time: 2.408s
[2K
| RMSProp | epoch: 009 | loss: 0.45847 - acc: 0.7984 -- iter: 0064/2226
[A[ATraining Step: 563  | total loss: [1m[32m0.44693[0m[0m | time: 3.539s
[2K
| RMSProp | epoch: 009 | loss: 0.44693 - acc: 0.8061 -- iter: 0096/2226
[A[ATraining Step: 564  | total loss: [1m[32m0.45243[0m[0m | time: 4.673s
[2K
| RMSProp | epoch: 009 | loss: 0.45243 - acc: 0.8067 -- iter: 0128/2226
[A[ATraining Step: 565  | total loss: [1m[32m0.46059[0m[0m | time: 5.778s
[2K
| RMSProp | epoch: 009 | loss: 0.46059 - acc: 0.8042 -- iter: 0160/2226
[A[ATraining Step: 566  | total loss: [1m[32m0.46191[0m[0m | time: 7.050s
[2K
| RMSProp | epoch: 009 | loss: 0.46191 - acc: 0.8019 -- iter: 0192/2226
[A[ATraining Step: 567  | total loss: [1m[32m0.47756[0m[0m | time: 9.211s
[2K
| RMSProp | epoch: 009 | loss: 0.47756 - acc: 0.7936 -- iter: 0224/2226
[A[ATraining Step: 568  | total loss: [1m[32m0.46399[0m[0m | time: 12.597s
[2K
| RMSProp | epoch: 009 | loss: 0.46399 - acc: 0.8087 -- iter: 0256/2226
[A[ATraining Step: 569  | total loss: [1m[32m0.44160[0m[0m | time: 15.430s
[2K
| RMSProp | epoch: 009 | loss: 0.44160 - acc: 0.8278 -- iter: 0288/2226
[A[ATraining Step: 570  | total loss: [1m[32m0.43599[0m[0m | time: 16.496s
[2K
| RMSProp | epoch: 009 | loss: 0.43599 - acc: 0.8294 -- iter: 0320/2226
[A[ATraining Step: 571  | total loss: [1m[32m0.42813[0m[0m | time: 17.589s
[2K
| RMSProp | epoch: 009 | loss: 0.42813 - acc: 0.8277 -- iter: 0352/2226
[A[ATraining Step: 572  | total loss: [1m[32m0.42462[0m[0m | time: 18.759s
[2K
| RMSProp | epoch: 009 | loss: 0.42462 - acc: 0.8231 -- iter: 0384/2226
[A[ATraining Step: 573  | total loss: [1m[32m0.43374[0m[0m | time: 20.011s
[2K
| RMSProp | epoch: 009 | loss: 0.43374 - acc: 0.8189 -- iter: 0416/2226
[A[ATraining Step: 574  | total loss: [1m[32m0.43750[0m[0m | time: 21.227s
[2K
| RMSProp | epoch: 009 | loss: 0.43750 - acc: 0.8151 -- iter: 0448/2226
[A[ATraining Step: 575  | total loss: [1m[32m0.43536[0m[0m | time: 22.846s
[2K
| RMSProp | epoch: 009 | loss: 0.43536 - acc: 0.8180 -- iter: 0480/2226
[A[ATraining Step: 576  | total loss: [1m[32m0.41540[0m[0m | time: 24.255s
[2K
| RMSProp | epoch: 009 | loss: 0.41540 - acc: 0.8299 -- iter: 0512/2226
[A[ATraining Step: 577  | total loss: [1m[32m0.40467[0m[0m | time: 25.079s
[2K
| RMSProp | epoch: 009 | loss: 0.40467 - acc: 0.8344 -- iter: 0544/2226
[A[ATraining Step: 578  | total loss: [1m[32m0.41827[0m[0m | time: 26.245s
[2K
| RMSProp | epoch: 009 | loss: 0.41827 - acc: 0.8166 -- iter: 0576/2226
[A[ATraining Step: 579  | total loss: [1m[32m0.42153[0m[0m | time: 27.484s
[2K
| RMSProp | epoch: 009 | loss: 0.42153 - acc: 0.8193 -- iter: 0608/2226
[A[ATraining Step: 580  | total loss: [1m[32m0.41139[0m[0m | time: 28.516s
[2K
| RMSProp | epoch: 009 | loss: 0.41139 - acc: 0.8280 -- iter: 0640/2226
[A[ATraining Step: 581  | total loss: [1m[32m0.41640[0m[0m | time: 29.533s
[2K
| RMSProp | epoch: 009 | loss: 0.41640 - acc: 0.8171 -- iter: 0672/2226
[A[ATraining Step: 582  | total loss: [1m[32m0.42580[0m[0m | time: 30.552s
[2K
| RMSProp | epoch: 009 | loss: 0.42580 - acc: 0.8104 -- iter: 0704/2226
[A[ATraining Step: 583  | total loss: [1m[32m0.42398[0m[0m | time: 31.729s
[2K
| RMSProp | epoch: 009 | loss: 0.42398 - acc: 0.8137 -- iter: 0736/2226
[A[ATraining Step: 584  | total loss: [1m[32m0.42626[0m[0m | time: 32.909s
[2K
| RMSProp | epoch: 009 | loss: 0.42626 - acc: 0.8105 -- iter: 0768/2226
[A[ATraining Step: 585  | total loss: [1m[32m0.39826[0m[0m | time: 34.011s
[2K
| RMSProp | epoch: 009 | loss: 0.39826 - acc: 0.8294 -- iter: 0800/2226
[A[ATraining Step: 586  | total loss: [1m[32m0.39190[0m[0m | time: 35.023s
[2K
| RMSProp | epoch: 009 | loss: 0.39190 - acc: 0.8340 -- iter: 0832/2226
[A[ATraining Step: 587  | total loss: [1m[32m0.40877[0m[0m | time: 36.356s
[2K
| RMSProp | epoch: 009 | loss: 0.40877 - acc: 0.8256 -- iter: 0864/2226
[A[ATraining Step: 588  | total loss: [1m[32m0.42583[0m[0m | time: 37.463s
[2K
| RMSProp | epoch: 009 | loss: 0.42583 - acc: 0.8149 -- iter: 0896/2226
[A[ATraining Step: 589  | total loss: [1m[32m0.43004[0m[0m | time: 38.687s
[2K
| RMSProp | epoch: 009 | loss: 0.43004 - acc: 0.8178 -- iter: 0928/2226
[A[ATraining Step: 590  | total loss: [1m[32m0.44522[0m[0m | time: 39.968s
[2K
| RMSProp | epoch: 009 | loss: 0.44522 - acc: 0.8079 -- iter: 0960/2226
[A[ATraining Step: 591  | total loss: [1m[32m0.45446[0m[0m | time: 41.072s
[2K
| RMSProp | epoch: 009 | loss: 0.45446 - acc: 0.8021 -- iter: 0992/2226
[A[ATraining Step: 592  | total loss: [1m[32m0.45007[0m[0m | time: 42.160s
[2K
| RMSProp | epoch: 009 | loss: 0.45007 - acc: 0.8094 -- iter: 1024/2226
[A[ATraining Step: 593  | total loss: [1m[32m0.43408[0m[0m | time: 43.272s
[2K
| RMSProp | epoch: 009 | loss: 0.43408 - acc: 0.8222 -- iter: 1056/2226
[A[ATraining Step: 594  | total loss: [1m[32m0.42690[0m[0m | time: 44.472s
[2K
| RMSProp | epoch: 009 | loss: 0.42690 - acc: 0.8275 -- iter: 1088/2226
[A[ATraining Step: 595  | total loss: [1m[32m0.41034[0m[0m | time: 45.692s
[2K
| RMSProp | epoch: 009 | loss: 0.41034 - acc: 0.8385 -- iter: 1120/2226
[A[ATraining Step: 596  | total loss: [1m[32m0.41121[0m[0m | time: 46.981s
[2K
| RMSProp | epoch: 009 | loss: 0.41121 - acc: 0.8390 -- iter: 1152/2226
[A[ATraining Step: 597  | total loss: [1m[32m0.40283[0m[0m | time: 48.183s
[2K
| RMSProp | epoch: 009 | loss: 0.40283 - acc: 0.8457 -- iter: 1184/2226
[A[ATraining Step: 598  | total loss: [1m[32m0.39929[0m[0m | time: 49.355s
[2K
| RMSProp | epoch: 009 | loss: 0.39929 - acc: 0.8455 -- iter: 1216/2226
[A[ATraining Step: 599  | total loss: [1m[32m0.41964[0m[0m | time: 50.543s
[2K
| RMSProp | epoch: 009 | loss: 0.41964 - acc: 0.8391 -- iter: 1248/2226
[A[ATraining Step: 600  | total loss: [1m[32m0.43794[0m[0m | time: 56.288s
[2K
| RMSProp | epoch: 009 | loss: 0.43794 - acc: 0.8271 | val_loss: 0.36054 - val_acc: 0.8534 -- iter: 1280/2226
--
Training Step: 601  | total loss: [1m[32m0.42070[0m[0m | time: 57.416s
[2K
| RMSProp | epoch: 009 | loss: 0.42070 - acc: 0.8412 -- iter: 1312/2226
[A[ATraining Step: 602  | total loss: [1m[32m0.39638[0m[0m | time: 58.521s
[2K
| RMSProp | epoch: 009 | loss: 0.39638 - acc: 0.8540 -- iter: 1344/2226
[A[ATraining Step: 603  | total loss: [1m[32m0.38548[0m[0m | time: 59.650s
[2K
| RMSProp | epoch: 009 | loss: 0.38548 - acc: 0.8561 -- iter: 1376/2226
[A[ATraining Step: 604  | total loss: [1m[32m0.38043[0m[0m | time: 60.911s
[2K
| RMSProp | epoch: 009 | loss: 0.38043 - acc: 0.8642 -- iter: 1408/2226
[A[ATraining Step: 605  | total loss: [1m[32m0.40027[0m[0m | time: 62.104s
[2K
| RMSProp | epoch: 009 | loss: 0.40027 - acc: 0.8528 -- iter: 1440/2226
[A[ATraining Step: 606  | total loss: [1m[32m0.39873[0m[0m | time: 63.158s
[2K
| RMSProp | epoch: 009 | loss: 0.39873 - acc: 0.8519 -- iter: 1472/2226
[A[ATraining Step: 607  | total loss: [1m[32m0.39367[0m[0m | time: 64.317s
[2K
| RMSProp | epoch: 009 | loss: 0.39367 - acc: 0.8573 -- iter: 1504/2226
[A[ATraining Step: 608  | total loss: [1m[32m0.37831[0m[0m | time: 65.654s
[2K
| RMSProp | epoch: 009 | loss: 0.37831 - acc: 0.8654 -- iter: 1536/2226
[A[ATraining Step: 609  | total loss: [1m[32m0.37854[0m[0m | time: 66.937s
[2K
| RMSProp | epoch: 009 | loss: 0.37854 - acc: 0.8694 -- iter: 1568/2226
[A[ATraining Step: 610  | total loss: [1m[32m0.41840[0m[0m | time: 68.142s
[2K
| RMSProp | epoch: 009 | loss: 0.41840 - acc: 0.8481 -- iter: 1600/2226
[A[ATraining Step: 611  | total loss: [1m[32m0.40646[0m[0m | time: 69.259s
[2K
| RMSProp | epoch: 009 | loss: 0.40646 - acc: 0.8477 -- iter: 1632/2226
[A[ATraining Step: 612  | total loss: [1m[32m0.39026[0m[0m | time: 70.147s
[2K
| RMSProp | epoch: 009 | loss: 0.39026 - acc: 0.8535 -- iter: 1664/2226
[A[ATraining Step: 613  | total loss: [1m[32m0.38844[0m[0m | time: 71.118s
[2K
| RMSProp | epoch: 009 | loss: 0.38844 - acc: 0.8494 -- iter: 1696/2226
[A[ATraining Step: 614  | total loss: [1m[32m0.38901[0m[0m | time: 72.242s
[2K
| RMSProp | epoch: 009 | loss: 0.38901 - acc: 0.8489 -- iter: 1728/2226
[A[ATraining Step: 615  | total loss: [1m[32m0.37678[0m[0m | time: 73.127s
[2K
| RMSProp | epoch: 009 | loss: 0.37678 - acc: 0.8515 -- iter: 1760/2226
[A[ATraining Step: 616  | total loss: [1m[32m0.37232[0m[0m | time: 73.849s
[2K
| RMSProp | epoch: 009 | loss: 0.37232 - acc: 0.8538 -- iter: 1792/2226
[A[ATraining Step: 617  | total loss: [1m[32m0.36538[0m[0m | time: 74.636s
[2K
| RMSProp | epoch: 009 | loss: 0.36538 - acc: 0.8560 -- iter: 1824/2226
[A[ATraining Step: 618  | total loss: [1m[32m0.41930[0m[0m | time: 75.439s
[2K
| RMSProp | epoch: 009 | loss: 0.41930 - acc: 0.8329 -- iter: 1856/2226
[A[ATraining Step: 619  | total loss: [1m[32m0.41138[0m[0m | time: 76.300s
[2K
| RMSProp | epoch: 009 | loss: 0.41138 - acc: 0.8339 -- iter: 1888/2226
[A[ATraining Step: 620  | total loss: [1m[32m0.40114[0m[0m | time: 77.136s
[2K
| RMSProp | epoch: 009 | loss: 0.40114 - acc: 0.8381 -- iter: 1920/2226
[A[ATraining Step: 621  | total loss: [1m[32m0.41437[0m[0m | time: 78.029s
[2K
| RMSProp | epoch: 009 | loss: 0.41437 - acc: 0.8324 -- iter: 1952/2226
[A[ATraining Step: 622  | total loss: [1m[32m0.39796[0m[0m | time: 78.869s
[2K
| RMSProp | epoch: 009 | loss: 0.39796 - acc: 0.8429 -- iter: 1984/2226
[A[ATraining Step: 623  | total loss: [1m[32m0.39345[0m[0m | time: 79.740s
[2K
| RMSProp | epoch: 009 | loss: 0.39345 - acc: 0.8430 -- iter: 2016/2226
[A[ATraining Step: 624  | total loss: [1m[32m0.39234[0m[0m | time: 80.618s
[2K
| RMSProp | epoch: 009 | loss: 0.39234 - acc: 0.8430 -- iter: 2048/2226
[A[ATraining Step: 625  | total loss: [1m[32m0.39926[0m[0m | time: 81.489s
[2K
| RMSProp | epoch: 009 | loss: 0.39926 - acc: 0.8369 -- iter: 2080/2226
[A[ATraining Step: 626  | total loss: [1m[32m0.39810[0m[0m | time: 82.303s
[2K
| RMSProp | epoch: 009 | loss: 0.39810 - acc: 0.8407 -- iter: 2112/2226
[A[ATraining Step: 627  | total loss: [1m[32m0.39839[0m[0m | time: 83.139s
[2K
| RMSProp | epoch: 009 | loss: 0.39839 - acc: 0.8410 -- iter: 2144/2226
[A[ATraining Step: 628  | total loss: [1m[32m0.39873[0m[0m | time: 83.991s
[2K
| RMSProp | epoch: 009 | loss: 0.39873 - acc: 0.8413 -- iter: 2176/2226
[A[ATraining Step: 629  | total loss: [1m[32m0.41145[0m[0m | time: 84.806s
[2K
| RMSProp | epoch: 009 | loss: 0.41145 - acc: 0.8321 -- iter: 2208/2226
[A[ATraining Step: 630  | total loss: [1m[32m0.40135[0m[0m | time: 90.152s
[2K
| RMSProp | epoch: 009 | loss: 0.40135 - acc: 0.8395 | val_loss: 0.57019 - val_acc: 0.6911 -- iter: 2226/2226
--
Training Step: 631  | total loss: [1m[32m0.42239[0m[0m | time: 2.684s
[2K
| RMSProp | epoch: 010 | loss: 0.42239 - acc: 0.8243 -- iter: 0032/2226
[A[ATraining Step: 632  | total loss: [1m[32m0.43639[0m[0m | time: 5.441s
[2K
| RMSProp | epoch: 010 | loss: 0.43639 - acc: 0.8138 -- iter: 0064/2226
[A[ATraining Step: 633  | total loss: [1m[32m0.42062[0m[0m | time: 9.752s
[2K
| RMSProp | epoch: 010 | loss: 0.42062 - acc: 0.8262 -- iter: 0096/2226
[A[ATraining Step: 634  | total loss: [1m[32m0.41381[0m[0m | time: 14.735s
[2K
| RMSProp | epoch: 010 | loss: 0.41381 - acc: 0.8342 -- iter: 0128/2226
[A[ATraining Step: 635  | total loss: [1m[32m0.39206[0m[0m | time: 19.171s
[2K
| RMSProp | epoch: 010 | loss: 0.39206 - acc: 0.8445 -- iter: 0160/2226
[A[ATraining Step: 636  | total loss: [1m[32m0.39856[0m[0m | time: 21.654s
[2K
| RMSProp | epoch: 010 | loss: 0.39856 - acc: 0.8444 -- iter: 0192/2226
[A[ATraining Step: 637  | total loss: [1m[32m0.39657[0m[0m | time: 25.340s
[2K
| RMSProp | epoch: 010 | loss: 0.39657 - acc: 0.8444 -- iter: 0224/2226
[A[ATraining Step: 638  | total loss: [1m[32m0.40335[0m[0m | time: 26.522s
[2K
| RMSProp | epoch: 010 | loss: 0.40335 - acc: 0.8412 -- iter: 0256/2226
[A[ATraining Step: 639  | total loss: [1m[32m0.41643[0m[0m | time: 28.714s
[2K
| RMSProp | epoch: 010 | loss: 0.41643 - acc: 0.8348 -- iter: 0288/2226
[A[ATraining Step: 640  | total loss: [1m[32m0.42398[0m[0m | time: 29.737s
[2K
| RMSProp | epoch: 010 | loss: 0.42398 - acc: 0.8291 -- iter: 0320/2226
[A[ATraining Step: 641  | total loss: [1m[32m0.40814[0m[0m | time: 30.816s
[2K
| RMSProp | epoch: 010 | loss: 0.40814 - acc: 0.8431 -- iter: 0352/2226
[A[ATraining Step: 642  | total loss: [1m[32m0.39608[0m[0m | time: 31.908s
[2K
| RMSProp | epoch: 010 | loss: 0.39608 - acc: 0.8494 -- iter: 0384/2226
[A[ATraining Step: 643  | total loss: [1m[32m0.39395[0m[0m | time: 33.020s
[2K
| RMSProp | epoch: 010 | loss: 0.39395 - acc: 0.8520 -- iter: 0416/2226
[A[ATraining Step: 644  | total loss: [1m[32m0.39000[0m[0m | time: 34.112s
[2K
| RMSProp | epoch: 010 | loss: 0.39000 - acc: 0.8511 -- iter: 0448/2226
[A[ATraining Step: 645  | total loss: [1m[32m0.36842[0m[0m | time: 35.323s
[2K
| RMSProp | epoch: 010 | loss: 0.36842 - acc: 0.8598 -- iter: 0480/2226
[A[ATraining Step: 646  | total loss: [1m[32m0.35583[0m[0m | time: 36.475s
[2K
| RMSProp | epoch: 010 | loss: 0.35583 - acc: 0.8613 -- iter: 0512/2226
[A[ATraining Step: 647  | total loss: [1m[32m0.33916[0m[0m | time: 37.647s
[2K
| RMSProp | epoch: 010 | loss: 0.33916 - acc: 0.8627 -- iter: 0544/2226
[A[ATraining Step: 648  | total loss: [1m[32m0.33164[0m[0m | time: 38.717s
[2K
| RMSProp | epoch: 010 | loss: 0.33164 - acc: 0.8670 -- iter: 0576/2226
[A[ATraining Step: 649  | total loss: [1m[32m0.32499[0m[0m | time: 40.800s
[2K
| RMSProp | epoch: 010 | loss: 0.32499 - acc: 0.8741 -- iter: 0608/2226
[A[ATraining Step: 650  | total loss: [1m[32m0.32557[0m[0m | time: 42.590s
[2K
| RMSProp | epoch: 010 | loss: 0.32557 - acc: 0.8742 -- iter: 0640/2226
[A[ATraining Step: 651  | total loss: [1m[32m0.31686[0m[0m | time: 47.659s
[2K
| RMSProp | epoch: 010 | loss: 0.31686 - acc: 0.8774 -- iter: 0672/2226
[A[ATraining Step: 652  | total loss: [1m[32m0.31086[0m[0m | time: 51.073s
[2K
| RMSProp | epoch: 010 | loss: 0.31086 - acc: 0.8771 -- iter: 0704/2226
[A[ATraining Step: 653  | total loss: [1m[32m0.31154[0m[0m | time: 52.334s
[2K
| RMSProp | epoch: 010 | loss: 0.31154 - acc: 0.8707 -- iter: 0736/2226
[A[ATraining Step: 654  | total loss: [1m[32m0.33501[0m[0m | time: 53.481s
[2K
| RMSProp | epoch: 010 | loss: 0.33501 - acc: 0.8680 -- iter: 0768/2226
[A[ATraining Step: 655  | total loss: [1m[32m0.32614[0m[0m | time: 54.566s
[2K
| RMSProp | epoch: 010 | loss: 0.32614 - acc: 0.8718 -- iter: 0800/2226
[A[ATraining Step: 656  | total loss: [1m[32m0.32834[0m[0m | time: 55.738s
[2K
| RMSProp | epoch: 010 | loss: 0.32834 - acc: 0.8690 -- iter: 0832/2226
[A[ATraining Step: 657  | total loss: [1m[32m0.36133[0m[0m | time: 56.926s
[2K
| RMSProp | epoch: 010 | loss: 0.36133 - acc: 0.8509 -- iter: 0864/2226
[A[ATraining Step: 658  | total loss: [1m[32m0.36399[0m[0m | time: 58.133s
[2K
| RMSProp | epoch: 010 | loss: 0.36399 - acc: 0.8533 -- iter: 0896/2226
[A[ATraining Step: 659  | total loss: [1m[32m0.39270[0m[0m | time: 59.335s
[2K
| RMSProp | epoch: 010 | loss: 0.39270 - acc: 0.8367 -- iter: 0928/2226
[A[ATraining Step: 660  | total loss: [1m[32m0.39244[0m[0m | time: 60.446s
[2K
| RMSProp | epoch: 010 | loss: 0.39244 - acc: 0.8280 -- iter: 0960/2226
[A[ATraining Step: 661  | total loss: [1m[32m0.39027[0m[0m | time: 61.556s
[2K
| RMSProp | epoch: 010 | loss: 0.39027 - acc: 0.8327 -- iter: 0992/2226
[A[ATraining Step: 662  | total loss: [1m[32m0.37745[0m[0m | time: 62.604s
[2K
| RMSProp | epoch: 010 | loss: 0.37745 - acc: 0.8432 -- iter: 1024/2226
[A[ATraining Step: 663  | total loss: [1m[32m0.38697[0m[0m | time: 63.628s
[2K
| RMSProp | epoch: 010 | loss: 0.38697 - acc: 0.8339 -- iter: 1056/2226
[A[ATraining Step: 664  | total loss: [1m[32m0.39687[0m[0m | time: 64.684s
[2K
| RMSProp | epoch: 010 | loss: 0.39687 - acc: 0.8286 -- iter: 1088/2226
[A[ATraining Step: 665  | total loss: [1m[32m0.38352[0m[0m | time: 65.692s
[2K
| RMSProp | epoch: 010 | loss: 0.38352 - acc: 0.8395 -- iter: 1120/2226
[A[ATraining Step: 666  | total loss: [1m[32m0.38607[0m[0m | time: 66.737s
[2K
| RMSProp | epoch: 010 | loss: 0.38607 - acc: 0.8368 -- iter: 1152/2226
[A[ATraining Step: 667  | total loss: [1m[32m0.39561[0m[0m | time: 67.724s
[2K
| RMSProp | epoch: 010 | loss: 0.39561 - acc: 0.8344 -- iter: 1184/2226
[A[ATraining Step: 668  | total loss: [1m[32m0.40127[0m[0m | time: 68.757s
[2K
| RMSProp | epoch: 010 | loss: 0.40127 - acc: 0.8322 -- iter: 1216/2226
[A[ATraining Step: 669  | total loss: [1m[32m0.37967[0m[0m | time: 69.937s
[2K
| RMSProp | epoch: 010 | loss: 0.37967 - acc: 0.8458 -- iter: 1248/2226
[A[ATraining Step: 670  | total loss: [1m[32m0.38808[0m[0m | time: 70.994s
[2K
| RMSProp | epoch: 010 | loss: 0.38808 - acc: 0.8456 -- iter: 1280/2226
[A[ATraining Step: 671  | total loss: [1m[32m0.38373[0m[0m | time: 72.121s
[2K
| RMSProp | epoch: 010 | loss: 0.38373 - acc: 0.8454 -- iter: 1312/2226
[A[ATraining Step: 672  | total loss: [1m[32m0.37790[0m[0m | time: 73.443s
[2K
| RMSProp | epoch: 010 | loss: 0.37790 - acc: 0.8484 -- iter: 1344/2226
[A[ATraining Step: 673  | total loss: [1m[32m0.36028[0m[0m | time: 83.438s
[2K
| RMSProp | epoch: 010 | loss: 0.36028 - acc: 0.8573 -- iter: 1376/2226
[A[ATraining Step: 674  | total loss: [1m[32m0.34411[0m[0m | time: 88.819s
[2K
| RMSProp | epoch: 010 | loss: 0.34411 - acc: 0.8622 -- iter: 1408/2226
[A[ATraining Step: 675  | total loss: [1m[32m0.38716[0m[0m | time: 89.872s
[2K
| RMSProp | epoch: 010 | loss: 0.38716 - acc: 0.8479 -- iter: 1440/2226
[A[ATraining Step: 676  | total loss: [1m[32m0.38231[0m[0m | time: 90.860s
[2K
| RMSProp | epoch: 010 | loss: 0.38231 - acc: 0.8506 -- iter: 1472/2226
[A[ATraining Step: 677  | total loss: [1m[32m0.36856[0m[0m | time: 91.948s
[2K
| RMSProp | epoch: 010 | loss: 0.36856 - acc: 0.8624 -- iter: 1504/2226
[A[ATraining Step: 678  | total loss: [1m[32m0.35701[0m[0m | time: 93.023s
[2K
| RMSProp | epoch: 010 | loss: 0.35701 - acc: 0.8668 -- iter: 1536/2226
[A[ATraining Step: 679  | total loss: [1m[32m0.36330[0m[0m | time: 94.136s
[2K
| RMSProp | epoch: 010 | loss: 0.36330 - acc: 0.8582 -- iter: 1568/2226
[A[ATraining Step: 680  | total loss: [1m[32m0.35267[0m[0m | time: 95.199s
[2K
| RMSProp | epoch: 010 | loss: 0.35267 - acc: 0.8630 -- iter: 1600/2226
[A[ATraining Step: 681  | total loss: [1m[32m0.34616[0m[0m | time: 96.481s
[2K
| RMSProp | epoch: 010 | loss: 0.34616 - acc: 0.8642 -- iter: 1632/2226
[A[ATraining Step: 682  | total loss: [1m[32m0.33770[0m[0m | time: 97.564s
[2K
| RMSProp | epoch: 010 | loss: 0.33770 - acc: 0.8653 -- iter: 1664/2226
[A[ATraining Step: 683  | total loss: [1m[32m0.36531[0m[0m | time: 98.650s
[2K
| RMSProp | epoch: 010 | loss: 0.36531 - acc: 0.8506 -- iter: 1696/2226
[A[ATraining Step: 684  | total loss: [1m[32m0.36474[0m[0m | time: 100.023s
[2K
| RMSProp | epoch: 010 | loss: 0.36474 - acc: 0.8500 -- iter: 1728/2226
[A[ATraining Step: 685  | total loss: [1m[32m0.36096[0m[0m | time: 105.124s
[2K
| RMSProp | epoch: 010 | loss: 0.36096 - acc: 0.8525 -- iter: 1760/2226
[A[ATraining Step: 686  | total loss: [1m[32m0.37407[0m[0m | time: 108.089s
[2K
| RMSProp | epoch: 010 | loss: 0.37407 - acc: 0.8453 -- iter: 1792/2226
[A[ATraining Step: 687  | total loss: [1m[32m0.37255[0m[0m | time: 111.573s
[2K
| RMSProp | epoch: 010 | loss: 0.37255 - acc: 0.8452 -- iter: 1824/2226
[A[ATraining Step: 688  | total loss: [1m[32m0.35931[0m[0m | time: 115.880s
[2K
| RMSProp | epoch: 010 | loss: 0.35931 - acc: 0.8513 -- iter: 1856/2226
[A[ATraining Step: 689  | total loss: [1m[32m0.35538[0m[0m | time: 118.359s
[2K
| RMSProp | epoch: 010 | loss: 0.35538 - acc: 0.8537 -- iter: 1888/2226
[A[ATraining Step: 690  | total loss: [1m[32m0.35074[0m[0m | time: 119.394s
[2K
| RMSProp | epoch: 010 | loss: 0.35074 - acc: 0.8527 -- iter: 1920/2226
[A[ATraining Step: 691  | total loss: [1m[32m0.35959[0m[0m | time: 120.541s
[2K
| RMSProp | epoch: 010 | loss: 0.35959 - acc: 0.8362 -- iter: 1952/2226
[A[ATraining Step: 692  | total loss: [1m[32m0.36785[0m[0m | time: 121.678s
[2K
| RMSProp | epoch: 010 | loss: 0.36785 - acc: 0.8338 -- iter: 1984/2226
[A[ATraining Step: 693  | total loss: [1m[32m0.36076[0m[0m | time: 122.812s
[2K
| RMSProp | epoch: 010 | loss: 0.36076 - acc: 0.8348 -- iter: 2016/2226
[A[ATraining Step: 694  | total loss: [1m[32m0.34637[0m[0m | time: 124.052s
[2K
| RMSProp | epoch: 010 | loss: 0.34637 - acc: 0.8451 -- iter: 2048/2226
[A[ATraining Step: 695  | total loss: [1m[32m0.34232[0m[0m | time: 125.268s
[2K
| RMSProp | epoch: 010 | loss: 0.34232 - acc: 0.8418 -- iter: 2080/2226
[A[ATraining Step: 696  | total loss: [1m[32m0.38093[0m[0m | time: 126.592s
[2K
| RMSProp | epoch: 010 | loss: 0.38093 - acc: 0.8139 -- iter: 2112/2226
[A[ATraining Step: 697  | total loss: [1m[32m0.38546[0m[0m | time: 127.693s
[2K
| RMSProp | epoch: 010 | loss: 0.38546 - acc: 0.8137 -- iter: 2144/2226
[A[ATraining Step: 698  | total loss: [1m[32m0.38551[0m[0m | time: 128.886s
[2K
| RMSProp | epoch: 010 | loss: 0.38551 - acc: 0.8167 -- iter: 2176/2226
[A[ATraining Step: 699  | total loss: [1m[32m0.38558[0m[0m | time: 130.043s
[2K
| RMSProp | epoch: 010 | loss: 0.38558 - acc: 0.8163 -- iter: 2208/2226
[A[ATraining Step: 700  | total loss: [1m[32m0.40205[0m[0m | time: 155.208s
[2K
| RMSProp | epoch: 010 | loss: 0.40205 - acc: 0.8066 | val_loss: 0.34247 - val_acc: 0.8592 -- iter: 2226/2226
--
Training Step: 701  | total loss: [1m[32m0.39641[0m[0m | time: 1.226s
[2K
| RMSProp | epoch: 011 | loss: 0.39641 - acc: 0.8134 -- iter: 0032/2226
[A[ATraining Step: 702  | total loss: [1m[32m0.38409[0m[0m | time: 2.463s
[2K
| RMSProp | epoch: 011 | loss: 0.38409 - acc: 0.8196 -- iter: 0064/2226
[A[ATraining Step: 703  | total loss: [1m[32m0.39059[0m[0m | time: 3.648s
[2K
| RMSProp | epoch: 011 | loss: 0.39059 - acc: 0.8189 -- iter: 0096/2226
[A[ATraining Step: 704  | total loss: [1m[32m0.39094[0m[0m | time: 4.780s
[2K
| RMSProp | epoch: 011 | loss: 0.39094 - acc: 0.8276 -- iter: 0128/2226
[A[ATraining Step: 705  | total loss: [1m[32m0.38267[0m[0m | time: 6.049s
[2K
| RMSProp | epoch: 011 | loss: 0.38267 - acc: 0.8386 -- iter: 0160/2226
[A[ATraining Step: 706  | total loss: [1m[32m0.38555[0m[0m | time: 7.335s
[2K
| RMSProp | epoch: 011 | loss: 0.38555 - acc: 0.8329 -- iter: 0192/2226
[A[ATraining Step: 707  | total loss: [1m[32m0.37761[0m[0m | time: 13.347s
[2K
| RMSProp | epoch: 011 | loss: 0.37761 - acc: 0.8402 -- iter: 0224/2226
[A[ATraining Step: 708  | total loss: [1m[32m0.35832[0m[0m | time: 15.299s
[2K
| RMSProp | epoch: 011 | loss: 0.35832 - acc: 0.8530 -- iter: 0256/2226
[A[ATraining Step: 709  | total loss: [1m[32m0.34841[0m[0m | time: 16.322s
[2K
| RMSProp | epoch: 011 | loss: 0.34841 - acc: 0.8584 -- iter: 0288/2226
[A[ATraining Step: 710  | total loss: [1m[32m0.36927[0m[0m | time: 18.927s
[2K
| RMSProp | epoch: 011 | loss: 0.36927 - acc: 0.8448 -- iter: 0320/2226
[A[ATraining Step: 711  | total loss: [1m[32m0.34921[0m[0m | time: 19.890s
[2K
| RMSProp | epoch: 011 | loss: 0.34921 - acc: 0.8547 -- iter: 0352/2226
[A[ATraining Step: 712  | total loss: [1m[32m0.36524[0m[0m | time: 21.049s
[2K
| RMSProp | epoch: 011 | loss: 0.36524 - acc: 0.8474 -- iter: 0384/2226
[A[ATraining Step: 713  | total loss: [1m[32m0.36949[0m[0m | time: 22.145s
[2K
| RMSProp | epoch: 011 | loss: 0.36949 - acc: 0.8501 -- iter: 0416/2226
[A[ATraining Step: 714  | total loss: [1m[32m0.35455[0m[0m | time: 23.324s
[2K
| RMSProp | epoch: 011 | loss: 0.35455 - acc: 0.8620 -- iter: 0448/2226
[A[ATraining Step: 715  | total loss: [1m[32m0.34372[0m[0m | time: 24.465s
[2K
| RMSProp | epoch: 011 | loss: 0.34372 - acc: 0.8664 -- iter: 0480/2226
[A[ATraining Step: 716  | total loss: [1m[32m0.32626[0m[0m | time: 25.633s
[2K
| RMSProp | epoch: 011 | loss: 0.32626 - acc: 0.8735 -- iter: 0512/2226
[A[ATraining Step: 717  | total loss: [1m[32m0.31793[0m[0m | time: 26.721s
[2K
| RMSProp | epoch: 011 | loss: 0.31793 - acc: 0.8768 -- iter: 0544/2226
[A[ATraining Step: 718  | total loss: [1m[32m0.30872[0m[0m | time: 27.888s
[2K
| RMSProp | epoch: 011 | loss: 0.30872 - acc: 0.8829 -- iter: 0576/2226
[A[ATraining Step: 719  | total loss: [1m[32m0.30509[0m[0m | time: 28.932s
[2K
| RMSProp | epoch: 011 | loss: 0.30509 - acc: 0.8883 -- iter: 0608/2226
[A[ATraining Step: 720  | total loss: [1m[32m0.29919[0m[0m | time: 30.107s
[2K
| RMSProp | epoch: 011 | loss: 0.29919 - acc: 0.8933 -- iter: 0640/2226
[A[ATraining Step: 721  | total loss: [1m[32m0.31271[0m[0m | time: 31.358s
[2K
| RMSProp | epoch: 011 | loss: 0.31271 - acc: 0.8821 -- iter: 0672/2226
[A[ATraining Step: 722  | total loss: [1m[32m0.32489[0m[0m | time: 32.659s
[2K
| RMSProp | epoch: 011 | loss: 0.32489 - acc: 0.8782 -- iter: 0704/2226
[A[ATraining Step: 723  | total loss: [1m[32m0.33803[0m[0m | time: 33.867s
[2K
| RMSProp | epoch: 011 | loss: 0.33803 - acc: 0.8685 -- iter: 0736/2226
[A[ATraining Step: 724  | total loss: [1m[32m0.33375[0m[0m | time: 34.945s
[2K
| RMSProp | epoch: 011 | loss: 0.33375 - acc: 0.8692 -- iter: 0768/2226
[A[ATraining Step: 725  | total loss: [1m[32m0.33376[0m[0m | time: 35.797s
[2K
| RMSProp | epoch: 011 | loss: 0.33376 - acc: 0.8729 -- iter: 0800/2226
[A[ATraining Step: 726  | total loss: [1m[32m0.32318[0m[0m | time: 36.641s
[2K
| RMSProp | epoch: 011 | loss: 0.32318 - acc: 0.8731 -- iter: 0832/2226
[A[ATraining Step: 727  | total loss: [1m[32m0.31011[0m[0m | time: 37.450s
[2K
| RMSProp | epoch: 011 | loss: 0.31011 - acc: 0.8764 -- iter: 0864/2226
[A[ATraining Step: 728  | total loss: [1m[32m0.29809[0m[0m | time: 38.302s
[2K
| RMSProp | epoch: 011 | loss: 0.29809 - acc: 0.8825 -- iter: 0896/2226
[A[ATraining Step: 729  | total loss: [1m[32m0.30394[0m[0m | time: 39.175s
[2K
| RMSProp | epoch: 011 | loss: 0.30394 - acc: 0.8786 -- iter: 0928/2226
[A[ATraining Step: 730  | total loss: [1m[32m0.29393[0m[0m | time: 40.125s
[2K
| RMSProp | epoch: 011 | loss: 0.29393 - acc: 0.8814 -- iter: 0960/2226
[A[ATraining Step: 731  | total loss: [1m[32m0.29384[0m[0m | time: 41.211s
[2K
| RMSProp | epoch: 011 | loss: 0.29384 - acc: 0.8839 -- iter: 0992/2226
[A[ATraining Step: 732  | total loss: [1m[32m0.34157[0m[0m | time: 41.984s
[2K
| RMSProp | epoch: 011 | loss: 0.34157 - acc: 0.8674 -- iter: 1024/2226
[A[ATraining Step: 733  | total loss: [1m[32m0.33134[0m[0m | time: 42.780s
[2K
| RMSProp | epoch: 011 | loss: 0.33134 - acc: 0.8681 -- iter: 1056/2226
[A[ATraining Step: 734  | total loss: [1m[32m0.32982[0m[0m | time: 43.626s
[2K
| RMSProp | epoch: 011 | loss: 0.32982 - acc: 0.8657 -- iter: 1088/2226
[A[ATraining Step: 735  | total loss: [1m[32m0.32226[0m[0m | time: 44.522s
[2K
| RMSProp | epoch: 011 | loss: 0.32226 - acc: 0.8698 -- iter: 1120/2226
[A[ATraining Step: 736  | total loss: [1m[32m0.35086[0m[0m | time: 45.313s
[2K
| RMSProp | epoch: 011 | loss: 0.35086 - acc: 0.8578 -- iter: 1152/2226
[A[ATraining Step: 737  | total loss: [1m[32m0.34436[0m[0m | time: 46.126s
[2K
| RMSProp | epoch: 011 | loss: 0.34436 - acc: 0.8595 -- iter: 1184/2226
[A[ATraining Step: 738  | total loss: [1m[32m0.33193[0m[0m | time: 46.987s
[2K
| RMSProp | epoch: 011 | loss: 0.33193 - acc: 0.8642 -- iter: 1216/2226
[A[ATraining Step: 739  | total loss: [1m[32m0.33915[0m[0m | time: 47.881s
[2K
| RMSProp | epoch: 011 | loss: 0.33915 - acc: 0.8590 -- iter: 1248/2226
[A[ATraining Step: 740  | total loss: [1m[32m0.33354[0m[0m | time: 48.702s
[2K
| RMSProp | epoch: 011 | loss: 0.33354 - acc: 0.8606 -- iter: 1280/2226
[A[ATraining Step: 741  | total loss: [1m[32m0.39512[0m[0m | time: 49.517s
[2K
| RMSProp | epoch: 011 | loss: 0.39512 - acc: 0.8370 -- iter: 1312/2226
[A[ATraining Step: 742  | total loss: [1m[32m0.38692[0m[0m | time: 50.352s
[2K
| RMSProp | epoch: 011 | loss: 0.38692 - acc: 0.8408 -- iter: 1344/2226
[A[ATraining Step: 743  | total loss: [1m[32m0.37650[0m[0m | time: 51.176s
[2K
| RMSProp | epoch: 011 | loss: 0.37650 - acc: 0.8443 -- iter: 1376/2226
[A[ATraining Step: 744  | total loss: [1m[32m0.36642[0m[0m | time: 51.932s
[2K
| RMSProp | epoch: 011 | loss: 0.36642 - acc: 0.8473 -- iter: 1408/2226
[A[ATraining Step: 745  | total loss: [1m[32m0.35503[0m[0m | time: 53.090s
[2K
| RMSProp | epoch: 011 | loss: 0.35503 - acc: 0.8532 -- iter: 1440/2226
[A[ATraining Step: 746  | total loss: [1m[32m0.34787[0m[0m | time: 54.394s
[2K
| RMSProp | epoch: 011 | loss: 0.34787 - acc: 0.8585 -- iter: 1472/2226
[A[ATraining Step: 747  | total loss: [1m[32m0.35759[0m[0m | time: 55.770s
[2K
| RMSProp | epoch: 011 | loss: 0.35759 - acc: 0.8570 -- iter: 1504/2226
[A[ATraining Step: 748  | total loss: [1m[32m0.38438[0m[0m | time: 59.394s
[2K
| RMSProp | epoch: 011 | loss: 0.38438 - acc: 0.8526 -- iter: 1536/2226
[A[ATraining Step: 749  | total loss: [1m[32m0.39660[0m[0m | time: 60.679s
[2K
| RMSProp | epoch: 011 | loss: 0.39660 - acc: 0.8517 -- iter: 1568/2226
[A[ATraining Step: 750  | total loss: [1m[32m0.37667[0m[0m | time: 61.772s
[2K
| RMSProp | epoch: 011 | loss: 0.37667 - acc: 0.8665 -- iter: 1600/2226
[A[ATraining Step: 751  | total loss: [1m[32m0.36651[0m[0m | time: 62.965s
[2K
| RMSProp | epoch: 011 | loss: 0.36651 - acc: 0.8674 -- iter: 1632/2226
[A[ATraining Step: 752  | total loss: [1m[32m0.36733[0m[0m | time: 64.112s
[2K
| RMSProp | epoch: 011 | loss: 0.36733 - acc: 0.8681 -- iter: 1664/2226
[A[ATraining Step: 753  | total loss: [1m[32m0.36528[0m[0m | time: 65.313s
[2K
| RMSProp | epoch: 011 | loss: 0.36528 - acc: 0.8626 -- iter: 1696/2226
[A[ATraining Step: 754  | total loss: [1m[32m0.35008[0m[0m | time: 66.476s
[2K
| RMSProp | epoch: 011 | loss: 0.35008 - acc: 0.8701 -- iter: 1728/2226
[A[ATraining Step: 755  | total loss: [1m[32m0.33256[0m[0m | time: 67.652s
[2K
| RMSProp | epoch: 011 | loss: 0.33256 - acc: 0.8737 -- iter: 1760/2226
[A[ATraining Step: 756  | total loss: [1m[32m0.31796[0m[0m | time: 68.782s
[2K
| RMSProp | epoch: 011 | loss: 0.31796 - acc: 0.8769 -- iter: 1792/2226
[A[ATraining Step: 757  | total loss: [1m[32m0.30420[0m[0m | time: 69.914s
[2K
| RMSProp | epoch: 011 | loss: 0.30420 - acc: 0.8830 -- iter: 1824/2226
[A[ATraining Step: 758  | total loss: [1m[32m0.32563[0m[0m | time: 71.108s
[2K
| RMSProp | epoch: 011 | loss: 0.32563 - acc: 0.8728 -- iter: 1856/2226
[A[ATraining Step: 759  | total loss: [1m[32m0.31621[0m[0m | time: 72.388s
[2K
| RMSProp | epoch: 011 | loss: 0.31621 - acc: 0.8793 -- iter: 1888/2226
[A[ATraining Step: 760  | total loss: [1m[32m0.31513[0m[0m | time: 73.313s
[2K
| RMSProp | epoch: 011 | loss: 0.31513 - acc: 0.8789 -- iter: 1920/2226
[A[ATraining Step: 761  | total loss: [1m[32m0.33395[0m[0m | time: 74.402s
[2K
| RMSProp | epoch: 011 | loss: 0.33395 - acc: 0.8660 -- iter: 1952/2226
[A[ATraining Step: 762  | total loss: [1m[32m0.32388[0m[0m | time: 75.549s
[2K
| RMSProp | epoch: 011 | loss: 0.32388 - acc: 0.8731 -- iter: 1984/2226
[A[ATraining Step: 763  | total loss: [1m[32m0.31516[0m[0m | time: 76.716s
[2K
| RMSProp | epoch: 011 | loss: 0.31516 - acc: 0.8764 -- iter: 2016/2226
[A[ATraining Step: 764  | total loss: [1m[32m0.30017[0m[0m | time: 77.883s
[2K
| RMSProp | epoch: 011 | loss: 0.30017 - acc: 0.8857 -- iter: 2048/2226
[A[ATraining Step: 765  | total loss: [1m[32m0.29494[0m[0m | time: 79.031s
[2K
| RMSProp | epoch: 011 | loss: 0.29494 - acc: 0.8877 -- iter: 2080/2226
[A[ATraining Step: 766  | total loss: [1m[32m0.27707[0m[0m | time: 80.211s
[2K
| RMSProp | epoch: 011 | loss: 0.27707 - acc: 0.8990 -- iter: 2112/2226
[A[ATraining Step: 767  | total loss: [1m[32m0.27827[0m[0m | time: 81.340s
[2K
| RMSProp | epoch: 011 | loss: 0.27827 - acc: 0.8997 -- iter: 2144/2226
[A[ATraining Step: 768  | total loss: [1m[32m0.28196[0m[0m | time: 82.419s
[2K
| RMSProp | epoch: 011 | loss: 0.28196 - acc: 0.8941 -- iter: 2176/2226
[A[ATraining Step: 769  | total loss: [1m[32m0.30171[0m[0m | time: 83.719s
[2K
| RMSProp | epoch: 011 | loss: 0.30171 - acc: 0.8828 -- iter: 2208/2226
[A[ATraining Step: 770  | total loss: [1m[32m0.29906[0m[0m | time: 107.519s
[2K
| RMSProp | epoch: 011 | loss: 0.29906 - acc: 0.8820 | val_loss: 0.31158 - val_acc: 0.8721 -- iter: 2226/2226
--
Training Step: 771  | total loss: [1m[32m0.28477[0m[0m | time: 1.323s
[2K
| RMSProp | epoch: 012 | loss: 0.28477 - acc: 0.8907 -- iter: 0032/2226
[A[ATraining Step: 772  | total loss: [1m[32m0.28429[0m[0m | time: 6.458s
[2K
| RMSProp | epoch: 012 | loss: 0.28429 - acc: 0.8923 -- iter: 0064/2226
[A[ATraining Step: 773  | total loss: [1m[32m0.27701[0m[0m | time: 15.598s
[2K
| RMSProp | epoch: 012 | loss: 0.27701 - acc: 0.8937 -- iter: 0096/2226
[A[ATraining Step: 774  | total loss: [1m[32m0.28561[0m[0m | time: 18.592s
[2K
| RMSProp | epoch: 012 | loss: 0.28561 - acc: 0.8855 -- iter: 0128/2226
[A[ATraining Step: 775  | total loss: [1m[32m0.31295[0m[0m | time: 23.120s
[2K
| RMSProp | epoch: 012 | loss: 0.31295 - acc: 0.8689 -- iter: 0160/2226
[A[ATraining Step: 776  | total loss: [1m[32m0.29928[0m[0m | time: 24.067s
[2K
| RMSProp | epoch: 012 | loss: 0.29928 - acc: 0.8757 -- iter: 0192/2226
[A[ATraining Step: 777  | total loss: [1m[32m0.30261[0m[0m | time: 24.991s
[2K
| RMSProp | epoch: 012 | loss: 0.30261 - acc: 0.8788 -- iter: 0224/2226
[A[ATraining Step: 778  | total loss: [1m[32m0.28122[0m[0m | time: 25.960s
[2K
| RMSProp | epoch: 012 | loss: 0.28122 - acc: 0.8909 -- iter: 0256/2226
[A[ATraining Step: 779  | total loss: [1m[32m0.26935[0m[0m | time: 26.816s
[2K
| RMSProp | epoch: 012 | loss: 0.26935 - acc: 0.8924 -- iter: 0288/2226
[A[ATraining Step: 780  | total loss: [1m[32m0.30877[0m[0m | time: 27.384s
[2K
| RMSProp | epoch: 012 | loss: 0.30877 - acc: 0.8751 -- iter: 0320/2226
[A[ATraining Step: 781  | total loss: [1m[32m0.28898[0m[0m | time: 27.968s
[2K
| RMSProp | epoch: 012 | loss: 0.28898 - acc: 0.8876 -- iter: 0352/2226
[A[ATraining Step: 782  | total loss: [1m[32m0.26580[0m[0m | time: 28.935s
[2K
| RMSProp | epoch: 012 | loss: 0.26580 - acc: 0.8988 -- iter: 0384/2226
[A[ATraining Step: 783  | total loss: [1m[32m0.25256[0m[0m | time: 29.927s
[2K
| RMSProp | epoch: 012 | loss: 0.25256 - acc: 0.9089 -- iter: 0416/2226
[A[ATraining Step: 784  | total loss: [1m[32m0.26662[0m[0m | time: 31.020s
[2K
| RMSProp | epoch: 012 | loss: 0.26662 - acc: 0.8993 -- iter: 0448/2226
[A[ATraining Step: 785  | total loss: [1m[32m0.28620[0m[0m | time: 32.039s
[2K
| RMSProp | epoch: 012 | loss: 0.28620 - acc: 0.8906 -- iter: 0480/2226
[A[ATraining Step: 786  | total loss: [1m[32m0.27183[0m[0m | time: 32.807s
[2K
| RMSProp | epoch: 012 | loss: 0.27183 - acc: 0.8984 -- iter: 0512/2226
[A[ATraining Step: 787  | total loss: [1m[32m0.24999[0m[0m | time: 33.763s
[2K
| RMSProp | epoch: 012 | loss: 0.24999 - acc: 0.9086 -- iter: 0544/2226
[A[ATraining Step: 788  | total loss: [1m[32m0.24884[0m[0m | time: 34.787s
[2K
| RMSProp | epoch: 012 | loss: 0.24884 - acc: 0.9115 -- iter: 0576/2226
[A[ATraining Step: 789  | total loss: [1m[32m0.25587[0m[0m | time: 35.787s
[2K
| RMSProp | epoch: 012 | loss: 0.25587 - acc: 0.9078 -- iter: 0608/2226
[A[ATraining Step: 790  | total loss: [1m[32m0.25081[0m[0m | time: 36.822s
[2K
| RMSProp | epoch: 012 | loss: 0.25081 - acc: 0.9077 -- iter: 0640/2226
[A[ATraining Step: 791  | total loss: [1m[32m0.24938[0m[0m | time: 37.810s
[2K
| RMSProp | epoch: 012 | loss: 0.24938 - acc: 0.9106 -- iter: 0672/2226
[A[ATraining Step: 792  | total loss: [1m[32m0.25378[0m[0m | time: 38.842s
[2K
| RMSProp | epoch: 012 | loss: 0.25378 - acc: 0.9040 -- iter: 0704/2226
[A[ATraining Step: 793  | total loss: [1m[32m0.26518[0m[0m | time: 39.756s
[2K
| RMSProp | epoch: 012 | loss: 0.26518 - acc: 0.8979 -- iter: 0736/2226
[A[ATraining Step: 794  | total loss: [1m[32m0.26939[0m[0m | time: 40.861s
[2K
| RMSProp | epoch: 012 | loss: 0.26939 - acc: 0.8956 -- iter: 0768/2226
[A[ATraining Step: 795  | total loss: [1m[32m0.27453[0m[0m | time: 41.948s
[2K
| RMSProp | epoch: 012 | loss: 0.27453 - acc: 0.8936 -- iter: 0800/2226
[A[ATraining Step: 796  | total loss: [1m[32m0.27352[0m[0m | time: 42.823s
[2K
| RMSProp | epoch: 012 | loss: 0.27352 - acc: 0.8948 -- iter: 0832/2226
[A[ATraining Step: 797  | total loss: [1m[32m0.27254[0m[0m | time: 43.699s
[2K
| RMSProp | epoch: 012 | loss: 0.27254 - acc: 0.8929 -- iter: 0864/2226
[A[ATraining Step: 798  | total loss: [1m[32m0.27154[0m[0m | time: 44.719s
[2K
| RMSProp | epoch: 012 | loss: 0.27154 - acc: 0.8911 -- iter: 0896/2226
[A[ATraining Step: 799  | total loss: [1m[32m0.26369[0m[0m | time: 45.724s
[2K
| RMSProp | epoch: 012 | loss: 0.26369 - acc: 0.8957 -- iter: 0928/2226
[A[ATraining Step: 800  | total loss: [1m[32m0.25598[0m[0m | time: 50.927s
[2K
| RMSProp | epoch: 012 | loss: 0.25598 - acc: 0.8999 | val_loss: 0.41510 - val_acc: 0.8348 -- iter: 0960/2226
--
Training Step: 801  | total loss: [1m[32m0.27212[0m[0m | time: 52.036s
[2K
| RMSProp | epoch: 012 | loss: 0.27212 - acc: 0.8943 -- iter: 0992/2226
[A[ATraining Step: 802  | total loss: [1m[32m0.27761[0m[0m | time: 52.932s
[2K
| RMSProp | epoch: 012 | loss: 0.27761 - acc: 0.8924 -- iter: 1024/2226
[A[ATraining Step: 803  | total loss: [1m[32m0.28101[0m[0m | time: 53.830s
[2K
| RMSProp | epoch: 012 | loss: 0.28101 - acc: 0.8906 -- iter: 1056/2226
[A[ATraining Step: 804  | total loss: [1m[32m0.27851[0m[0m | time: 54.784s
[2K
| RMSProp | epoch: 012 | loss: 0.27851 - acc: 0.8891 -- iter: 1088/2226
[A[ATraining Step: 805  | total loss: [1m[32m0.29176[0m[0m | time: 55.746s
[2K
| RMSProp | epoch: 012 | loss: 0.29176 - acc: 0.8814 -- iter: 1120/2226
[A[ATraining Step: 806  | total loss: [1m[32m0.28464[0m[0m | time: 56.698s
[2K
| RMSProp | epoch: 012 | loss: 0.28464 - acc: 0.8808 -- iter: 1152/2226
[A[ATraining Step: 807  | total loss: [1m[32m0.27523[0m[0m | time: 57.721s
[2K
| RMSProp | epoch: 012 | loss: 0.27523 - acc: 0.8864 -- iter: 1184/2226
[A[ATraining Step: 808  | total loss: [1m[32m0.26146[0m[0m | time: 58.723s
[2K
| RMSProp | epoch: 012 | loss: 0.26146 - acc: 0.8947 -- iter: 1216/2226
[A[ATraining Step: 809  | total loss: [1m[32m0.28741[0m[0m | time: 59.689s
[2K
| RMSProp | epoch: 012 | loss: 0.28741 - acc: 0.8802 -- iter: 1248/2226
[A[ATraining Step: 810  | total loss: [1m[32m0.28896[0m[0m | time: 60.679s
[2K
| RMSProp | epoch: 012 | loss: 0.28896 - acc: 0.8797 -- iter: 1280/2226
[A[ATraining Step: 811  | total loss: [1m[32m0.27849[0m[0m | time: 61.748s
[2K
| RMSProp | epoch: 012 | loss: 0.27849 - acc: 0.8855 -- iter: 1312/2226
[A[ATraining Step: 812  | total loss: [1m[32m0.26375[0m[0m | time: 62.743s
[2K
| RMSProp | epoch: 012 | loss: 0.26375 - acc: 0.8938 -- iter: 1344/2226
[A[ATraining Step: 813  | total loss: [1m[32m0.25454[0m[0m | time: 63.835s
[2K
| RMSProp | epoch: 012 | loss: 0.25454 - acc: 0.9013 -- iter: 1376/2226
[A[ATraining Step: 814  | total loss: [1m[32m0.24056[0m[0m | time: 64.912s
[2K
| RMSProp | epoch: 012 | loss: 0.24056 - acc: 0.9080 -- iter: 1408/2226
[A[ATraining Step: 815  | total loss: [1m[32m0.27410[0m[0m | time: 65.686s
[2K
| RMSProp | epoch: 012 | loss: 0.27410 - acc: 0.9016 -- iter: 1440/2226
[A[ATraining Step: 816  | total loss: [1m[32m0.29684[0m[0m | time: 66.399s
[2K
| RMSProp | epoch: 012 | loss: 0.29684 - acc: 0.8927 -- iter: 1472/2226
[A[ATraining Step: 817  | total loss: [1m[32m0.28607[0m[0m | time: 67.108s
[2K
| RMSProp | epoch: 012 | loss: 0.28607 - acc: 0.8940 -- iter: 1504/2226
[A[ATraining Step: 818  | total loss: [1m[32m0.28579[0m[0m | time: 67.843s
[2K
| RMSProp | epoch: 012 | loss: 0.28579 - acc: 0.8921 -- iter: 1536/2226
[A[ATraining Step: 819  | total loss: [1m[32m0.28538[0m[0m | time: 68.548s
[2K
| RMSProp | epoch: 012 | loss: 0.28538 - acc: 0.8936 -- iter: 1568/2226
[A[ATraining Step: 820  | total loss: [1m[32m0.27600[0m[0m | time: 69.252s
[2K
| RMSProp | epoch: 012 | loss: 0.27600 - acc: 0.8979 -- iter: 1600/2226
[A[ATraining Step: 821  | total loss: [1m[32m0.26959[0m[0m | time: 69.934s
[2K
| RMSProp | epoch: 012 | loss: 0.26959 - acc: 0.8988 -- iter: 1632/2226
[A[ATraining Step: 822  | total loss: [1m[32m0.27936[0m[0m | time: 70.647s
[2K
| RMSProp | epoch: 012 | loss: 0.27936 - acc: 0.8933 -- iter: 1664/2226
[A[ATraining Step: 823  | total loss: [1m[32m0.27308[0m[0m | time: 71.441s
[2K
| RMSProp | epoch: 012 | loss: 0.27308 - acc: 0.8946 -- iter: 1696/2226
[A[ATraining Step: 824  | total loss: [1m[32m0.28374[0m[0m | time: 72.308s
[2K
| RMSProp | epoch: 012 | loss: 0.28374 - acc: 0.8832 -- iter: 1728/2226
[A[ATraining Step: 825  | total loss: [1m[32m0.29300[0m[0m | time: 73.024s
[2K
| RMSProp | epoch: 012 | loss: 0.29300 - acc: 0.8855 -- iter: 1760/2226
[A[ATraining Step: 826  | total loss: [1m[32m0.29872[0m[0m | time: 73.680s
[2K
| RMSProp | epoch: 012 | loss: 0.29872 - acc: 0.8814 -- iter: 1792/2226
[A[ATraining Step: 827  | total loss: [1m[32m0.28624[0m[0m | time: 74.388s
[2K
| RMSProp | epoch: 012 | loss: 0.28624 - acc: 0.8870 -- iter: 1824/2226
[A[ATraining Step: 828  | total loss: [1m[32m0.26875[0m[0m | time: 75.090s
[2K
| RMSProp | epoch: 012 | loss: 0.26875 - acc: 0.8952 -- iter: 1856/2226
[A[ATraining Step: 829  | total loss: [1m[32m0.25337[0m[0m | time: 75.782s
[2K
| RMSProp | epoch: 012 | loss: 0.25337 - acc: 0.9025 -- iter: 1888/2226
[A[ATraining Step: 830  | total loss: [1m[32m0.24259[0m[0m | time: 76.484s
[2K
| RMSProp | epoch: 012 | loss: 0.24259 - acc: 0.9060 -- iter: 1920/2226
[A[ATraining Step: 831  | total loss: [1m[32m0.24543[0m[0m | time: 77.180s
[2K
| RMSProp | epoch: 012 | loss: 0.24543 - acc: 0.9060 -- iter: 1952/2226
[A[ATraining Step: 832  | total loss: [1m[32m0.24933[0m[0m | time: 77.871s
[2K
| RMSProp | epoch: 012 | loss: 0.24933 - acc: 0.9061 -- iter: 1984/2226
[A[ATraining Step: 833  | total loss: [1m[32m0.24619[0m[0m | time: 78.577s
[2K
| RMSProp | epoch: 012 | loss: 0.24619 - acc: 0.9061 -- iter: 2016/2226
[A[ATraining Step: 834  | total loss: [1m[32m0.24246[0m[0m | time: 79.274s
[2K
| RMSProp | epoch: 012 | loss: 0.24246 - acc: 0.9061 -- iter: 2048/2226
[A[ATraining Step: 835  | total loss: [1m[32m0.22720[0m[0m | time: 79.979s
[2K
| RMSProp | epoch: 012 | loss: 0.22720 - acc: 0.9124 -- iter: 2080/2226
[A[ATraining Step: 836  | total loss: [1m[32m0.22757[0m[0m | time: 80.666s
[2K
| RMSProp | epoch: 012 | loss: 0.22757 - acc: 0.9117 -- iter: 2112/2226
[A[ATraining Step: 837  | total loss: [1m[32m0.24767[0m[0m | time: 81.469s
[2K
| RMSProp | epoch: 012 | loss: 0.24767 - acc: 0.9049 -- iter: 2144/2226
[A[ATraining Step: 838  | total loss: [1m[32m0.24892[0m[0m | time: 82.154s
[2K
| RMSProp | epoch: 012 | loss: 0.24892 - acc: 0.9082 -- iter: 2176/2226
[A[ATraining Step: 839  | total loss: [1m[32m0.23761[0m[0m | time: 82.840s
[2K
| RMSProp | epoch: 012 | loss: 0.23761 - acc: 0.9111 -- iter: 2208/2226
[A[ATraining Step: 840  | total loss: [1m[32m0.23953[0m[0m | time: 86.764s
[2K
| RMSProp | epoch: 012 | loss: 0.23953 - acc: 0.9106 | val_loss: 0.30623 - val_acc: 0.8736 -- iter: 2226/2226
--
Training Step: 841  | total loss: [1m[32m0.25218[0m[0m | time: 0.794s
[2K
| RMSProp | epoch: 013 | loss: 0.25218 - acc: 0.9008 -- iter: 0032/2226
[A[ATraining Step: 842  | total loss: [1m[32m0.24991[0m[0m | time: 1.781s
[2K
| RMSProp | epoch: 013 | loss: 0.24991 - acc: 0.9045 -- iter: 0064/2226
[A[ATraining Step: 843  | total loss: [1m[32m0.25144[0m[0m | time: 2.794s
[2K
| RMSProp | epoch: 013 | loss: 0.25144 - acc: 0.9047 -- iter: 0096/2226
[A[ATraining Step: 844  | total loss: [1m[32m0.24420[0m[0m | time: 3.708s
[2K
| RMSProp | epoch: 013 | loss: 0.24420 - acc: 0.9111 -- iter: 0128/2226
[A[ATraining Step: 845  | total loss: [1m[32m0.23897[0m[0m | time: 4.749s
[2K
| RMSProp | epoch: 013 | loss: 0.23897 - acc: 0.9106 -- iter: 0160/2226
[A[ATraining Step: 846  | total loss: [1m[32m0.24390[0m[0m | time: 5.844s
[2K
| RMSProp | epoch: 013 | loss: 0.24390 - acc: 0.9070 -- iter: 0192/2226
[A[ATraining Step: 847  | total loss: [1m[32m0.23392[0m[0m | time: 6.852s
[2K
| RMSProp | epoch: 013 | loss: 0.23392 - acc: 0.9101 -- iter: 0224/2226
[A[ATraining Step: 848  | total loss: [1m[32m0.22616[0m[0m | time: 7.794s
[2K
| RMSProp | epoch: 013 | loss: 0.22616 - acc: 0.9128 -- iter: 0256/2226
[A[ATraining Step: 849  | total loss: [1m[32m0.22023[0m[0m | time: 8.850s
[2K
| RMSProp | epoch: 013 | loss: 0.22023 - acc: 0.9184 -- iter: 0288/2226
[A[ATraining Step: 850  | total loss: [1m[32m0.23525[0m[0m | time: 10.054s
[2K
| RMSProp | epoch: 013 | loss: 0.23525 - acc: 0.9110 -- iter: 0320/2226
[A[ATraining Step: 851  | total loss: [1m[32m0.24691[0m[0m | time: 10.610s
[2K
| RMSProp | epoch: 013 | loss: 0.24691 - acc: 0.9011 -- iter: 0352/2226
[A[ATraining Step: 852  | total loss: [1m[32m0.25376[0m[0m | time: 11.124s
[2K
| RMSProp | epoch: 013 | loss: 0.25376 - acc: 0.8943 -- iter: 0384/2226
[A[ATraining Step: 853  | total loss: [1m[32m0.24551[0m[0m | time: 12.068s
[2K
| RMSProp | epoch: 013 | loss: 0.24551 - acc: 0.8993 -- iter: 0416/2226
[A[ATraining Step: 854  | total loss: [1m[32m0.24402[0m[0m | time: 13.001s
[2K
| RMSProp | epoch: 013 | loss: 0.24402 - acc: 0.9000 -- iter: 0448/2226
[A[ATraining Step: 855  | total loss: [1m[32m0.24419[0m[0m | time: 14.057s
[2K
| RMSProp | epoch: 013 | loss: 0.24419 - acc: 0.9007 -- iter: 0480/2226
[A[ATraining Step: 856  | total loss: [1m[32m0.24748[0m[0m | time: 15.071s
[2K
| RMSProp | epoch: 013 | loss: 0.24748 - acc: 0.9012 -- iter: 0512/2226
[A[ATraining Step: 857  | total loss: [1m[32m0.25206[0m[0m | time: 16.169s
[2K
| RMSProp | epoch: 013 | loss: 0.25206 - acc: 0.9017 -- iter: 0544/2226
[A[ATraining Step: 858  | total loss: [1m[32m0.23711[0m[0m | time: 17.199s
[2K
| RMSProp | epoch: 013 | loss: 0.23711 - acc: 0.9115 -- iter: 0576/2226
[A[ATraining Step: 859  | total loss: [1m[32m0.22191[0m[0m | time: 18.187s
[2K
| RMSProp | epoch: 013 | loss: 0.22191 - acc: 0.9204 -- iter: 0608/2226
[A[ATraining Step: 860  | total loss: [1m[32m0.21198[0m[0m | time: 19.297s
[2K
| RMSProp | epoch: 013 | loss: 0.21198 - acc: 0.9221 -- iter: 0640/2226
[A[ATraining Step: 861  | total loss: [1m[32m0.22988[0m[0m | time: 20.354s
[2K
| RMSProp | epoch: 013 | loss: 0.22988 - acc: 0.9080 -- iter: 0672/2226
[A[ATraining Step: 862  | total loss: [1m[32m0.25655[0m[0m | time: 21.237s
[2K
| RMSProp | epoch: 013 | loss: 0.25655 - acc: 0.8985 -- iter: 0704/2226
[A[ATraining Step: 863  | total loss: [1m[32m0.26544[0m[0m | time: 22.155s
[2K
| RMSProp | epoch: 013 | loss: 0.26544 - acc: 0.8961 -- iter: 0736/2226
[A[ATraining Step: 864  | total loss: [1m[32m0.25730[0m[0m | time: 23.103s
[2K
| RMSProp | epoch: 013 | loss: 0.25730 - acc: 0.9034 -- iter: 0768/2226
[A[ATraining Step: 865  | total loss: [1m[32m0.23889[0m[0m | time: 24.141s
[2K
| RMSProp | epoch: 013 | loss: 0.23889 - acc: 0.9130 -- iter: 0800/2226
[A[ATraining Step: 866  | total loss: [1m[32m0.22121[0m[0m | time: 25.042s
[2K
| RMSProp | epoch: 013 | loss: 0.22121 - acc: 0.9217 -- iter: 0832/2226
[A[ATraining Step: 867  | total loss: [1m[32m0.22465[0m[0m | time: 26.006s
[2K
| RMSProp | epoch: 013 | loss: 0.22465 - acc: 0.9202 -- iter: 0864/2226
[A[ATraining Step: 868  | total loss: [1m[32m0.23927[0m[0m | time: 27.040s
[2K
| RMSProp | epoch: 013 | loss: 0.23927 - acc: 0.9125 -- iter: 0896/2226
[A[ATraining Step: 869  | total loss: [1m[32m0.24164[0m[0m | time: 28.067s
[2K
| RMSProp | epoch: 013 | loss: 0.24164 - acc: 0.9057 -- iter: 0928/2226
[A[ATraining Step: 870  | total loss: [1m[32m0.24303[0m[0m | time: 29.063s
[2K
| RMSProp | epoch: 013 | loss: 0.24303 - acc: 0.8995 -- iter: 0960/2226
[A[ATraining Step: 871  | total loss: [1m[32m0.28474[0m[0m | time: 30.151s
[2K
| RMSProp | epoch: 013 | loss: 0.28474 - acc: 0.8845 -- iter: 0992/2226
[A[ATraining Step: 872  | total loss: [1m[32m0.27752[0m[0m | time: 31.167s
[2K
| RMSProp | epoch: 013 | loss: 0.27752 - acc: 0.8898 -- iter: 1024/2226
[A[ATraining Step: 873  | total loss: [1m[32m0.27346[0m[0m | time: 32.101s
[2K
| RMSProp | epoch: 013 | loss: 0.27346 - acc: 0.8915 -- iter: 1056/2226
[A[ATraining Step: 874  | total loss: [1m[32m0.26445[0m[0m | time: 32.957s
[2K
| RMSProp | epoch: 013 | loss: 0.26445 - acc: 0.8929 -- iter: 1088/2226
[A[ATraining Step: 875  | total loss: [1m[32m0.24636[0m[0m | time: 33.953s
[2K
| RMSProp | epoch: 013 | loss: 0.24636 - acc: 0.9037 -- iter: 1120/2226
[A[ATraining Step: 876  | total loss: [1m[32m0.24313[0m[0m | time: 34.992s
[2K
| RMSProp | epoch: 013 | loss: 0.24313 - acc: 0.9039 -- iter: 1152/2226
[A[ATraining Step: 877  | total loss: [1m[32m0.23203[0m[0m | time: 35.982s
[2K
| RMSProp | epoch: 013 | loss: 0.23203 - acc: 0.9104 -- iter: 1184/2226
[A[ATraining Step: 878  | total loss: [1m[32m0.21497[0m[0m | time: 37.052s
[2K
| RMSProp | epoch: 013 | loss: 0.21497 - acc: 0.9194 -- iter: 1216/2226
[A[ATraining Step: 879  | total loss: [1m[32m0.21353[0m[0m | time: 38.105s
[2K
| RMSProp | epoch: 013 | loss: 0.21353 - acc: 0.9212 -- iter: 1248/2226
[A[ATraining Step: 880  | total loss: [1m[32m0.20451[0m[0m | time: 39.123s
[2K
| RMSProp | epoch: 013 | loss: 0.20451 - acc: 0.9228 -- iter: 1280/2226
[A[ATraining Step: 881  | total loss: [1m[32m0.21480[0m[0m | time: 40.082s
[2K
| RMSProp | epoch: 013 | loss: 0.21480 - acc: 0.9149 -- iter: 1312/2226
[A[ATraining Step: 882  | total loss: [1m[32m0.22863[0m[0m | time: 40.984s
[2K
| RMSProp | epoch: 013 | loss: 0.22863 - acc: 0.9109 -- iter: 1344/2226
[A[ATraining Step: 883  | total loss: [1m[32m0.22194[0m[0m | time: 41.814s
[2K
| RMSProp | epoch: 013 | loss: 0.22194 - acc: 0.9136 -- iter: 1376/2226
[A[ATraining Step: 884  | total loss: [1m[32m0.22507[0m[0m | time: 42.808s
[2K
| RMSProp | epoch: 013 | loss: 0.22507 - acc: 0.9160 -- iter: 1408/2226
[A[ATraining Step: 885  | total loss: [1m[32m0.22493[0m[0m | time: 43.727s
[2K
| RMSProp | epoch: 013 | loss: 0.22493 - acc: 0.9150 -- iter: 1440/2226
[A[ATraining Step: 886  | total loss: [1m[32m0.22649[0m[0m | time: 44.693s
[2K
| RMSProp | epoch: 013 | loss: 0.22649 - acc: 0.9172 -- iter: 1472/2226
[A[ATraining Step: 887  | total loss: [1m[32m0.22316[0m[0m | time: 45.678s
[2K
| RMSProp | epoch: 013 | loss: 0.22316 - acc: 0.9161 -- iter: 1504/2226
[A[ATraining Step: 888  | total loss: [1m[32m0.22535[0m[0m | time: 46.634s
[2K
| RMSProp | epoch: 013 | loss: 0.22535 - acc: 0.9120 -- iter: 1536/2226
[A[ATraining Step: 889  | total loss: [1m[32m0.22974[0m[0m | time: 47.540s
[2K
| RMSProp | epoch: 013 | loss: 0.22974 - acc: 0.9052 -- iter: 1568/2226
[A[ATraining Step: 890  | total loss: [1m[32m0.23983[0m[0m | time: 48.499s
[2K
| RMSProp | epoch: 013 | loss: 0.23983 - acc: 0.9022 -- iter: 1600/2226
[A[ATraining Step: 891  | total loss: [1m[32m0.24602[0m[0m | time: 49.416s
[2K
| RMSProp | epoch: 013 | loss: 0.24602 - acc: 0.9057 -- iter: 1632/2226
[A[ATraining Step: 892  | total loss: [1m[32m0.25196[0m[0m | time: 50.575s
[2K
| RMSProp | epoch: 013 | loss: 0.25196 - acc: 0.8995 -- iter: 1664/2226
[A[ATraining Step: 893  | total loss: [1m[32m0.25176[0m[0m | time: 51.653s
[2K
| RMSProp | epoch: 013 | loss: 0.25176 - acc: 0.9002 -- iter: 1696/2226
[A[ATraining Step: 894  | total loss: [1m[32m0.23918[0m[0m | time: 52.475s
[2K
| RMSProp | epoch: 013 | loss: 0.23918 - acc: 0.9070 -- iter: 1728/2226
[A[ATraining Step: 895  | total loss: [1m[32m0.23171[0m[0m | time: 53.346s
[2K
| RMSProp | epoch: 013 | loss: 0.23171 - acc: 0.9007 -- iter: 1760/2226
[A[ATraining Step: 896  | total loss: [1m[32m0.22325[0m[0m | time: 54.290s
[2K
| RMSProp | epoch: 013 | loss: 0.22325 - acc: 0.9075 -- iter: 1792/2226
[A[ATraining Step: 897  | total loss: [1m[32m0.23297[0m[0m | time: 55.200s
[2K
| RMSProp | epoch: 013 | loss: 0.23297 - acc: 0.9011 -- iter: 1824/2226
[A[ATraining Step: 898  | total loss: [1m[32m0.25633[0m[0m | time: 56.231s
[2K
| RMSProp | epoch: 013 | loss: 0.25633 - acc: 0.8923 -- iter: 1856/2226
[A[ATraining Step: 899  | total loss: [1m[32m0.24532[0m[0m | time: 57.223s
[2K
| RMSProp | epoch: 013 | loss: 0.24532 - acc: 0.8968 -- iter: 1888/2226
[A[ATraining Step: 900  | total loss: [1m[32m0.24528[0m[0m | time: 58.269s
[2K
| RMSProp | epoch: 013 | loss: 0.24528 - acc: 0.8977 -- iter: 1920/2226
[A[ATraining Step: 901  | total loss: [1m[32m0.28496[0m[0m | time: 59.229s
[2K
| RMSProp | epoch: 013 | loss: 0.28496 - acc: 0.8798 -- iter: 1952/2226
[A[ATraining Step: 902  | total loss: [1m[32m0.27159[0m[0m | time: 60.232s
[2K
| RMSProp | epoch: 013 | loss: 0.27159 - acc: 0.8856 -- iter: 1984/2226
[A[ATraining Step: 903  | total loss: [1m[32m0.26391[0m[0m | time: 61.379s
[2K
| RMSProp | epoch: 013 | loss: 0.26391 - acc: 0.8877 -- iter: 2016/2226
[A[ATraining Step: 904  | total loss: [1m[32m0.26703[0m[0m | time: 62.458s
[2K
| RMSProp | epoch: 013 | loss: 0.26703 - acc: 0.8833 -- iter: 2048/2226
[A[ATraining Step: 905  | total loss: [1m[32m0.25040[0m[0m | time: 63.330s
[2K
| RMSProp | epoch: 013 | loss: 0.25040 - acc: 0.8918 -- iter: 2080/2226
[A[ATraining Step: 906  | total loss: [1m[32m0.24087[0m[0m | time: 64.319s
[2K
| RMSProp | epoch: 013 | loss: 0.24087 - acc: 0.8995 -- iter: 2112/2226
[A[ATraining Step: 907  | total loss: [1m[32m0.22426[0m[0m | time: 65.391s
[2K
| RMSProp | epoch: 013 | loss: 0.22426 - acc: 0.9096 -- iter: 2144/2226
[A[ATraining Step: 908  | total loss: [1m[32m0.20865[0m[0m | time: 66.427s
[2K
| RMSProp | epoch: 013 | loss: 0.20865 - acc: 0.9186 -- iter: 2176/2226
[A[ATraining Step: 909  | total loss: [1m[32m0.19602[0m[0m | time: 67.437s
[2K
| RMSProp | epoch: 013 | loss: 0.19602 - acc: 0.9236 -- iter: 2208/2226
[A[ATraining Step: 910  | total loss: [1m[32m0.20353[0m[0m | time: 72.585s
[2K
| RMSProp | epoch: 013 | loss: 0.20353 - acc: 0.9156 | val_loss: 0.27599 - val_acc: 0.8980 -- iter: 2226/2226
--
Training Step: 911  | total loss: [1m[32m0.22544[0m[0m | time: 1.126s
[2K
| RMSProp | epoch: 014 | loss: 0.22544 - acc: 0.9147 -- iter: 0032/2226
[A[ATraining Step: 912  | total loss: [1m[32m0.21694[0m[0m | time: 2.032s
[2K
| RMSProp | epoch: 014 | loss: 0.21694 - acc: 0.9201 -- iter: 0064/2226
[A[ATraining Step: 913  | total loss: [1m[32m0.21354[0m[0m | time: 2.735s
[2K
| RMSProp | epoch: 014 | loss: 0.21354 - acc: 0.9218 -- iter: 0096/2226
[A[ATraining Step: 914  | total loss: [1m[32m0.19771[0m[0m | time: 3.466s
[2K
| RMSProp | epoch: 014 | loss: 0.19771 - acc: 0.9297 -- iter: 0128/2226
[A[ATraining Step: 915  | total loss: [1m[32m0.18555[0m[0m | time: 4.172s
[2K
| RMSProp | epoch: 014 | loss: 0.18555 - acc: 0.9336 -- iter: 0160/2226
[A[ATraining Step: 916  | total loss: [1m[32m0.17070[0m[0m | time: 4.879s
[2K
| RMSProp | epoch: 014 | loss: 0.17070 - acc: 0.9402 -- iter: 0192/2226
[A[ATraining Step: 917  | total loss: [1m[32m0.15969[0m[0m | time: 5.574s
[2K
| RMSProp | epoch: 014 | loss: 0.15969 - acc: 0.9431 -- iter: 0224/2226
[A[ATraining Step: 918  | total loss: [1m[32m0.15245[0m[0m | time: 6.299s
[2K
| RMSProp | epoch: 014 | loss: 0.15245 - acc: 0.9456 -- iter: 0256/2226
[A[ATraining Step: 919  | total loss: [1m[32m0.14648[0m[0m | time: 6.989s
[2K
| RMSProp | epoch: 014 | loss: 0.14648 - acc: 0.9479 -- iter: 0288/2226
[A[ATraining Step: 920  | total loss: [1m[32m0.25551[0m[0m | time: 7.676s
[2K
| RMSProp | epoch: 014 | loss: 0.25551 - acc: 0.9188 -- iter: 0320/2226
[A[ATraining Step: 921  | total loss: [1m[32m0.24833[0m[0m | time: 8.383s
[2K
| RMSProp | epoch: 014 | loss: 0.24833 - acc: 0.9238 -- iter: 0352/2226
[A[ATraining Step: 922  | total loss: [1m[32m0.24690[0m[0m | time: 8.806s
[2K
| RMSProp | epoch: 014 | loss: 0.24690 - acc: 0.9189 -- iter: 0384/2226
[A[ATraining Step: 923  | total loss: [1m[32m0.24007[0m[0m | time: 9.235s
[2K
| RMSProp | epoch: 014 | loss: 0.24007 - acc: 0.9215 -- iter: 0416/2226
[A[ATraining Step: 924  | total loss: [1m[32m0.22291[0m[0m | time: 9.955s
[2K
| RMSProp | epoch: 014 | loss: 0.22291 - acc: 0.9293 -- iter: 0448/2226
[A[ATraining Step: 925  | total loss: [1m[32m0.24155[0m[0m | time: 10.650s
[2K
| RMSProp | epoch: 014 | loss: 0.24155 - acc: 0.9208 -- iter: 0480/2226
[A[ATraining Step: 926  | total loss: [1m[32m0.23371[0m[0m | time: 11.353s
[2K
| RMSProp | epoch: 014 | loss: 0.23371 - acc: 0.9256 -- iter: 0512/2226
[A[ATraining Step: 927  | total loss: [1m[32m0.22609[0m[0m | time: 12.064s
[2K
| RMSProp | epoch: 014 | loss: 0.22609 - acc: 0.9267 -- iter: 0544/2226
[A[ATraining Step: 928  | total loss: [1m[32m0.23517[0m[0m | time: 12.768s
[2K
| RMSProp | epoch: 014 | loss: 0.23517 - acc: 0.9216 -- iter: 0576/2226
[A[ATraining Step: 929  | total loss: [1m[32m0.23163[0m[0m | time: 13.460s
[2K
| RMSProp | epoch: 014 | loss: 0.23163 - acc: 0.9232 -- iter: 0608/2226
[A[ATraining Step: 930  | total loss: [1m[32m0.22995[0m[0m | time: 14.182s
[2K
| RMSProp | epoch: 014 | loss: 0.22995 - acc: 0.9215 -- iter: 0640/2226
[A[ATraining Step: 931  | total loss: [1m[32m0.23080[0m[0m | time: 14.886s
[2K
| RMSProp | epoch: 014 | loss: 0.23080 - acc: 0.9231 -- iter: 0672/2226
[A[ATraining Step: 932  | total loss: [1m[32m0.22513[0m[0m | time: 15.578s
[2K
| RMSProp | epoch: 014 | loss: 0.22513 - acc: 0.9276 -- iter: 0704/2226
[A[ATraining Step: 933  | total loss: [1m[32m0.20949[0m[0m | time: 16.288s
[2K
| RMSProp | epoch: 014 | loss: 0.20949 - acc: 0.9349 -- iter: 0736/2226
[A[ATraining Step: 934  | total loss: [1m[32m0.20680[0m[0m | time: 16.991s
[2K
| RMSProp | epoch: 014 | loss: 0.20680 - acc: 0.9289 -- iter: 0768/2226
[A[ATraining Step: 935  | total loss: [1m[32m0.20225[0m[0m | time: 17.765s
[2K
| RMSProp | epoch: 014 | loss: 0.20225 - acc: 0.9298 -- iter: 0800/2226
[A[ATraining Step: 936  | total loss: [1m[32m0.18675[0m[0m | time: 18.834s
[2K
| RMSProp | epoch: 014 | loss: 0.18675 - acc: 0.9368 -- iter: 0832/2226
[A[ATraining Step: 937  | total loss: [1m[32m0.19741[0m[0m | time: 19.947s
[2K
| RMSProp | epoch: 014 | loss: 0.19741 - acc: 0.9275 -- iter: 0864/2226
[A[ATraining Step: 938  | total loss: [1m[32m0.21715[0m[0m | time: 20.799s
[2K
| RMSProp | epoch: 014 | loss: 0.21715 - acc: 0.9129 -- iter: 0896/2226
[A[ATraining Step: 939  | total loss: [1m[32m0.19868[0m[0m | time: 21.782s
[2K
| RMSProp | epoch: 014 | loss: 0.19868 - acc: 0.9216 -- iter: 0928/2226
[A[ATraining Step: 940  | total loss: [1m[32m0.18195[0m[0m | time: 22.806s
[2K
| RMSProp | epoch: 014 | loss: 0.18195 - acc: 0.9294 -- iter: 0960/2226
[A[ATraining Step: 941  | total loss: [1m[32m0.19795[0m[0m | time: 23.763s
[2K
| RMSProp | epoch: 014 | loss: 0.19795 - acc: 0.9240 -- iter: 0992/2226
[A[ATraining Step: 942  | total loss: [1m[32m0.21385[0m[0m | time: 24.753s
[2K
| RMSProp | epoch: 014 | loss: 0.21385 - acc: 0.9191 -- iter: 1024/2226
[A[ATraining Step: 943  | total loss: [1m[32m0.19835[0m[0m | time: 25.773s
[2K
| RMSProp | epoch: 014 | loss: 0.19835 - acc: 0.9272 -- iter: 1056/2226
[A[ATraining Step: 944  | total loss: [1m[32m0.18868[0m[0m | time: 26.833s
[2K
| RMSProp | epoch: 014 | loss: 0.18868 - acc: 0.9282 -- iter: 1088/2226
[A[ATraining Step: 945  | total loss: [1m[32m0.18676[0m[0m | time: 27.786s
[2K
| RMSProp | epoch: 014 | loss: 0.18676 - acc: 0.9291 -- iter: 1120/2226
[A[ATraining Step: 946  | total loss: [1m[32m0.18156[0m[0m | time: 28.854s
[2K
| RMSProp | epoch: 014 | loss: 0.18156 - acc: 0.9331 -- iter: 1152/2226
[A[ATraining Step: 947  | total loss: [1m[32m0.19151[0m[0m | time: 29.954s
[2K
| RMSProp | epoch: 014 | loss: 0.19151 - acc: 0.9273 -- iter: 1184/2226
[A[ATraining Step: 948  | total loss: [1m[32m0.19470[0m[0m | time: 30.893s
[2K
| RMSProp | epoch: 014 | loss: 0.19470 - acc: 0.9283 -- iter: 1216/2226
[A[ATraining Step: 949  | total loss: [1m[32m0.20453[0m[0m | time: 31.767s
[2K
| RMSProp | epoch: 014 | loss: 0.20453 - acc: 0.9292 -- iter: 1248/2226
[A[ATraining Step: 950  | total loss: [1m[32m0.21332[0m[0m | time: 32.753s
[2K
| RMSProp | epoch: 014 | loss: 0.21332 - acc: 0.9269 -- iter: 1280/2226
[A[ATraining Step: 951  | total loss: [1m[32m0.19712[0m[0m | time: 33.735s
[2K
| RMSProp | epoch: 014 | loss: 0.19712 - acc: 0.9342 -- iter: 1312/2226
[A[ATraining Step: 952  | total loss: [1m[32m0.18430[0m[0m | time: 34.706s
[2K
| RMSProp | epoch: 014 | loss: 0.18430 - acc: 0.9408 -- iter: 1344/2226
[A[ATraining Step: 953  | total loss: [1m[32m0.17022[0m[0m | time: 35.720s
[2K
| RMSProp | epoch: 014 | loss: 0.17022 - acc: 0.9467 -- iter: 1376/2226
[A[ATraining Step: 954  | total loss: [1m[32m0.16389[0m[0m | time: 36.752s
[2K
| RMSProp | epoch: 014 | loss: 0.16389 - acc: 0.9489 -- iter: 1408/2226
[A[ATraining Step: 955  | total loss: [1m[32m0.17724[0m[0m | time: 37.754s
[2K
| RMSProp | epoch: 014 | loss: 0.17724 - acc: 0.9415 -- iter: 1440/2226
[A[ATraining Step: 956  | total loss: [1m[32m0.17628[0m[0m | time: 38.659s
[2K
| RMSProp | epoch: 014 | loss: 0.17628 - acc: 0.9411 -- iter: 1472/2226
[A[ATraining Step: 957  | total loss: [1m[32m0.17629[0m[0m | time: 39.771s
[2K
| RMSProp | epoch: 014 | loss: 0.17629 - acc: 0.9376 -- iter: 1504/2226
[A[ATraining Step: 958  | total loss: [1m[32m0.16395[0m[0m | time: 40.856s
[2K
| RMSProp | epoch: 014 | loss: 0.16395 - acc: 0.9439 -- iter: 1536/2226
[A[ATraining Step: 959  | total loss: [1m[32m0.17746[0m[0m | time: 41.696s
[2K
| RMSProp | epoch: 014 | loss: 0.17746 - acc: 0.9370 -- iter: 1568/2226
[A[ATraining Step: 960  | total loss: [1m[32m0.16803[0m[0m | time: 42.704s
[2K
| RMSProp | epoch: 014 | loss: 0.16803 - acc: 0.9433 -- iter: 1600/2226
[A[ATraining Step: 961  | total loss: [1m[32m0.19104[0m[0m | time: 43.735s
[2K
| RMSProp | epoch: 014 | loss: 0.19104 - acc: 0.9333 -- iter: 1632/2226
[A[ATraining Step: 962  | total loss: [1m[32m0.21971[0m[0m | time: 44.668s
[2K
| RMSProp | epoch: 014 | loss: 0.21971 - acc: 0.9244 -- iter: 1664/2226
[A[ATraining Step: 963  | total loss: [1m[32m0.22059[0m[0m | time: 45.629s
[2K
| RMSProp | epoch: 014 | loss: 0.22059 - acc: 0.9226 -- iter: 1696/2226
[A[ATraining Step: 964  | total loss: [1m[32m0.24214[0m[0m | time: 46.630s
[2K
| RMSProp | epoch: 014 | loss: 0.24214 - acc: 0.9147 -- iter: 1728/2226
[A[ATraining Step: 965  | total loss: [1m[32m0.23647[0m[0m | time: 47.663s
[2K
| RMSProp | epoch: 014 | loss: 0.23647 - acc: 0.9138 -- iter: 1760/2226
[A[ATraining Step: 966  | total loss: [1m[32m0.22189[0m[0m | time: 48.594s
[2K
| RMSProp | epoch: 014 | loss: 0.22189 - acc: 0.9225 -- iter: 1792/2226
[A[ATraining Step: 967  | total loss: [1m[32m0.20742[0m[0m | time: 49.527s
[2K
| RMSProp | epoch: 014 | loss: 0.20742 - acc: 0.9271 -- iter: 1824/2226
[A[ATraining Step: 968  | total loss: [1m[32m0.19957[0m[0m | time: 50.640s
[2K
| RMSProp | epoch: 014 | loss: 0.19957 - acc: 0.9313 -- iter: 1856/2226
[A[ATraining Step: 969  | total loss: [1m[32m0.19383[0m[0m | time: 51.720s
[2K
| RMSProp | epoch: 014 | loss: 0.19383 - acc: 0.9319 -- iter: 1888/2226
[A[ATraining Step: 970  | total loss: [1m[32m0.19681[0m[0m | time: 52.542s
[2K
| RMSProp | epoch: 014 | loss: 0.19681 - acc: 0.9293 -- iter: 1920/2226
[A[ATraining Step: 971  | total loss: [1m[32m0.20589[0m[0m | time: 53.573s
[2K
| RMSProp | epoch: 014 | loss: 0.20589 - acc: 0.9239 -- iter: 1952/2226
[A[ATraining Step: 972  | total loss: [1m[32m0.20568[0m[0m | time: 54.569s
[2K
| RMSProp | epoch: 014 | loss: 0.20568 - acc: 0.9252 -- iter: 1984/2226
[A[ATraining Step: 973  | total loss: [1m[32m0.20435[0m[0m | time: 55.584s
[2K
| RMSProp | epoch: 014 | loss: 0.20435 - acc: 0.9265 -- iter: 2016/2226
[A[ATraining Step: 974  | total loss: [1m[32m0.18977[0m[0m | time: 56.617s
[2K
| RMSProp | epoch: 014 | loss: 0.18977 - acc: 0.9338 -- iter: 2048/2226
[A[ATraining Step: 975  | total loss: [1m[32m0.17530[0m[0m | time: 57.655s
[2K
| RMSProp | epoch: 014 | loss: 0.17530 - acc: 0.9404 -- iter: 2080/2226
[A[ATraining Step: 976  | total loss: [1m[32m0.17503[0m[0m | time: 58.678s
[2K
| RMSProp | epoch: 014 | loss: 0.17503 - acc: 0.9370 -- iter: 2112/2226
[A[ATraining Step: 977  | total loss: [1m[32m0.16172[0m[0m | time: 59.640s
[2K
| RMSProp | epoch: 014 | loss: 0.16172 - acc: 0.9433 -- iter: 2144/2226
[A[ATraining Step: 978  | total loss: [1m[32m0.14940[0m[0m | time: 60.700s
[2K
| RMSProp | epoch: 014 | loss: 0.14940 - acc: 0.9490 -- iter: 2176/2226
[A[ATraining Step: 979  | total loss: [1m[32m0.14414[0m[0m | time: 61.753s
[2K
| RMSProp | epoch: 014 | loss: 0.14414 - acc: 0.9510 -- iter: 2208/2226
[A[ATraining Step: 980  | total loss: [1m[32m0.14569[0m[0m | time: 66.701s
[2K
| RMSProp | epoch: 014 | loss: 0.14569 - acc: 0.9527 | val_loss: 0.29658 - val_acc: 0.8980 -- iter: 2226/2226
--
Training Step: 981  | total loss: [1m[32m0.13365[0m[0m | time: 1.008s
[2K
| RMSProp | epoch: 015 | loss: 0.13365 - acc: 0.9575 -- iter: 0032/2226
[A[ATraining Step: 982  | total loss: [1m[32m0.12700[0m[0m | time: 2.010s
[2K
| RMSProp | epoch: 015 | loss: 0.12700 - acc: 0.9586 -- iter: 0064/2226
[A[ATraining Step: 983  | total loss: [1m[32m0.15709[0m[0m | time: 3.007s
[2K
| RMSProp | epoch: 015 | loss: 0.15709 - acc: 0.9502 -- iter: 0096/2226
[A[ATraining Step: 984  | total loss: [1m[32m0.16932[0m[0m | time: 3.869s
[2K
| RMSProp | epoch: 015 | loss: 0.16932 - acc: 0.9458 -- iter: 0128/2226
[A[ATraining Step: 985  | total loss: [1m[32m0.16507[0m[0m | time: 4.783s
[2K
| RMSProp | epoch: 015 | loss: 0.16507 - acc: 0.9481 -- iter: 0160/2226
[A[ATraining Step: 986  | total loss: [1m[32m0.15686[0m[0m | time: 5.734s
[2K
| RMSProp | epoch: 015 | loss: 0.15686 - acc: 0.9502 -- iter: 0192/2226
[A[ATraining Step: 987  | total loss: [1m[32m0.14693[0m[0m | time: 6.598s
[2K
| RMSProp | epoch: 015 | loss: 0.14693 - acc: 0.9552 -- iter: 0224/2226
[A[ATraining Step: 988  | total loss: [1m[32m0.13707[0m[0m | time: 7.478s
[2K
| RMSProp | epoch: 015 | loss: 0.13707 - acc: 0.9597 -- iter: 0256/2226
[A[ATraining Step: 989  | total loss: [1m[32m0.13092[0m[0m | time: 8.503s
[2K
| RMSProp | epoch: 015 | loss: 0.13092 - acc: 0.9606 -- iter: 0288/2226
[A[ATraining Step: 990  | total loss: [1m[32m0.17796[0m[0m | time: 9.502s
[2K
| RMSProp | epoch: 015 | loss: 0.17796 - acc: 0.9364 -- iter: 0320/2226
[A[ATraining Step: 991  | total loss: [1m[32m0.19955[0m[0m | time: 10.456s
[2K
| RMSProp | epoch: 015 | loss: 0.19955 - acc: 0.9240 -- iter: 0352/2226
[A[ATraining Step: 992  | total loss: [1m[32m0.20467[0m[0m | time: 11.428s
[2K
| RMSProp | epoch: 015 | loss: 0.20467 - acc: 0.9222 -- iter: 0384/2226
[A[ATraining Step: 993  | total loss: [1m[32m0.22473[0m[0m | time: 11.966s
[2K
| RMSProp | epoch: 015 | loss: 0.22473 - acc: 0.9175 -- iter: 0416/2226
[A[ATraining Step: 994  | total loss: [1m[32m0.21251[0m[0m | time: 12.501s
[2K
| RMSProp | epoch: 015 | loss: 0.21251 - acc: 0.9202 -- iter: 0448/2226
[A[ATraining Step: 995  | total loss: [1m[32m0.19364[0m[0m | time: 13.544s
[2K
| RMSProp | epoch: 015 | loss: 0.19364 - acc: 0.9282 -- iter: 0480/2226
[A[ATraining Step: 996  | total loss: [1m[32m0.19128[0m[0m | time: 14.675s
[2K
| RMSProp | epoch: 015 | loss: 0.19128 - acc: 0.9260 -- iter: 0512/2226
[A[ATraining Step: 997  | total loss: [1m[32m0.22780[0m[0m | time: 15.615s
[2K
| RMSProp | epoch: 015 | loss: 0.22780 - acc: 0.9115 -- iter: 0544/2226
[A[ATraining Step: 998  | total loss: [1m[32m0.20931[0m[0m | time: 16.515s
[2K
| RMSProp | epoch: 015 | loss: 0.20931 - acc: 0.9204 -- iter: 0576/2226
[A[ATraining Step: 999  | total loss: [1m[32m0.20699[0m[0m | time: 17.468s
[2K
| RMSProp | epoch: 015 | loss: 0.20699 - acc: 0.9221 -- iter: 0608/2226
[A[ATraining Step: 1000  | total loss: [1m[32m0.19558[0m[0m | time: 22.902s
[2K
| RMSProp | epoch: 015 | loss: 0.19558 - acc: 0.9267 | val_loss: 0.37000 - val_acc: 0.8506 -- iter: 0640/2226
--
Training Step: 1001  | total loss: [1m[32m0.19657[0m[0m | time: 23.928s
[2K
| RMSProp | epoch: 015 | loss: 0.19657 - acc: 0.9247 -- iter: 0672/2226
[A[ATraining Step: 1002  | total loss: [1m[32m0.20932[0m[0m | time: 25.073s
[2K
| RMSProp | epoch: 015 | loss: 0.20932 - acc: 0.9135 -- iter: 0704/2226
[A[ATraining Step: 1003  | total loss: [1m[32m0.20128[0m[0m | time: 26.195s
[2K
| RMSProp | epoch: 015 | loss: 0.20128 - acc: 0.9159 -- iter: 0736/2226
[A[ATraining Step: 1004  | total loss: [1m[32m0.19025[0m[0m | time: 27.314s
[2K
| RMSProp | epoch: 015 | loss: 0.19025 - acc: 0.9212 -- iter: 0768/2226
[A[ATraining Step: 1005  | total loss: [1m[32m0.19790[0m[0m | time: 28.370s
[2K
| RMSProp | epoch: 015 | loss: 0.19790 - acc: 0.9197 -- iter: 0800/2226
[A[ATraining Step: 1006  | total loss: [1m[32m0.20490[0m[0m | time: 29.120s
[2K
| RMSProp | epoch: 015 | loss: 0.20490 - acc: 0.9183 -- iter: 0832/2226
[A[ATraining Step: 1007  | total loss: [1m[32m0.21193[0m[0m | time: 29.808s
[2K
| RMSProp | epoch: 015 | loss: 0.21193 - acc: 0.9140 -- iter: 0864/2226
[A[ATraining Step: 1008  | total loss: [1m[32m0.20126[0m[0m | time: 30.536s
[2K
| RMSProp | epoch: 015 | loss: 0.20126 - acc: 0.9195 -- iter: 0896/2226
[A[ATraining Step: 1009  | total loss: [1m[32m0.19469[0m[0m | time: 31.235s
[2K
| RMSProp | epoch: 015 | loss: 0.19469 - acc: 0.9213 -- iter: 0928/2226
[A[ATraining Step: 1010  | total loss: [1m[32m0.20842[0m[0m | time: 31.954s
[2K
| RMSProp | epoch: 015 | loss: 0.20842 - acc: 0.9073 -- iter: 0960/2226
[A[ATraining Step: 1011  | total loss: [1m[32m0.20513[0m[0m | time: 32.652s
[2K
| RMSProp | epoch: 015 | loss: 0.20513 - acc: 0.9072 -- iter: 0992/2226
[A[ATraining Step: 1012  | total loss: [1m[32m0.19230[0m[0m | time: 33.345s
[2K
| RMSProp | epoch: 015 | loss: 0.19230 - acc: 0.9165 -- iter: 1024/2226
[A[ATraining Step: 1013  | total loss: [1m[32m0.17674[0m[0m | time: 34.010s
[2K
| RMSProp | epoch: 015 | loss: 0.17674 - acc: 0.9248 -- iter: 1056/2226
[A[ATraining Step: 1014  | total loss: [1m[32m0.16485[0m[0m | time: 34.734s
[2K
| RMSProp | epoch: 015 | loss: 0.16485 - acc: 0.9292 -- iter: 1088/2226
[A[ATraining Step: 1015  | total loss: [1m[32m0.16975[0m[0m | time: 35.466s
[2K
| RMSProp | epoch: 015 | loss: 0.16975 - acc: 0.9300 -- iter: 1120/2226
[A[ATraining Step: 1016  | total loss: [1m[32m0.17481[0m[0m | time: 36.191s
[2K
| RMSProp | epoch: 015 | loss: 0.17481 - acc: 0.9277 -- iter: 1152/2226
[A[ATraining Step: 1017  | total loss: [1m[32m0.16133[0m[0m | time: 36.897s
[2K
| RMSProp | epoch: 015 | loss: 0.16133 - acc: 0.9349 -- iter: 1184/2226
[A[ATraining Step: 1018  | total loss: [1m[32m0.15186[0m[0m | time: 37.619s
[2K
| RMSProp | epoch: 015 | loss: 0.15186 - acc: 0.9383 -- iter: 1216/2226
[A[ATraining Step: 1019  | total loss: [1m[32m0.14384[0m[0m | time: 38.405s
[2K
| RMSProp | epoch: 015 | loss: 0.14384 - acc: 0.9413 -- iter: 1248/2226
[A[ATraining Step: 1020  | total loss: [1m[32m0.13231[0m[0m | time: 39.135s
[2K
| RMSProp | epoch: 015 | loss: 0.13231 - acc: 0.9472 -- iter: 1280/2226
[A[ATraining Step: 1021  | total loss: [1m[32m0.12089[0m[0m | time: 39.845s
[2K
| RMSProp | epoch: 015 | loss: 0.12089 - acc: 0.9525 -- iter: 1312/2226
[A[ATraining Step: 1022  | total loss: [1m[32m0.11059[0m[0m | time: 40.582s
[2K
| RMSProp | epoch: 015 | loss: 0.11059 - acc: 0.9572 -- iter: 1344/2226
[A[ATraining Step: 1023  | total loss: [1m[32m0.10677[0m[0m | time: 41.277s
[2K
| RMSProp | epoch: 015 | loss: 0.10677 - acc: 0.9584 -- iter: 1376/2226
[A[ATraining Step: 1024  | total loss: [1m[32m0.10015[0m[0m | time: 41.963s
[2K
| RMSProp | epoch: 015 | loss: 0.10015 - acc: 0.9625 -- iter: 1408/2226
[A[ATraining Step: 1025  | total loss: [1m[32m0.09994[0m[0m | time: 42.709s
[2K
| RMSProp | epoch: 015 | loss: 0.09994 - acc: 0.9632 -- iter: 1440/2226
[A[ATraining Step: 1026  | total loss: [1m[32m0.10683[0m[0m | time: 43.399s
[2K
| RMSProp | epoch: 015 | loss: 0.10683 - acc: 0.9606 -- iter: 1472/2226
[A[ATraining Step: 1027  | total loss: [1m[32m0.15616[0m[0m | time: 44.108s
[2K
| RMSProp | epoch: 015 | loss: 0.15616 - acc: 0.9395 -- iter: 1504/2226
[A[ATraining Step: 1028  | total loss: [1m[32m0.14633[0m[0m | time: 44.820s
[2K
| RMSProp | epoch: 015 | loss: 0.14633 - acc: 0.9425 -- iter: 1536/2226
[A[ATraining Step: 1029  | total loss: [1m[32m0.13806[0m[0m | time: 45.557s
[2K
| RMSProp | epoch: 015 | loss: 0.13806 - acc: 0.9451 -- iter: 1568/2226
[A[ATraining Step: 1030  | total loss: [1m[32m0.12663[0m[0m | time: 46.396s
[2K
| RMSProp | epoch: 015 | loss: 0.12663 - acc: 0.9506 -- iter: 1600/2226
[A[ATraining Step: 1031  | total loss: [1m[32m0.12332[0m[0m | time: 47.449s
[2K
| RMSProp | epoch: 015 | loss: 0.12332 - acc: 0.9524 -- iter: 1632/2226
[A[ATraining Step: 1032  | total loss: [1m[32m0.11354[0m[0m | time: 48.550s
[2K
| RMSProp | epoch: 015 | loss: 0.11354 - acc: 0.9572 -- iter: 1664/2226
[A[ATraining Step: 1033  | total loss: [1m[32m0.11382[0m[0m | time: 49.391s
[2K
| RMSProp | epoch: 015 | loss: 0.11382 - acc: 0.9583 -- iter: 1696/2226
[A[ATraining Step: 1034  | total loss: [1m[32m0.10383[0m[0m | time: 50.357s
[2K
| RMSProp | epoch: 015 | loss: 0.10383 - acc: 0.9625 -- iter: 1728/2226
[A[ATraining Step: 1035  | total loss: [1m[32m0.09434[0m[0m | time: 51.335s
[2K
| RMSProp | epoch: 015 | loss: 0.09434 - acc: 0.9662 -- iter: 1760/2226
[A[ATraining Step: 1036  | total loss: [1m[32m0.08688[0m[0m | time: 52.338s
[2K
| RMSProp | epoch: 015 | loss: 0.08688 - acc: 0.9696 -- iter: 1792/2226
[A[ATraining Step: 1037  | total loss: [1m[32m0.08196[0m[0m | time: 53.267s
[2K
| RMSProp | epoch: 015 | loss: 0.08196 - acc: 0.9726 -- iter: 1824/2226
[A[ATraining Step: 1038  | total loss: [1m[32m0.08863[0m[0m | time: 54.294s
[2K
| RMSProp | epoch: 015 | loss: 0.08863 - acc: 0.9723 -- iter: 1856/2226
[A[ATraining Step: 1039  | total loss: [1m[32m0.10118[0m[0m | time: 55.298s
[2K
| RMSProp | epoch: 015 | loss: 0.10118 - acc: 0.9657 -- iter: 1888/2226
[A[ATraining Step: 1040  | total loss: [1m[32m0.16145[0m[0m | time: 56.223s
[2K
| RMSProp | epoch: 015 | loss: 0.16145 - acc: 0.9441 -- iter: 1920/2226
[A[ATraining Step: 1041  | total loss: [1m[32m0.16279[0m[0m | time: 57.218s
[2K
| RMSProp | epoch: 015 | loss: 0.16279 - acc: 0.9466 -- iter: 1952/2226
[A[ATraining Step: 1042  | total loss: [1m[32m0.16138[0m[0m | time: 58.332s
[2K
| RMSProp | epoch: 015 | loss: 0.16138 - acc: 0.9425 -- iter: 1984/2226
[A[ATraining Step: 1043  | total loss: [1m[32m0.14779[0m[0m | time: 59.327s
[2K
| RMSProp | epoch: 015 | loss: 0.14779 - acc: 0.9483 -- iter: 2016/2226
[A[ATraining Step: 1044  | total loss: [1m[32m0.14407[0m[0m | time: 60.201s
[2K
| RMSProp | epoch: 015 | loss: 0.14407 - acc: 0.9472 -- iter: 2048/2226
[A[ATraining Step: 1045  | total loss: [1m[32m0.17866[0m[0m | time: 61.197s
[2K
| RMSProp | epoch: 015 | loss: 0.17866 - acc: 0.9337 -- iter: 2080/2226
[A[ATraining Step: 1046  | total loss: [1m[32m0.17239[0m[0m | time: 62.210s
[2K
| RMSProp | epoch: 015 | loss: 0.17239 - acc: 0.9372 -- iter: 2112/2226
[A[ATraining Step: 1047  | total loss: [1m[32m0.16532[0m[0m | time: 63.183s
[2K
| RMSProp | epoch: 015 | loss: 0.16532 - acc: 0.9404 -- iter: 2144/2226
[A[ATraining Step: 1048  | total loss: [1m[32m0.16077[0m[0m | time: 64.212s
[2K
| RMSProp | epoch: 015 | loss: 0.16077 - acc: 0.9432 -- iter: 2176/2226
[A[ATraining Step: 1049  | total loss: [1m[32m0.15001[0m[0m | time: 65.209s
[2K
| RMSProp | epoch: 015 | loss: 0.15001 - acc: 0.9458 -- iter: 2208/2226
[A[ATraining Step: 1050  | total loss: [1m[32m0.13997[0m[0m | time: 70.345s
[2K
| RMSProp | epoch: 015 | loss: 0.13997 - acc: 0.9481 | val_loss: 0.23948 - val_acc: 0.9195 -- iter: 2226/2226
--
2018-09-06 17:12:26.163426: W tensorflow/core/framework/allocator.cc:101] Allocation of 6303510528 exceeds 10% of system memory.
2018-09-06 17:12:54.059656: W tensorflow/core/framework/allocator.cc:101] Allocation of 6303510528 exceeds 10% of system memory.
Validation AUC:0.9698521093614526
Validation AUPRC:0.9718204318254469
Test AUC:0.9641781411359724
Test AUPRC:0.9610269908183098
BestTestF1Score	0.92	0.85	0.93	0.92	0.92	306	26	338	26	0.69
BestTestMCCScore	0.92	0.85	0.93	0.92	0.92	306	26	338	26	0.69
BestTestAccuracyScore	0.92	0.85	0.93	0.92	0.92	306	26	338	26	0.69
BestValidationF1Score	0.92	0.85	0.92	0.93	0.91	316	22	327	31	0.69
BestValidationMCC	0.92	0.85	0.92	0.93	0.91	316	22	327	31	0.69
BestValidationAccuracy	0.92	0.85	0.92	0.93	0.91	316	22	327	31	0.69
TestPredictions (Threshold:0.69)
CHEMBL274501,TN,INACT,0.009999999776482582	CHEMBL95408,TP,ACT,1.0	CHEMBL606737,TP,ACT,0.9900000095367432	CHEMBL75514,TN,INACT,0.009999999776482582	CHEMBL338045,TN,INACT,0.009999999776482582	CHEMBL267215,TP,ACT,0.9900000095367432	CHEMBL2042551,FP,INACT,0.8999999761581421	CHEMBL2112953,TP,ACT,1.0	CHEMBL2113653,TP,ACT,0.9900000095367432	CHEMBL266632,TN,INACT,0.5299999713897705	CHEMBL2178089,TP,ACT,0.9900000095367432	CHEMBL2113470,TN,INACT,0.0	CHEMBL142822,TN,INACT,0.07000000029802322	CHEMBL135405,TN,INACT,0.009999999776482582	CHEMBL2181144,TN,INACT,0.019999999552965164	CHEMBL319356,TN,INACT,0.019999999552965164	CHEMBL407250,TP,ACT,0.9599999785423279	CHEMBL619,TN,INACT,0.05999999865889549	CHEMBL2425428,TP,ACT,1.0	CHEMBL436039,TP,ACT,0.9599999785423279	CHEMBL344752,TN,INACT,0.009999999776482582	CHEMBL3104109,FN,ACT,0.0	CHEMBL2436814,TN,INACT,0.0	CHEMBL413343,TN,INACT,0.0	CHEMBL335295,TP,ACT,0.9900000095367432	CHEMBL98539,TP,ACT,0.9800000190734863	CHEMBL28057,TN,INACT,0.0	CHEMBL412921,TP,ACT,0.9900000095367432	CHEMBL2442639,TN,INACT,0.0	CHEMBL330984,TP,ACT,1.0	CHEMBL110695,TN,INACT,0.0	CHEMBL412924,TP,ACT,1.0	CHEMBL3770024,TP,ACT,0.9700000286102295	CHEMBL391718,TN,INACT,0.0	CHEMBL12765,TP,ACT,0.9900000095367432	CHEMBL3314885,TN,INACT,0.019999999552965164	CHEMBL608245,TP,ACT,0.9900000095367432	CHEMBL307595,TP,ACT,0.9900000095367432	CHEMBL22321,TP,ACT,0.9900000095367432	CHEMBL458208,TP,ACT,0.949999988079071	CHEMBL2436825,TN,INACT,0.009999999776482582	CHEMBL2113659,TP,ACT,1.0	CHEMBL2373008,TP,ACT,0.949999988079071	CHEMBL1956200,TN,INACT,0.0	CHEMBL610261,TP,ACT,1.0	CHEMBL120371,FP,INACT,0.949999988079071	CHEMBL317536,TP,ACT,0.9900000095367432	CHEMBL2062851,TN,INACT,0.009999999776482582	CHEMBL46031,TP,ACT,0.9900000095367432	CHEMBL3679619,TP,ACT,0.9700000286102295	CHEMBL13146,TP,ACT,1.0	CHEMBL422877,FN,ACT,0.009999999776482582	CHEMBL2112573,TP,ACT,1.0	CHEMBL3038229,TP,ACT,0.9900000095367432	CHEMBL1213992,TP,ACT,1.0	CHEMBL318395,TP,ACT,1.0	CHEMBL549770,TP,ACT,0.9900000095367432	CHEMBL110214,TN,INACT,0.0	CHEMBL3410297,TN,INACT,0.009999999776482582	CHEMBL170317,FN,ACT,0.07999999821186066	CHEMBL344602,TN,INACT,0.10999999940395355	CHEMBL452150,TN,INACT,0.0	CHEMBL75126,TN,INACT,0.0	CHEMBL557749,TP,ACT,0.9900000095367432	CHEMBL593620,TN,INACT,0.009999999776482582	CHEMBL717,TN,INACT,0.30000001192092896	CHEMBL297059,TP,ACT,1.0	CHEMBL421474,TN,INACT,0.009999999776482582	CHEMBL3409740,TP,ACT,0.9900000095367432	CHEMBL72060,TN,INACT,0.009999999776482582	CHEMBL3403333,TN,INACT,0.14000000059604645	CHEMBL2111766,TN,INACT,0.0	CHEMBL2181969,TN,INACT,0.0	CHEMBL302359,TN,INACT,0.009999999776482582	CHEMBL3233193,TP,ACT,0.9900000095367432	CHEMBL2028970,TP,ACT,1.0	CHEMBL2436818,TN,INACT,0.0	CHEMBL51561,TN,INACT,0.0	CHEMBL103433,TN,INACT,0.6600000262260437	CHEMBL355580,TP,ACT,0.8799999952316284	CHEMBL2372207,TP,ACT,0.9900000095367432	CHEMBL296749,TP,ACT,0.7200000286102295	CHEMBL195684,TP,ACT,0.9900000095367432	CHEMBL170935,TP,ACT,0.8700000047683716	CHEMBL610619,TP,ACT,1.0	CHEMBL275987,TN,INACT,0.0	CHEMBL48024,TN,INACT,0.029999999329447746	CHEMBL2113689,TN,INACT,0.019999999552965164	CHEMBL3409751,TP,ACT,1.0	CHEMBL275469,TN,INACT,0.0	CHEMBL2112571,TP,ACT,1.0	CHEMBL66039,TP,ACT,0.9900000095367432	CHEMBL610030,TP,ACT,1.0	CHEMBL3218122,TN,INACT,0.6899999976158142	CHEMBL2322567,TP,ACT,0.8999999761581421	CHEMBL356923,FP,INACT,0.8700000047683716	CHEMBL104981,TN,INACT,0.03999999910593033	CHEMBL223244,TP,ACT,1.0	CHEMBL3627863,TN,INACT,0.029999999329447746	CHEMBL312150,TN,INACT,0.5400000214576721	CHEMBL373668,FN,ACT,0.4699999988079071	CHEMBL280669,TP,ACT,1.0	CHEMBL59,TN,INACT,0.009999999776482582	CHEMBL2331806,TN,INACT,0.0	CHEMBL260563,TN,INACT,0.019999999552965164	CHEMBL125289,TP,ACT,0.9800000190734863	CHEMBL266829,FN,ACT,0.3400000035762787	CHEMBL332645,TN,INACT,0.019999999552965164	CHEMBL2112568,TP,ACT,1.0	CHEMBL1916700,TN,INACT,0.009999999776482582	CHEMBL70246,TN,INACT,0.009999999776482582	CHEMBL2442640,TN,INACT,0.0	CHEMBL610882,TP,ACT,1.0	CHEMBL149256,TP,ACT,1.0	CHEMBL263670,TP,ACT,1.0	CHEMBL34862,TN,INACT,0.0	CHEMBL368133,TN,INACT,0.029999999329447746	CHEMBL2372410,TP,ACT,1.0	CHEMBL13055,TP,ACT,1.0	CHEMBL151803,TN,INACT,0.0	CHEMBL118553,TN,INACT,0.009999999776482582	CHEMBL136560,TN,INACT,0.019999999552965164	CHEMBL40643,FN,ACT,0.14000000059604645	CHEMBL1922023,TN,INACT,0.0	CHEMBL2159122,TP,ACT,0.8700000047683716	CHEMBL290095,TP,ACT,0.8799999952316284	CHEMBL607480,TP,ACT,1.0	CHEMBL172354,TN,INACT,0.0	CHEMBL26061,TN,INACT,0.0	CHEMBL293879,FP,INACT,0.8399999737739563	CHEMBL289157,TP,ACT,0.7300000190734863	CHEMBL2115465,TP,ACT,0.9900000095367432	CHEMBL177546,FP,INACT,0.7799999713897705	CHEMBL461088,TN,INACT,0.0	CHEMBL611433,TP,ACT,0.9900000095367432	CHEMBL2372819,TP,ACT,1.0	CHEMBL575160,TN,INACT,0.009999999776482582	CHEMBL280178,TP,ACT,0.7799999713897705	CHEMBL240279,TN,INACT,0.07000000029802322	CHEMBL102059,TP,ACT,1.0	CHEMBL591679,FN,ACT,0.0	CHEMBL337243,TN,INACT,0.009999999776482582	CHEMBL556230,TP,ACT,1.0	CHEMBL74902,FP,INACT,0.75	CHEMBL76403,TN,INACT,0.05000000074505806	CHEMBL86765,TP,ACT,0.9800000190734863	CHEMBL2163568,TN,INACT,0.0	CHEMBL234543,TN,INACT,0.009999999776482582	CHEMBL425735,TP,ACT,0.9900000095367432	CHEMBL226694,FP,INACT,0.9700000286102295	CHEMBL327745,TP,ACT,0.8700000047683716	CHEMBL312194,TP,ACT,1.0	CHEMBL607483,TP,ACT,1.0	CHEMBL120758,TP,ACT,0.8399999737739563	CHEMBL1223054,TN,INACT,0.0	CHEMBL227378,TN,INACT,0.009999999776482582	CHEMBL440864,TN,INACT,0.009999999776482582	CHEMBL88688,TN,INACT,0.07999999821186066	CHEMBL123785,TN,INACT,0.009999999776482582	CHEMBL373192,TP,ACT,0.9800000190734863	CHEMBL440061,TN,INACT,0.009999999776482582	CHEMBL559518,TP,ACT,0.9599999785423279	CHEMBL578850,FN,ACT,0.10999999940395355	CHEMBL405102,FN,ACT,0.38999998569488525	CHEMBL3823054,TP,ACT,0.9900000095367432	CHEMBL444197,TP,ACT,1.0	CHEMBL3797616,TP,ACT,0.9900000095367432	CHEMBL156230,TP,ACT,0.9100000262260437	CHEMBL100624,TN,INACT,0.0	CHEMBL2436824,TN,INACT,0.0	CHEMBL3087709,FP,INACT,0.699999988079071	CHEMBL3634256,TP,ACT,0.9800000190734863	CHEMBL3038180,TP,ACT,1.0	CHEMBL319964,TP,ACT,0.9900000095367432	CHEMBL583274,TP,ACT,1.0	CHEMBL16496,TN,INACT,0.009999999776482582	CHEMBL95032,TN,INACT,0.009999999776482582	CHEMBL370966,TP,ACT,0.9900000095367432	CHEMBL423666,TN,INACT,0.0	CHEMBL2062854,TN,INACT,0.0	CHEMBL611434,TP,ACT,0.9900000095367432	CHEMBL3327376,TN,INACT,0.10999999940395355	CHEMBL3634253,TP,ACT,0.9800000190734863	CHEMBL3323005,TN,INACT,0.0	CHEMBL208311,TN,INACT,0.019999999552965164	CHEMBL3765059,TP,ACT,1.0	CHEMBL450463,TN,INACT,0.0	CHEMBL163743,TN,INACT,0.07999999821186066	CHEMBL258704,TP,ACT,0.9900000095367432	CHEMBL218215,FN,ACT,0.1899999976158142	CHEMBL3237720,TP,ACT,1.0	CHEMBL64461,TN,INACT,0.0	CHEMBL3233198,TP,ACT,0.9700000286102295	CHEMBL195411,TP,ACT,1.0	CHEMBL2115123,TN,INACT,0.009999999776482582	CHEMBL80807,TN,INACT,0.0	CHEMBL1782798,TN,INACT,0.0	CHEMBL175698,TN,INACT,0.10000000149011612	CHEMBL338462,TP,ACT,1.0	CHEMBL105457,TN,INACT,0.0	CHEMBL241101,TN,INACT,0.009999999776482582	CHEMBL249492,TN,INACT,0.0	CHEMBL455493,TN,INACT,0.009999999776482582	CHEMBL334511,FN,ACT,0.2199999988079071	CHEMBL410090,FN,ACT,0.5899999737739563	CHEMBL3589800,TP,ACT,0.9800000190734863	CHEMBL2371714,TP,ACT,0.7699999809265137	CHEMBL300725,TN,INACT,0.0	CHEMBL75141,TN,INACT,0.05999999865889549	CHEMBL2375157,TP,ACT,0.9399999976158142	CHEMBL3634257,TP,ACT,0.9200000166893005	CHEMBL594376,TN,INACT,0.0	CHEMBL38791,FN,ACT,0.07999999821186066	CHEMBL305313,TN,INACT,0.0	CHEMBL2369870,TP,ACT,0.9800000190734863	CHEMBL593443,TN,INACT,0.0	CHEMBL62066,TN,INACT,0.0	CHEMBL59702,TN,INACT,0.0	CHEMBL143939,TP,ACT,1.0	CHEMBL3634395,TP,ACT,0.9700000286102295	CHEMBL67060,TN,INACT,0.0	CHEMBL336256,TN,INACT,0.0	CHEMBL155748,TP,ACT,0.9900000095367432	CHEMBL163,FP,INACT,0.9900000095367432	CHEMBL431276,TP,ACT,1.0	CHEMBL353806,FN,ACT,0.33000001311302185	CHEMBL2391352,TN,INACT,0.0	CHEMBL339226,TP,ACT,0.949999988079071	CHEMBL115853,TN,INACT,0.0	CHEMBL3408736,TP,ACT,0.949999988079071	CHEMBL1213991,TP,ACT,1.0	CHEMBL105594,TN,INACT,0.0	CHEMBL25976,TN,INACT,0.0	CHEMBL48325,TP,ACT,0.75	CHEMBL3581740,TP,ACT,0.9800000190734863	CHEMBL75002,TN,INACT,0.0	CHEMBL3403728,TN,INACT,0.2199999988079071	CHEMBL396271,TN,INACT,0.20000000298023224	CHEMBL323216,TP,ACT,0.949999988079071	CHEMBL2371240,FP,INACT,1.0	CHEMBL2112576,TP,ACT,1.0	CHEMBL320254,TN,INACT,0.05000000074505806	CHEMBL575027,FP,INACT,0.8700000047683716	CHEMBL277285,TN,INACT,0.019999999552965164	CHEMBL307175,TN,INACT,0.0	CHEMBL3423401,TN,INACT,0.0	CHEMBL316247,TN,INACT,0.009999999776482582	CHEMBL2371935,TP,ACT,1.0	CHEMBL538457,TP,ACT,0.9700000286102295	CHEMBL440961,FP,INACT,0.9800000190734863	CHEMBL373654,TN,INACT,0.019999999552965164	CHEMBL419144,TP,ACT,1.0	CHEMBL208400,TP,ACT,0.9800000190734863	CHEMBL437842,TN,INACT,0.009999999776482582	CHEMBL64120,TN,INACT,0.0	CHEMBL1927443,TN,INACT,0.12999999523162842	CHEMBL611399,TP,ACT,1.0	CHEMBL1672090,TP,ACT,0.9900000095367432	CHEMBL354652,TP,ACT,0.7900000214576721	CHEMBL2237151,TN,INACT,0.17000000178813934	CHEMBL2042403,TN,INACT,0.09000000357627869	CHEMBL164055,TP,ACT,0.9900000095367432	CHEMBL104180,TN,INACT,0.0	CHEMBL71385,TN,INACT,0.019999999552965164	CHEMBL2369626,TP,ACT,0.9800000190734863	CHEMBL27253,TP,ACT,1.0	CHEMBL331007,TN,INACT,0.6100000143051147	CHEMBL241080,TN,INACT,0.2199999988079071	CHEMBL148291,TP,ACT,0.9599999785423279	CHEMBL109248,TN,INACT,0.12999999523162842	CHEMBL335678,TP,ACT,0.9900000095367432	CHEMBL433720,TP,ACT,1.0	CHEMBL438915,FP,INACT,1.0	CHEMBL251541,TN,INACT,0.05999999865889549	CHEMBL51838,TP,ACT,0.9700000286102295	CHEMBL536800,TN,INACT,0.0	CHEMBL342256,TN,INACT,0.4300000071525574	CHEMBL98072,TN,INACT,0.0	CHEMBL73133,TN,INACT,0.009999999776482582	CHEMBL611398,TP,ACT,1.0	CHEMBL355239,TP,ACT,1.0	CHEMBL75773,TN,INACT,0.0	CHEMBL16639,TN,INACT,0.0	CHEMBL78365,TP,ACT,1.0	CHEMBL97883,TP,ACT,0.9800000190734863	CHEMBL157095,TP,ACT,0.9100000262260437	CHEMBL15689,TN,INACT,0.6299999952316284	CHEMBL222830,TP,ACT,1.0	CHEMBL15153,TN,INACT,0.0	CHEMBL458417,TP,ACT,0.9800000190734863	CHEMBL64000,TN,INACT,0.07999999821186066	CHEMBL15100,TP,ACT,0.9900000095367432	CHEMBL2370601,TP,ACT,1.0	CHEMBL3822963,TP,ACT,0.9900000095367432	CHEMBL538514,TP,ACT,0.9900000095367432	CHEMBL505960,TP,ACT,0.9800000190734863	CHEMBL168855,FP,INACT,0.800000011920929	CHEMBL3236677,TP,ACT,1.0	CHEMBL1082036,TN,INACT,0.0	CHEMBL61120,TN,INACT,0.0	CHEMBL281633,TN,INACT,0.0	CHEMBL13616,TP,ACT,1.0	CHEMBL606987,TP,ACT,0.9900000095367432	CHEMBL1672087,TP,ACT,0.9900000095367432	CHEMBL461089,TN,INACT,0.0	CHEMBL118487,TP,ACT,0.9200000166893005	CHEMBL177524,FP,INACT,0.9399999976158142	CHEMBL318682,TP,ACT,0.9399999976158142	CHEMBL21192,TP,ACT,1.0	CHEMBL2112488,TN,INACT,0.019999999552965164	CHEMBL108659,TP,ACT,0.9900000095367432	CHEMBL2370387,TP,ACT,0.9900000095367432	CHEMBL154068,TN,INACT,0.009999999776482582	CHEMBL535602,TN,INACT,0.15000000596046448	CHEMBL149938,TN,INACT,0.029999999329447746	CHEMBL167497,TP,ACT,1.0	CHEMBL62808,TN,INACT,0.0	CHEMBL1094468,TP,ACT,1.0	CHEMBL312551,TN,INACT,0.0	CHEMBL607351,TP,ACT,1.0	CHEMBL195082,TP,ACT,0.9700000286102295	CHEMBL349505,TN,INACT,0.0	CHEMBL67384,FN,ACT,0.0	CHEMBL303278,TP,ACT,0.9599999785423279	CHEMBL1672086,TP,ACT,0.9900000095367432	CHEMBL2153720,TN,INACT,0.05000000074505806	CHEMBL339437,TP,ACT,0.9800000190734863	CHEMBL593685,TN,INACT,0.009999999776482582	CHEMBL3290985,TN,INACT,0.0	CHEMBL74430,TN,INACT,0.27000001072883606	CHEMBL3797228,TP,ACT,0.9800000190734863	CHEMBL1790572,FN,ACT,0.07000000029802322	CHEMBL540360,TN,INACT,0.03999999910593033	CHEMBL430732,TP,ACT,0.949999988079071	CHEMBL221691,TN,INACT,0.0	CHEMBL73740,TN,INACT,0.3799999952316284	CHEMBL78326,TN,INACT,0.10000000149011612	CHEMBL444307,TN,INACT,0.0	CHEMBL1834245,TP,ACT,0.9900000095367432	CHEMBL1275791,TN,INACT,0.019999999552965164	CHEMBL606601,TP,ACT,0.9900000095367432	CHEMBL2062861,TN,INACT,0.009999999776482582	CHEMBL544638,TP,ACT,0.949999988079071	CHEMBL162690,TP,ACT,0.9900000095367432	CHEMBL469856,TN,INACT,0.009999999776482582	CHEMBL18797,TN,INACT,0.07000000029802322	CHEMBL499141,FN,ACT,0.07999999821186066	CHEMBL289006,TP,ACT,0.9700000286102295	CHEMBL92152,TN,INACT,0.0	CHEMBL573688,TP,ACT,0.9800000190734863	CHEMBL43661,TN,INACT,0.0	CHEMBL88584,TN,INACT,0.0	CHEMBL475496,TN,INACT,0.009999999776482582	CHEMBL143592,TP,ACT,0.949999988079071	CHEMBL300926,TN,INACT,0.09000000357627869	CHEMBL2436817,TN,INACT,0.029999999329447746	CHEMBL401143,TP,ACT,0.9300000071525574	CHEMBL2181191,TP,ACT,0.9900000095367432	CHEMBL2372076,FP,INACT,0.7599999904632568	CHEMBL606821,TP,ACT,1.0	CHEMBL2370207,FN,ACT,0.6899999976158142	CHEMBL3350741,FP,INACT,1.0	CHEMBL3604301,TN,INACT,0.09000000357627869	CHEMBL358534,TP,ACT,1.0	CHEMBL471663,TN,INACT,0.03999999910593033	CHEMBL354641,TP,ACT,0.7699999809265137	CHEMBL2323443,TN,INACT,0.019999999552965164	CHEMBL505502,TP,ACT,0.9800000190734863	CHEMBL313752,TP,ACT,1.0	CHEMBL76779,TN,INACT,0.0	CHEMBL251301,TN,INACT,0.009999999776482582	CHEMBL3759167,TP,ACT,0.9900000095367432	CHEMBL145584,TN,INACT,0.0	CHEMBL51473,TP,ACT,1.0	CHEMBL2370671,TP,ACT,1.0	CHEMBL3394007,TN,INACT,0.009999999776482582	CHEMBL1214118,TP,ACT,1.0	CHEMBL423260,TN,INACT,0.0	CHEMBL312958,TN,INACT,0.0	CHEMBL93041,TP,ACT,1.0	CHEMBL346987,TP,ACT,0.9800000190734863	CHEMBL146436,TP,ACT,0.9900000095367432	CHEMBL82194,TN,INACT,0.0	CHEMBL400481,TN,INACT,0.029999999329447746	CHEMBL153466,TP,ACT,0.9700000286102295	CHEMBL2115043,TP,ACT,0.8199999928474426	CHEMBL594802,TN,INACT,0.0	CHEMBL78601,TN,INACT,0.0	CHEMBL147746,TP,ACT,0.9800000190734863	CHEMBL147238,TN,INACT,0.009999999776482582	CHEMBL510201,FP,INACT,0.9300000071525574	CHEMBL1091777,TN,INACT,0.10000000149011612	CHEMBL24781,TN,INACT,0.4099999964237213	CHEMBL3577344,TN,INACT,0.0	CHEMBL375781,TN,INACT,0.0	CHEMBL169183,FN,ACT,0.2199999988079071	CHEMBL44615,TN,INACT,0.09000000357627869	CHEMBL147646,TP,ACT,0.9900000095367432	CHEMBL291394,TN,INACT,0.0	CHEMBL3403332,FP,INACT,0.9599999785423279	CHEMBL268941,TP,ACT,1.0	CHEMBL413040,FP,INACT,0.8399999737739563	CHEMBL1689550,TP,ACT,1.0	CHEMBL228512,TP,ACT,1.0	CHEMBL471806,TP,ACT,1.0	CHEMBL408680,TP,ACT,0.9800000190734863	CHEMBL207743,TP,ACT,0.9900000095367432	CHEMBL107681,TN,INACT,0.0	CHEMBL133836,TP,ACT,0.7799999713897705	CHEMBL279520,TN,INACT,0.009999999776482582	CHEMBL250715,TN,INACT,0.30000001192092896	CHEMBL287321,TN,INACT,0.10000000149011612	CHEMBL73272,TN,INACT,0.009999999776482582	CHEMBL218433,TP,ACT,0.9399999976158142	CHEMBL169178,TN,INACT,0.009999999776482582	CHEMBL123002,TN,INACT,0.009999999776482582	CHEMBL76949,TN,INACT,0.009999999776482582	CHEMBL2369710,TN,INACT,0.4000000059604645	CHEMBL39632,TN,INACT,0.12999999523162842	CHEMBL552849,TP,ACT,0.9300000071525574	CHEMBL302196,TN,INACT,0.0	CHEMBL192443,TP,ACT,0.9700000286102295	CHEMBL352083,TN,INACT,0.10000000149011612	CHEMBL501202,TP,ACT,1.0	CHEMBL485576,TP,ACT,0.9800000190734863	CHEMBL606740,TP,ACT,1.0	CHEMBL45308,TP,ACT,0.9599999785423279	CHEMBL339254,TN,INACT,0.019999999552965164	CHEMBL552615,TN,INACT,0.0	CHEMBL462650,TN,INACT,0.009999999776482582	CHEMBL124764,TN,INACT,0.019999999552965164	CHEMBL328812,TN,INACT,0.029999999329447746	CHEMBL416747,TN,INACT,0.0	CHEMBL169675,TN,INACT,0.0	CHEMBL679,TN,INACT,0.029999999329447746	CHEMBL78080,TN,INACT,0.0	CHEMBL2115042,TP,ACT,0.9800000190734863	CHEMBL3634249,TP,ACT,0.9800000190734863	CHEMBL1782812,TN,INACT,0.0	CHEMBL33720,TN,INACT,0.0	CHEMBL117457,TP,ACT,0.9900000095367432	CHEMBL340537,TP,ACT,1.0	CHEMBL3394008,TN,INACT,0.019999999552965164	CHEMBL2370328,TP,ACT,0.8700000047683716	CHEMBL333393,TP,ACT,1.0	CHEMBL169439,TP,ACT,1.0	CHEMBL322332,TN,INACT,0.009999999776482582	CHEMBL389948,TP,ACT,0.9900000095367432	CHEMBL333350,TP,ACT,0.9900000095367432	CHEMBL610620,TP,ACT,0.9900000095367432	CHEMBL297998,TP,ACT,0.7400000095367432	CHEMBL602269,TN,INACT,0.029999999329447746	CHEMBL114886,TN,INACT,0.0	CHEMBL33224,TN,INACT,0.0	CHEMBL3087714,TN,INACT,0.23999999463558197	CHEMBL2372213,TP,ACT,1.0	CHEMBL324652,TN,INACT,0.03999999910593033	CHEMBL59517,TN,INACT,0.0	CHEMBL114891,TN,INACT,0.0	CHEMBL240657,FP,INACT,0.949999988079071	CHEMBL246003,TP,ACT,0.9399999976158142	CHEMBL365026,TN,INACT,0.009999999776482582	CHEMBL45875,TN,INACT,0.07999999821186066	CHEMBL99136,TP,ACT,1.0	CHEMBL418375,TN,INACT,0.0	CHEMBL3264204,TN,INACT,0.0	CHEMBL50456,TN,INACT,0.009999999776482582	CHEMBL240021,TN,INACT,0.0	CHEMBL2112373,TP,ACT,0.949999988079071	CHEMBL27995,TN,INACT,0.0	CHEMBL296603,TP,ACT,1.0	CHEMBL2062860,TN,INACT,0.10000000149011612	CHEMBL606674,TP,ACT,0.9900000095367432	CHEMBL73791,TN,INACT,0.18000000715255737	CHEMBL74330,TN,INACT,0.009999999776482582	CHEMBL80505,TN,INACT,0.0	CHEMBL195756,TP,ACT,0.9399999976158142	CHEMBL104377,TN,INACT,0.0	CHEMBL95590,TN,INACT,0.0	CHEMBL557978,TP,ACT,0.9900000095367432	CHEMBL162095,TN,INACT,0.07000000029802322	CHEMBL510728,TP,ACT,1.0	CHEMBL1672083,TP,ACT,1.0	CHEMBL2323575,TP,ACT,0.9900000095367432	CHEMBL317298,TP,ACT,1.0	CHEMBL426320,TP,ACT,0.9800000190734863	CHEMBL38861,TN,INACT,0.0	CHEMBL91073,TN,INACT,0.009999999776482582	CHEMBL170335,TN,INACT,0.0	CHEMBL3355779,TP,ACT,1.0	CHEMBL377069,TP,ACT,0.9399999976158142	CHEMBL3290994,TN,INACT,0.0	CHEMBL3679601,TP,ACT,1.0	CHEMBL362010,TN,INACT,0.1599999964237213	CHEMBL3741465,TN,INACT,0.23999999463558197	CHEMBL2113646,TP,ACT,1.0	CHEMBL611437,TP,ACT,1.0	CHEMBL3409748,TP,ACT,0.9800000190734863	CHEMBL2181195,TP,ACT,0.8199999928474426	CHEMBL146983,TN,INACT,0.0	CHEMBL220333,TP,ACT,1.0	CHEMBL228409,TP,ACT,0.9599999785423279	CHEMBL2397389,TN,INACT,0.0	CHEMBL29541,TN,INACT,0.009999999776482582	CHEMBL170594,TP,ACT,0.9800000190734863	CHEMBL2113734,TP,ACT,0.9300000071525574	CHEMBL3234532,TN,INACT,0.4399999976158142	CHEMBL611688,TP,ACT,0.9900000095367432	CHEMBL3349043,TP,ACT,0.9800000190734863	CHEMBL2111777,TN,INACT,0.009999999776482582	CHEMBL2371713,TP,ACT,1.0	CHEMBL167032,TN,INACT,0.0	CHEMBL438681,TP,ACT,1.0	CHEMBL3121474,TN,INACT,0.0	CHEMBL140365,TN,INACT,0.029999999329447746	CHEMBL71626,TN,INACT,0.009999999776482582	CHEMBL3216936,TP,ACT,0.9900000095367432	CHEMBL3604310,TN,INACT,0.0	CHEMBL611430,TP,ACT,0.9900000095367432	CHEMBL1765671,TN,INACT,0.3400000035762787	CHEMBL447871,TP,ACT,0.9900000095367432	CHEMBL254505,TN,INACT,0.05999999865889549	CHEMBL33108,TN,INACT,0.009999999776482582	CHEMBL540724,TP,ACT,1.0	CHEMBL319036,TN,INACT,0.019999999552965164	CHEMBL113,TN,INACT,0.009999999776482582	CHEMBL141381,TP,ACT,0.949999988079071	CHEMBL3798018,TN,INACT,0.009999999776482582	CHEMBL65461,TN,INACT,0.0	CHEMBL317661,TP,ACT,1.0	CHEMBL2331793,TN,INACT,0.0	CHEMBL611429,TP,ACT,1.0	CHEMBL102613,TN,INACT,0.0	CHEMBL26632,TP,ACT,1.0	CHEMBL419617,TN,INACT,0.009999999776482582	CHEMBL399203,TN,INACT,0.0	CHEMBL2112340,TN,INACT,0.009999999776482582	CHEMBL13309,TP,ACT,1.0	CHEMBL3763905,TP,ACT,1.0	CHEMBL109203,FN,ACT,0.0	CHEMBL421349,TN,INACT,0.15000000596046448	CHEMBL312670,TN,INACT,0.0	CHEMBL105567,TN,INACT,0.0	CHEMBL2372196,TP,ACT,0.9900000095367432	CHEMBL312567,TN,INACT,0.009999999776482582	CHEMBL13143,TP,ACT,1.0	CHEMBL1437,TN,INACT,0.009999999776482582	CHEMBL115670,TN,INACT,0.0	CHEMBL419414,TP,ACT,0.9800000190734863	CHEMBL70728,TN,INACT,0.6200000047683716	CHEMBL95847,TP,ACT,0.9700000286102295	CHEMBL120530,FN,ACT,0.5	CHEMBL278129,TN,INACT,0.009999999776482582	CHEMBL441145,TN,INACT,0.009999999776482582	CHEMBL311781,TN,INACT,0.009999999776482582	CHEMBL610095,TN,INACT,0.4099999964237213	CHEMBL1201353,TN,INACT,0.20000000298023224	CHEMBL3679606,TP,ACT,0.9900000095367432	CHEMBL3236673,TP,ACT,0.9800000190734863	CHEMBL279225,TN,INACT,0.07999999821186066	CHEMBL328285,TN,INACT,0.44999998807907104	CHEMBL509619,TP,ACT,0.9800000190734863	CHEMBL106483,TN,INACT,0.0	CHEMBL3780776,TP,ACT,0.9900000095367432	CHEMBL109206,TN,INACT,0.0	CHEMBL3114145,TN,INACT,0.0	CHEMBL3409743,TP,ACT,0.9900000095367432	CHEMBL348011,TP,ACT,1.0	CHEMBL368629,TN,INACT,0.6700000166893005	CHEMBL3350316,TP,ACT,0.8600000143051147	CHEMBL59347,TN,INACT,0.009999999776482582	CHEMBL3423403,TN,INACT,0.0	CHEMBL337081,TN,INACT,0.009999999776482582	CHEMBL171108,TN,INACT,0.009999999776482582	CHEMBL15675,TN,INACT,0.009999999776482582	CHEMBL374818,TP,ACT,0.9800000190734863	CHEMBL2322561,TP,ACT,0.9800000190734863	CHEMBL608248,TP,ACT,0.9900000095367432	CHEMBL415617,TP,ACT,1.0	CHEMBL3763495,TP,ACT,1.0	CHEMBL3589799,TP,ACT,0.9800000190734863	CHEMBL311995,TP,ACT,0.8700000047683716	CHEMBL2114470,TP,ACT,0.949999988079071	CHEMBL607405,TP,ACT,0.9900000095367432	CHEMBL3589707,TP,ACT,0.7599999904632568	CHEMBL3237721,TP,ACT,1.0	CHEMBL77962,TN,INACT,0.0	CHEMBL317276,TP,ACT,1.0	CHEMBL81593,TN,INACT,0.0	CHEMBL3823231,TP,ACT,1.0	CHEMBL27384,TN,INACT,0.009999999776482582	CHEMBL333357,TP,ACT,0.9800000190734863	CHEMBL148143,TP,ACT,1.0	CHEMBL3633650,TN,INACT,0.1599999964237213	CHEMBL3403334,FP,INACT,0.8700000047683716	CHEMBL415247,TP,ACT,1.0	CHEMBL95727,TN,INACT,0.0	CHEMBL78487,TN,INACT,0.2199999988079071	CHEMBL3403733,TN,INACT,0.0	CHEMBL20993,TN,INACT,0.0	CHEMBL2371284,TP,ACT,0.9300000071525574	CHEMBL386492,TP,ACT,1.0	CHEMBL2113133,TN,INACT,0.6600000262260437	CHEMBL2011441,TN,INACT,0.0	CHEMBL552133,FN,ACT,0.1899999976158142	CHEMBL3393999,TN,INACT,0.0	CHEMBL2159112,TP,ACT,0.699999988079071	CHEMBL149763,TN,INACT,0.0	CHEMBL536034,TN,INACT,0.03999999910593033	CHEMBL254500,TN,INACT,0.1599999964237213	CHEMBL114478,TN,INACT,0.0	CHEMBL397176,TP,ACT,0.7099999785423279	CHEMBL2112674,TN,INACT,0.05000000074505806	CHEMBL326789,TN,INACT,0.029999999329447746	CHEMBL283057,TN,INACT,0.019999999552965164	CHEMBL2436819,TN,INACT,0.009999999776482582	CHEMBL343018,FN,ACT,0.05000000074505806	CHEMBL2370386,TP,ACT,0.9900000095367432	CHEMBL542877,TN,INACT,0.009999999776482582	CHEMBL2323447,TN,INACT,0.019999999552965164	CHEMBL73096,TN,INACT,0.009999999776482582	CHEMBL1500,TP,ACT,1.0	CHEMBL101554,TN,INACT,0.009999999776482582	CHEMBL302523,TP,ACT,0.9700000286102295	CHEMBL307659,TN,INACT,0.0	CHEMBL48932,TP,ACT,0.9900000095367432	CHEMBL3679603,TP,ACT,0.9900000095367432	CHEMBL527880,TN,INACT,0.0	CHEMBL600610,TN,INACT,0.0	CHEMBL90491,FP,INACT,0.9800000190734863	CHEMBL42799,TN,INACT,0.0	CHEMBL129052,TP,ACT,0.9399999976158142	CHEMBL319231,TN,INACT,0.0	CHEMBL275288,TP,ACT,0.9900000095367432	CHEMBL140693,TN,INACT,0.029999999329447746	CHEMBL341825,TN,INACT,0.0	CHEMBL9219,TN,INACT,0.0	CHEMBL1689548,TP,ACT,0.9700000286102295	CHEMBL216821,TN,INACT,0.0	CHEMBL160092,TN,INACT,0.0	CHEMBL60294,FN,ACT,0.15000000596046448	CHEMBL3665444,TN,INACT,0.10999999940395355	CHEMBL258906,TN,INACT,0.009999999776482582	CHEMBL76816,TP,ACT,0.9300000071525574	CHEMBL72710,TN,INACT,0.0	CHEMBL300357,FN,ACT,0.44999998807907104	CHEMBL1180343,TN,INACT,0.0	CHEMBL389129,TN,INACT,0.2800000011920929	CHEMBL422701,TN,INACT,0.009999999776482582	CHEMBL354966,TP,ACT,1.0	CHEMBL284969,TN,INACT,0.0	CHEMBL143304,TN,INACT,0.009999999776482582	CHEMBL281232,TN,INACT,0.0	CHEMBL172788,TN,INACT,0.009999999776482582	CHEMBL3236675,TP,ACT,1.0	CHEMBL607484,TP,ACT,1.0	CHEMBL141354,TN,INACT,0.41999998688697815	CHEMBL101231,TP,ACT,0.9900000095367432	CHEMBL58241,TN,INACT,0.0	CHEMBL229441,TP,ACT,1.0	CHEMBL3104110,FN,ACT,0.0	CHEMBL441889,TP,ACT,0.8799999952316284	CHEMBL378173,TN,INACT,0.07999999821186066	CHEMBL606676,TP,ACT,0.9900000095367432	CHEMBL295618,TP,ACT,0.7200000286102295	CHEMBL196127,FP,INACT,0.9800000190734863	CHEMBL3679615,TP,ACT,0.9900000095367432	CHEMBL606925,TP,ACT,0.9900000095367432	CHEMBL3327375,TN,INACT,0.25999999046325684	CHEMBL97750,TN,INACT,0.009999999776482582	CHEMBL422959,TN,INACT,0.0	CHEMBL3764246,TN,INACT,0.009999999776482582	CHEMBL3634398,TP,ACT,0.9800000190734863	CHEMBL282181,TP,ACT,1.0	CHEMBL2369477,TP,ACT,1.0	CHEMBL555618,TP,ACT,0.9900000095367432	CHEMBL78669,TN,INACT,0.0	CHEMBL64043,TN,INACT,0.0	CHEMBL100358,TP,ACT,0.9800000190734863	CHEMBL381793,TP,ACT,0.9599999785423279	CHEMBL3634516,TP,ACT,0.9599999785423279	CHEMBL3233195,TP,ACT,1.0	CHEMBL2369871,TP,ACT,1.0	CHEMBL539334,TN,INACT,0.009999999776482582	CHEMBL2370502,TP,ACT,1.0	CHEMBL441049,TP,ACT,0.9599999785423279	CHEMBL321170,FP,INACT,0.9800000190734863	CHEMBL406036,TP,ACT,0.9900000095367432	CHEMBL3781336,TP,ACT,1.0	CHEMBL62840,TN,INACT,0.0	CHEMBL574602,FP,INACT,0.8899999856948853	CHEMBL117899,TN,INACT,0.009999999776482582	CHEMBL2372816,TP,ACT,1.0	CHEMBL332803,TP,ACT,1.0	CHEMBL179638,TN,INACT,0.029999999329447746	CHEMBL610798,TP,ACT,0.9800000190734863	

