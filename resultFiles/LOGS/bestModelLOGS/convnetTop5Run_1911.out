CNNModel CHEMBL3858 adam 0.001 15 128 0 0.8 False True
Number of active compounds :	183
Number of inactive compounds :	122
---------------------------------
Run id: CNNModel_CHEMBL3858_adam_0.001_15_128_0_0.8_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL3858_adam_0.001_15_128_0.8_True/
---------------------------------
Training samples: 192
Validation samples: 61
--
Training Step: 1  | time: 9.951s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/192
[A[ATraining Step: 2  | total loss: [1m[32m0.62387[0m[0m | time: 11.131s
[2K
| Adam | epoch: 001 | loss: 0.62387 - acc: 0.4781 -- iter: 064/192
[A[ATraining Step: 3  | total loss: [1m[32m0.67925[0m[0m | time: 17.090s
[2K
| Adam | epoch: 001 | loss: 0.67925 - acc: 0.5727 -- iter: 096/192
[A[ATraining Step: 4  | total loss: [1m[32m0.68818[0m[0m | time: 19.385s
[2K
| Adam | epoch: 001 | loss: 0.68818 - acc: 0.5416 -- iter: 128/192
[A[ATraining Step: 5  | total loss: [1m[32m0.68046[0m[0m | time: 20.455s
[2K
| Adam | epoch: 001 | loss: 0.68046 - acc: 0.5777 -- iter: 160/192
[A[ATraining Step: 6  | total loss: [1m[32m0.64528[0m[0m | time: 22.780s
[2K
| Adam | epoch: 001 | loss: 0.64528 - acc: 0.6483 | val_loss: 0.63060 - val_acc: 0.6721 -- iter: 192/192
--
Training Step: 7  | total loss: [1m[32m0.84147[0m[0m | time: 1.292s
[2K
| Adam | epoch: 002 | loss: 0.84147 - acc: 0.5406 -- iter: 032/192
[A[ATraining Step: 8  | total loss: [1m[32m0.81105[0m[0m | time: 2.750s
[2K
| Adam | epoch: 002 | loss: 0.81105 - acc: 0.5002 -- iter: 064/192
[A[ATraining Step: 9  | total loss: [1m[32m0.76065[0m[0m | time: 5.250s
[2K
| Adam | epoch: 002 | loss: 0.76065 - acc: 0.4835 -- iter: 096/192
[A[ATraining Step: 10  | total loss: [1m[32m0.71416[0m[0m | time: 22.870s
[2K
| Adam | epoch: 002 | loss: 0.71416 - acc: 0.5855 -- iter: 128/192
[A[ATraining Step: 11  | total loss: [1m[32m0.70445[0m[0m | time: 35.790s
[2K
| Adam | epoch: 002 | loss: 0.70445 - acc: 0.5450 -- iter: 160/192
[A[ATraining Step: 12  | total loss: [1m[32m0.70005[0m[0m | time: 41.034s
[2K
| Adam | epoch: 002 | loss: 0.70005 - acc: 0.5107 | val_loss: 0.69125 - val_acc: 0.6721 -- iter: 192/192
--
Training Step: 13  | total loss: [1m[32m0.69671[0m[0m | time: 0.979s
[2K
| Adam | epoch: 003 | loss: 0.69671 - acc: 0.5329 -- iter: 032/192
[A[ATraining Step: 14  | total loss: [1m[32m0.69545[0m[0m | time: 1.962s
[2K
| Adam | epoch: 003 | loss: 0.69545 - acc: 0.5067 -- iter: 064/192
[A[ATraining Step: 15  | total loss: [1m[32m0.69459[0m[0m | time: 2.942s
[2K
| Adam | epoch: 003 | loss: 0.69459 - acc: 0.4918 -- iter: 096/192
[A[ATraining Step: 16  | total loss: [1m[32m0.69382[0m[0m | time: 3.985s
[2K
| Adam | epoch: 003 | loss: 0.69382 - acc: 0.5535 -- iter: 128/192
[A[ATraining Step: 17  | total loss: [1m[32m0.69314[0m[0m | time: 4.855s
[2K
| Adam | epoch: 003 | loss: 0.69314 - acc: 0.6242 -- iter: 160/192
[A[ATraining Step: 18  | total loss: [1m[32m0.69277[0m[0m | time: 6.910s
[2K
| Adam | epoch: 003 | loss: 0.69277 - acc: 0.6245 | val_loss: 0.69083 - val_acc: 0.6721 -- iter: 192/192
--
Training Step: 19  | total loss: [1m[32m0.69326[0m[0m | time: 0.894s
[2K
| Adam | epoch: 004 | loss: 0.69326 - acc: 0.5517 -- iter: 032/192
[A[ATraining Step: 20  | total loss: [1m[32m0.69279[0m[0m | time: 1.826s
[2K
| Adam | epoch: 004 | loss: 0.69279 - acc: 0.5652 -- iter: 064/192
[A[ATraining Step: 21  | total loss: [1m[32m0.69287[0m[0m | time: 2.837s
[2K
| Adam | epoch: 004 | loss: 0.69287 - acc: 0.5450 -- iter: 096/192
[A[ATraining Step: 22  | total loss: [1m[32m0.69295[0m[0m | time: 3.862s
[2K
| Adam | epoch: 004 | loss: 0.69295 - acc: 0.5315 -- iter: 128/192
[A[ATraining Step: 23  | total loss: [1m[32m0.69318[0m[0m | time: 4.928s
[2K
| Adam | epoch: 004 | loss: 0.69318 - acc: 0.5133 -- iter: 160/192
[A[ATraining Step: 24  | total loss: [1m[32m0.69244[0m[0m | time: 6.989s
[2K
| Adam | epoch: 004 | loss: 0.69244 - acc: 0.5447 | val_loss: 0.68861 - val_acc: 0.6721 -- iter: 192/192
--
Training Step: 25  | total loss: [1m[32m0.69168[0m[0m | time: 4.321s
[2K
| Adam | epoch: 005 | loss: 0.69168 - acc: 0.5751 -- iter: 032/192
[A[ATraining Step: 26  | total loss: [1m[32m0.69168[0m[0m | time: 9.295s
[2K
| Adam | epoch: 005 | loss: 0.69168 - acc: 0.5718 -- iter: 064/192
[A[ATraining Step: 27  | total loss: [1m[32m0.69128[0m[0m | time: 10.375s
[2K
| Adam | epoch: 005 | loss: 0.69128 - acc: 0.5774 -- iter: 096/192
[A[ATraining Step: 28  | total loss: [1m[32m0.69197[0m[0m | time: 11.381s
[2K
| Adam | epoch: 005 | loss: 0.69197 - acc: 0.5503 -- iter: 128/192
[A[ATraining Step: 29  | total loss: [1m[32m0.69258[0m[0m | time: 12.465s
[2K
| Adam | epoch: 005 | loss: 0.69258 - acc: 0.5304 -- iter: 160/192
[A[ATraining Step: 30  | total loss: [1m[32m0.69339[0m[0m | time: 14.563s
[2K
| Adam | epoch: 005 | loss: 0.69339 - acc: 0.5084 | val_loss: 0.68529 - val_acc: 0.6721 -- iter: 192/192
--
Training Step: 31  | total loss: [1m[32m0.69242[0m[0m | time: 1.085s
[2K
| Adam | epoch: 006 | loss: 0.69242 - acc: 0.5281 -- iter: 032/192
[A[ATraining Step: 32  | total loss: [1m[32m0.69166[0m[0m | time: 2.131s
[2K
| Adam | epoch: 006 | loss: 0.69166 - acc: 0.5429 -- iter: 064/192
[A[ATraining Step: 33  | total loss: [1m[32m0.69203[0m[0m | time: 3.299s
[2K
| Adam | epoch: 006 | loss: 0.69203 - acc: 0.5335 -- iter: 096/192
[A[ATraining Step: 34  | total loss: [1m[32m0.69198[0m[0m | time: 5.214s
[2K
| Adam | epoch: 006 | loss: 0.69198 - acc: 0.5330 -- iter: 128/192
[A[ATraining Step: 35  | total loss: [1m[32m0.69114[0m[0m | time: 6.658s
[2K
| Adam | epoch: 006 | loss: 0.69114 - acc: 0.5457 -- iter: 160/192
[A[ATraining Step: 36  | total loss: [1m[32m0.69035[0m[0m | time: 10.594s
[2K
| Adam | epoch: 006 | loss: 0.69035 - acc: 0.5555 | val_loss: 0.67916 - val_acc: 0.6721 -- iter: 192/192
--
Training Step: 37  | total loss: [1m[32m0.69040[0m[0m | time: 1.049s
[2K
| Adam | epoch: 007 | loss: 0.69040 - acc: 0.5507 -- iter: 032/192
[A[ATraining Step: 38  | total loss: [1m[32m0.68892[0m[0m | time: 2.387s
[2K
| Adam | epoch: 007 | loss: 0.68892 - acc: 0.5652 -- iter: 064/192
[A[ATraining Step: 39  | total loss: [1m[32m0.68874[0m[0m | time: 3.749s
[2K
| Adam | epoch: 007 | loss: 0.68874 - acc: 0.5647 -- iter: 096/192
[A[ATraining Step: 40  | total loss: [1m[32m0.68908[0m[0m | time: 5.109s
[2K
| Adam | epoch: 007 | loss: 0.68908 - acc: 0.5584 -- iter: 128/192
[A[ATraining Step: 41  | total loss: [1m[32m0.68859[0m[0m | time: 6.411s
[2K
| Adam | epoch: 007 | loss: 0.68859 - acc: 0.5592 -- iter: 160/192
[A[ATraining Step: 42  | total loss: [1m[32m0.68894[0m[0m | time: 8.767s
[2K
| Adam | epoch: 007 | loss: 0.68894 - acc: 0.5541 | val_loss: 0.65621 - val_acc: 0.6721 -- iter: 192/192
--
Training Step: 43  | total loss: [1m[32m0.68933[0m[0m | time: 10.461s
[2K
| Adam | epoch: 008 | loss: 0.68933 - acc: 0.5501 -- iter: 032/192
[A[ATraining Step: 44  | total loss: [1m[32m0.68961[0m[0m | time: 24.123s
[2K
| Adam | epoch: 008 | loss: 0.68961 - acc: 0.5468 -- iter: 064/192
[A[ATraining Step: 45  | total loss: [1m[32m0.68994[0m[0m | time: 25.393s
[2K
| Adam | epoch: 008 | loss: 0.68994 - acc: 0.5442 -- iter: 096/192
[A[ATraining Step: 46  | total loss: [1m[32m0.69389[0m[0m | time: 26.659s
[2K
| Adam | epoch: 008 | loss: 0.69389 - acc: 0.5264 -- iter: 128/192
[A[ATraining Step: 47  | total loss: [1m[32m0.69368[0m[0m | time: 34.415s
[2K
| Adam | epoch: 008 | loss: 0.69368 - acc: 0.5272 -- iter: 160/192
[A[ATraining Step: 48  | total loss: [1m[32m0.69057[0m[0m | time: 40.124s
[2K
| Adam | epoch: 008 | loss: 0.69057 - acc: 0.5429 | val_loss: 0.66936 - val_acc: 0.6721 -- iter: 192/192
--
Training Step: 49  | total loss: [1m[32m0.68949[0m[0m | time: 1.197s
[2K
| Adam | epoch: 009 | loss: 0.68949 - acc: 0.5460 -- iter: 032/192
[A[ATraining Step: 50  | total loss: [1m[32m0.68876[0m[0m | time: 2.497s
[2K
| Adam | epoch: 009 | loss: 0.68876 - acc: 0.5486 -- iter: 064/192
[A[ATraining Step: 51  | total loss: [1m[32m0.68620[0m[0m | time: 3.815s
[2K
| Adam | epoch: 009 | loss: 0.68620 - acc: 0.5650 -- iter: 096/192
[A[ATraining Step: 52  | total loss: [1m[32m0.68600[0m[0m | time: 5.068s
[2K
| Adam | epoch: 009 | loss: 0.68600 - acc: 0.5646 -- iter: 128/192
[A[ATraining Step: 53  | total loss: [1m[32m0.68562[0m[0m | time: 6.254s
[2K
| Adam | epoch: 009 | loss: 0.68562 - acc: 0.5643 -- iter: 160/192
[A[ATraining Step: 54  | total loss: [1m[32m0.68811[0m[0m | time: 8.362s
[2K
| Adam | epoch: 009 | loss: 0.68811 - acc: 0.5504 | val_loss: 0.66079 - val_acc: 0.6721 -- iter: 192/192
--
Training Step: 55  | total loss: [1m[32m0.68569[0m[0m | time: 3.995s
[2K
| Adam | epoch: 010 | loss: 0.68569 - acc: 0.5611 -- iter: 032/192
[A[ATraining Step: 56  | total loss: [1m[32m0.68219[0m[0m | time: 5.058s
[2K
| Adam | epoch: 010 | loss: 0.68219 - acc: 0.5745 -- iter: 064/192
[A[ATraining Step: 57  | total loss: [1m[32m0.67875[0m[0m | time: 6.238s
[2K
| Adam | epoch: 010 | loss: 0.67875 - acc: 0.5858 -- iter: 096/192
[A[ATraining Step: 58  | total loss: [1m[32m0.68319[0m[0m | time: 7.537s
[2K
| Adam | epoch: 010 | loss: 0.68319 - acc: 0.5698 -- iter: 128/192
[A[ATraining Step: 59  | total loss: [1m[32m0.68779[0m[0m | time: 8.701s
[2K
| Adam | epoch: 010 | loss: 0.68779 - acc: 0.5521 -- iter: 160/192
[A[ATraining Step: 60  | total loss: [1m[32m0.68908[0m[0m | time: 10.986s
[2K
| Adam | epoch: 010 | loss: 0.68908 - acc: 0.5452 | val_loss: 0.65002 - val_acc: 0.6721 -- iter: 192/192
--
Training Step: 61  | total loss: [1m[32m0.68780[0m[0m | time: 4.851s
[2K
| Adam | epoch: 011 | loss: 0.68780 - acc: 0.5474 -- iter: 032/192
[A[ATraining Step: 62  | total loss: [1m[32m0.68382[0m[0m | time: 6.186s
[2K
| Adam | epoch: 011 | loss: 0.68382 - acc: 0.5614 -- iter: 064/192
[A[ATraining Step: 63  | total loss: [1m[32m0.68185[0m[0m | time: 7.637s
[2K
| Adam | epoch: 011 | loss: 0.68185 - acc: 0.5655 -- iter: 096/192
[A[ATraining Step: 64  | total loss: [1m[32m0.68030[0m[0m | time: 10.696s
[2K
| Adam | epoch: 011 | loss: 0.68030 - acc: 0.5690 -- iter: 128/192
[A[ATraining Step: 65  | total loss: [1m[32m0.67880[0m[0m | time: 15.997s
[2K
| Adam | epoch: 011 | loss: 0.67880 - acc: 0.5721 -- iter: 160/192
[A[ATraining Step: 66  | total loss: [1m[32m0.67336[0m[0m | time: 19.619s
[2K
| Adam | epoch: 011 | loss: 0.67336 - acc: 0.5861 | val_loss: 0.62216 - val_acc: 0.6721 -- iter: 192/192
--
Training Step: 67  | total loss: [1m[32m0.67602[0m[0m | time: 0.934s
[2K
| Adam | epoch: 012 | loss: 0.67602 - acc: 0.5758 -- iter: 032/192
[A[ATraining Step: 68  | total loss: [1m[32m0.67125[0m[0m | time: 1.859s
[2K
| Adam | epoch: 012 | loss: 0.67125 - acc: 0.5816 -- iter: 064/192
[A[ATraining Step: 69  | total loss: [1m[32m0.67392[0m[0m | time: 2.899s
[2K
| Adam | epoch: 012 | loss: 0.67392 - acc: 0.5757 -- iter: 096/192
[A[ATraining Step: 70  | total loss: [1m[32m0.67995[0m[0m | time: 3.836s
[2K
| Adam | epoch: 012 | loss: 0.67995 - acc: 0.5634 -- iter: 128/192
[A[ATraining Step: 71  | total loss: [1m[32m0.68348[0m[0m | time: 4.733s
[2K
| Adam | epoch: 012 | loss: 0.68348 - acc: 0.5526 -- iter: 160/192
[A[ATraining Step: 72  | total loss: [1m[32m0.68287[0m[0m | time: 6.698s
[2K
| Adam | epoch: 012 | loss: 0.68287 - acc: 0.5502 | val_loss: 0.64994 - val_acc: 0.6721 -- iter: 192/192
--
Training Step: 73  | total loss: [1m[32m0.68383[0m[0m | time: 1.028s
[2K
| Adam | epoch: 013 | loss: 0.68383 - acc: 0.5412 -- iter: 032/192
[A[ATraining Step: 74  | total loss: [1m[32m0.68329[0m[0m | time: 2.033s
[2K
| Adam | epoch: 013 | loss: 0.68329 - acc: 0.5401 -- iter: 064/192
[A[ATraining Step: 75  | total loss: [1m[32m0.67980[0m[0m | time: 3.090s
[2K
| Adam | epoch: 013 | loss: 0.67980 - acc: 0.5561 -- iter: 096/192
[A[ATraining Step: 76  | total loss: [1m[32m0.67912[0m[0m | time: 4.040s
[2K
| Adam | epoch: 013 | loss: 0.67912 - acc: 0.5567 -- iter: 128/192
[A[ATraining Step: 77  | total loss: [1m[32m0.67857[0m[0m | time: 4.688s
[2K
| Adam | epoch: 013 | loss: 0.67857 - acc: 0.5507 -- iter: 160/192
[A[ATraining Step: 78  | total loss: [1m[32m0.67776[0m[0m | time: 6.362s
[2K
| Adam | epoch: 013 | loss: 0.67776 - acc: 0.5454 | val_loss: 0.61845 - val_acc: 0.6721 -- iter: 192/192
--
Training Step: 79  | total loss: [1m[32m0.67504[0m[0m | time: 0.724s
[2K
| Adam | epoch: 014 | loss: 0.67504 - acc: 0.5504 -- iter: 032/192
[A[ATraining Step: 80  | total loss: [1m[32m0.67739[0m[0m | time: 1.443s
[2K
| Adam | epoch: 014 | loss: 0.67739 - acc: 0.5421 -- iter: 064/192
[A[ATraining Step: 81  | total loss: [1m[32m0.66995[0m[0m | time: 2.090s
[2K
| Adam | epoch: 014 | loss: 0.66995 - acc: 0.5536 -- iter: 096/192
[A[ATraining Step: 82  | total loss: [1m[32m0.66639[0m[0m | time: 2.712s
[2K
| Adam | epoch: 014 | loss: 0.66639 - acc: 0.5514 -- iter: 128/192
[A[ATraining Step: 83  | total loss: [1m[32m0.66319[0m[0m | time: 3.384s
[2K
| Adam | epoch: 014 | loss: 0.66319 - acc: 0.5494 -- iter: 160/192
[A[ATraining Step: 84  | total loss: [1m[32m0.65469[0m[0m | time: 5.024s
[2K
| Adam | epoch: 014 | loss: 0.65469 - acc: 0.5569 | val_loss: 0.52511 - val_acc: 0.6721 -- iter: 192/192
--
Training Step: 85  | total loss: [1m[32m0.64564[0m[0m | time: 0.691s
[2K
| Adam | epoch: 015 | loss: 0.64564 - acc: 0.5637 -- iter: 032/192
[A[ATraining Step: 86  | total loss: [1m[32m0.64810[0m[0m | time: 1.345s
[2K
| Adam | epoch: 015 | loss: 0.64810 - acc: 0.5605 -- iter: 064/192
[A[ATraining Step: 87  | total loss: [1m[32m0.63234[0m[0m | time: 2.016s
[2K
| Adam | epoch: 015 | loss: 0.63234 - acc: 0.5732 -- iter: 096/192
[A[ATraining Step: 88  | total loss: [1m[32m0.62527[0m[0m | time: 2.713s
[2K
| Adam | epoch: 015 | loss: 0.62527 - acc: 0.5721 -- iter: 128/192
[A[ATraining Step: 89  | total loss: [1m[32m0.61780[0m[0m | time: 3.362s
[2K
| Adam | epoch: 015 | loss: 0.61780 - acc: 0.5712 -- iter: 160/192
[A[ATraining Step: 90  | total loss: [1m[32m0.61610[0m[0m | time: 5.028s
[2K
| Adam | epoch: 015 | loss: 0.61610 - acc: 0.5640 | val_loss: 0.46582 - val_acc: 0.7705 -- iter: 192/192
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.8792682926829268
Validation AUPRC:0.9475160880845249
Test AUC:0.937070938215103
Test AUPRC:0.9675450507600135
BestTestF1Score	0.93	0.83	0.92	0.97	0.89	34	1	22	4	0.66
BestTestMCCScore	0.93	0.83	0.92	0.97	0.89	34	1	22	4	0.66
BestTestAccuracyScore	0.93	0.83	0.92	0.97	0.89	34	1	22	4	0.66
BestValidationF1Score	0.91	0.77	0.89	0.97	0.85	35	1	19	6	0.66
BestValidationMCC	0.91	0.77	0.89	0.97	0.85	35	1	19	6	0.66
BestValidationAccuracy	0.91	0.77	0.89	0.97	0.85	35	1	19	6	0.66
TestPredictions (Threshold:0.66)
CHEMBL3431645,TP,ACT,0.7799999713897705	CHEMBL2430741,TN,INACT,0.6000000238418579	CHEMBL3431689,TP,ACT,0.8500000238418579	CHEMBL3431613,TP,ACT,0.6800000071525574	CHEMBL3431877,TP,ACT,0.7799999713897705	CHEMBL2059006,TN,INACT,0.49000000953674316	CHEMBL3431494,TP,ACT,0.8799999952316284	CHEMBL3431628,TP,ACT,0.8999999761581421	CHEMBL3431859,TP,ACT,0.6800000071525574	CHEMBL3431508,TP,ACT,0.6700000166893005	CHEMBL3431750,TP,ACT,0.6800000071525574	CHEMBL3431506,TP,ACT,0.8299999833106995	CHEMBL3431764,TP,ACT,0.8899999856948853	CHEMBL3431905,TP,ACT,0.800000011920929	CHEMBL3431680,TP,ACT,0.8899999856948853	CHEMBL3431919,TP,ACT,0.8999999761581421	CHEMBL83103,TN,INACT,0.49000000953674316	CHEMBL3431512,TP,ACT,0.8899999856948853	CHEMBL3431523,TP,ACT,0.7200000286102295	CHEMBL2430730,TN,INACT,0.6200000047683716	CHEMBL3431641,FN,ACT,0.6499999761581421	CHEMBL486329,TN,INACT,0.49000000953674316	CHEMBL2059010,TN,INACT,0.49000000953674316	CHEMBL3431882,TP,ACT,0.8899999856948853	CHEMBL3358882,TN,INACT,0.49000000953674316	CHEMBL3819561,TN,INACT,0.5099999904632568	CHEMBL12812,FN,ACT,0.49000000953674316	CHEMBL3431647,TP,ACT,0.8999999761581421	CHEMBL2382399,TN,INACT,0.49000000953674316	CHEMBL127645,TN,INACT,0.49000000953674316	CHEMBL3358865,TN,INACT,0.49000000953674316	CHEMBL2059005,TN,INACT,0.49000000953674316	CHEMBL2059007,TN,INACT,0.49000000953674316	CHEMBL3431599,TP,ACT,0.8299999833106995	CHEMBL3431912,TP,ACT,0.8999999761581421	CHEMBL3431889,FN,ACT,0.49000000953674316	CHEMBL3431625,TP,ACT,0.8399999737739563	CHEMBL3431555,TP,ACT,0.6600000262260437	CHEMBL3431687,TP,ACT,0.7099999785423279	CHEMBL3608325,TN,INACT,0.49000000953674316	CHEMBL3431701,TP,ACT,0.7099999785423279	CHEMBL2316889,TN,INACT,0.49000000953674316	CHEMBL3431477,FN,ACT,0.5899999737739563	CHEMBL3431488,TP,ACT,0.75	CHEMBL3431653,TP,ACT,0.699999988079071	CHEMBL3431795,TP,ACT,0.6800000071525574	CHEMBL15976,TN,INACT,0.49000000953674316	CHEMBL75880,TN,INACT,0.5	CHEMBL3431464,TP,ACT,0.7099999785423279	CHEMBL3431470,TP,ACT,0.8899999856948853	CHEMBL210497,TN,INACT,0.49000000953674316	CHEMBL3431755,TP,ACT,0.7900000214576721	CHEMBL3431547,TP,ACT,0.7699999809265137	CHEMBL3431575,TP,ACT,0.8999999761581421	CHEMBL2382394,TN,INACT,0.5199999809265137	CHEMBL1159897,FP,INACT,0.75	CHEMBL3431708,TP,ACT,0.7400000095367432	CHEMBL3431751,TP,ACT,0.6800000071525574	CHEMBL2316888,TN,INACT,0.49000000953674316	CHEMBL3318420,TN,INACT,0.49000000953674316	CHEMBL2430726,TN,INACT,0.5899999737739563	

