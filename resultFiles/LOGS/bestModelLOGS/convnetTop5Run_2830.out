CNNModel CHEMBL4005 adam 0.001 15 256 0 0.6 False True
Number of active compounds :	369
Number of inactive compounds :	246
---------------------------------
Run id: CNNModel_CHEMBL4005_adam_0.001_15_256_0_0.6_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL4005_adam_0.001_15_256_0.6_True/
---------------------------------
Training samples: 382
Validation samples: 120
--
Training Step: 1  | time: 24.484s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/382
[A[ATraining Step: 2  | total loss: [1m[32m0.62398[0m[0m | time: 48.008s
[2K
| Adam | epoch: 001 | loss: 0.62398 - acc: 0.3375 -- iter: 064/382
[A[ATraining Step: 3  | total loss: [1m[32m0.67599[0m[0m | time: 64.186s
[2K
| Adam | epoch: 001 | loss: 0.67599 - acc: 0.6239 -- iter: 096/382
[A[ATraining Step: 4  | total loss: [1m[32m0.67944[0m[0m | time: 79.641s
[2K
| Adam | epoch: 001 | loss: 0.67944 - acc: 0.6013 -- iter: 128/382
[A[ATraining Step: 5  | total loss: [1m[32m0.69241[0m[0m | time: 91.764s
[2K
| Adam | epoch: 001 | loss: 0.69241 - acc: 0.5744 -- iter: 160/382
[A[ATraining Step: 6  | total loss: [1m[32m0.67277[0m[0m | time: 106.958s
[2K
| Adam | epoch: 001 | loss: 0.67277 - acc: 0.6069 -- iter: 192/382
[A[ATraining Step: 7  | total loss: [1m[32m0.66138[0m[0m | time: 116.863s
[2K
| Adam | epoch: 001 | loss: 0.66138 - acc: 0.6365 -- iter: 224/382
[A[ATraining Step: 8  | total loss: [1m[32m0.70059[0m[0m | time: 126.848s
[2K
| Adam | epoch: 001 | loss: 0.70059 - acc: 0.5422 -- iter: 256/382
[A[ATraining Step: 9  | total loss: [1m[32m0.68680[0m[0m | time: 131.724s
[2K
| Adam | epoch: 001 | loss: 0.68680 - acc: 0.5695 -- iter: 288/382
[A[ATraining Step: 10  | total loss: [1m[32m0.69342[0m[0m | time: 141.560s
[2K
| Adam | epoch: 001 | loss: 0.69342 - acc: 0.5347 -- iter: 320/382
[A[ATraining Step: 11  | total loss: [1m[32m0.69210[0m[0m | time: 154.006s
[2K
| Adam | epoch: 001 | loss: 0.69210 - acc: 0.5331 -- iter: 352/382
[A[ATraining Step: 12  | total loss: [1m[32m0.68097[0m[0m | time: 173.761s
[2K
| Adam | epoch: 001 | loss: 0.68097 - acc: 0.6166 | val_loss: 0.69146 - val_acc: 0.5250 -- iter: 382/382
--
Training Step: 13  | total loss: [1m[32m0.68312[0m[0m | time: 11.262s
[2K
| Adam | epoch: 002 | loss: 0.68312 - acc: 0.5952 -- iter: 032/382
[A[ATraining Step: 14  | total loss: [1m[32m0.68449[0m[0m | time: 18.499s
[2K
| Adam | epoch: 002 | loss: 0.68449 - acc: 0.5835 -- iter: 064/382
[A[ATraining Step: 15  | total loss: [1m[32m0.68318[0m[0m | time: 27.263s
[2K
| Adam | epoch: 002 | loss: 0.68318 - acc: 0.5875 -- iter: 096/382
[A[ATraining Step: 16  | total loss: [1m[32m0.67099[0m[0m | time: 36.560s
[2K
| Adam | epoch: 002 | loss: 0.67099 - acc: 0.6602 -- iter: 128/382
[A[ATraining Step: 17  | total loss: [1m[32m0.67327[0m[0m | time: 48.058s
[2K
| Adam | epoch: 002 | loss: 0.67327 - acc: 0.6363 -- iter: 160/382
[A[ATraining Step: 18  | total loss: [1m[32m0.66839[0m[0m | time: 53.591s
[2K
| Adam | epoch: 002 | loss: 0.66839 - acc: 0.6324 -- iter: 192/382
[A[ATraining Step: 19  | total loss: [1m[32m0.68359[0m[0m | time: 64.001s
[2K
| Adam | epoch: 002 | loss: 0.68359 - acc: 0.5987 -- iter: 224/382
[A[ATraining Step: 20  | total loss: [1m[32m0.64533[0m[0m | time: 70.348s
[2K
| Adam | epoch: 002 | loss: 0.64533 - acc: 0.6674 -- iter: 256/382
[A[ATraining Step: 21  | total loss: [1m[32m0.67283[0m[0m | time: 77.124s
[2K
| Adam | epoch: 002 | loss: 0.67283 - acc: 0.6251 -- iter: 288/382
[A[ATraining Step: 22  | total loss: [1m[32m0.68343[0m[0m | time: 86.567s
[2K
| Adam | epoch: 002 | loss: 0.68343 - acc: 0.6063 -- iter: 320/382
[A[ATraining Step: 23  | total loss: [1m[32m0.69052[0m[0m | time: 98.586s
[2K
| Adam | epoch: 002 | loss: 0.69052 - acc: 0.5845 -- iter: 352/382
[A[ATraining Step: 24  | total loss: [1m[32m0.67865[0m[0m | time: 135.063s
[2K
| Adam | epoch: 002 | loss: 0.67865 - acc: 0.6047 | val_loss: 0.69252 - val_acc: 0.5250 -- iter: 382/382
--
Training Step: 25  | total loss: [1m[32m0.68875[0m[0m | time: 9.787s
[2K
| Adam | epoch: 003 | loss: 0.68875 - acc: 0.5676 -- iter: 032/382
[A[ATraining Step: 26  | total loss: [1m[32m0.68307[0m[0m | time: 14.497s
[2K
| Adam | epoch: 003 | loss: 0.68307 - acc: 0.5850 -- iter: 064/382
[A[ATraining Step: 27  | total loss: [1m[32m0.68008[0m[0m | time: 26.235s
[2K
| Adam | epoch: 003 | loss: 0.68008 - acc: 0.5974 -- iter: 096/382
[A[ATraining Step: 28  | total loss: [1m[32m0.68116[0m[0m | time: 36.098s
[2K
| Adam | epoch: 003 | loss: 0.68116 - acc: 0.5887 -- iter: 128/382
[A[ATraining Step: 29  | total loss: [1m[32m0.67854[0m[0m | time: 44.175s
[2K
| Adam | epoch: 003 | loss: 0.67854 - acc: 0.6051 -- iter: 160/382
[A[ATraining Step: 30  | total loss: [1m[32m0.67685[0m[0m | time: 50.562s
[2K
| Adam | epoch: 003 | loss: 0.67685 - acc: 0.6172 -- iter: 192/382
[A[ATraining Step: 31  | total loss: [1m[32m0.67958[0m[0m | time: 61.637s
[2K
| Adam | epoch: 003 | loss: 0.67958 - acc: 0.5974 -- iter: 224/382
[A[ATraining Step: 32  | total loss: [1m[32m0.68053[0m[0m | time: 70.875s
[2K
| Adam | epoch: 003 | loss: 0.68053 - acc: 0.5895 -- iter: 256/382
[A[ATraining Step: 33  | total loss: [1m[32m0.67943[0m[0m | time: 73.124s
[2K
| Adam | epoch: 003 | loss: 0.67943 - acc: 0.5973 -- iter: 288/382
[A[ATraining Step: 34  | total loss: [1m[32m0.68497[0m[0m | time: 90.634s
[2K
| Adam | epoch: 003 | loss: 0.68497 - acc: 0.5631 -- iter: 320/382
[A[ATraining Step: 35  | total loss: [1m[32m0.68484[0m[0m | time: 102.152s
[2K
| Adam | epoch: 003 | loss: 0.68484 - acc: 0.5630 -- iter: 352/382
[A[ATraining Step: 36  | total loss: [1m[32m0.68467[0m[0m | time: 126.519s
[2K
| Adam | epoch: 003 | loss: 0.68467 - acc: 0.5629 | val_loss: 0.69128 - val_acc: 0.5250 -- iter: 382/382
--
Training Step: 37  | total loss: [1m[32m0.68088[0m[0m | time: 8.242s
[2K
| Adam | epoch: 004 | loss: 0.68088 - acc: 0.5815 -- iter: 032/382
[A[ATraining Step: 38  | total loss: [1m[32m0.67866[0m[0m | time: 14.658s
[2K
| Adam | epoch: 004 | loss: 0.67866 - acc: 0.5900 -- iter: 064/382
[A[ATraining Step: 39  | total loss: [1m[32m0.67898[0m[0m | time: 19.860s
[2K
| Adam | epoch: 004 | loss: 0.67898 - acc: 0.5856 -- iter: 096/382
[A[ATraining Step: 40  | total loss: [1m[32m0.67930[0m[0m | time: 31.611s
[2K
| Adam | epoch: 004 | loss: 0.67930 - acc: 0.5820 -- iter: 128/382
[A[ATraining Step: 41  | total loss: [1m[32m0.67712[0m[0m | time: 37.259s
[2K
| Adam | epoch: 004 | loss: 0.67712 - acc: 0.5842 -- iter: 160/382
[A[ATraining Step: 42  | total loss: [1m[32m0.67117[0m[0m | time: 52.508s
[2K
| Adam | epoch: 004 | loss: 0.67117 - acc: 0.5972 -- iter: 192/382
[A[ATraining Step: 43  | total loss: [1m[32m0.66137[0m[0m | time: 66.498s
[2K
| Adam | epoch: 004 | loss: 0.66137 - acc: 0.6131 -- iter: 224/382
[A[ATraining Step: 44  | total loss: [1m[32m0.66241[0m[0m | time: 69.683s
[2K
| Adam | epoch: 004 | loss: 0.66241 - acc: 0.6152 -- iter: 256/382
[A[ATraining Step: 45  | total loss: [1m[32m0.67024[0m[0m | time: 83.287s
[2K
| Adam | epoch: 004 | loss: 0.67024 - acc: 0.6062 -- iter: 288/382
[A[ATraining Step: 46  | total loss: [1m[32m0.65708[0m[0m | time: 91.628s
[2K
| Adam | epoch: 004 | loss: 0.65708 - acc: 0.6250 -- iter: 320/382
[A[ATraining Step: 47  | total loss: [1m[32m0.65114[0m[0m | time: 103.655s
[2K
| Adam | epoch: 004 | loss: 0.65114 - acc: 0.6301 -- iter: 352/382
[A[ATraining Step: 48  | total loss: [1m[32m0.64987[0m[0m | time: 137.680s
[2K
| Adam | epoch: 004 | loss: 0.64987 - acc: 0.6293 | val_loss: 0.70767 - val_acc: 0.5250 -- iter: 382/382
--
Training Step: 49  | total loss: [1m[32m0.64746[0m[0m | time: 7.044s
[2K
| Adam | epoch: 005 | loss: 0.64746 - acc: 0.6335 -- iter: 032/382
[A[ATraining Step: 50  | total loss: [1m[32m0.64890[0m[0m | time: 12.064s
[2K
| Adam | epoch: 005 | loss: 0.64890 - acc: 0.6274 -- iter: 064/382
[A[ATraining Step: 51  | total loss: [1m[32m0.65062[0m[0m | time: 18.009s
[2K
| Adam | epoch: 005 | loss: 0.65062 - acc: 0.6222 -- iter: 096/382
[A[ATraining Step: 52  | total loss: [1m[32m0.66120[0m[0m | time: 22.284s
[2K
| Adam | epoch: 005 | loss: 0.66120 - acc: 0.5939 -- iter: 128/382
[A[ATraining Step: 53  | total loss: [1m[32m0.66704[0m[0m | time: 31.657s
[2K
| Adam | epoch: 005 | loss: 0.66704 - acc: 0.5702 -- iter: 160/382
[A[ATraining Step: 54  | total loss: [1m[32m0.66769[0m[0m | time: 36.866s
[2K
| Adam | epoch: 005 | loss: 0.66769 - acc: 0.5736 -- iter: 192/382
[A[ATraining Step: 55  | total loss: [1m[32m0.66753[0m[0m | time: 41.467s
[2K
| Adam | epoch: 005 | loss: 0.66753 - acc: 0.5854 -- iter: 224/382
[A[ATraining Step: 56  | total loss: [1m[32m0.67142[0m[0m | time: 50.305s
[2K
| Adam | epoch: 005 | loss: 0.67142 - acc: 0.5646 -- iter: 256/382
[A[ATraining Step: 57  | total loss: [1m[32m0.67441[0m[0m | time: 63.511s
[2K
| Adam | epoch: 005 | loss: 0.67441 - acc: 0.5557 -- iter: 288/382
[A[ATraining Step: 58  | total loss: [1m[32m0.67268[0m[0m | time: 71.675s
[2K
| Adam | epoch: 005 | loss: 0.67268 - acc: 0.5737 -- iter: 320/382
[A[ATraining Step: 59  | total loss: [1m[32m0.66978[0m[0m | time: 83.256s
[2K
| Adam | epoch: 005 | loss: 0.66978 - acc: 0.5931 -- iter: 352/382
[A[ATraining Step: 60  | total loss: [1m[32m0.67103[0m[0m | time: 119.663s
[2K
| Adam | epoch: 005 | loss: 0.67103 - acc: 0.5850 | val_loss: 0.68120 - val_acc: 0.5250 -- iter: 382/382
--
Training Step: 61  | total loss: [1m[32m0.67319[0m[0m | time: 4.889s
[2K
| Adam | epoch: 006 | loss: 0.67319 - acc: 0.5739 -- iter: 032/382
[A[ATraining Step: 62  | total loss: [1m[32m0.67432[0m[0m | time: 18.435s
[2K
| Adam | epoch: 006 | loss: 0.67432 - acc: 0.5644 -- iter: 064/382
[A[ATraining Step: 63  | total loss: [1m[32m0.66726[0m[0m | time: 27.235s
[2K
| Adam | epoch: 006 | loss: 0.66726 - acc: 0.5721 -- iter: 096/382
[A[ATraining Step: 64  | total loss: [1m[32m0.66449[0m[0m | time: 34.793s
[2K
| Adam | epoch: 006 | loss: 0.66449 - acc: 0.5709 -- iter: 128/382
[A[ATraining Step: 65  | total loss: [1m[32m0.65925[0m[0m | time: 38.656s
[2K
| Adam | epoch: 006 | loss: 0.65925 - acc: 0.5786 -- iter: 160/382
[A[ATraining Step: 66  | total loss: [1m[32m0.65338[0m[0m | time: 45.467s
[2K
| Adam | epoch: 006 | loss: 0.65338 - acc: 0.5852 -- iter: 192/382
[A[ATraining Step: 67  | total loss: [1m[32m0.64985[0m[0m | time: 61.738s
[2K
| Adam | epoch: 006 | loss: 0.64985 - acc: 0.5825 -- iter: 224/382
[A[ATraining Step: 68  | total loss: [1m[32m0.64680[0m[0m | time: 64.654s
[2K
| Adam | epoch: 006 | loss: 0.64680 - acc: 0.5875 -- iter: 256/382
[A[ATraining Step: 69  | total loss: [1m[32m0.64173[0m[0m | time: 68.498s
[2K
| Adam | epoch: 006 | loss: 0.64173 - acc: 0.5992 -- iter: 288/382
[A[ATraining Step: 70  | total loss: [1m[32m0.63892[0m[0m | time: 75.096s
[2K
| Adam | epoch: 006 | loss: 0.63892 - acc: 0.5914 -- iter: 320/382
[A[ATraining Step: 71  | total loss: [1m[32m0.63127[0m[0m | time: 89.835s
[2K
| Adam | epoch: 006 | loss: 0.63127 - acc: 0.6023 -- iter: 352/382
[A[ATraining Step: 72  | total loss: [1m[32m0.62727[0m[0m | time: 116.253s
[2K
| Adam | epoch: 006 | loss: 0.62727 - acc: 0.6014 | val_loss: 0.65072 - val_acc: 0.5250 -- iter: 382/382
--
Training Step: 73  | total loss: [1m[32m0.62549[0m[0m | time: 7.310s
[2K
| Adam | epoch: 007 | loss: 0.62549 - acc: 0.5970 -- iter: 032/382
[A[ATraining Step: 74  | total loss: [1m[32m0.62200[0m[0m | time: 12.764s
[2K
| Adam | epoch: 007 | loss: 0.62200 - acc: 0.6138 -- iter: 064/382
[A[ATraining Step: 75  | total loss: [1m[32m0.61390[0m[0m | time: 15.575s
[2K
| Adam | epoch: 007 | loss: 0.61390 - acc: 0.6252 -- iter: 096/382
[A[ATraining Step: 76  | total loss: [1m[32m0.61135[0m[0m | time: 28.467s
[2K
| Adam | epoch: 007 | loss: 0.61135 - acc: 0.6185 -- iter: 128/382
[A[ATraining Step: 77  | total loss: [1m[32m0.61176[0m[0m | time: 33.897s
[2K
| Adam | epoch: 007 | loss: 0.61176 - acc: 0.6093 -- iter: 160/382
[A[ATraining Step: 78  | total loss: [1m[32m0.62772[0m[0m | time: 38.860s
[2K
| Adam | epoch: 007 | loss: 0.62772 - acc: 0.5943 -- iter: 192/382
[A[ATraining Step: 79  | total loss: [1m[32m0.63200[0m[0m | time: 45.804s
[2K
| Adam | epoch: 007 | loss: 0.63200 - acc: 0.5742 -- iter: 224/382
[A[ATraining Step: 80  | total loss: [1m[32m0.63766[0m[0m | time: 55.631s
[2K
| Adam | epoch: 007 | loss: 0.63766 - acc: 0.5730 -- iter: 256/382
[A[ATraining Step: 81  | total loss: [1m[32m0.64278[0m[0m | time: 61.652s
[2K
| Adam | epoch: 007 | loss: 0.64278 - acc: 0.5751 -- iter: 288/382
[A[ATraining Step: 82  | total loss: [1m[32m0.64846[0m[0m | time: 70.832s
[2K
| Adam | epoch: 007 | loss: 0.64846 - acc: 0.5614 -- iter: 320/382
[A[ATraining Step: 83  | total loss: [1m[32m0.65616[0m[0m | time: 76.579s
[2K
| Adam | epoch: 007 | loss: 0.65616 - acc: 0.5240 -- iter: 352/382
[A[ATraining Step: 84  | total loss: [1m[32m0.66151[0m[0m | time: 105.844s
[2K
| Adam | epoch: 007 | loss: 0.66151 - acc: 0.4997 | val_loss: 0.69344 - val_acc: 0.4750 -- iter: 382/382
--
Training Step: 85  | total loss: [1m[32m0.66578[0m[0m | time: 8.467s
[2K
| Adam | epoch: 008 | loss: 0.66578 - acc: 0.4747 -- iter: 032/382
[A[ATraining Step: 86  | total loss: [1m[32m0.66849[0m[0m | time: 11.457s
[2K
| Adam | epoch: 008 | loss: 0.66849 - acc: 0.4773 -- iter: 064/382
[A[ATraining Step: 87  | total loss: [1m[32m0.67094[0m[0m | time: 18.532s
[2K
| Adam | epoch: 008 | loss: 0.67094 - acc: 0.4795 -- iter: 096/382
[A[ATraining Step: 88  | total loss: [1m[32m0.67311[0m[0m | time: 26.421s
[2K
| Adam | epoch: 008 | loss: 0.67311 - acc: 0.4847 -- iter: 128/382
[A[ATraining Step: 89  | total loss: [1m[32m0.67481[0m[0m | time: 35.879s
[2K
| Adam | epoch: 008 | loss: 0.67481 - acc: 0.4894 -- iter: 160/382
[A[ATraining Step: 90  | total loss: [1m[32m0.67534[0m[0m | time: 42.547s
[2K
| Adam | epoch: 008 | loss: 0.67534 - acc: 0.5029 -- iter: 192/382
[A[ATraining Step: 91  | total loss: [1m[32m0.67534[0m[0m | time: 48.578s
[2K
| Adam | epoch: 008 | loss: 0.67534 - acc: 0.5160 -- iter: 224/382
[A[ATraining Step: 92  | total loss: [1m[32m0.67431[0m[0m | time: 59.829s
[2K
| Adam | epoch: 008 | loss: 0.67431 - acc: 0.5277 -- iter: 256/382
[A[ATraining Step: 93  | total loss: [1m[32m0.67180[0m[0m | time: 63.742s
[2K
| Adam | epoch: 008 | loss: 0.67180 - acc: 0.5406 -- iter: 288/382
[A[ATraining Step: 94  | total loss: [1m[32m0.68191[0m[0m | time: 72.452s
[2K
| Adam | epoch: 008 | loss: 0.68191 - acc: 0.5303 -- iter: 320/382
[A[ATraining Step: 95  | total loss: [1m[32m0.67604[0m[0m | time: 82.459s
[2K
| Adam | epoch: 008 | loss: 0.67604 - acc: 0.5460 -- iter: 352/382
[A[ATraining Step: 96  | total loss: [1m[32m0.67674[0m[0m | time: 107.019s
[2K
| Adam | epoch: 008 | loss: 0.67674 - acc: 0.5476 | val_loss: 0.69983 - val_acc: 0.5250 -- iter: 382/382
--
Training Step: 97  | total loss: [1m[32m0.67503[0m[0m | time: 5.729s
[2K
| Adam | epoch: 009 | loss: 0.67503 - acc: 0.5554 -- iter: 032/382
[A[ATraining Step: 98  | total loss: [1m[32m0.67653[0m[0m | time: 13.470s
[2K
| Adam | epoch: 009 | loss: 0.67653 - acc: 0.5561 -- iter: 064/382
[A[ATraining Step: 99  | total loss: [1m[32m0.67364[0m[0m | time: 26.947s
[2K
| Adam | epoch: 009 | loss: 0.67364 - acc: 0.5661 -- iter: 096/382
[A[ATraining Step: 100  | total loss: [1m[32m0.67278[0m[0m | time: 28.342s
[2K
| Adam | epoch: 009 | loss: 0.67278 - acc: 0.5689 -- iter: 128/382
[A[ATraining Step: 101  | total loss: [1m[32m0.67222[0m[0m | time: 33.005s
[2K
| Adam | epoch: 009 | loss: 0.67222 - acc: 0.5714 -- iter: 160/382
[A[ATraining Step: 102  | total loss: [1m[32m0.66948[0m[0m | time: 39.863s
[2K
| Adam | epoch: 009 | loss: 0.66948 - acc: 0.5798 -- iter: 192/382
[A[ATraining Step: 103  | total loss: [1m[32m0.67373[0m[0m | time: 41.152s
[2K
| Adam | epoch: 009 | loss: 0.67373 - acc: 0.5719 -- iter: 224/382
[A[ATraining Step: 104  | total loss: [1m[32m0.67818[0m[0m | time: 47.531s
[2K
| Adam | epoch: 009 | loss: 0.67818 - acc: 0.5613 -- iter: 256/382
[A[ATraining Step: 105  | total loss: [1m[32m0.68119[0m[0m | time: 49.978s
[2K
| Adam | epoch: 009 | loss: 0.68119 - acc: 0.5519 -- iter: 288/382
[A[ATraining Step: 106  | total loss: [1m[32m0.67837[0m[0m | time: 60.265s
[2K
| Adam | epoch: 009 | loss: 0.67837 - acc: 0.5592 -- iter: 320/382
[A[ATraining Step: 107  | total loss: [1m[32m0.67796[0m[0m | time: 70.005s
[2K
| Adam | epoch: 009 | loss: 0.67796 - acc: 0.5595 -- iter: 352/382
[A[ATraining Step: 108  | total loss: [1m[32m0.67552[0m[0m | time: 96.398s
[2K
| Adam | epoch: 009 | loss: 0.67552 - acc: 0.5692 | val_loss: 0.68381 - val_acc: 0.5250 -- iter: 382/382
--
Training Step: 109  | total loss: [1m[32m0.67102[0m[0m | time: 3.633s
[2K
| Adam | epoch: 010 | loss: 0.67102 - acc: 0.5841 -- iter: 032/382
[A[ATraining Step: 110  | total loss: [1m[32m0.67232[0m[0m | time: 11.030s
[2K
| Adam | epoch: 010 | loss: 0.67232 - acc: 0.5820 -- iter: 064/382
[A[ATraining Step: 111  | total loss: [1m[32m0.67082[0m[0m | time: 28.772s
[2K
| Adam | epoch: 010 | loss: 0.67082 - acc: 0.5800 -- iter: 096/382
[A[ATraining Step: 112  | total loss: [1m[32m0.66111[0m[0m | time: 41.561s
[2K
| Adam | epoch: 010 | loss: 0.66111 - acc: 0.5939 -- iter: 128/382
[A[ATraining Step: 113  | total loss: [1m[32m0.66226[0m[0m | time: 52.340s
[2K
| Adam | epoch: 010 | loss: 0.66226 - acc: 0.5939 -- iter: 160/382
[A[ATraining Step: 114  | total loss: [1m[32m0.65219[0m[0m | time: 57.616s
[2K
| Adam | epoch: 010 | loss: 0.65219 - acc: 0.6064 -- iter: 192/382
[A[ATraining Step: 115  | total loss: [1m[32m0.67916[0m[0m | time: 67.885s
[2K
| Adam | epoch: 010 | loss: 0.67916 - acc: 0.5832 -- iter: 224/382
[A[ATraining Step: 116  | total loss: [1m[32m0.68490[0m[0m | time: 73.336s
[2K
| Adam | epoch: 010 | loss: 0.68490 - acc: 0.5749 -- iter: 256/382
[A[ATraining Step: 117  | total loss: [1m[32m0.68243[0m[0m | time: 76.499s
[2K
| Adam | epoch: 010 | loss: 0.68243 - acc: 0.5741 -- iter: 288/382
[A[ATraining Step: 118  | total loss: [1m[32m0.68131[0m[0m | time: 88.341s
[2K
| Adam | epoch: 010 | loss: 0.68131 - acc: 0.5733 -- iter: 320/382
[A[ATraining Step: 119  | total loss: [1m[32m0.68222[0m[0m | time: 99.694s
[2K
| Adam | epoch: 010 | loss: 0.68222 - acc: 0.5598 -- iter: 352/382
[A[ATraining Step: 120  | total loss: [1m[32m0.68197[0m[0m | time: 131.688s
[2K
| Adam | epoch: 010 | loss: 0.68197 - acc: 0.5725 | val_loss: 0.69163 - val_acc: 0.5250 -- iter: 382/382
--
Training Step: 121  | total loss: [1m[32m0.68250[0m[0m | time: 4.826s
[2K
| Adam | epoch: 011 | loss: 0.68250 - acc: 0.5747 -- iter: 032/382
[A[ATraining Step: 122  | total loss: [1m[32m0.68291[0m[0m | time: 11.609s
[2K
| Adam | epoch: 011 | loss: 0.68291 - acc: 0.5797 -- iter: 064/382
[A[ATraining Step: 123  | total loss: [1m[32m0.68246[0m[0m | time: 17.941s
[2K
| Adam | epoch: 011 | loss: 0.68246 - acc: 0.5998 -- iter: 096/382
[A[ATraining Step: 124  | total loss: [1m[32m0.68315[0m[0m | time: 27.268s
[2K
| Adam | epoch: 011 | loss: 0.68315 - acc: 0.5961 -- iter: 128/382
[A[ATraining Step: 125  | total loss: [1m[32m0.68356[0m[0m | time: 30.072s
[2K
| Adam | epoch: 011 | loss: 0.68356 - acc: 0.5959 -- iter: 160/382
[A[ATraining Step: 126  | total loss: [1m[32m0.68408[0m[0m | time: 38.724s
[2K
| Adam | epoch: 011 | loss: 0.68408 - acc: 0.5925 -- iter: 192/382
[A[ATraining Step: 127  | total loss: [1m[32m0.68335[0m[0m | time: 42.040s
[2K
| Adam | epoch: 011 | loss: 0.68335 - acc: 0.6020 -- iter: 224/382
[A[ATraining Step: 128  | total loss: [1m[32m0.68301[0m[0m | time: 44.673s
[2K
| Adam | epoch: 011 | loss: 0.68301 - acc: 0.6043 -- iter: 256/382
[A[ATraining Step: 129  | total loss: [1m[32m0.68166[0m[0m | time: 51.211s
[2K
| Adam | epoch: 011 | loss: 0.68166 - acc: 0.6095 -- iter: 288/382
[A[ATraining Step: 130  | total loss: [1m[32m0.68167[0m[0m | time: 61.042s
[2K
| Adam | epoch: 011 | loss: 0.68167 - acc: 0.6052 -- iter: 320/382
[A[ATraining Step: 131  | total loss: [1m[32m0.68169[0m[0m | time: 67.720s
[2K
| Adam | epoch: 011 | loss: 0.68169 - acc: 0.6014 -- iter: 352/382
[A[ATraining Step: 132  | total loss: [1m[32m0.67629[0m[0m | time: 83.549s
[2K
| Adam | epoch: 011 | loss: 0.67629 - acc: 0.6100 | val_loss: 0.72003 - val_acc: 0.5250 -- iter: 382/382
--
Training Step: 133  | total loss: [1m[32m0.68219[0m[0m | time: 3.498s
[2K
| Adam | epoch: 012 | loss: 0.68219 - acc: 0.6021 -- iter: 032/382
[A[ATraining Step: 134  | total loss: [1m[32m0.67745[0m[0m | time: 13.788s
[2K
| Adam | epoch: 012 | loss: 0.67745 - acc: 0.6075 -- iter: 064/382
[A[ATraining Step: 135  | total loss: [1m[32m0.67708[0m[0m | time: 19.061s
[2K
| Adam | epoch: 012 | loss: 0.67708 - acc: 0.6062 -- iter: 096/382
[A[ATraining Step: 136  | total loss: [1m[32m0.67965[0m[0m | time: 22.217s
[2K
| Adam | epoch: 012 | loss: 0.67965 - acc: 0.5987 -- iter: 128/382
[A[ATraining Step: 137  | total loss: [1m[32m0.67864[0m[0m | time: 29.898s
[2K
| Adam | epoch: 012 | loss: 0.67864 - acc: 0.5982 -- iter: 160/382
[A[ATraining Step: 138  | total loss: [1m[32m0.67892[0m[0m | time: 44.945s
[2K
| Adam | epoch: 012 | loss: 0.67892 - acc: 0.5946 -- iter: 192/382
[A[ATraining Step: 139  | total loss: [1m[32m0.67376[0m[0m | time: 58.743s
[2K
| Adam | epoch: 012 | loss: 0.67376 - acc: 0.6101 -- iter: 224/382
[A[ATraining Step: 140  | total loss: [1m[32m0.67692[0m[0m | time: 66.658s
[2K
| Adam | epoch: 012 | loss: 0.67692 - acc: 0.5960 -- iter: 256/382
[A[ATraining Step: 141  | total loss: [1m[32m0.67512[0m[0m | time: 75.923s
[2K
| Adam | epoch: 012 | loss: 0.67512 - acc: 0.5989 -- iter: 288/382
[A[ATraining Step: 142  | total loss: [1m[32m0.67189[0m[0m | time: 79.281s
[2K
| Adam | epoch: 012 | loss: 0.67189 - acc: 0.6046 -- iter: 320/382
[A[ATraining Step: 143  | total loss: [1m[32m0.67230[0m[0m | time: 89.389s
[2K
| Adam | epoch: 012 | loss: 0.67230 - acc: 0.6008 -- iter: 352/382
[A[ATraining Step: 144  | total loss: [1m[32m0.67264[0m[0m | time: 118.099s
[2K
| Adam | epoch: 012 | loss: 0.67264 - acc: 0.5974 | val_loss: 0.71217 - val_acc: 0.5250 -- iter: 382/382
--
Training Step: 145  | total loss: [1m[32m0.66959[0m[0m | time: 5.027s
[2K
| Adam | epoch: 013 | loss: 0.66959 - acc: 0.6002 -- iter: 032/382
[A[ATraining Step: 146  | total loss: [1m[32m0.67211[0m[0m | time: 13.615s
[2K
| Adam | epoch: 013 | loss: 0.67211 - acc: 0.5933 -- iter: 064/382
[A[ATraining Step: 147  | total loss: [1m[32m0.67582[0m[0m | time: 18.280s
[2K
| Adam | epoch: 013 | loss: 0.67582 - acc: 0.5840 -- iter: 096/382
[A[ATraining Step: 148  | total loss: [1m[32m0.67053[0m[0m | time: 29.388s
[2K
| Adam | epoch: 013 | loss: 0.67053 - acc: 0.5943 -- iter: 128/382
[A[ATraining Step: 149  | total loss: [1m[32m0.66978[0m[0m | time: 35.445s
[2K
| Adam | epoch: 013 | loss: 0.66978 - acc: 0.5943 -- iter: 160/382
[A[ATraining Step: 150  | total loss: [1m[32m0.67153[0m[0m | time: 43.823s
[2K
| Adam | epoch: 013 | loss: 0.67153 - acc: 0.5848 -- iter: 192/382
[A[ATraining Step: 151  | total loss: [1m[32m0.66835[0m[0m | time: 49.192s
[2K
| Adam | epoch: 013 | loss: 0.66835 - acc: 0.5920 -- iter: 224/382
[A[ATraining Step: 152  | total loss: [1m[32m0.66619[0m[0m | time: 64.529s
[2K
| Adam | epoch: 013 | loss: 0.66619 - acc: 0.5922 -- iter: 256/382
[A[ATraining Step: 153  | total loss: [1m[32m0.66774[0m[0m | time: 71.042s
[2K
| Adam | epoch: 013 | loss: 0.66774 - acc: 0.5829 -- iter: 288/382
[A[ATraining Step: 154  | total loss: [1m[32m0.67239[0m[0m | time: 75.670s
[2K
| Adam | epoch: 013 | loss: 0.67239 - acc: 0.5684 -- iter: 320/382
[A[ATraining Step: 155  | total loss: [1m[32m0.67156[0m[0m | time: 84.928s
[2K
| Adam | epoch: 013 | loss: 0.67156 - acc: 0.5678 -- iter: 352/382
[A[ATraining Step: 156  | total loss: [1m[32m0.66798[0m[0m | time: 112.037s
[2K
| Adam | epoch: 013 | loss: 0.66798 - acc: 0.5710 | val_loss: 0.67016 - val_acc: 0.5250 -- iter: 382/382
--
Training Step: 157  | total loss: [1m[32m0.66422[0m[0m | time: 14.646s
[2K
| Adam | epoch: 014 | loss: 0.66422 - acc: 0.5739 -- iter: 032/382
[A[ATraining Step: 158  | total loss: [1m[32m0.65888[0m[0m | time: 26.921s
[2K
| Adam | epoch: 014 | loss: 0.65888 - acc: 0.5822 -- iter: 064/382
[A[ATraining Step: 159  | total loss: [1m[32m0.65285[0m[0m | time: 37.652s
[2K
| Adam | epoch: 014 | loss: 0.65285 - acc: 0.5864 -- iter: 096/382
[A[ATraining Step: 160  | total loss: [1m[32m0.65222[0m[0m | time: 44.899s
[2K
| Adam | epoch: 014 | loss: 0.65222 - acc: 0.5903 -- iter: 128/382
[A[ATraining Step: 161  | total loss: [1m[32m0.64695[0m[0m | time: 52.045s
[2K
| Adam | epoch: 014 | loss: 0.64695 - acc: 0.5969 -- iter: 160/382
[A[ATraining Step: 162  | total loss: [1m[32m0.63348[0m[0m | time: 57.901s
[2K
| Adam | epoch: 014 | loss: 0.63348 - acc: 0.6122 -- iter: 192/382
[A[ATraining Step: 163  | total loss: [1m[32m0.61523[0m[0m | time: 60.470s
[2K
| Adam | epoch: 014 | loss: 0.61523 - acc: 0.6291 -- iter: 224/382
[A[ATraining Step: 164  | total loss: [1m[32m0.63740[0m[0m | time: 62.993s
[2K
| Adam | epoch: 014 | loss: 0.63740 - acc: 0.6099 -- iter: 256/382
[A[ATraining Step: 165  | total loss: [1m[32m0.63194[0m[0m | time: 66.629s
[2K
| Adam | epoch: 014 | loss: 0.63194 - acc: 0.6083 -- iter: 288/382
[A[ATraining Step: 166  | total loss: [1m[32m0.62846[0m[0m | time: 71.081s
[2K
| Adam | epoch: 014 | loss: 0.62846 - acc: 0.6100 -- iter: 320/382
[A[ATraining Step: 167  | total loss: [1m[32m0.62940[0m[0m | time: 76.220s
[2K
| Adam | epoch: 014 | loss: 0.62940 - acc: 0.6115 -- iter: 352/382
[A[ATraining Step: 168  | total loss: [1m[32m0.62705[0m[0m | time: 103.261s
[2K
| Adam | epoch: 014 | loss: 0.62705 - acc: 0.6191 | val_loss: 0.63995 - val_acc: 0.5250 -- iter: 382/382
--
Training Step: 169  | total loss: [1m[32m0.62581[0m[0m | time: 7.908s
[2K
| Adam | epoch: 015 | loss: 0.62581 - acc: 0.6105 -- iter: 032/382
[A[ATraining Step: 170  | total loss: [1m[32m0.62339[0m[0m | time: 13.204s
[2K
| Adam | epoch: 015 | loss: 0.62339 - acc: 0.6028 -- iter: 064/382
[A[ATraining Step: 171  | total loss: [1m[32m0.62061[0m[0m | time: 17.269s
[2K
| Adam | epoch: 015 | loss: 0.62061 - acc: 0.6050 -- iter: 096/382
[A[ATraining Step: 172  | total loss: [1m[32m0.63160[0m[0m | time: 27.861s
[2K
| Adam | epoch: 015 | loss: 0.63160 - acc: 0.5851 -- iter: 128/382
[A[ATraining Step: 173  | total loss: [1m[32m0.61626[0m[0m | time: 34.834s
[2K
| Adam | epoch: 015 | loss: 0.61626 - acc: 0.6016 -- iter: 160/382
[A[ATraining Step: 174  | total loss: [1m[32m0.60586[0m[0m | time: 42.888s
[2K
| Adam | epoch: 015 | loss: 0.60586 - acc: 0.6008 -- iter: 192/382
[A[ATraining Step: 175  | total loss: [1m[32m0.61092[0m[0m | time: 50.508s
[2K
| Adam | epoch: 015 | loss: 0.61092 - acc: 0.5908 -- iter: 224/382
[A[ATraining Step: 176  | total loss: [1m[32m0.60162[0m[0m | time: 55.633s
[2K
| Adam | epoch: 015 | loss: 0.60162 - acc: 0.5911 -- iter: 256/382
[A[ATraining Step: 177  | total loss: [1m[32m0.60594[0m[0m | time: 58.633s
[2K
| Adam | epoch: 015 | loss: 0.60594 - acc: 0.5757 -- iter: 288/382
[A[ATraining Step: 178  | total loss: [1m[32m0.60163[0m[0m | time: 61.416s
[2K
| Adam | epoch: 015 | loss: 0.60163 - acc: 0.5838 -- iter: 320/382
[A[ATraining Step: 179  | total loss: [1m[32m0.59840[0m[0m | time: 69.638s
[2K
| Adam | epoch: 015 | loss: 0.59840 - acc: 0.5973 -- iter: 352/382
[A[ATraining Step: 180  | total loss: [1m[32m0.58978[0m[0m | time: 118.396s
[2K
| Adam | epoch: 015 | loss: 0.58978 - acc: 0.6000 | val_loss: 0.77703 - val_acc: 0.5250 -- iter: 382/382
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.7541074909495963
Validation AUPRC:0.7771960225744385
Test AUC:0.7944004524886878
Test AUPRC:0.8274040186472407
BestTestF1Score	0.78	0.39	0.69	0.66	0.96	65	34	18	3	0.61
BestTestMCCScore	0.74	0.43	0.72	0.77	0.71	48	14	38	20	0.87
BestTestAccuracyScore	0.74	0.43	0.72	0.77	0.71	48	14	38	20	0.87
BestValidationF1Score	0.73	0.36	0.68	0.64	0.86	54	30	27	9	0.61
BestValidationMCC	0.67	0.43	0.7	0.8	0.57	36	9	48	27	0.87
BestValidationAccuracy	0.67	0.43	0.7	0.8	0.57	36	9	48	27	0.87
TestPredictions (Threshold:0.87)
CHEMBL3645913,TP,ACT,0.9900000095367432	CHEMBL2172632,TP,ACT,0.9100000262260437	CHEMBL3640976,TP,ACT,0.9900000095367432	CHEMBL41013,TN,INACT,0.7400000095367432	CHEMBL1916045,TN,INACT,0.7300000190734863	CHEMBL3640969,FN,ACT,0.8199999928474426	CHEMBL3590225,TP,ACT,0.9700000286102295	CHEMBL601689,TP,ACT,0.9399999976158142	CHEMBL1916042,TN,INACT,0.5199999809265137	CHEMBL3646527,TP,ACT,0.9200000166893005	CHEMBL3681132,TP,ACT,0.9900000095367432	CHEMBL3753037,FP,INACT,0.8799999952316284	CHEMBL388946,FP,INACT,0.9800000190734863	CHEMBL3643362,TP,ACT,0.9399999976158142	CHEMBL1644599,FN,ACT,0.8100000023841858	CHEMBL1090434,FN,ACT,0.8500000238418579	CHEMBL1914722,TP,ACT,0.9300000071525574	CHEMBL3652257,TP,ACT,0.9900000095367432	CHEMBL3643155,TP,ACT,0.9900000095367432	CHEMBL3408277,FN,ACT,0.8100000023841858	CHEMBL551440,TP,ACT,0.9800000190734863	CHEMBL2165027,TP,ACT,0.8999999761581421	CHEMBL3698296,TP,ACT,0.9900000095367432	CHEMBL3799345,TN,INACT,0.5099999904632568	CHEMBL3799956,TN,INACT,0.5099999904632568	CHEMBL512309,TN,INACT,0.6499999761581421	CHEMBL3645918,TP,ACT,0.9700000286102295	CHEMBL3641006,TP,ACT,1.0	CHEMBL3646566,TP,ACT,0.9900000095367432	CHEMBL511807,TN,INACT,0.7300000190734863	CHEMBL1092546,TP,ACT,0.8999999761581421	CHEMBL3134267,TP,ACT,0.8899999856948853	CHEMBL3753665,FN,ACT,0.5099999904632568	CHEMBL2064567,TN,INACT,0.8600000143051147	CHEMBL2030444,FP,INACT,0.9800000190734863	CHEMBL450243,TN,INACT,0.8500000238418579	CHEMBL2204551,TN,INACT,0.8100000023841858	CHEMBL3648885,TP,ACT,1.0	CHEMBL1644594,TP,ACT,0.9900000095367432	CHEMBL2158428,TN,INACT,0.8299999833106995	CHEMBL575576,TP,ACT,0.9900000095367432	CHEMBL1914737,TP,ACT,0.949999988079071	CHEMBL1242382,FP,INACT,0.9200000166893005	CHEMBL3221622,FN,ACT,0.7699999809265137	CHEMBL3600785,TN,INACT,0.75	CHEMBL3770633,TP,ACT,0.9900000095367432	CHEMBL1914733,FN,ACT,0.6200000047683716	CHEMBL1241389,TN,INACT,0.7900000214576721	CHEMBL589142,TP,ACT,0.9700000286102295	CHEMBL3752760,FN,ACT,0.5099999904632568	CHEMBL594793,TP,ACT,0.8700000047683716	CHEMBL3670212,TP,ACT,0.9399999976158142	CHEMBL1242209,FN,ACT,0.6100000143051147	CHEMBL2375954,FN,ACT,0.8399999737739563	CHEMBL2314282,FP,INACT,0.9800000190734863	CHEMBL1914728,TP,ACT,0.9399999976158142	CHEMBL1938641,TN,INACT,0.6000000238418579	CHEMBL1241440,TN,INACT,0.5400000214576721	CHEMBL3646588,TP,ACT,1.0	CHEMBL3600782,FP,INACT,0.8999999761581421	CHEMBL3643185,TP,ACT,0.9700000286102295	CHEMBL583194,TP,ACT,0.9300000071525574	CHEMBL2322777,FN,ACT,0.7799999713897705	CHEMBL3360231,TN,INACT,0.5	CHEMBL3589320,TN,INACT,0.6100000143051147	CHEMBL3643168,TP,ACT,1.0	CHEMBL3754755,FP,INACT,0.9399999976158142	CHEMBL3646565,FN,ACT,0.75	CHEMBL3698346,TP,ACT,0.9599999785423279	CHEMBL3648993,FN,ACT,0.8199999928474426	CHEMBL1889525,TN,INACT,0.6200000047683716	CHEMBL3765681,FP,INACT,0.8899999856948853	CHEMBL1327614,TN,INACT,0.6800000071525574	CHEMBL3641008,TP,ACT,1.0	CHEMBL2152138,FN,ACT,0.75	CHEMBL3698628,FN,ACT,0.7599999904632568	CHEMBL2152264,TP,ACT,0.8999999761581421	CHEMBL1213084,TP,ACT,0.9800000190734863	CHEMBL58782,TN,INACT,0.5099999904632568	CHEMBL1258634,TP,ACT,0.9800000190734863	CHEMBL1760168,TP,ACT,0.9700000286102295	CHEMBL3318706,TP,ACT,0.9800000190734863	CHEMBL2064343,TP,ACT,0.8999999761581421	CHEMBL210991,FN,ACT,0.7900000214576721	CHEMBL565432,TN,INACT,0.550000011920929	CHEMBL1645098,TN,INACT,0.5	CHEMBL1762242,TN,INACT,0.5099999904632568	CHEMBL2071336,FN,ACT,0.5099999904632568	CHEMBL3589373,FP,INACT,0.9200000166893005	CHEMBL3643129,TP,ACT,1.0	CHEMBL3109125,TN,INACT,0.8500000238418579	CHEMBL3643089,TP,ACT,1.0	CHEMBL2087474,TP,ACT,0.9300000071525574	CHEMBL3649439,FN,ACT,0.8199999928474426	CHEMBL72683,FP,INACT,0.9800000190734863	CHEMBL3589378,TN,INACT,0.6100000143051147	CHEMBL3347670,TN,INACT,0.5099999904632568	CHEMBL1916044,TN,INACT,0.6100000143051147	CHEMBL173928,TN,INACT,0.5299999713897705	CHEMBL3112863,FN,ACT,0.800000011920929	CHEMBL270638,TP,ACT,0.9800000190734863	CHEMBL2018271,TN,INACT,0.5799999833106995	CHEMBL3765248,FP,INACT,0.9700000286102295	CHEMBL1242750,FN,ACT,0.8399999737739563	CHEMBL1242204,TN,INACT,0.7599999904632568	CHEMBL3589321,TN,INACT,0.7400000095367432	CHEMBL3645830,TP,ACT,0.9800000190734863	CHEMBL3589322,FP,INACT,1.0	CHEMBL1938635,FP,INACT,0.9300000071525574	CHEMBL2206918,TP,ACT,0.9599999785423279	CHEMBL3589375,TN,INACT,0.75	CHEMBL2018413,TN,INACT,0.5099999904632568	CHEMBL3643105,TP,ACT,1.0	CHEMBL3649436,TP,ACT,0.9700000286102295	CHEMBL3765457,FP,INACT,0.9300000071525574	CHEMBL2158424,TN,INACT,0.5299999713897705	CHEMBL260122,TN,INACT,0.5199999809265137	CHEMBL1945950,TN,INACT,0.8199999928474426	CHEMBL1242661,TN,INACT,0.5799999833106995	CHEMBL1938639,TN,INACT,0.550000011920929	

