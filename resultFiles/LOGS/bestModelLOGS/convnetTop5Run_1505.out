CNNModel CHEMBL4361 adam 0.001 15 256 0 0.6 False True
Number of active compounds :	503
Number of inactive compounds :	338
---------------------------------
Run id: CNNModel_CHEMBL4361_adam_0.001_15_256_0_0.6_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL4361_adam_0.001_15_256_0.6_True/
---------------------------------
Training samples: 522
Validation samples: 164
--
Training Step: 1  | time: 0.763s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/522
[A[ATraining Step: 2  | total loss: [1m[32m0.62410[0m[0m | time: 1.412s
[2K
| Adam | epoch: 001 | loss: 0.62410 - acc: 0.4500 -- iter: 064/522
[A[ATraining Step: 3  | total loss: [1m[32m0.67700[0m[0m | time: 2.375s
[2K
| Adam | epoch: 001 | loss: 0.67700 - acc: 0.5420 -- iter: 096/522
[A[ATraining Step: 4  | total loss: [1m[32m0.65675[0m[0m | time: 2.980s
[2K
| Adam | epoch: 001 | loss: 0.65675 - acc: 0.6511 -- iter: 128/522
[A[ATraining Step: 5  | total loss: [1m[32m0.73454[0m[0m | time: 3.607s
[2K
| Adam | epoch: 001 | loss: 0.73454 - acc: 0.5681 -- iter: 160/522
[A[ATraining Step: 6  | total loss: [1m[32m0.72022[0m[0m | time: 4.243s
[2K
| Adam | epoch: 001 | loss: 0.72022 - acc: 0.5444 -- iter: 192/522
[A[ATraining Step: 7  | total loss: [1m[32m0.69016[0m[0m | time: 4.866s
[2K
| Adam | epoch: 001 | loss: 0.69016 - acc: 0.5928 -- iter: 224/522
[A[ATraining Step: 8  | total loss: [1m[32m0.68101[0m[0m | time: 5.513s
[2K
| Adam | epoch: 001 | loss: 0.68101 - acc: 0.6285 -- iter: 256/522
[A[ATraining Step: 9  | total loss: [1m[32m0.68281[0m[0m | time: 6.111s
[2K
| Adam | epoch: 001 | loss: 0.68281 - acc: 0.6101 -- iter: 288/522
[A[ATraining Step: 10  | total loss: [1m[32m0.68333[0m[0m | time: 6.747s
[2K
| Adam | epoch: 001 | loss: 0.68333 - acc: 0.6175 -- iter: 320/522
[A[ATraining Step: 11  | total loss: [1m[32m0.68581[0m[0m | time: 7.386s
[2K
| Adam | epoch: 001 | loss: 0.68581 - acc: 0.5915 -- iter: 352/522
[A[ATraining Step: 12  | total loss: [1m[32m0.68717[0m[0m | time: 7.996s
[2K
| Adam | epoch: 001 | loss: 0.68717 - acc: 0.5784 -- iter: 384/522
[A[ATraining Step: 13  | total loss: [1m[32m0.68665[0m[0m | time: 8.611s
[2K
| Adam | epoch: 001 | loss: 0.68665 - acc: 0.5850 -- iter: 416/522
[A[ATraining Step: 14  | total loss: [1m[32m0.69082[0m[0m | time: 9.221s
[2K
| Adam | epoch: 001 | loss: 0.69082 - acc: 0.5374 -- iter: 448/522
[A[ATraining Step: 15  | total loss: [1m[32m0.68877[0m[0m | time: 9.823s
[2K
| Adam | epoch: 001 | loss: 0.68877 - acc: 0.5595 -- iter: 480/522
[A[ATraining Step: 16  | total loss: [1m[32m0.68214[0m[0m | time: 10.441s
[2K
| Adam | epoch: 001 | loss: 0.68214 - acc: 0.6192 -- iter: 512/522
[A[ATraining Step: 17  | total loss: [1m[32m0.68224[0m[0m | time: 11.688s
[2K
| Adam | epoch: 001 | loss: 0.68224 - acc: 0.6100 | val_loss: 0.67757 - val_acc: 0.5976 -- iter: 522/522
--
Training Step: 18  | total loss: [1m[32m0.67086[0m[0m | time: 0.244s
[2K
| Adam | epoch: 002 | loss: 0.67086 - acc: 0.6758 -- iter: 032/522
[A[ATraining Step: 19  | total loss: [1m[32m0.65823[0m[0m | time: 0.857s
[2K
| Adam | epoch: 002 | loss: 0.65823 - acc: 0.7172 -- iter: 064/522
[A[ATraining Step: 20  | total loss: [1m[32m0.66371[0m[0m | time: 1.500s
[2K
| Adam | epoch: 002 | loss: 0.66371 - acc: 0.6775 -- iter: 096/522
[A[ATraining Step: 21  | total loss: [1m[32m0.66355[0m[0m | time: 2.199s
[2K
| Adam | epoch: 002 | loss: 0.66355 - acc: 0.6612 -- iter: 128/522
[A[ATraining Step: 22  | total loss: [1m[32m0.65014[0m[0m | time: 2.805s
[2K
| Adam | epoch: 002 | loss: 0.65014 - acc: 0.6691 -- iter: 160/522
[A[ATraining Step: 23  | total loss: [1m[32m0.68792[0m[0m | time: 3.419s
[2K
| Adam | epoch: 002 | loss: 0.68792 - acc: 0.6291 -- iter: 192/522
[A[ATraining Step: 24  | total loss: [1m[32m0.70643[0m[0m | time: 4.055s
[2K
| Adam | epoch: 002 | loss: 0.70643 - acc: 0.6016 -- iter: 224/522
[A[ATraining Step: 25  | total loss: [1m[32m0.73232[0m[0m | time: 4.659s
[2K
| Adam | epoch: 002 | loss: 0.73232 - acc: 0.5398 -- iter: 256/522
[A[ATraining Step: 26  | total loss: [1m[32m0.70340[0m[0m | time: 5.265s
[2K
| Adam | epoch: 002 | loss: 0.70340 - acc: 0.5954 -- iter: 288/522
[A[ATraining Step: 27  | total loss: [1m[32m0.70067[0m[0m | time: 5.875s
[2K
| Adam | epoch: 002 | loss: 0.70067 - acc: 0.5789 -- iter: 320/522
[A[ATraining Step: 28  | total loss: [1m[32m0.70117[0m[0m | time: 6.480s
[2K
| Adam | epoch: 002 | loss: 0.70117 - acc: 0.5514 -- iter: 352/522
[A[ATraining Step: 29  | total loss: [1m[32m0.68757[0m[0m | time: 7.084s
[2K
| Adam | epoch: 002 | loss: 0.68757 - acc: 0.6225 -- iter: 384/522
[A[ATraining Step: 30  | total loss: [1m[32m0.68576[0m[0m | time: 7.684s
[2K
| Adam | epoch: 002 | loss: 0.68576 - acc: 0.6231 -- iter: 416/522
[A[ATraining Step: 31  | total loss: [1m[32m0.68614[0m[0m | time: 8.292s
[2K
| Adam | epoch: 002 | loss: 0.68614 - acc: 0.6091 -- iter: 448/522
[A[ATraining Step: 32  | total loss: [1m[32m0.68641[0m[0m | time: 8.904s
[2K
| Adam | epoch: 002 | loss: 0.68641 - acc: 0.5986 -- iter: 480/522
[A[ATraining Step: 33  | total loss: [1m[32m0.68612[0m[0m | time: 9.512s
[2K
| Adam | epoch: 002 | loss: 0.68612 - acc: 0.5976 -- iter: 512/522
[A[ATraining Step: 34  | total loss: [1m[32m0.68281[0m[0m | time: 11.127s
[2K
| Adam | epoch: 002 | loss: 0.68281 - acc: 0.6235 | val_loss: 0.68362 - val_acc: 0.5976 -- iter: 522/522
--
Training Step: 35  | total loss: [1m[32m0.68304[0m[0m | time: 0.224s
[2K
| Adam | epoch: 003 | loss: 0.68304 - acc: 0.6173 -- iter: 032/522
[A[ATraining Step: 36  | total loss: [1m[32m0.68299[0m[0m | time: 0.453s
[2K
| Adam | epoch: 003 | loss: 0.68299 - acc: 0.6138 -- iter: 064/522
[A[ATraining Step: 37  | total loss: [1m[32m0.68299[0m[0m | time: 1.075s
[2K
| Adam | epoch: 003 | loss: 0.68299 - acc: 0.6110 -- iter: 096/522
[A[ATraining Step: 38  | total loss: [1m[32m0.68369[0m[0m | time: 1.669s
[2K
| Adam | epoch: 003 | loss: 0.68369 - acc: 0.6015 -- iter: 128/522
[A[ATraining Step: 39  | total loss: [1m[32m0.68511[0m[0m | time: 2.263s
[2K
| Adam | epoch: 003 | loss: 0.68511 - acc: 0.5881 -- iter: 160/522
[A[ATraining Step: 40  | total loss: [1m[32m0.68553[0m[0m | time: 2.867s
[2K
| Adam | epoch: 003 | loss: 0.68553 - acc: 0.5833 -- iter: 192/522
[A[ATraining Step: 41  | total loss: [1m[32m0.68338[0m[0m | time: 3.466s
[2K
| Adam | epoch: 003 | loss: 0.68338 - acc: 0.5967 -- iter: 224/522
[A[ATraining Step: 42  | total loss: [1m[32m0.68066[0m[0m | time: 4.076s
[2K
| Adam | epoch: 003 | loss: 0.68066 - acc: 0.6130 -- iter: 256/522
[A[ATraining Step: 43  | total loss: [1m[32m0.68443[0m[0m | time: 4.677s
[2K
| Adam | epoch: 003 | loss: 0.68443 - acc: 0.5876 -- iter: 288/522
[A[ATraining Step: 44  | total loss: [1m[32m0.68741[0m[0m | time: 5.281s
[2K
| Adam | epoch: 003 | loss: 0.68741 - acc: 0.5670 -- iter: 320/522
[A[ATraining Step: 45  | total loss: [1m[32m0.68363[0m[0m | time: 5.896s
[2K
| Adam | epoch: 003 | loss: 0.68363 - acc: 0.5875 -- iter: 352/522
[A[ATraining Step: 46  | total loss: [1m[32m0.68026[0m[0m | time: 6.497s
[2K
| Adam | epoch: 003 | loss: 0.68026 - acc: 0.6041 -- iter: 384/522
[A[ATraining Step: 47  | total loss: [1m[32m0.67741[0m[0m | time: 7.103s
[2K
| Adam | epoch: 003 | loss: 0.67741 - acc: 0.6178 -- iter: 416/522
[A[ATraining Step: 48  | total loss: [1m[32m0.67440[0m[0m | time: 7.683s
[2K
| Adam | epoch: 003 | loss: 0.67440 - acc: 0.6290 -- iter: 448/522
[A[ATraining Step: 49  | total loss: [1m[32m0.67732[0m[0m | time: 8.298s
[2K
| Adam | epoch: 003 | loss: 0.67732 - acc: 0.6135 -- iter: 480/522
[A[ATraining Step: 50  | total loss: [1m[32m0.67701[0m[0m | time: 8.912s
[2K
| Adam | epoch: 003 | loss: 0.67701 - acc: 0.6105 -- iter: 512/522
[A[ATraining Step: 51  | total loss: [1m[32m0.68030[0m[0m | time: 10.543s
[2K
| Adam | epoch: 003 | loss: 0.68030 - acc: 0.5984 | val_loss: 0.67388 - val_acc: 0.5976 -- iter: 522/522
--
Training Step: 52  | total loss: [1m[32m0.67771[0m[0m | time: 0.613s
[2K
| Adam | epoch: 004 | loss: 0.67771 - acc: 0.6024 -- iter: 032/522
[A[ATraining Step: 53  | total loss: [1m[32m0.66988[0m[0m | time: 0.841s
[2K
| Adam | epoch: 004 | loss: 0.66988 - acc: 0.6196 -- iter: 064/522
[A[ATraining Step: 54  | total loss: [1m[32m0.66914[0m[0m | time: 1.071s
[2K
| Adam | epoch: 004 | loss: 0.66914 - acc: 0.6167 -- iter: 096/522
[A[ATraining Step: 55  | total loss: [1m[32m0.67008[0m[0m | time: 1.679s
[2K
| Adam | epoch: 004 | loss: 0.67008 - acc: 0.6143 -- iter: 128/522
[A[ATraining Step: 56  | total loss: [1m[32m0.67872[0m[0m | time: 2.294s
[2K
| Adam | epoch: 004 | loss: 0.67872 - acc: 0.6026 -- iter: 160/522
[A[ATraining Step: 57  | total loss: [1m[32m0.68620[0m[0m | time: 2.901s
[2K
| Adam | epoch: 004 | loss: 0.68620 - acc: 0.5884 -- iter: 192/522
[A[ATraining Step: 58  | total loss: [1m[32m0.68516[0m[0m | time: 3.543s
[2K
| Adam | epoch: 004 | loss: 0.68516 - acc: 0.5892 -- iter: 224/522
[A[ATraining Step: 59  | total loss: [1m[32m0.67663[0m[0m | time: 4.154s
[2K
| Adam | epoch: 004 | loss: 0.67663 - acc: 0.6108 -- iter: 256/522
[A[ATraining Step: 60  | total loss: [1m[32m0.67692[0m[0m | time: 4.751s
[2K
| Adam | epoch: 004 | loss: 0.67692 - acc: 0.6085 -- iter: 288/522
[A[ATraining Step: 61  | total loss: [1m[32m0.67469[0m[0m | time: 5.391s
[2K
| Adam | epoch: 004 | loss: 0.67469 - acc: 0.6147 -- iter: 320/522
[A[ATraining Step: 62  | total loss: [1m[32m0.67813[0m[0m | time: 5.985s
[2K
| Adam | epoch: 004 | loss: 0.67813 - acc: 0.6000 -- iter: 352/522
[A[ATraining Step: 63  | total loss: [1m[32m0.67689[0m[0m | time: 6.597s
[2K
| Adam | epoch: 004 | loss: 0.67689 - acc: 0.6032 -- iter: 384/522
[A[ATraining Step: 64  | total loss: [1m[32m0.67392[0m[0m | time: 7.212s
[2K
| Adam | epoch: 004 | loss: 0.67392 - acc: 0.6137 -- iter: 416/522
[A[ATraining Step: 65  | total loss: [1m[32m0.67857[0m[0m | time: 7.860s
[2K
| Adam | epoch: 004 | loss: 0.67857 - acc: 0.5958 -- iter: 448/522
[A[ATraining Step: 66  | total loss: [1m[32m0.67840[0m[0m | time: 8.464s
[2K
| Adam | epoch: 004 | loss: 0.67840 - acc: 0.5956 -- iter: 480/522
[A[ATraining Step: 67  | total loss: [1m[32m0.68214[0m[0m | time: 9.105s
[2K
| Adam | epoch: 004 | loss: 0.68214 - acc: 0.5804 -- iter: 512/522
[A[ATraining Step: 68  | total loss: [1m[32m0.68245[0m[0m | time: 10.812s
[2K
| Adam | epoch: 004 | loss: 0.68245 - acc: 0.5782 | val_loss: 0.67649 - val_acc: 0.5976 -- iter: 522/522
--
Training Step: 69  | total loss: [1m[32m0.68455[0m[0m | time: 0.637s
[2K
| Adam | epoch: 005 | loss: 0.68455 - acc: 0.5691 -- iter: 032/522
[A[ATraining Step: 70  | total loss: [1m[32m0.68468[0m[0m | time: 1.235s
[2K
| Adam | epoch: 005 | loss: 0.68468 - acc: 0.5683 -- iter: 064/522
[A[ATraining Step: 71  | total loss: [1m[32m0.68024[0m[0m | time: 1.467s
[2K
| Adam | epoch: 005 | loss: 0.68024 - acc: 0.5855 -- iter: 096/522
[A[ATraining Step: 72  | total loss: [1m[32m0.67685[0m[0m | time: 1.682s
[2K
| Adam | epoch: 005 | loss: 0.67685 - acc: 0.5984 -- iter: 128/522
[A[ATraining Step: 73  | total loss: [1m[32m0.67424[0m[0m | time: 2.295s
[2K
| Adam | epoch: 005 | loss: 0.67424 - acc: 0.6096 -- iter: 160/522
[A[ATraining Step: 74  | total loss: [1m[32m0.67462[0m[0m | time: 2.898s
[2K
| Adam | epoch: 005 | loss: 0.67462 - acc: 0.6079 -- iter: 192/522
[A[ATraining Step: 75  | total loss: [1m[32m0.67105[0m[0m | time: 3.519s
[2K
| Adam | epoch: 005 | loss: 0.67105 - acc: 0.6199 -- iter: 224/522
[A[ATraining Step: 76  | total loss: [1m[32m0.67084[0m[0m | time: 4.119s
[2K
| Adam | epoch: 005 | loss: 0.67084 - acc: 0.6205 -- iter: 256/522
[A[ATraining Step: 77  | total loss: [1m[32m0.67261[0m[0m | time: 4.732s
[2K
| Adam | epoch: 005 | loss: 0.67261 - acc: 0.6143 -- iter: 288/522
[A[ATraining Step: 78  | total loss: [1m[32m0.68009[0m[0m | time: 5.333s
[2K
| Adam | epoch: 005 | loss: 0.68009 - acc: 0.5893 -- iter: 320/522
[A[ATraining Step: 79  | total loss: [1m[32m0.67560[0m[0m | time: 5.926s
[2K
| Adam | epoch: 005 | loss: 0.67560 - acc: 0.6027 -- iter: 352/522
[A[ATraining Step: 80  | total loss: [1m[32m0.67564[0m[0m | time: 6.527s
[2K
| Adam | epoch: 005 | loss: 0.67564 - acc: 0.6018 -- iter: 384/522
[A[ATraining Step: 81  | total loss: [1m[32m0.67558[0m[0m | time: 7.115s
[2K
| Adam | epoch: 005 | loss: 0.67558 - acc: 0.6010 -- iter: 416/522
[A[ATraining Step: 82  | total loss: [1m[32m0.67349[0m[0m | time: 7.713s
[2K
| Adam | epoch: 005 | loss: 0.67349 - acc: 0.6065 -- iter: 448/522
[A[ATraining Step: 83  | total loss: [1m[32m0.67719[0m[0m | time: 8.550s
[2K
| Adam | epoch: 005 | loss: 0.67719 - acc: 0.5958 -- iter: 480/522
[A[ATraining Step: 84  | total loss: [1m[32m0.68014[0m[0m | time: 9.132s
[2K
| Adam | epoch: 005 | loss: 0.68014 - acc: 0.5863 -- iter: 512/522
[A[ATraining Step: 85  | total loss: [1m[32m0.67745[0m[0m | time: 10.726s
[2K
| Adam | epoch: 005 | loss: 0.67745 - acc: 0.5933 | val_loss: 0.67396 - val_acc: 0.5976 -- iter: 522/522
--
Training Step: 86  | total loss: [1m[32m0.67823[0m[0m | time: 0.608s
[2K
| Adam | epoch: 006 | loss: 0.67823 - acc: 0.5902 -- iter: 032/522
[A[ATraining Step: 87  | total loss: [1m[32m0.67567[0m[0m | time: 1.452s
[2K
| Adam | epoch: 006 | loss: 0.67567 - acc: 0.5968 -- iter: 064/522
[A[ATraining Step: 88  | total loss: [1m[32m0.66985[0m[0m | time: 2.054s
[2K
| Adam | epoch: 006 | loss: 0.66985 - acc: 0.6121 -- iter: 096/522
[A[ATraining Step: 89  | total loss: [1m[32m0.66929[0m[0m | time: 2.280s
[2K
| Adam | epoch: 006 | loss: 0.66929 - acc: 0.6134 -- iter: 128/522
[A[ATraining Step: 90  | total loss: [1m[32m0.67758[0m[0m | time: 2.495s
[2K
| Adam | epoch: 006 | loss: 0.67758 - acc: 0.5921 -- iter: 160/522
[A[ATraining Step: 91  | total loss: [1m[32m0.68501[0m[0m | time: 3.267s
[2K
| Adam | epoch: 006 | loss: 0.68501 - acc: 0.5728 -- iter: 192/522
[A[ATraining Step: 92  | total loss: [1m[32m0.68061[0m[0m | time: 3.868s
[2K
| Adam | epoch: 006 | loss: 0.68061 - acc: 0.5843 -- iter: 224/522
[A[ATraining Step: 93  | total loss: [1m[32m0.67878[0m[0m | time: 4.477s
[2K
| Adam | epoch: 006 | loss: 0.67878 - acc: 0.5884 -- iter: 256/522
[A[ATraining Step: 94  | total loss: [1m[32m0.67484[0m[0m | time: 5.080s
[2K
| Adam | epoch: 006 | loss: 0.67484 - acc: 0.5983 -- iter: 288/522
[A[ATraining Step: 95  | total loss: [1m[32m0.67599[0m[0m | time: 5.689s
[2K
| Adam | epoch: 006 | loss: 0.67599 - acc: 0.5947 -- iter: 320/522
[A[ATraining Step: 96  | total loss: [1m[32m0.67575[0m[0m | time: 6.281s
[2K
| Adam | epoch: 006 | loss: 0.67575 - acc: 0.5946 -- iter: 352/522
[A[ATraining Step: 97  | total loss: [1m[32m0.67673[0m[0m | time: 6.897s
[2K
| Adam | epoch: 006 | loss: 0.67673 - acc: 0.5914 -- iter: 384/522
[A[ATraining Step: 98  | total loss: [1m[32m0.67651[0m[0m | time: 7.499s
[2K
| Adam | epoch: 006 | loss: 0.67651 - acc: 0.5916 -- iter: 416/522
[A[ATraining Step: 99  | total loss: [1m[32m0.67991[0m[0m | time: 8.084s
[2K
| Adam | epoch: 006 | loss: 0.67991 - acc: 0.5825 -- iter: 448/522
[A[ATraining Step: 100  | total loss: [1m[32m0.68278[0m[0m | time: 8.677s
[2K
| Adam | epoch: 006 | loss: 0.68278 - acc: 0.5742 -- iter: 480/522
[A[ATraining Step: 101  | total loss: [1m[32m0.67849[0m[0m | time: 9.281s
[2K
| Adam | epoch: 006 | loss: 0.67849 - acc: 0.5856 -- iter: 512/522
[A[ATraining Step: 102  | total loss: [1m[32m0.67685[0m[0m | time: 10.900s
[2K
| Adam | epoch: 006 | loss: 0.67685 - acc: 0.5895 | val_loss: 0.67368 - val_acc: 0.5976 -- iter: 522/522
--
Training Step: 103  | total loss: [1m[32m0.67216[0m[0m | time: 0.611s
[2K
| Adam | epoch: 007 | loss: 0.67216 - acc: 0.6024 -- iter: 032/522
[A[ATraining Step: 104  | total loss: [1m[32m0.67115[0m[0m | time: 1.209s
[2K
| Adam | epoch: 007 | loss: 0.67115 - acc: 0.6047 -- iter: 064/522
[A[ATraining Step: 105  | total loss: [1m[32m0.67289[0m[0m | time: 1.816s
[2K
| Adam | epoch: 007 | loss: 0.67289 - acc: 0.6005 -- iter: 096/522
[A[ATraining Step: 106  | total loss: [1m[32m0.67306[0m[0m | time: 2.412s
[2K
| Adam | epoch: 007 | loss: 0.67306 - acc: 0.5998 -- iter: 128/522
[A[ATraining Step: 107  | total loss: [1m[32m0.68058[0m[0m | time: 2.630s
[2K
| Adam | epoch: 007 | loss: 0.68058 - acc: 0.5804 -- iter: 160/522
[A[ATraining Step: 108  | total loss: [1m[32m0.68383[0m[0m | time: 2.859s
[2K
| Adam | epoch: 007 | loss: 0.68383 - acc: 0.5724 -- iter: 192/522
[A[ATraining Step: 109  | total loss: [1m[32m0.68642[0m[0m | time: 3.482s
[2K
| Adam | epoch: 007 | loss: 0.68642 - acc: 0.5652 -- iter: 224/522
[A[ATraining Step: 110  | total loss: [1m[32m0.68107[0m[0m | time: 4.090s
[2K
| Adam | epoch: 007 | loss: 0.68107 - acc: 0.5805 -- iter: 256/522
[A[ATraining Step: 111  | total loss: [1m[32m0.67623[0m[0m | time: 4.714s
[2K
| Adam | epoch: 007 | loss: 0.67623 - acc: 0.5943 -- iter: 288/522
[A[ATraining Step: 112  | total loss: [1m[32m0.67940[0m[0m | time: 5.316s
[2K
| Adam | epoch: 007 | loss: 0.67940 - acc: 0.5849 -- iter: 320/522
[A[ATraining Step: 113  | total loss: [1m[32m0.67571[0m[0m | time: 5.916s
[2K
| Adam | epoch: 007 | loss: 0.67571 - acc: 0.5952 -- iter: 352/522
[A[ATraining Step: 114  | total loss: [1m[32m0.67996[0m[0m | time: 6.520s
[2K
| Adam | epoch: 007 | loss: 0.67996 - acc: 0.5825 -- iter: 384/522
[A[ATraining Step: 115  | total loss: [1m[32m0.67645[0m[0m | time: 7.109s
[2K
| Adam | epoch: 007 | loss: 0.67645 - acc: 0.5930 -- iter: 416/522
[A[ATraining Step: 116  | total loss: [1m[32m0.67325[0m[0m | time: 7.713s
[2K
| Adam | epoch: 007 | loss: 0.67325 - acc: 0.6025 -- iter: 448/522
[A[ATraining Step: 117  | total loss: [1m[32m0.67154[0m[0m | time: 8.312s
[2K
| Adam | epoch: 007 | loss: 0.67154 - acc: 0.6078 -- iter: 480/522
[A[ATraining Step: 118  | total loss: [1m[32m0.67662[0m[0m | time: 8.914s
[2K
| Adam | epoch: 007 | loss: 0.67662 - acc: 0.5939 -- iter: 512/522
[A[ATraining Step: 119  | total loss: [1m[32m0.67405[0m[0m | time: 10.532s
[2K
| Adam | epoch: 007 | loss: 0.67405 - acc: 0.6002 | val_loss: 0.67302 - val_acc: 0.5976 -- iter: 522/522
--
Training Step: 120  | total loss: [1m[32m0.67302[0m[0m | time: 0.614s
[2K
| Adam | epoch: 008 | loss: 0.67302 - acc: 0.6027 -- iter: 032/522
[A[ATraining Step: 121  | total loss: [1m[32m0.67207[0m[0m | time: 1.227s
[2K
| Adam | epoch: 008 | loss: 0.67207 - acc: 0.6049 -- iter: 064/522
[A[ATraining Step: 122  | total loss: [1m[32m0.67474[0m[0m | time: 1.834s
[2K
| Adam | epoch: 008 | loss: 0.67474 - acc: 0.5975 -- iter: 096/522
[A[ATraining Step: 123  | total loss: [1m[32m0.67592[0m[0m | time: 2.438s
[2K
| Adam | epoch: 008 | loss: 0.67592 - acc: 0.5940 -- iter: 128/522
[A[ATraining Step: 124  | total loss: [1m[32m0.68068[0m[0m | time: 3.037s
[2K
| Adam | epoch: 008 | loss: 0.68068 - acc: 0.5815 -- iter: 160/522
[A[ATraining Step: 125  | total loss: [1m[32m0.68003[0m[0m | time: 3.273s
[2K
| Adam | epoch: 008 | loss: 0.68003 - acc: 0.5827 -- iter: 192/522
[A[ATraining Step: 126  | total loss: [1m[32m0.68670[0m[0m | time: 3.498s
[2K
| Adam | epoch: 008 | loss: 0.68670 - acc: 0.5644 -- iter: 224/522
[A[ATraining Step: 127  | total loss: [1m[32m0.69239[0m[0m | time: 4.286s
[2K
| Adam | epoch: 008 | loss: 0.69239 - acc: 0.5480 -- iter: 256/522
[A[ATraining Step: 128  | total loss: [1m[32m0.68453[0m[0m | time: 4.888s
[2K
| Adam | epoch: 008 | loss: 0.68453 - acc: 0.5713 -- iter: 288/522
[A[ATraining Step: 129  | total loss: [1m[32m0.68059[0m[0m | time: 5.495s
[2K
| Adam | epoch: 008 | loss: 0.68059 - acc: 0.5829 -- iter: 320/522
[A[ATraining Step: 130  | total loss: [1m[32m0.67817[0m[0m | time: 6.092s
[2K
| Adam | epoch: 008 | loss: 0.67817 - acc: 0.5903 -- iter: 352/522
[A[ATraining Step: 131  | total loss: [1m[32m0.67804[0m[0m | time: 6.698s
[2K
| Adam | epoch: 008 | loss: 0.67804 - acc: 0.5906 -- iter: 384/522
[A[ATraining Step: 132  | total loss: [1m[32m0.67960[0m[0m | time: 7.298s
[2K
| Adam | epoch: 008 | loss: 0.67960 - acc: 0.5847 -- iter: 416/522
[A[ATraining Step: 133  | total loss: [1m[32m0.68010[0m[0m | time: 7.897s
[2K
| Adam | epoch: 008 | loss: 0.68010 - acc: 0.5825 -- iter: 448/522
[A[ATraining Step: 134  | total loss: [1m[32m0.67867[0m[0m | time: 8.510s
[2K
| Adam | epoch: 008 | loss: 0.67867 - acc: 0.5867 -- iter: 480/522
[A[ATraining Step: 135  | total loss: [1m[32m0.67527[0m[0m | time: 9.101s
[2K
| Adam | epoch: 008 | loss: 0.67527 - acc: 0.5968 -- iter: 512/522
[A[ATraining Step: 136  | total loss: [1m[32m0.67625[0m[0m | time: 10.725s
[2K
| Adam | epoch: 008 | loss: 0.67625 - acc: 0.5934 | val_loss: 0.67226 - val_acc: 0.5976 -- iter: 522/522
--
Training Step: 137  | total loss: [1m[32m0.67694[0m[0m | time: 0.608s
[2K
| Adam | epoch: 009 | loss: 0.67694 - acc: 0.5903 -- iter: 032/522
[A[ATraining Step: 138  | total loss: [1m[32m0.67746[0m[0m | time: 1.208s
[2K
| Adam | epoch: 009 | loss: 0.67746 - acc: 0.5875 -- iter: 064/522
[A[ATraining Step: 139  | total loss: [1m[32m0.67292[0m[0m | time: 1.809s
[2K
| Adam | epoch: 009 | loss: 0.67292 - acc: 0.6006 -- iter: 096/522
[A[ATraining Step: 140  | total loss: [1m[32m0.66848[0m[0m | time: 2.614s
[2K
| Adam | epoch: 009 | loss: 0.66848 - acc: 0.6124 -- iter: 128/522
[A[ATraining Step: 141  | total loss: [1m[32m0.67136[0m[0m | time: 3.220s
[2K
| Adam | epoch: 009 | loss: 0.67136 - acc: 0.6043 -- iter: 160/522
[A[ATraining Step: 142  | total loss: [1m[32m0.67402[0m[0m | time: 3.814s
[2K
| Adam | epoch: 009 | loss: 0.67402 - acc: 0.5970 -- iter: 192/522
[A[ATraining Step: 143  | total loss: [1m[32m0.67801[0m[0m | time: 4.048s
[2K
| Adam | epoch: 009 | loss: 0.67801 - acc: 0.5873 -- iter: 224/522
[A[ATraining Step: 144  | total loss: [1m[32m0.67659[0m[0m | time: 4.262s
[2K
| Adam | epoch: 009 | loss: 0.67659 - acc: 0.5886 -- iter: 256/522
[A[ATraining Step: 145  | total loss: [1m[32m0.67493[0m[0m | time: 4.874s
[2K
| Adam | epoch: 009 | loss: 0.67493 - acc: 0.5897 -- iter: 288/522
[A[ATraining Step: 146  | total loss: [1m[32m0.67194[0m[0m | time: 5.494s
[2K
| Adam | epoch: 009 | loss: 0.67194 - acc: 0.5964 -- iter: 320/522
[A[ATraining Step: 147  | total loss: [1m[32m0.67014[0m[0m | time: 6.169s
[2K
| Adam | epoch: 009 | loss: 0.67014 - acc: 0.5992 -- iter: 352/522
[A[ATraining Step: 148  | total loss: [1m[32m0.66664[0m[0m | time: 6.751s
[2K
| Adam | epoch: 009 | loss: 0.66664 - acc: 0.6049 -- iter: 384/522
[A[ATraining Step: 149  | total loss: [1m[32m0.66405[0m[0m | time: 7.346s
[2K
| Adam | epoch: 009 | loss: 0.66405 - acc: 0.6101 -- iter: 416/522
[A[ATraining Step: 150  | total loss: [1m[32m0.66879[0m[0m | time: 7.945s
[2K
| Adam | epoch: 009 | loss: 0.66879 - acc: 0.6022 -- iter: 448/522
[A[ATraining Step: 151  | total loss: [1m[32m0.67429[0m[0m | time: 8.539s
[2K
| Adam | epoch: 009 | loss: 0.67429 - acc: 0.5920 -- iter: 480/522
[A[ATraining Step: 152  | total loss: [1m[32m0.67351[0m[0m | time: 9.156s
[2K
| Adam | epoch: 009 | loss: 0.67351 - acc: 0.5921 -- iter: 512/522
[A[ATraining Step: 153  | total loss: [1m[32m0.67814[0m[0m | time: 10.764s
[2K
| Adam | epoch: 009 | loss: 0.67814 - acc: 0.5798 | val_loss: 0.67066 - val_acc: 0.5976 -- iter: 522/522
--
Training Step: 154  | total loss: [1m[32m0.67615[0m[0m | time: 0.602s
[2K
| Adam | epoch: 010 | loss: 0.67615 - acc: 0.5843 -- iter: 032/522
[A[ATraining Step: 155  | total loss: [1m[32m0.67384[0m[0m | time: 1.203s
[2K
| Adam | epoch: 010 | loss: 0.67384 - acc: 0.5915 -- iter: 064/522
[A[ATraining Step: 156  | total loss: [1m[32m0.67302[0m[0m | time: 1.806s
[2K
| Adam | epoch: 010 | loss: 0.67302 - acc: 0.5949 -- iter: 096/522
[A[ATraining Step: 157  | total loss: [1m[32m0.67225[0m[0m | time: 2.413s
[2K
| Adam | epoch: 010 | loss: 0.67225 - acc: 0.5979 -- iter: 128/522
[A[ATraining Step: 158  | total loss: [1m[32m0.67527[0m[0m | time: 3.041s
[2K
| Adam | epoch: 010 | loss: 0.67527 - acc: 0.5881 -- iter: 160/522
[A[ATraining Step: 159  | total loss: [1m[32m0.67531[0m[0m | time: 3.671s
[2K
| Adam | epoch: 010 | loss: 0.67531 - acc: 0.5887 -- iter: 192/522
[A[ATraining Step: 160  | total loss: [1m[32m0.67163[0m[0m | time: 4.277s
[2K
| Adam | epoch: 010 | loss: 0.67163 - acc: 0.6017 -- iter: 224/522
[A[ATraining Step: 161  | total loss: [1m[32m0.66985[0m[0m | time: 4.499s
[2K
| Adam | epoch: 010 | loss: 0.66985 - acc: 0.6071 -- iter: 256/522
[A[ATraining Step: 162  | total loss: [1m[32m0.67294[0m[0m | time: 4.710s
[2K
| Adam | epoch: 010 | loss: 0.67294 - acc: 0.5964 -- iter: 288/522
[A[ATraining Step: 163  | total loss: [1m[32m0.67582[0m[0m | time: 5.312s
[2K
| Adam | epoch: 010 | loss: 0.67582 - acc: 0.5868 -- iter: 320/522
[A[ATraining Step: 164  | total loss: [1m[32m0.67431[0m[0m | time: 5.910s
[2K
| Adam | epoch: 010 | loss: 0.67431 - acc: 0.5906 -- iter: 352/522
[A[ATraining Step: 165  | total loss: [1m[32m0.67281[0m[0m | time: 6.514s
[2K
| Adam | epoch: 010 | loss: 0.67281 - acc: 0.5940 -- iter: 384/522
[A[ATraining Step: 166  | total loss: [1m[32m0.67391[0m[0m | time: 7.117s
[2K
| Adam | epoch: 010 | loss: 0.67391 - acc: 0.5909 -- iter: 416/522
[A[ATraining Step: 167  | total loss: [1m[32m0.67220[0m[0m | time: 7.726s
[2K
| Adam | epoch: 010 | loss: 0.67220 - acc: 0.5943 -- iter: 448/522
[A[ATraining Step: 168  | total loss: [1m[32m0.66955[0m[0m | time: 8.330s
[2K
| Adam | epoch: 010 | loss: 0.66955 - acc: 0.6005 -- iter: 480/522
[A[ATraining Step: 169  | total loss: [1m[32m0.66421[0m[0m | time: 8.928s
[2K
| Adam | epoch: 010 | loss: 0.66421 - acc: 0.6092 -- iter: 512/522
[A[ATraining Step: 170  | total loss: [1m[32m0.66652[0m[0m | time: 10.531s
[2K
| Adam | epoch: 010 | loss: 0.66652 - acc: 0.6045 | val_loss: 0.66672 - val_acc: 0.5976 -- iter: 522/522
--
Training Step: 171  | total loss: [1m[32m0.66538[0m[0m | time: 0.608s
[2K
| Adam | epoch: 011 | loss: 0.66538 - acc: 0.6066 -- iter: 032/522
[A[ATraining Step: 172  | total loss: [1m[32m0.66718[0m[0m | time: 1.202s
[2K
| Adam | epoch: 011 | loss: 0.66718 - acc: 0.6022 -- iter: 064/522
[A[ATraining Step: 173  | total loss: [1m[32m0.65163[0m[0m | time: 1.799s
[2K
| Adam | epoch: 011 | loss: 0.65163 - acc: 0.6232 -- iter: 096/522
[A[ATraining Step: 174  | total loss: [1m[32m0.65083[0m[0m | time: 2.404s
[2K
| Adam | epoch: 011 | loss: 0.65083 - acc: 0.6234 -- iter: 128/522
[A[ATraining Step: 175  | total loss: [1m[32m0.65002[0m[0m | time: 3.164s
[2K
| Adam | epoch: 011 | loss: 0.65002 - acc: 0.6235 -- iter: 160/522
[A[ATraining Step: 176  | total loss: [1m[32m0.66593[0m[0m | time: 3.772s
[2K
| Adam | epoch: 011 | loss: 0.66593 - acc: 0.6081 -- iter: 192/522
[A[ATraining Step: 177  | total loss: [1m[32m0.66865[0m[0m | time: 4.383s
[2K
| Adam | epoch: 011 | loss: 0.66865 - acc: 0.6004 -- iter: 224/522
[A[ATraining Step: 178  | total loss: [1m[32m0.66925[0m[0m | time: 4.965s
[2K
| Adam | epoch: 011 | loss: 0.66925 - acc: 0.5935 -- iter: 256/522
[A[ATraining Step: 179  | total loss: [1m[32m0.67186[0m[0m | time: 5.178s
[2K
| Adam | epoch: 011 | loss: 0.67186 - acc: 0.5841 -- iter: 288/522
[A[ATraining Step: 180  | total loss: [1m[32m0.67639[0m[0m | time: 5.403s
[2K
| Adam | epoch: 011 | loss: 0.67639 - acc: 0.5657 -- iter: 320/522
[A[ATraining Step: 181  | total loss: [1m[32m0.68076[0m[0m | time: 6.160s
[2K
| Adam | epoch: 011 | loss: 0.68076 - acc: 0.5491 -- iter: 352/522
[A[ATraining Step: 182  | total loss: [1m[32m0.67978[0m[0m | time: 6.759s
[2K
| Adam | epoch: 011 | loss: 0.67978 - acc: 0.5567 -- iter: 384/522
[A[ATraining Step: 183  | total loss: [1m[32m0.67770[0m[0m | time: 7.354s
[2K
| Adam | epoch: 011 | loss: 0.67770 - acc: 0.5698 -- iter: 416/522
[A[ATraining Step: 184  | total loss: [1m[32m0.67929[0m[0m | time: 7.952s
[2K
| Adam | epoch: 011 | loss: 0.67929 - acc: 0.5628 -- iter: 448/522
[A[ATraining Step: 185  | total loss: [1m[32m0.67808[0m[0m | time: 8.560s
[2K
| Adam | epoch: 011 | loss: 0.67808 - acc: 0.5690 -- iter: 480/522
[A[ATraining Step: 186  | total loss: [1m[32m0.67951[0m[0m | time: 9.159s
[2K
| Adam | epoch: 011 | loss: 0.67951 - acc: 0.5621 -- iter: 512/522
[A[ATraining Step: 187  | total loss: [1m[32m0.67370[0m[0m | time: 10.782s
[2K
| Adam | epoch: 011 | loss: 0.67370 - acc: 0.5840 | val_loss: 0.66475 - val_acc: 0.5976 -- iter: 522/522
--
Training Step: 188  | total loss: [1m[32m0.67248[0m[0m | time: 0.604s
[2K
| Adam | epoch: 012 | loss: 0.67248 - acc: 0.5881 -- iter: 032/522
[A[ATraining Step: 189  | total loss: [1m[32m0.67850[0m[0m | time: 1.293s
[2K
| Adam | epoch: 012 | loss: 0.67850 - acc: 0.5700 -- iter: 064/522
[A[ATraining Step: 190  | total loss: [1m[32m0.68080[0m[0m | time: 1.902s
[2K
| Adam | epoch: 012 | loss: 0.68080 - acc: 0.5630 -- iter: 096/522
[A[ATraining Step: 191  | total loss: [1m[32m0.68178[0m[0m | time: 2.603s
[2K
| Adam | epoch: 012 | loss: 0.68178 - acc: 0.5598 -- iter: 128/522
[A[ATraining Step: 192  | total loss: [1m[32m0.68318[0m[0m | time: 3.323s
[2K
| Adam | epoch: 012 | loss: 0.68318 - acc: 0.5569 -- iter: 160/522
[A[ATraining Step: 193  | total loss: [1m[32m0.68058[0m[0m | time: 3.918s
[2K
| Adam | epoch: 012 | loss: 0.68058 - acc: 0.5637 -- iter: 192/522
[A[ATraining Step: 194  | total loss: [1m[32m0.67454[0m[0m | time: 4.723s
[2K
| Adam | epoch: 012 | loss: 0.67454 - acc: 0.5824 -- iter: 224/522
[A[ATraining Step: 195  | total loss: [1m[32m0.66912[0m[0m | time: 5.334s
[2K
| Adam | epoch: 012 | loss: 0.66912 - acc: 0.5960 -- iter: 256/522
[A[ATraining Step: 196  | total loss: [1m[32m0.66351[0m[0m | time: 5.937s
[2K
| Adam | epoch: 012 | loss: 0.66351 - acc: 0.6083 -- iter: 288/522
[A[ATraining Step: 197  | total loss: [1m[32m0.67016[0m[0m | time: 6.151s
[2K
| Adam | epoch: 012 | loss: 0.67016 - acc: 0.5943 -- iter: 320/522
[A[ATraining Step: 198  | total loss: [1m[32m0.67005[0m[0m | time: 6.380s
[2K
| Adam | epoch: 012 | loss: 0.67005 - acc: 0.5949 -- iter: 352/522
[A[ATraining Step: 199  | total loss: [1m[32m0.66850[0m[0m | time: 6.991s
[2K
| Adam | epoch: 012 | loss: 0.66850 - acc: 0.5954 -- iter: 384/522
[A[ATraining Step: 200  | total loss: [1m[32m0.66304[0m[0m | time: 8.625s
[2K
| Adam | epoch: 012 | loss: 0.66304 - acc: 0.5984 | val_loss: 0.65598 - val_acc: 0.5976 -- iter: 416/522
--
Training Step: 201  | total loss: [1m[32m0.65397[0m[0m | time: 9.260s
[2K
| Adam | epoch: 012 | loss: 0.65397 - acc: 0.6104 -- iter: 448/522
[A[ATraining Step: 202  | total loss: [1m[32m0.65166[0m[0m | time: 9.861s
[2K
| Adam | epoch: 012 | loss: 0.65166 - acc: 0.6119 -- iter: 480/522
[A[ATraining Step: 203  | total loss: [1m[32m0.65080[0m[0m | time: 10.460s
[2K
| Adam | epoch: 012 | loss: 0.65080 - acc: 0.6101 -- iter: 512/522
[A[ATraining Step: 204  | total loss: [1m[32m0.65025[0m[0m | time: 12.098s
[2K
| Adam | epoch: 012 | loss: 0.65025 - acc: 0.6084 | val_loss: 0.63228 - val_acc: 0.5976 -- iter: 522/522
--
Training Step: 205  | total loss: [1m[32m0.63890[0m[0m | time: 0.619s
[2K
| Adam | epoch: 013 | loss: 0.63890 - acc: 0.6226 -- iter: 032/522
[A[ATraining Step: 206  | total loss: [1m[32m0.63583[0m[0m | time: 1.281s
[2K
| Adam | epoch: 013 | loss: 0.63583 - acc: 0.6259 -- iter: 064/522
[A[ATraining Step: 207  | total loss: [1m[32m0.62527[0m[0m | time: 1.876s
[2K
| Adam | epoch: 013 | loss: 0.62527 - acc: 0.6384 -- iter: 096/522
[A[ATraining Step: 208  | total loss: [1m[32m0.64106[0m[0m | time: 2.483s
[2K
| Adam | epoch: 013 | loss: 0.64106 - acc: 0.6276 -- iter: 128/522
[A[ATraining Step: 209  | total loss: [1m[32m0.63848[0m[0m | time: 3.085s
[2K
| Adam | epoch: 013 | loss: 0.63848 - acc: 0.6274 -- iter: 160/522
[A[ATraining Step: 210  | total loss: [1m[32m0.63444[0m[0m | time: 3.685s
[2K
| Adam | epoch: 013 | loss: 0.63444 - acc: 0.6271 -- iter: 192/522
[A[ATraining Step: 211  | total loss: [1m[32m0.63320[0m[0m | time: 4.296s
[2K
| Adam | epoch: 013 | loss: 0.63320 - acc: 0.6238 -- iter: 224/522
[A[ATraining Step: 212  | total loss: [1m[32m0.63690[0m[0m | time: 4.898s
[2K
| Adam | epoch: 013 | loss: 0.63690 - acc: 0.6083 -- iter: 256/522
[A[ATraining Step: 213  | total loss: [1m[32m0.64036[0m[0m | time: 5.496s
[2K
| Adam | epoch: 013 | loss: 0.64036 - acc: 0.5943 -- iter: 288/522
[A[ATraining Step: 214  | total loss: [1m[32m0.63913[0m[0m | time: 6.096s
[2K
| Adam | epoch: 013 | loss: 0.63913 - acc: 0.5943 -- iter: 320/522
[A[ATraining Step: 215  | total loss: [1m[32m0.63405[0m[0m | time: 6.311s
[2K
| Adam | epoch: 013 | loss: 0.63405 - acc: 0.5911 -- iter: 352/522
[A[ATraining Step: 216  | total loss: [1m[32m0.64546[0m[0m | time: 6.541s
[2K
| Adam | epoch: 013 | loss: 0.64546 - acc: 0.5520 -- iter: 384/522
[A[ATraining Step: 217  | total loss: [1m[32m0.64819[0m[0m | time: 7.148s
[2K
| Adam | epoch: 013 | loss: 0.64819 - acc: 0.5168 -- iter: 416/522
[A[ATraining Step: 218  | total loss: [1m[32m0.65192[0m[0m | time: 7.763s
[2K
| Adam | epoch: 013 | loss: 0.65192 - acc: 0.5120 -- iter: 448/522
[A[ATraining Step: 219  | total loss: [1m[32m0.65740[0m[0m | time: 8.376s
[2K
| Adam | epoch: 013 | loss: 0.65740 - acc: 0.4952 -- iter: 480/522
[A[ATraining Step: 220  | total loss: [1m[32m0.66064[0m[0m | time: 8.966s
[2K
| Adam | epoch: 013 | loss: 0.66064 - acc: 0.5019 -- iter: 512/522
[A[ATraining Step: 221  | total loss: [1m[32m0.66567[0m[0m | time: 10.562s
[2K
| Adam | epoch: 013 | loss: 0.66567 - acc: 0.4830 | val_loss: 0.69916 - val_acc: 0.4024 -- iter: 522/522
--
Training Step: 222  | total loss: [1m[32m0.66813[0m[0m | time: 0.615s
[2K
| Adam | epoch: 014 | loss: 0.66813 - acc: 0.4878 -- iter: 032/522
[A[ATraining Step: 223  | total loss: [1m[32m0.67102[0m[0m | time: 1.219s
[2K
| Adam | epoch: 014 | loss: 0.67102 - acc: 0.4828 -- iter: 064/522
[A[ATraining Step: 224  | total loss: [1m[32m0.67431[0m[0m | time: 1.794s
[2K
| Adam | epoch: 014 | loss: 0.67431 - acc: 0.4626 -- iter: 096/522
[A[ATraining Step: 225  | total loss: [1m[32m0.67629[0m[0m | time: 2.393s
[2K
| Adam | epoch: 014 | loss: 0.67629 - acc: 0.4538 -- iter: 128/522
[A[ATraining Step: 226  | total loss: [1m[32m0.67734[0m[0m | time: 2.997s
[2K
| Adam | epoch: 014 | loss: 0.67734 - acc: 0.4647 -- iter: 160/522
[A[ATraining Step: 227  | total loss: [1m[32m0.67704[0m[0m | time: 3.594s
[2K
| Adam | epoch: 014 | loss: 0.67704 - acc: 0.4932 -- iter: 192/522
[A[ATraining Step: 228  | total loss: [1m[32m0.67463[0m[0m | time: 4.199s
[2K
| Adam | epoch: 014 | loss: 0.67463 - acc: 0.4970 -- iter: 224/522
[A[ATraining Step: 229  | total loss: [1m[32m0.66584[0m[0m | time: 4.798s
[2K
| Adam | epoch: 014 | loss: 0.66584 - acc: 0.5067 -- iter: 256/522
[A[ATraining Step: 230  | total loss: [1m[32m0.67549[0m[0m | time: 5.398s
[2K
| Adam | epoch: 014 | loss: 0.67549 - acc: 0.5092 -- iter: 288/522
[A[ATraining Step: 231  | total loss: [1m[32m0.66904[0m[0m | time: 6.012s
[2K
| Adam | epoch: 014 | loss: 0.66904 - acc: 0.5176 -- iter: 320/522
[A[ATraining Step: 232  | total loss: [1m[32m0.66381[0m[0m | time: 6.608s
[2K
| Adam | epoch: 014 | loss: 0.66381 - acc: 0.5252 -- iter: 352/522
[A[ATraining Step: 233  | total loss: [1m[32m0.65665[0m[0m | time: 6.818s
[2K
| Adam | epoch: 014 | loss: 0.65665 - acc: 0.5321 -- iter: 384/522
[A[ATraining Step: 234  | total loss: [1m[32m0.65836[0m[0m | time: 7.037s
[2K
| Adam | epoch: 014 | loss: 0.65836 - acc: 0.5189 -- iter: 416/522
[A[ATraining Step: 235  | total loss: [1m[32m0.65965[0m[0m | time: 7.741s
[2K
| Adam | epoch: 014 | loss: 0.65965 - acc: 0.5070 -- iter: 448/522
[A[ATraining Step: 236  | total loss: [1m[32m0.65046[0m[0m | time: 8.345s
[2K
| Adam | epoch: 014 | loss: 0.65046 - acc: 0.5188 -- iter: 480/522
[A[ATraining Step: 237  | total loss: [1m[32m0.64966[0m[0m | time: 8.944s
[2K
| Adam | epoch: 014 | loss: 0.64966 - acc: 0.5294 -- iter: 512/522
[A[ATraining Step: 238  | total loss: [1m[32m0.64188[0m[0m | time: 10.545s
[2K
| Adam | epoch: 014 | loss: 0.64188 - acc: 0.5421 | val_loss: 0.58307 - val_acc: 0.5976 -- iter: 522/522
--
Training Step: 239  | total loss: [1m[32m0.64427[0m[0m | time: 0.596s
[2K
| Adam | epoch: 015 | loss: 0.64427 - acc: 0.5379 -- iter: 032/522
[A[ATraining Step: 240  | total loss: [1m[32m0.63162[0m[0m | time: 1.216s
[2K
| Adam | epoch: 015 | loss: 0.63162 - acc: 0.5466 -- iter: 064/522
[A[ATraining Step: 241  | total loss: [1m[32m0.61793[0m[0m | time: 1.830s
[2K
| Adam | epoch: 015 | loss: 0.61793 - acc: 0.5544 -- iter: 096/522
[A[ATraining Step: 242  | total loss: [1m[32m0.61087[0m[0m | time: 2.430s
[2K
| Adam | epoch: 015 | loss: 0.61087 - acc: 0.5615 -- iter: 128/522
[A[ATraining Step: 243  | total loss: [1m[32m0.59659[0m[0m | time: 3.038s
[2K
| Adam | epoch: 015 | loss: 0.59659 - acc: 0.5772 -- iter: 160/522
[A[ATraining Step: 244  | total loss: [1m[32m0.59625[0m[0m | time: 3.634s
[2K
| Adam | epoch: 015 | loss: 0.59625 - acc: 0.5820 -- iter: 192/522
[A[ATraining Step: 245  | total loss: [1m[32m0.59009[0m[0m | time: 4.231s
[2K
| Adam | epoch: 015 | loss: 0.59009 - acc: 0.5894 -- iter: 224/522
[A[ATraining Step: 246  | total loss: [1m[32m0.58760[0m[0m | time: 4.830s
[2K
| Adam | epoch: 015 | loss: 0.58760 - acc: 0.5930 -- iter: 256/522
[A[ATraining Step: 247  | total loss: [1m[32m0.58668[0m[0m | time: 5.467s
[2K
| Adam | epoch: 015 | loss: 0.58668 - acc: 0.6087 -- iter: 288/522
[A[ATraining Step: 248  | total loss: [1m[32m0.58300[0m[0m | time: 6.061s
[2K
| Adam | epoch: 015 | loss: 0.58300 - acc: 0.6166 -- iter: 320/522
[A[ATraining Step: 249  | total loss: [1m[32m0.58701[0m[0m | time: 6.680s
[2K
| Adam | epoch: 015 | loss: 0.58701 - acc: 0.6112 -- iter: 352/522
[A[ATraining Step: 250  | total loss: [1m[32m0.57842[0m[0m | time: 7.272s
[2K
| Adam | epoch: 015 | loss: 0.57842 - acc: 0.6375 -- iter: 384/522
[A[ATraining Step: 251  | total loss: [1m[32m0.57697[0m[0m | time: 7.488s
[2K
| Adam | epoch: 015 | loss: 0.57697 - acc: 0.6457 -- iter: 416/522
[A[ATraining Step: 252  | total loss: [1m[32m0.57598[0m[0m | time: 7.706s
[2K
| Adam | epoch: 015 | loss: 0.57598 - acc: 0.6411 -- iter: 448/522
[A[ATraining Step: 253  | total loss: [1m[32m0.57335[0m[0m | time: 8.320s
[2K
| Adam | epoch: 015 | loss: 0.57335 - acc: 0.6470 -- iter: 480/522
[A[ATraining Step: 254  | total loss: [1m[32m0.57066[0m[0m | time: 8.908s
[2K
| Adam | epoch: 015 | loss: 0.57066 - acc: 0.6604 -- iter: 512/522
[A[ATraining Step: 255  | total loss: [1m[32m0.56541[0m[0m | time: 10.512s
[2K
| Adam | epoch: 015 | loss: 0.56541 - acc: 0.6694 | val_loss: 0.70436 - val_acc: 0.7073 -- iter: 522/522
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.8385899814471243
Validation AUPRC:0.8487779018467354
Test AUC:0.8180322966507176
Test AUPRC:0.8266033205449066
BestTestF1Score	0.8	0.52	0.75	0.7	0.92	81	34	42	7	0.77
BestTestMCCScore	0.8	0.52	0.75	0.7	0.92	81	34	42	7	0.77
BestTestAccuracyScore	0.8	0.52	0.75	0.7	0.92	81	34	42	7	0.77
BestValidationF1Score	0.86	0.61	0.81	0.79	0.94	92	25	41	6	0.77
BestValidationMCC	0.86	0.61	0.81	0.79	0.94	92	25	41	6	0.77
BestValidationAccuracy	0.86	0.61	0.81	0.79	0.94	92	25	41	6	0.77
TestPredictions (Threshold:0.77)
CHEMBL1957101,FP,INACT,0.7799999713897705	CHEMBL2207938,TN,INACT,0.7699999809265137	CHEMBL2314191,TP,ACT,0.9800000190734863	CHEMBL2314206,TP,ACT,0.9800000190734863	CHEMBL2314371,TN,INACT,0.4699999988079071	CHEMBL219366,TN,INACT,0.7300000190734863	CHEMBL505165,FP,INACT,0.9200000166893005	CHEMBL3780803,TP,ACT,0.9900000095367432	CHEMBL188794,FP,INACT,0.949999988079071	CHEMBL25927,TN,INACT,0.4699999988079071	CHEMBL2398295,FP,INACT,0.9100000262260437	CHEMBL3780225,TP,ACT,1.0	CHEMBL1094809,FP,INACT,0.8500000238418579	CHEMBL2314180,TP,ACT,0.9800000190734863	CHEMBL335265,TN,INACT,0.47999998927116394	CHEMBL3781868,TP,ACT,0.8500000238418579	CHEMBL3654307,FP,INACT,0.8299999833106995	CHEMBL209148,FP,INACT,0.7900000214576721	CHEMBL2333151,TP,ACT,0.9399999976158142	CHEMBL395653,TN,INACT,0.4699999988079071	CHEMBL51483,FN,ACT,0.6700000166893005	CHEMBL1828984,TN,INACT,0.4699999988079071	CHEMBL408732,TN,INACT,0.6499999761581421	CHEMBL3813804,TP,ACT,0.8600000143051147	CHEMBL1373095,TP,ACT,0.9900000095367432	CHEMBL1374667,FP,INACT,0.800000011920929	CHEMBL3787127,TP,ACT,0.9800000190734863	CHEMBL2171491,TP,ACT,0.9599999785423279	CHEMBL40835,TN,INACT,0.4699999988079071	CHEMBL3813743,FP,INACT,0.8199999928474426	CHEMBL3780113,TP,ACT,0.8500000238418579	CHEMBL3798160,TP,ACT,0.8199999928474426	CHEMBL1824826,TP,ACT,0.9800000190734863	CHEMBL3221336,FP,INACT,0.9300000071525574	CHEMBL1371985,TN,INACT,0.6800000071525574	CHEMBL1824815,TP,ACT,0.9900000095367432	CHEMBL1269074,TP,ACT,0.8899999856948853	CHEMBL2321969,FP,INACT,0.9200000166893005	CHEMBL3780058,FN,ACT,0.5400000214576721	CHEMBL216213,FP,INACT,0.9900000095367432	CHEMBL1828986,TN,INACT,0.47999998927116394	CHEMBL3814238,FP,INACT,0.9100000262260437	CHEMBL2314523,TN,INACT,0.4699999988079071	CHEMBL538616,TP,ACT,0.9800000190734863	CHEMBL3265301,TP,ACT,0.9800000190734863	CHEMBL270268,FN,ACT,0.6600000262260437	CHEMBL3785514,TP,ACT,1.0	CHEMBL3781894,TP,ACT,0.800000011920929	CHEMBL3265292,TP,ACT,0.7900000214576721	CHEMBL2374449,FP,INACT,0.949999988079071	CHEMBL3265303,FN,ACT,0.6200000047683716	CHEMBL3786306,TP,ACT,1.0	CHEMBL1824813,TP,ACT,1.0	CHEMBL3770890,TN,INACT,0.4699999988079071	CHEMBL3426331,TP,ACT,0.9399999976158142	CHEMBL3785892,TP,ACT,0.9900000095367432	CHEMBL3779915,TN,INACT,0.550000011920929	CHEMBL2312484,TP,ACT,0.9599999785423279	CHEMBL437577,TN,INACT,0.7599999904632568	CHEMBL3798676,TP,ACT,0.9800000190734863	CHEMBL1539493,TN,INACT,0.4699999988079071	CHEMBL1574133,TN,INACT,0.7599999904632568	CHEMBL2436886,FN,ACT,0.6200000047683716	CHEMBL1966952,TP,ACT,0.9800000190734863	CHEMBL226678,FN,ACT,0.4699999988079071	CHEMBL3780614,TN,INACT,0.49000000953674316	CHEMBL3426335,TP,ACT,0.9399999976158142	CHEMBL1081198,FP,INACT,0.8299999833106995	CHEMBL2180319,TP,ACT,0.9399999976158142	CHEMBL1824186,TP,ACT,0.9700000286102295	CHEMBL193114,TN,INACT,0.4699999988079071	CHEMBL2334796,FP,INACT,0.8500000238418579	CHEMBL3417405,FP,INACT,0.8799999952316284	CHEMBL1385808,TP,ACT,0.8600000143051147	CHEMBL1310309,FP,INACT,0.9100000262260437	CHEMBL1401406,TP,ACT,0.8700000047683716	CHEMBL3126311,TP,ACT,0.9399999976158142	CHEMBL3193581,FP,INACT,0.949999988079071	CHEMBL2314369,TN,INACT,0.5	CHEMBL449317,TN,INACT,0.75	CHEMBL572922,TN,INACT,0.5699999928474426	CHEMBL3417580,TP,ACT,0.7900000214576721	CHEMBL1095758,FP,INACT,0.8600000143051147	CHEMBL3189416,TP,ACT,0.9599999785423279	CHEMBL312065,TN,INACT,0.4699999988079071	CHEMBL1269110,TP,ACT,0.8700000047683716	CHEMBL3126118,TP,ACT,0.9300000071525574	CHEMBL3770132,TN,INACT,0.5400000214576721	CHEMBL3265304,TN,INACT,0.6299999952316284	CHEMBL3417691,TP,ACT,1.0	CHEMBL3417703,TP,ACT,0.9599999785423279	CHEMBL1672073,TP,ACT,0.9399999976158142	CHEMBL2312471,TP,ACT,0.9599999785423279	CHEMBL3298854,TP,ACT,0.9399999976158142	CHEMBL3771369,TN,INACT,0.6899999976158142	CHEMBL3780094,TN,INACT,0.7400000095367432	CHEMBL177299,TN,INACT,0.4699999988079071	CHEMBL2314517,TP,ACT,0.8600000143051147	CHEMBL1311053,FN,ACT,0.5299999713897705	CHEMBL1417070,TP,ACT,0.8100000023841858	CHEMBL2334798,FP,INACT,0.8500000238418579	CHEMBL3126117,FP,INACT,0.9200000166893005	CHEMBL410802,TP,ACT,0.8100000023841858	CHEMBL3781669,TP,ACT,0.9800000190734863	CHEMBL3786317,TP,ACT,1.0	CHEMBL3265311,TP,ACT,0.8299999833106995	CHEMBL243190,FP,INACT,0.9399999976158142	CHEMBL1672076,TP,ACT,0.9900000095367432	CHEMBL1094250,TP,ACT,0.949999988079071	CHEMBL242339,TN,INACT,0.47999998927116394	CHEMBL497196,TN,INACT,0.75	CHEMBL2314520,FP,INACT,0.8700000047683716	CHEMBL2314506,TN,INACT,0.4699999988079071	CHEMBL186213,FP,INACT,0.949999988079071	CHEMBL2314172,TP,ACT,0.9900000095367432	CHEMBL3260033,TP,ACT,0.9800000190734863	CHEMBL3786706,TP,ACT,1.0	CHEMBL3786582,TP,ACT,1.0	CHEMBL1957944,FP,INACT,0.8100000023841858	CHEMBL3781059,TP,ACT,0.8299999833106995	CHEMBL508790,TN,INACT,0.4699999988079071	CHEMBL3417693,TP,ACT,1.0	CHEMBL2314202,TP,ACT,0.949999988079071	CHEMBL185516,TN,INACT,0.5099999904632568	CHEMBL3126119,TP,ACT,0.9599999785423279	CHEMBL3787037,TP,ACT,1.0	CHEMBL3401816,TP,ACT,0.9700000286102295	CHEMBL3126324,TP,ACT,0.8100000023841858	CHEMBL3260041,FP,INACT,0.9100000262260437	CHEMBL1824801,FP,INACT,0.9800000190734863	CHEMBL2334786,TN,INACT,0.75	CHEMBL1331211,TP,ACT,0.8999999761581421	CHEMBL3401800,TP,ACT,0.9800000190734863	CHEMBL3126323,TP,ACT,0.800000011920929	CHEMBL143525,TN,INACT,0.4699999988079071	CHEMBL2334797,FP,INACT,0.8500000238418579	CHEMBL1824822,FP,INACT,0.9900000095367432	CHEMBL1410420,TN,INACT,0.4699999988079071	CHEMBL3629660,TP,ACT,0.9900000095367432	CHEMBL1330042,TN,INACT,0.47999998927116394	CHEMBL1957095,FP,INACT,0.8700000047683716	CHEMBL219234,TN,INACT,0.6200000047683716	CHEMBL178179,FP,INACT,0.8600000143051147	CHEMBL1230189,TN,INACT,0.4699999988079071	CHEMBL3787178,TP,ACT,1.0	CHEMBL3401801,TP,ACT,0.9800000190734863	CHEMBL1940678,TP,ACT,0.9300000071525574	CHEMBL3417409,FP,INACT,0.9800000190734863	CHEMBL3780355,TN,INACT,0.6899999976158142	CHEMBL571525,TN,INACT,0.47999998927116394	CHEMBL3265300,TP,ACT,0.8700000047683716	CHEMBL243203,TN,INACT,0.6800000071525574	CHEMBL3426325,TP,ACT,0.9800000190734863	CHEMBL140,FP,INACT,0.9900000095367432	CHEMBL1397916,TP,ACT,0.9700000286102295	CHEMBL2314194,TP,ACT,0.9599999785423279	CHEMBL3426328,TP,ACT,0.9399999976158142	CHEMBL3781264,TP,ACT,0.8299999833106995	CHEMBL3358892,TP,ACT,0.8299999833106995	CHEMBL1824811,TP,ACT,0.9900000095367432	CHEMBL2314522,TP,ACT,0.8100000023841858	CHEMBL541004,TP,ACT,0.9700000286102295	CHEMBL1940666,TP,ACT,0.9599999785423279	CHEMBL1824825,TP,ACT,0.9900000095367432	

