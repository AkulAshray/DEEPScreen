CNNModel CHEMBL1944 adam 0.001 15 128 0 0.8 False True
Number of active compounds :	184
Number of inactive compounds :	123
---------------------------------
Run id: CNNModel_CHEMBL1944_adam_0.001_15_128_0_0.8_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL1944_adam_0.001_15_128_0.8_True/
---------------------------------
Training samples: 185
Validation samples: 58
--
Training Step: 1  | time: 26.489s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/185
[A[ATraining Step: 2  | total loss: [1m[32m0.62396[0m[0m | time: 35.012s
[2K
| Adam | epoch: 001 | loss: 0.62396 - acc: 0.3094 -- iter: 064/185
[A[ATraining Step: 3  | total loss: [1m[32m0.67802[0m[0m | time: 36.226s
[2K
| Adam | epoch: 001 | loss: 0.67802 - acc: 0.5165 -- iter: 096/185
[A[ATraining Step: 4  | total loss: [1m[32m0.63405[0m[0m | time: 37.273s
[2K
| Adam | epoch: 001 | loss: 0.63405 - acc: 0.7385 -- iter: 128/185
[A[ATraining Step: 5  | total loss: [1m[32m0.65725[0m[0m | time: 38.377s
[2K
| Adam | epoch: 001 | loss: 0.65725 - acc: 0.6599 -- iter: 160/185
[A[ATraining Step: 6  | total loss: [1m[32m0.69692[0m[0m | time: 40.447s
[2K
| Adam | epoch: 001 | loss: 0.69692 - acc: 0.6174 | val_loss: 0.66374 - val_acc: 0.6379 -- iter: 185/185
--
Training Step: 7  | total loss: [1m[32m0.76186[0m[0m | time: 0.960s
[2K
| Adam | epoch: 002 | loss: 0.76186 - acc: 0.4870 -- iter: 032/185
[A[ATraining Step: 8  | total loss: [1m[32m0.74348[0m[0m | time: 1.949s
[2K
| Adam | epoch: 002 | loss: 0.74348 - acc: 0.4380 -- iter: 064/185
[A[ATraining Step: 9  | total loss: [1m[32m0.71639[0m[0m | time: 2.780s
[2K
| Adam | epoch: 002 | loss: 0.71639 - acc: 0.4874 -- iter: 096/185
[A[ATraining Step: 10  | total loss: [1m[32m0.70345[0m[0m | time: 4.018s
[2K
| Adam | epoch: 002 | loss: 0.70345 - acc: 0.5249 -- iter: 128/185
[A[ATraining Step: 11  | total loss: [1m[32m0.69570[0m[0m | time: 5.329s
[2K
| Adam | epoch: 002 | loss: 0.69570 - acc: 0.6019 -- iter: 160/185
[A[ATraining Step: 12  | total loss: [1m[32m0.69507[0m[0m | time: 28.351s
[2K
| Adam | epoch: 002 | loss: 0.69507 - acc: 0.5420 | val_loss: 0.69025 - val_acc: 0.6379 -- iter: 185/185
--
Training Step: 13  | total loss: [1m[32m0.69307[0m[0m | time: 12.593s
[2K
| Adam | epoch: 003 | loss: 0.69307 - acc: 0.5776 -- iter: 032/185
[A[ATraining Step: 14  | total loss: [1m[32m0.69293[0m[0m | time: 30.992s
[2K
| Adam | epoch: 003 | loss: 0.69293 - acc: 0.5540 -- iter: 064/185
[A[ATraining Step: 15  | total loss: [1m[32m0.69288[0m[0m | time: 55.468s
[2K
| Adam | epoch: 003 | loss: 0.69288 - acc: 0.5407 -- iter: 096/185
[A[ATraining Step: 16  | total loss: [1m[32m0.69142[0m[0m | time: 70.241s
[2K
| Adam | epoch: 003 | loss: 0.69142 - acc: 0.5958 -- iter: 128/185
[A[ATraining Step: 17  | total loss: [1m[32m0.69065[0m[0m | time: 86.367s
[2K
| Adam | epoch: 003 | loss: 0.69065 - acc: 0.6175 -- iter: 160/185
[A[ATraining Step: 18  | total loss: [1m[32m0.69033[0m[0m | time: 104.073s
[2K
| Adam | epoch: 003 | loss: 0.69033 - acc: 0.6201 | val_loss: 0.68803 - val_acc: 0.6379 -- iter: 185/185
--
Training Step: 19  | total loss: [1m[32m0.69095[0m[0m | time: 3.676s
[2K
| Adam | epoch: 004 | loss: 0.69095 - acc: 0.5905 -- iter: 032/185
[A[ATraining Step: 20  | total loss: [1m[32m0.69100[0m[0m | time: 7.471s
[2K
| Adam | epoch: 004 | loss: 0.69100 - acc: 0.5815 -- iter: 064/185
[A[ATraining Step: 21  | total loss: [1m[32m0.69044[0m[0m | time: 11.729s
[2K
| Adam | epoch: 004 | loss: 0.69044 - acc: 0.5872 -- iter: 096/185
[A[ATraining Step: 22  | total loss: [1m[32m0.68994[0m[0m | time: 12.747s
[2K
| Adam | epoch: 004 | loss: 0.68994 - acc: 0.5911 -- iter: 128/185
[A[ATraining Step: 23  | total loss: [1m[32m0.68927[0m[0m | time: 13.851s
[2K
| Adam | epoch: 004 | loss: 0.68927 - acc: 0.5918 -- iter: 160/185
[A[ATraining Step: 24  | total loss: [1m[32m0.68742[0m[0m | time: 16.081s
[2K
| Adam | epoch: 004 | loss: 0.68742 - acc: 0.6100 | val_loss: 0.68036 - val_acc: 0.6379 -- iter: 185/185
--
Training Step: 25  | total loss: [1m[32m0.68921[0m[0m | time: 1.524s
[2K
| Adam | epoch: 005 | loss: 0.68921 - acc: 0.5800 -- iter: 032/185
[A[ATraining Step: 26  | total loss: [1m[32m0.69305[0m[0m | time: 2.804s
[2K
| Adam | epoch: 005 | loss: 0.69305 - acc: 0.5340 -- iter: 064/185
[A[ATraining Step: 27  | total loss: [1m[32m0.69008[0m[0m | time: 3.749s
[2K
| Adam | epoch: 005 | loss: 0.69008 - acc: 0.5574 -- iter: 096/185
[A[ATraining Step: 28  | total loss: [1m[32m0.68385[0m[0m | time: 4.778s
[2K
| Adam | epoch: 005 | loss: 0.68385 - acc: 0.6080 -- iter: 128/185
[A[ATraining Step: 29  | total loss: [1m[32m0.67817[0m[0m | time: 6.521s
[2K
| Adam | epoch: 005 | loss: 0.67817 - acc: 0.6450 -- iter: 160/185
[A[ATraining Step: 30  | total loss: [1m[32m0.67657[0m[0m | time: 20.832s
[2K
| Adam | epoch: 005 | loss: 0.67657 - acc: 0.6477 | val_loss: 0.66304 - val_acc: 0.6379 -- iter: 185/185
--
Training Step: 31  | total loss: [1m[32m0.68026[0m[0m | time: 25.758s
[2K
| Adam | epoch: 006 | loss: 0.68026 - acc: 0.6208 -- iter: 032/185
[A[ATraining Step: 32  | total loss: [1m[32m0.67507[0m[0m | time: 40.415s
[2K
| Adam | epoch: 006 | loss: 0.67507 - acc: 0.6288 -- iter: 064/185
[A[ATraining Step: 33  | total loss: [1m[32m0.67513[0m[0m | time: 54.044s
[2K
| Adam | epoch: 006 | loss: 0.67513 - acc: 0.6211 -- iter: 096/185
[A[ATraining Step: 34  | total loss: [1m[32m0.67896[0m[0m | time: 58.941s
[2K
| Adam | epoch: 006 | loss: 0.67896 - acc: 0.6085 -- iter: 128/185
[A[ATraining Step: 35  | total loss: [1m[32m0.67832[0m[0m | time: 67.360s
[2K
| Adam | epoch: 006 | loss: 0.67832 - acc: 0.6067 -- iter: 160/185
[A[ATraining Step: 36  | total loss: [1m[32m0.67724[0m[0m | time: 78.551s
[2K
| Adam | epoch: 006 | loss: 0.67724 - acc: 0.6054 | val_loss: 0.65363 - val_acc: 0.6379 -- iter: 185/185
--
Training Step: 37  | total loss: [1m[32m0.67412[0m[0m | time: 7.097s
[2K
| Adam | epoch: 007 | loss: 0.67412 - acc: 0.6093 -- iter: 032/185
[A[ATraining Step: 38  | total loss: [1m[32m0.68337[0m[0m | time: 12.319s
[2K
| Adam | epoch: 007 | loss: 0.68337 - acc: 0.5879 -- iter: 064/185
[A[ATraining Step: 39  | total loss: [1m[32m0.68133[0m[0m | time: 19.923s
[2K
| Adam | epoch: 007 | loss: 0.68133 - acc: 0.5890 -- iter: 096/185
[A[ATraining Step: 40  | total loss: [1m[32m0.67860[0m[0m | time: 23.091s
[2K
| Adam | epoch: 007 | loss: 0.67860 - acc: 0.5958 -- iter: 128/185
[A[ATraining Step: 41  | total loss: [1m[32m0.67633[0m[0m | time: 32.494s
[2K
| Adam | epoch: 007 | loss: 0.67633 - acc: 0.6011 -- iter: 160/185
[A[ATraining Step: 42  | total loss: [1m[32m0.67615[0m[0m | time: 52.307s
[2K
| Adam | epoch: 007 | loss: 0.67615 - acc: 0.6009 | val_loss: 0.66239 - val_acc: 0.6379 -- iter: 185/185
--
Training Step: 43  | total loss: [1m[32m0.67588[0m[0m | time: 2.634s
[2K
| Adam | epoch: 008 | loss: 0.67588 - acc: 0.6008 -- iter: 032/185
[A[ATraining Step: 44  | total loss: [1m[32m0.67414[0m[0m | time: 4.974s
[2K
| Adam | epoch: 008 | loss: 0.67414 - acc: 0.6050 -- iter: 064/185
[A[ATraining Step: 45  | total loss: [1m[32m0.67475[0m[0m | time: 7.045s
[2K
| Adam | epoch: 008 | loss: 0.67475 - acc: 0.6031 -- iter: 096/185
[A[ATraining Step: 46  | total loss: [1m[32m0.67635[0m[0m | time: 12.816s
[2K
| Adam | epoch: 008 | loss: 0.67635 - acc: 0.5963 -- iter: 128/185
[A[ATraining Step: 47  | total loss: [1m[32m0.67148[0m[0m | time: 14.018s
[2K
| Adam | epoch: 008 | loss: 0.67148 - acc: 0.6112 -- iter: 160/185
[A[ATraining Step: 48  | total loss: [1m[32m0.67562[0m[0m | time: 16.196s
[2K
| Adam | epoch: 008 | loss: 0.67562 - acc: 0.5984 | val_loss: 0.65561 - val_acc: 0.6379 -- iter: 185/185
--
Training Step: 49  | total loss: [1m[32m0.67041[0m[0m | time: 1.593s
[2K
| Adam | epoch: 009 | loss: 0.67041 - acc: 0.6113 -- iter: 032/185
[A[ATraining Step: 50  | total loss: [1m[32m0.66558[0m[0m | time: 15.642s
[2K
| Adam | epoch: 009 | loss: 0.66558 - acc: 0.6219 -- iter: 064/185
[A[ATraining Step: 51  | total loss: [1m[32m0.67203[0m[0m | time: 20.641s
[2K
| Adam | epoch: 009 | loss: 0.67203 - acc: 0.6081 -- iter: 096/185
[A[ATraining Step: 52  | total loss: [1m[32m0.67286[0m[0m | time: 25.269s
[2K
| Adam | epoch: 009 | loss: 0.67286 - acc: 0.6059 -- iter: 128/185
[A[ATraining Step: 53  | total loss: [1m[32m0.68235[0m[0m | time: 32.131s
[2K
| Adam | epoch: 009 | loss: 0.68235 - acc: 0.5857 -- iter: 160/185
[A[ATraining Step: 54  | total loss: [1m[32m0.67505[0m[0m | time: 44.158s
[2K
| Adam | epoch: 009 | loss: 0.67505 - acc: 0.6005 | val_loss: 0.65521 - val_acc: 0.6379 -- iter: 185/185
--
Training Step: 55  | total loss: [1m[32m0.67663[0m[0m | time: 5.209s
[2K
| Adam | epoch: 010 | loss: 0.67663 - acc: 0.5951 -- iter: 032/185
[A[ATraining Step: 56  | total loss: [1m[32m0.67405[0m[0m | time: 10.863s
[2K
| Adam | epoch: 010 | loss: 0.67405 - acc: 0.6014 -- iter: 064/185
[A[ATraining Step: 57  | total loss: [1m[32m0.67150[0m[0m | time: 17.903s
[2K
| Adam | epoch: 010 | loss: 0.67150 - acc: 0.6067 -- iter: 096/185
[A[ATraining Step: 58  | total loss: [1m[32m0.66980[0m[0m | time: 20.982s
[2K
| Adam | epoch: 010 | loss: 0.66980 - acc: 0.6092 -- iter: 128/185
[A[ATraining Step: 59  | total loss: [1m[32m0.66540[0m[0m | time: 22.448s
[2K
| Adam | epoch: 010 | loss: 0.66540 - acc: 0.6197 -- iter: 160/185
[A[ATraining Step: 60  | total loss: [1m[32m0.66688[0m[0m | time: 24.742s
[2K
| Adam | epoch: 010 | loss: 0.66688 - acc: 0.6163 | val_loss: 0.65211 - val_acc: 0.6379 -- iter: 185/185
--
Training Step: 61  | total loss: [1m[32m0.66757[0m[0m | time: 2.594s
[2K
| Adam | epoch: 011 | loss: 0.66757 - acc: 0.6134 -- iter: 032/185
[A[ATraining Step: 62  | total loss: [1m[32m0.67226[0m[0m | time: 7.819s
[2K
| Adam | epoch: 011 | loss: 0.67226 - acc: 0.6028 -- iter: 064/185
[A[ATraining Step: 63  | total loss: [1m[32m0.67246[0m[0m | time: 20.251s
[2K
| Adam | epoch: 011 | loss: 0.67246 - acc: 0.6024 -- iter: 096/185
[A[ATraining Step: 64  | total loss: [1m[32m0.67272[0m[0m | time: 27.682s
[2K
| Adam | epoch: 011 | loss: 0.67272 - acc: 0.6021 -- iter: 128/185
[A[ATraining Step: 65  | total loss: [1m[32m0.67512[0m[0m | time: 30.554s
[2K
| Adam | epoch: 011 | loss: 0.67512 - acc: 0.5972 -- iter: 160/185
[A[ATraining Step: 66  | total loss: [1m[32m0.67664[0m[0m | time: 44.371s
[2K
| Adam | epoch: 011 | loss: 0.67664 - acc: 0.5930 | val_loss: 0.65912 - val_acc: 0.6379 -- iter: 185/185
--
Training Step: 67  | total loss: [1m[32m0.68075[0m[0m | time: 10.115s
[2K
| Adam | epoch: 012 | loss: 0.68075 - acc: 0.5819 -- iter: 032/185
[A[ATraining Step: 68  | total loss: [1m[32m0.68098[0m[0m | time: 25.422s
[2K
| Adam | epoch: 012 | loss: 0.68098 - acc: 0.5796 -- iter: 064/185
[A[ATraining Step: 69  | total loss: [1m[32m0.68031[0m[0m | time: 34.289s
[2K
| Adam | epoch: 012 | loss: 0.68031 - acc: 0.5812 -- iter: 096/185
[A[ATraining Step: 70  | total loss: [1m[32m0.67714[0m[0m | time: 40.234s
[2K
| Adam | epoch: 012 | loss: 0.67714 - acc: 0.5926 -- iter: 128/185
[A[ATraining Step: 71  | total loss: [1m[32m0.67435[0m[0m | time: 46.945s
[2K
| Adam | epoch: 012 | loss: 0.67435 - acc: 0.6026 -- iter: 160/185
[A[ATraining Step: 72  | total loss: [1m[32m0.67160[0m[0m | time: 58.690s
[2K
| Adam | epoch: 012 | loss: 0.67160 - acc: 0.6121 | val_loss: 0.66050 - val_acc: 0.6379 -- iter: 185/185
--
Training Step: 73  | total loss: [1m[32m0.67282[0m[0m | time: 9.034s
[2K
| Adam | epoch: 013 | loss: 0.67282 - acc: 0.6066 -- iter: 032/185
[A[ATraining Step: 74  | total loss: [1m[32m0.67308[0m[0m | time: 13.539s
[2K
| Adam | epoch: 013 | loss: 0.67308 - acc: 0.6052 -- iter: 064/185
[A[ATraining Step: 75  | total loss: [1m[32m0.67216[0m[0m | time: 20.470s
[2K
| Adam | epoch: 013 | loss: 0.67216 - acc: 0.6074 -- iter: 096/185
[A[ATraining Step: 76  | total loss: [1m[32m0.67105[0m[0m | time: 25.775s
[2K
| Adam | epoch: 013 | loss: 0.67105 - acc: 0.6092 -- iter: 128/185
[A[ATraining Step: 77  | total loss: [1m[32m0.67392[0m[0m | time: 27.421s
[2K
| Adam | epoch: 013 | loss: 0.67392 - acc: 0.5998 -- iter: 160/185
[A[ATraining Step: 78  | total loss: [1m[32m0.67666[0m[0m | time: 35.797s
[2K
| Adam | epoch: 013 | loss: 0.67666 - acc: 0.5914 | val_loss: 0.65293 - val_acc: 0.6379 -- iter: 185/185
--
Training Step: 79  | total loss: [1m[32m0.67731[0m[0m | time: 2.747s
[2K
| Adam | epoch: 014 | loss: 0.67731 - acc: 0.5884 -- iter: 032/185
[A[ATraining Step: 80  | total loss: [1m[32m0.67699[0m[0m | time: 5.872s
[2K
| Adam | epoch: 014 | loss: 0.67699 - acc: 0.5890 -- iter: 064/185
[A[ATraining Step: 81  | total loss: [1m[32m0.67398[0m[0m | time: 7.165s
[2K
| Adam | epoch: 014 | loss: 0.67398 - acc: 0.5958 -- iter: 096/185
[A[ATraining Step: 82  | total loss: [1m[32m0.67655[0m[0m | time: 8.643s
[2K
| Adam | epoch: 014 | loss: 0.67655 - acc: 0.5893 -- iter: 128/185
[A[ATraining Step: 83  | total loss: [1m[32m0.68077[0m[0m | time: 10.063s
[2K
| Adam | epoch: 014 | loss: 0.68077 - acc: 0.5773 -- iter: 160/185
[A[ATraining Step: 84  | total loss: [1m[32m0.67531[0m[0m | time: 22.244s
[2K
| Adam | epoch: 014 | loss: 0.67531 - acc: 0.5916 | val_loss: 0.64996 - val_acc: 0.6379 -- iter: 185/185
--
Training Step: 85  | total loss: [1m[32m0.67021[0m[0m | time: 5.969s
[2K
| Adam | epoch: 015 | loss: 0.67021 - acc: 0.6044 -- iter: 032/185
[A[ATraining Step: 86  | total loss: [1m[32m0.66871[0m[0m | time: 14.633s
[2K
| Adam | epoch: 015 | loss: 0.66871 - acc: 0.6065 -- iter: 064/185
[A[ATraining Step: 87  | total loss: [1m[32m0.67004[0m[0m | time: 19.447s
[2K
| Adam | epoch: 015 | loss: 0.67004 - acc: 0.6021 -- iter: 096/185
[A[ATraining Step: 88  | total loss: [1m[32m0.66433[0m[0m | time: 27.972s
[2K
| Adam | epoch: 015 | loss: 0.66433 - acc: 0.6137 -- iter: 128/185
[A[ATraining Step: 89  | total loss: [1m[32m0.66525[0m[0m | time: 32.776s
[2K
| Adam | epoch: 015 | loss: 0.66525 - acc: 0.6117 -- iter: 160/185
[A[ATraining Step: 90  | total loss: [1m[32m0.67417[0m[0m | time: 51.506s
[2K
| Adam | epoch: 015 | loss: 0.67417 - acc: 0.5974 | val_loss: 0.64239 - val_acc: 0.6379 -- iter: 185/185
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.8468468468468469
Validation AUPRC:0.9035827175799704
Test AUC:0.7857142857142857
Test AUPRC:0.7850136956524987
BestTestF1Score	0.75	0.47	0.71	0.63	0.93	26	15	15	2	0.62
BestTestMCCScore	0.75	0.47	0.71	0.63	0.93	26	15	15	2	0.62
BestTestAccuracyScore	0.75	0.47	0.71	0.63	0.93	26	15	15	2	0.62
BestValidationF1Score	0.86	0.63	0.83	0.86	0.86	32	5	16	5	0.62
BestValidationMCC	0.86	0.63	0.83	0.86	0.86	32	5	16	5	0.62
BestValidationAccuracy	0.86	0.63	0.83	0.86	0.86	32	5	16	5	0.62
TestPredictions (Threshold:0.62)
CHEMBL92632,TP,ACT,0.6399999856948853	CHEMBL58516,FP,INACT,0.6200000047683716	CHEMBL2051764,FP,INACT,0.6200000047683716	CHEMBL1829583,TP,ACT,0.6200000047683716	CHEMBL1161576,TN,INACT,0.6000000238418579	CHEMBL1939428,FP,INACT,0.6399999856948853	CHEMBL254493,TP,ACT,0.6200000047683716	CHEMBL3263103,FP,INACT,0.6200000047683716	CHEMBL389061,TP,ACT,0.6299999952316284	CHEMBL51438,TP,ACT,0.6299999952316284	CHEMBL1161558,TN,INACT,0.6000000238418579	CHEMBL2022230,FP,INACT,0.6200000047683716	CHEMBL1161542,TN,INACT,0.6000000238418579	CHEMBL77258,FP,INACT,0.6200000047683716	CHEMBL2177692,TN,INACT,0.6100000143051147	CHEMBL85430,FP,INACT,0.6299999952316284	CHEMBL284818,TP,ACT,0.6200000047683716	CHEMBL2024268,TN,INACT,0.6100000143051147	CHEMBL376425,TP,ACT,0.6299999952316284	CHEMBL11297,TP,ACT,0.6299999952316284	CHEMBL306353,FP,INACT,0.6299999952316284	CHEMBL289556,TP,ACT,0.6200000047683716	CHEMBL1160185,FP,INACT,0.6299999952316284	CHEMBL2370089,FP,INACT,0.6200000047683716	CHEMBL420253,TP,ACT,0.6399999856948853	CHEMBL85320,TP,ACT,0.6299999952316284	CHEMBL66481,TP,ACT,0.6200000047683716	CHEMBL3221959,FP,INACT,0.6299999952316284	CHEMBL499404,TN,INACT,0.6100000143051147	CHEMBL1201151,TN,INACT,0.6100000143051147	CHEMBL1939278,FP,INACT,0.6200000047683716	CHEMBL26703,TP,ACT,0.6299999952316284	CHEMBL1161575,TN,INACT,0.6000000238418579	CHEMBL145,TN,INACT,0.6100000143051147	CHEMBL261634,FP,INACT,0.6200000047683716	CHEMBL308333,TP,ACT,0.6200000047683716	CHEMBL70475,FP,INACT,0.6200000047683716	CHEMBL1161567,TN,INACT,0.6000000238418579	CHEMBL163454,TP,ACT,0.6399999856948853	CHEMBL303469,FP,INACT,0.6200000047683716	CHEMBL3298937,TP,ACT,0.6200000047683716	CHEMBL2022235,TN,INACT,0.6100000143051147	CHEMBL37820,TP,ACT,0.6399999856948853	CHEMBL254900,FN,ACT,0.6100000143051147	CHEMBL442337,TP,ACT,0.6200000047683716	CHEMBL417326,TP,ACT,0.6399999856948853	CHEMBL501385,TN,INACT,0.6100000143051147	CHEMBL149367,TP,ACT,0.6299999952316284	CHEMBL275690,TP,ACT,0.6200000047683716	CHEMBL1161552,TN,INACT,0.6000000238418579	CHEMBL1684165,TN,INACT,0.6100000143051147	CHEMBL328737,TP,ACT,0.6399999856948853	CHEMBL257026,TP,ACT,0.6299999952316284	CHEMBL1778527,TP,ACT,0.6200000047683716	CHEMBL328463,FN,ACT,0.6100000143051147	CHEMBL2022229,TN,INACT,0.6100000143051147	CHEMBL9963,TP,ACT,0.6200000047683716	CHEMBL211500,TP,ACT,0.6200000047683716	

