CNNModel CHEMBL5373 adam 0.0005 15 256 0 0.8 False True
Number of active compounds :	368
Number of inactive compounds :	368
---------------------------------
Run id: CNNModel_CHEMBL5373_adam_0.0005_15_256_0_0.8_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL5373_adam_0.0005_15_256_0.8_True/
---------------------------------
Training samples: 448
Validation samples: 140
--
Training Step: 1  | time: 1.338s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/448
[A[ATraining Step: 2  | total loss: [1m[32m0.62459[0m[0m | time: 2.368s
[2K
| Adam | epoch: 001 | loss: 0.62459 - acc: 0.2812 -- iter: 064/448
[A[ATraining Step: 3  | total loss: [1m[32m0.68013[0m[0m | time: 3.243s
[2K
| Adam | epoch: 001 | loss: 0.68013 - acc: 0.5114 -- iter: 096/448
[A[ATraining Step: 4  | total loss: [1m[32m0.69127[0m[0m | time: 4.183s
[2K
| Adam | epoch: 001 | loss: 0.69127 - acc: 0.4560 -- iter: 128/448
[A[ATraining Step: 5  | total loss: [1m[32m0.69502[0m[0m | time: 5.287s
[2K
| Adam | epoch: 001 | loss: 0.69502 - acc: 0.3783 -- iter: 160/448
[A[ATraining Step: 6  | total loss: [1m[32m0.69405[0m[0m | time: 6.338s
[2K
| Adam | epoch: 001 | loss: 0.69405 - acc: 0.4364 -- iter: 192/448
[A[ATraining Step: 7  | total loss: [1m[32m0.69357[0m[0m | time: 7.152s
[2K
| Adam | epoch: 001 | loss: 0.69357 - acc: 0.4183 -- iter: 224/448
[A[ATraining Step: 8  | total loss: [1m[32m0.69385[0m[0m | time: 8.008s
[2K
| Adam | epoch: 001 | loss: 0.69385 - acc: 0.3764 -- iter: 256/448
[A[ATraining Step: 9  | total loss: [1m[32m0.69331[0m[0m | time: 8.984s
[2K
| Adam | epoch: 001 | loss: 0.69331 - acc: 0.5080 -- iter: 288/448
[A[ATraining Step: 10  | total loss: [1m[32m0.69321[0m[0m | time: 9.925s
[2K
| Adam | epoch: 001 | loss: 0.69321 - acc: 0.5040 -- iter: 320/448
[A[ATraining Step: 11  | total loss: [1m[32m0.69271[0m[0m | time: 10.934s
[2K
| Adam | epoch: 001 | loss: 0.69271 - acc: 0.5761 -- iter: 352/448
[A[ATraining Step: 12  | total loss: [1m[32m0.69315[0m[0m | time: 11.955s
[2K
| Adam | epoch: 001 | loss: 0.69315 - acc: 0.5278 -- iter: 384/448
[A[ATraining Step: 13  | total loss: [1m[32m0.69225[0m[0m | time: 12.945s
[2K
| Adam | epoch: 001 | loss: 0.69225 - acc: 0.5695 -- iter: 416/448
[A[ATraining Step: 14  | total loss: [1m[32m0.69282[0m[0m | time: 15.017s
[2K
| Adam | epoch: 001 | loss: 0.69282 - acc: 0.5283 | val_loss: 0.69377 - val_acc: 0.4857 -- iter: 448/448
--
Training Step: 15  | total loss: [1m[32m0.69483[0m[0m | time: 0.996s
[2K
| Adam | epoch: 002 | loss: 0.69483 - acc: 0.4683 -- iter: 032/448
[A[ATraining Step: 16  | total loss: [1m[32m0.69577[0m[0m | time: 2.008s
[2K
| Adam | epoch: 002 | loss: 0.69577 - acc: 0.4333 -- iter: 064/448
[A[ATraining Step: 17  | total loss: [1m[32m0.69572[0m[0m | time: 2.979s
[2K
| Adam | epoch: 002 | loss: 0.69572 - acc: 0.4236 -- iter: 096/448
[A[ATraining Step: 18  | total loss: [1m[32m0.69485[0m[0m | time: 4.029s
[2K
| Adam | epoch: 002 | loss: 0.69485 - acc: 0.4500 -- iter: 128/448
[A[ATraining Step: 19  | total loss: [1m[32m0.69391[0m[0m | time: 4.969s
[2K
| Adam | epoch: 002 | loss: 0.69391 - acc: 0.4979 -- iter: 160/448
[A[ATraining Step: 20  | total loss: [1m[32m0.69360[0m[0m | time: 5.810s
[2K
| Adam | epoch: 002 | loss: 0.69360 - acc: 0.5086 -- iter: 192/448
[A[ATraining Step: 21  | total loss: [1m[32m0.69317[0m[0m | time: 6.593s
[2K
| Adam | epoch: 002 | loss: 0.69317 - acc: 0.5448 -- iter: 224/448
[A[ATraining Step: 22  | total loss: [1m[32m0.69332[0m[0m | time: 7.353s
[2K
| Adam | epoch: 002 | loss: 0.69332 - acc: 0.5126 -- iter: 256/448
[A[ATraining Step: 23  | total loss: [1m[32m0.69310[0m[0m | time: 8.240s
[2K
| Adam | epoch: 002 | loss: 0.69310 - acc: 0.5361 -- iter: 288/448
[A[ATraining Step: 24  | total loss: [1m[32m0.69316[0m[0m | time: 9.045s
[2K
| Adam | epoch: 002 | loss: 0.69316 - acc: 0.5260 -- iter: 320/448
[A[ATraining Step: 25  | total loss: [1m[32m0.69295[0m[0m | time: 10.207s
[2K
| Adam | epoch: 002 | loss: 0.69295 - acc: 0.5445 -- iter: 352/448
[A[ATraining Step: 26  | total loss: [1m[32m0.69279[0m[0m | time: 11.408s
[2K
| Adam | epoch: 002 | loss: 0.69279 - acc: 0.5575 -- iter: 384/448
[A[ATraining Step: 27  | total loss: [1m[32m0.69297[0m[0m | time: 12.722s
[2K
| Adam | epoch: 002 | loss: 0.69297 - acc: 0.5347 -- iter: 416/448
[A[ATraining Step: 28  | total loss: [1m[32m0.69262[0m[0m | time: 14.791s
[2K
| Adam | epoch: 002 | loss: 0.69262 - acc: 0.5573 | val_loss: 0.69335 - val_acc: 0.4857 -- iter: 448/448
--
Training Step: 29  | total loss: [1m[32m0.69306[0m[0m | time: 0.910s
[2K
| Adam | epoch: 003 | loss: 0.69306 - acc: 0.5205 -- iter: 032/448
[A[ATraining Step: 30  | total loss: [1m[32m0.69296[0m[0m | time: 1.883s
[2K
| Adam | epoch: 003 | loss: 0.69296 - acc: 0.5231 -- iter: 064/448
[A[ATraining Step: 31  | total loss: [1m[32m0.69293[0m[0m | time: 2.869s
[2K
| Adam | epoch: 003 | loss: 0.69293 - acc: 0.5250 -- iter: 096/448
[A[ATraining Step: 32  | total loss: [1m[32m0.69320[0m[0m | time: 3.761s
[2K
| Adam | epoch: 003 | loss: 0.69320 - acc: 0.5053 -- iter: 128/448
[A[ATraining Step: 33  | total loss: [1m[32m0.69308[0m[0m | time: 4.646s
[2K
| Adam | epoch: 003 | loss: 0.69308 - acc: 0.5110 -- iter: 160/448
[A[ATraining Step: 34  | total loss: [1m[32m0.69297[0m[0m | time: 5.676s
[2K
| Adam | epoch: 003 | loss: 0.69297 - acc: 0.5153 -- iter: 192/448
[A[ATraining Step: 35  | total loss: [1m[32m0.69273[0m[0m | time: 6.745s
[2K
| Adam | epoch: 003 | loss: 0.69273 - acc: 0.5252 -- iter: 224/448
[A[ATraining Step: 36  | total loss: [1m[32m0.69256[0m[0m | time: 7.521s
[2K
| Adam | epoch: 003 | loss: 0.69256 - acc: 0.5328 -- iter: 256/448
[A[ATraining Step: 37  | total loss: [1m[32m0.69294[0m[0m | time: 8.443s
[2K
| Adam | epoch: 003 | loss: 0.69294 - acc: 0.5138 -- iter: 288/448
[A[ATraining Step: 38  | total loss: [1m[32m0.69197[0m[0m | time: 9.386s
[2K
| Adam | epoch: 003 | loss: 0.69197 - acc: 0.5539 -- iter: 320/448
[A[ATraining Step: 39  | total loss: [1m[32m0.69268[0m[0m | time: 10.293s
[2K
| Adam | epoch: 003 | loss: 0.69268 - acc: 0.5256 -- iter: 352/448
[A[ATraining Step: 40  | total loss: [1m[32m0.69258[0m[0m | time: 11.201s
[2K
| Adam | epoch: 003 | loss: 0.69258 - acc: 0.5267 -- iter: 384/448
[A[ATraining Step: 41  | total loss: [1m[32m0.69282[0m[0m | time: 12.191s
[2K
| Adam | epoch: 003 | loss: 0.69282 - acc: 0.5160 -- iter: 416/448
[A[ATraining Step: 42  | total loss: [1m[32m0.69251[0m[0m | time: 14.146s
[2K
| Adam | epoch: 003 | loss: 0.69251 - acc: 0.5244 | val_loss: 0.69366 - val_acc: 0.4857 -- iter: 448/448
--
Training Step: 43  | total loss: [1m[32m0.69155[0m[0m | time: 0.907s
[2K
| Adam | epoch: 004 | loss: 0.69155 - acc: 0.5532 -- iter: 032/448
[A[ATraining Step: 44  | total loss: [1m[32m0.69136[0m[0m | time: 1.809s
[2K
| Adam | epoch: 004 | loss: 0.69136 - acc: 0.5548 -- iter: 064/448
[A[ATraining Step: 45  | total loss: [1m[32m0.69244[0m[0m | time: 2.742s
[2K
| Adam | epoch: 004 | loss: 0.69244 - acc: 0.5296 -- iter: 096/448
[A[ATraining Step: 46  | total loss: [1m[32m0.69335[0m[0m | time: 3.712s
[2K
| Adam | epoch: 004 | loss: 0.69335 - acc: 0.5090 -- iter: 128/448
[A[ATraining Step: 47  | total loss: [1m[32m0.69208[0m[0m | time: 4.671s
[2K
| Adam | epoch: 004 | loss: 0.69208 - acc: 0.5331 -- iter: 160/448
[A[ATraining Step: 48  | total loss: [1m[32m0.69278[0m[0m | time: 5.529s
[2K
| Adam | epoch: 004 | loss: 0.69278 - acc: 0.5177 -- iter: 192/448
[A[ATraining Step: 49  | total loss: [1m[32m0.69284[0m[0m | time: 6.398s
[2K
| Adam | epoch: 004 | loss: 0.69284 - acc: 0.5149 -- iter: 224/448
[A[ATraining Step: 50  | total loss: [1m[32m0.69250[0m[0m | time: 7.366s
[2K
| Adam | epoch: 004 | loss: 0.69250 - acc: 0.5175 -- iter: 256/448
[A[ATraining Step: 51  | total loss: [1m[32m0.69207[0m[0m | time: 8.384s
[2K
| Adam | epoch: 004 | loss: 0.69207 - acc: 0.5243 -- iter: 288/448
[A[ATraining Step: 52  | total loss: [1m[32m0.69137[0m[0m | time: 9.206s
[2K
| Adam | epoch: 004 | loss: 0.69137 - acc: 0.5348 -- iter: 320/448
[A[ATraining Step: 53  | total loss: [1m[32m0.69285[0m[0m | time: 10.050s
[2K
| Adam | epoch: 004 | loss: 0.69285 - acc: 0.5066 -- iter: 352/448
[A[ATraining Step: 54  | total loss: [1m[32m0.69175[0m[0m | time: 10.950s
[2K
| Adam | epoch: 004 | loss: 0.69175 - acc: 0.5238 -- iter: 384/448
[A[ATraining Step: 55  | total loss: [1m[32m0.69196[0m[0m | time: 11.866s
[2K
| Adam | epoch: 004 | loss: 0.69196 - acc: 0.5159 -- iter: 416/448
[A[ATraining Step: 56  | total loss: [1m[32m0.69238[0m[0m | time: 13.736s
[2K
| Adam | epoch: 004 | loss: 0.69238 - acc: 0.5049 | val_loss: 0.69225 - val_acc: 0.4857 -- iter: 448/448
--
Training Step: 57  | total loss: [1m[32m0.69235[0m[0m | time: 1.083s
[2K
| Adam | epoch: 005 | loss: 0.69235 - acc: 0.5042 -- iter: 032/448
[A[ATraining Step: 58  | total loss: [1m[32m0.69254[0m[0m | time: 2.327s
[2K
| Adam | epoch: 005 | loss: 0.69254 - acc: 0.4951 -- iter: 064/448
[A[ATraining Step: 59  | total loss: [1m[32m0.69206[0m[0m | time: 3.435s
[2K
| Adam | epoch: 005 | loss: 0.69206 - acc: 0.5000 -- iter: 096/448
[A[ATraining Step: 60  | total loss: [1m[32m0.69233[0m[0m | time: 4.631s
[2K
| Adam | epoch: 005 | loss: 0.69233 - acc: 0.4917 -- iter: 128/448
[A[ATraining Step: 61  | total loss: [1m[32m0.69251[0m[0m | time: 5.731s
[2K
| Adam | epoch: 005 | loss: 0.69251 - acc: 0.4846 -- iter: 160/448
[A[ATraining Step: 62  | total loss: [1m[32m0.69202[0m[0m | time: 6.903s
[2K
| Adam | epoch: 005 | loss: 0.69202 - acc: 0.4866 -- iter: 192/448
[A[ATraining Step: 63  | total loss: [1m[32m0.69109[0m[0m | time: 8.100s
[2K
| Adam | epoch: 005 | loss: 0.69109 - acc: 0.4962 -- iter: 224/448
[A[ATraining Step: 64  | total loss: [1m[32m0.68996[0m[0m | time: 9.280s
[2K
| Adam | epoch: 005 | loss: 0.68996 - acc: 0.5084 -- iter: 256/448
[A[ATraining Step: 65  | total loss: [1m[32m0.68993[0m[0m | time: 10.556s
[2K
| Adam | epoch: 005 | loss: 0.68993 - acc: 0.5035 -- iter: 288/448
[A[ATraining Step: 66  | total loss: [1m[32m0.68883[0m[0m | time: 11.768s
[2K
| Adam | epoch: 005 | loss: 0.68883 - acc: 0.5183 -- iter: 320/448
[A[ATraining Step: 67  | total loss: [1m[32m0.68723[0m[0m | time: 12.992s
[2K
| Adam | epoch: 005 | loss: 0.68723 - acc: 0.5536 -- iter: 352/448
[A[ATraining Step: 68  | total loss: [1m[32m0.68614[0m[0m | time: 14.177s
[2K
| Adam | epoch: 005 | loss: 0.68614 - acc: 0.5695 -- iter: 384/448
[A[ATraining Step: 69  | total loss: [1m[32m0.68343[0m[0m | time: 15.399s
[2K
| Adam | epoch: 005 | loss: 0.68343 - acc: 0.5760 -- iter: 416/448
[A[ATraining Step: 70  | total loss: [1m[32m0.68140[0m[0m | time: 17.593s
[2K
| Adam | epoch: 005 | loss: 0.68140 - acc: 0.5708 | val_loss: 0.66211 - val_acc: 0.6714 -- iter: 448/448
--
Training Step: 71  | total loss: [1m[32m0.68115[0m[0m | time: 1.167s
[2K
| Adam | epoch: 006 | loss: 0.68115 - acc: 0.5627 -- iter: 032/448
[A[ATraining Step: 72  | total loss: [1m[32m0.68011[0m[0m | time: 2.426s
[2K
| Adam | epoch: 006 | loss: 0.68011 - acc: 0.5697 -- iter: 064/448
[A[ATraining Step: 73  | total loss: [1m[32m0.67236[0m[0m | time: 3.614s
[2K
| Adam | epoch: 006 | loss: 0.67236 - acc: 0.5898 -- iter: 096/448
[A[ATraining Step: 74  | total loss: [1m[32m0.66592[0m[0m | time: 4.952s
[2K
| Adam | epoch: 006 | loss: 0.66592 - acc: 0.5971 -- iter: 128/448
[A[ATraining Step: 75  | total loss: [1m[32m0.67253[0m[0m | time: 6.317s
[2K
| Adam | epoch: 006 | loss: 0.67253 - acc: 0.5899 -- iter: 160/448
[A[ATraining Step: 76  | total loss: [1m[32m0.67586[0m[0m | time: 7.587s
[2K
| Adam | epoch: 006 | loss: 0.67586 - acc: 0.5836 -- iter: 192/448
[A[ATraining Step: 77  | total loss: [1m[32m0.67100[0m[0m | time: 8.794s
[2K
| Adam | epoch: 006 | loss: 0.67100 - acc: 0.5847 -- iter: 224/448
[A[ATraining Step: 78  | total loss: [1m[32m0.66468[0m[0m | time: 10.009s
[2K
| Adam | epoch: 006 | loss: 0.66468 - acc: 0.5987 -- iter: 256/448
[A[ATraining Step: 79  | total loss: [1m[32m0.66600[0m[0m | time: 11.338s
[2K
| Adam | epoch: 006 | loss: 0.66600 - acc: 0.5918 -- iter: 288/448
[A[ATraining Step: 80  | total loss: [1m[32m0.66179[0m[0m | time: 12.651s
[2K
| Adam | epoch: 006 | loss: 0.66179 - acc: 0.6015 -- iter: 320/448
[A[ATraining Step: 81  | total loss: [1m[32m0.65346[0m[0m | time: 14.960s
[2K
| Adam | epoch: 006 | loss: 0.65346 - acc: 0.6260 -- iter: 352/448
[A[ATraining Step: 82  | total loss: [1m[32m0.65123[0m[0m | time: 15.964s
[2K
| Adam | epoch: 006 | loss: 0.65123 - acc: 0.6322 -- iter: 384/448
[A[ATraining Step: 83  | total loss: [1m[32m0.65464[0m[0m | time: 17.056s
[2K
| Adam | epoch: 006 | loss: 0.65464 - acc: 0.6221 -- iter: 416/448
[A[ATraining Step: 84  | total loss: [1m[32m0.64692[0m[0m | time: 19.300s
[2K
| Adam | epoch: 006 | loss: 0.64692 - acc: 0.6349 | val_loss: 0.64186 - val_acc: 0.6214 -- iter: 448/448
--
Training Step: 85  | total loss: [1m[32m0.64606[0m[0m | time: 1.436s
[2K
| Adam | epoch: 007 | loss: 0.64606 - acc: 0.6339 -- iter: 032/448
[A[ATraining Step: 86  | total loss: [1m[32m0.64868[0m[0m | time: 2.720s
[2K
| Adam | epoch: 007 | loss: 0.64868 - acc: 0.6299 -- iter: 064/448
[A[ATraining Step: 87  | total loss: [1m[32m0.63668[0m[0m | time: 3.844s
[2K
| Adam | epoch: 007 | loss: 0.63668 - acc: 0.6450 -- iter: 096/448
[A[ATraining Step: 88  | total loss: [1m[32m0.62755[0m[0m | time: 5.110s
[2K
| Adam | epoch: 007 | loss: 0.62755 - acc: 0.6586 -- iter: 128/448
[A[ATraining Step: 89  | total loss: [1m[32m0.62593[0m[0m | time: 6.433s
[2K
| Adam | epoch: 007 | loss: 0.62593 - acc: 0.6678 -- iter: 160/448
[A[ATraining Step: 90  | total loss: [1m[32m0.62014[0m[0m | time: 7.683s
[2K
| Adam | epoch: 007 | loss: 0.62014 - acc: 0.6729 -- iter: 192/448
[A[ATraining Step: 91  | total loss: [1m[32m0.61242[0m[0m | time: 14.478s
[2K
| Adam | epoch: 007 | loss: 0.61242 - acc: 0.6775 -- iter: 224/448
[A[ATraining Step: 92  | total loss: [1m[32m0.60743[0m[0m | time: 22.285s
[2K
| Adam | epoch: 007 | loss: 0.60743 - acc: 0.6722 -- iter: 256/448
[A[ATraining Step: 93  | total loss: [1m[32m0.59610[0m[0m | time: 23.403s
[2K
| Adam | epoch: 007 | loss: 0.59610 - acc: 0.6862 -- iter: 288/448
[A[ATraining Step: 94  | total loss: [1m[32m0.58857[0m[0m | time: 24.780s
[2K
| Adam | epoch: 007 | loss: 0.58857 - acc: 0.6895 -- iter: 320/448
[A[ATraining Step: 95  | total loss: [1m[32m0.57770[0m[0m | time: 26.230s
[2K
| Adam | epoch: 007 | loss: 0.57770 - acc: 0.6987 -- iter: 352/448
[A[ATraining Step: 96  | total loss: [1m[32m0.56400[0m[0m | time: 27.588s
[2K
| Adam | epoch: 007 | loss: 0.56400 - acc: 0.7069 -- iter: 384/448
[A[ATraining Step: 97  | total loss: [1m[32m0.55917[0m[0m | time: 29.032s
[2K
| Adam | epoch: 007 | loss: 0.55917 - acc: 0.7112 -- iter: 416/448
[A[ATraining Step: 98  | total loss: [1m[32m0.57857[0m[0m | time: 31.933s
[2K
| Adam | epoch: 007 | loss: 0.57857 - acc: 0.7026 | val_loss: 0.57039 - val_acc: 0.7357 -- iter: 448/448
--
Training Step: 99  | total loss: [1m[32m0.55657[0m[0m | time: 1.221s
[2K
| Adam | epoch: 008 | loss: 0.55657 - acc: 0.7167 -- iter: 032/448
[A[ATraining Step: 100  | total loss: [1m[32m0.55436[0m[0m | time: 2.533s
[2K
| Adam | epoch: 008 | loss: 0.55436 - acc: 0.7232 -- iter: 064/448
[A[ATraining Step: 101  | total loss: [1m[32m0.55476[0m[0m | time: 3.813s
[2K
| Adam | epoch: 008 | loss: 0.55476 - acc: 0.7290 -- iter: 096/448
[A[ATraining Step: 102  | total loss: [1m[32m0.59619[0m[0m | time: 5.365s
[2K
| Adam | epoch: 008 | loss: 0.59619 - acc: 0.7155 -- iter: 128/448
[A[ATraining Step: 103  | total loss: [1m[32m0.58909[0m[0m | time: 6.589s
[2K
| Adam | epoch: 008 | loss: 0.58909 - acc: 0.7220 -- iter: 160/448
[A[ATraining Step: 104  | total loss: [1m[32m0.58249[0m[0m | time: 7.986s
[2K
| Adam | epoch: 008 | loss: 0.58249 - acc: 0.7248 -- iter: 192/448
[A[ATraining Step: 105  | total loss: [1m[32m0.58325[0m[0m | time: 9.352s
[2K
| Adam | epoch: 008 | loss: 0.58325 - acc: 0.7211 -- iter: 224/448
[A[ATraining Step: 106  | total loss: [1m[32m0.58289[0m[0m | time: 10.383s
[2K
| Adam | epoch: 008 | loss: 0.58289 - acc: 0.7177 -- iter: 256/448
[A[ATraining Step: 107  | total loss: [1m[32m0.57534[0m[0m | time: 11.436s
[2K
| Adam | epoch: 008 | loss: 0.57534 - acc: 0.7147 -- iter: 288/448
[A[ATraining Step: 108  | total loss: [1m[32m0.56299[0m[0m | time: 12.561s
[2K
| Adam | epoch: 008 | loss: 0.56299 - acc: 0.7307 -- iter: 320/448
[A[ATraining Step: 109  | total loss: [1m[32m0.56651[0m[0m | time: 13.847s
[2K
| Adam | epoch: 008 | loss: 0.56651 - acc: 0.7202 -- iter: 352/448
[A[ATraining Step: 110  | total loss: [1m[32m0.56455[0m[0m | time: 15.054s
[2K
| Adam | epoch: 008 | loss: 0.56455 - acc: 0.7232 -- iter: 384/448
[A[ATraining Step: 111  | total loss: [1m[32m0.55270[0m[0m | time: 16.072s
[2K
| Adam | epoch: 008 | loss: 0.55270 - acc: 0.7446 -- iter: 416/448
[A[ATraining Step: 112  | total loss: [1m[32m0.54534[0m[0m | time: 18.169s
[2K
| Adam | epoch: 008 | loss: 0.54534 - acc: 0.7576 | val_loss: 0.51396 - val_acc: 0.7571 -- iter: 448/448
--
Training Step: 113  | total loss: [1m[32m0.53581[0m[0m | time: 0.908s
[2K
| Adam | epoch: 009 | loss: 0.53581 - acc: 0.7662 -- iter: 032/448
[A[ATraining Step: 114  | total loss: [1m[32m0.53099[0m[0m | time: 1.812s
[2K
| Adam | epoch: 009 | loss: 0.53099 - acc: 0.7615 -- iter: 064/448
[A[ATraining Step: 115  | total loss: [1m[32m0.52063[0m[0m | time: 2.880s
[2K
| Adam | epoch: 009 | loss: 0.52063 - acc: 0.7666 -- iter: 096/448
[A[ATraining Step: 116  | total loss: [1m[32m0.50393[0m[0m | time: 3.882s
[2K
| Adam | epoch: 009 | loss: 0.50393 - acc: 0.7774 -- iter: 128/448
[A[ATraining Step: 117  | total loss: [1m[32m0.50152[0m[0m | time: 5.065s
[2K
| Adam | epoch: 009 | loss: 0.50152 - acc: 0.7747 -- iter: 160/448
[A[ATraining Step: 118  | total loss: [1m[32m0.51550[0m[0m | time: 6.055s
[2K
| Adam | epoch: 009 | loss: 0.51550 - acc: 0.7597 -- iter: 192/448
[A[ATraining Step: 119  | total loss: [1m[32m0.49789[0m[0m | time: 7.173s
[2K
| Adam | epoch: 009 | loss: 0.49789 - acc: 0.7712 -- iter: 224/448
[A[ATraining Step: 120  | total loss: [1m[32m0.49168[0m[0m | time: 8.340s
[2K
| Adam | epoch: 009 | loss: 0.49168 - acc: 0.7785 -- iter: 256/448
[A[ATraining Step: 121  | total loss: [1m[32m0.48500[0m[0m | time: 9.322s
[2K
| Adam | epoch: 009 | loss: 0.48500 - acc: 0.7850 -- iter: 288/448
[A[ATraining Step: 122  | total loss: [1m[32m0.46326[0m[0m | time: 10.267s
[2K
| Adam | epoch: 009 | loss: 0.46326 - acc: 0.7971 -- iter: 320/448
[A[ATraining Step: 123  | total loss: [1m[32m0.45943[0m[0m | time: 11.443s
[2K
| Adam | epoch: 009 | loss: 0.45943 - acc: 0.7956 -- iter: 352/448
[A[ATraining Step: 124  | total loss: [1m[32m0.43755[0m[0m | time: 12.751s
[2K
| Adam | epoch: 009 | loss: 0.43755 - acc: 0.8129 -- iter: 384/448
[A[ATraining Step: 125  | total loss: [1m[32m0.44005[0m[0m | time: 13.841s
[2K
| Adam | epoch: 009 | loss: 0.44005 - acc: 0.8097 -- iter: 416/448
[A[ATraining Step: 126  | total loss: [1m[32m0.44530[0m[0m | time: 15.708s
[2K
| Adam | epoch: 009 | loss: 0.44530 - acc: 0.8100 | val_loss: 0.43631 - val_acc: 0.8000 -- iter: 448/448
--
Training Step: 127  | total loss: [1m[32m0.45751[0m[0m | time: 1.044s
[2K
| Adam | epoch: 010 | loss: 0.45751 - acc: 0.8009 -- iter: 032/448
[A[ATraining Step: 128  | total loss: [1m[32m0.44186[0m[0m | time: 2.211s
[2K
| Adam | epoch: 010 | loss: 0.44186 - acc: 0.8020 -- iter: 064/448
[A[ATraining Step: 129  | total loss: [1m[32m0.42688[0m[0m | time: 3.372s
[2K
| Adam | epoch: 010 | loss: 0.42688 - acc: 0.8000 -- iter: 096/448
[A[ATraining Step: 130  | total loss: [1m[32m0.42091[0m[0m | time: 4.898s
[2K
| Adam | epoch: 010 | loss: 0.42091 - acc: 0.8043 -- iter: 128/448
[A[ATraining Step: 131  | total loss: [1m[32m0.41649[0m[0m | time: 6.281s
[2K
| Adam | epoch: 010 | loss: 0.41649 - acc: 0.8083 -- iter: 160/448
[A[ATraining Step: 132  | total loss: [1m[32m0.43196[0m[0m | time: 7.851s
[2K
| Adam | epoch: 010 | loss: 0.43196 - acc: 0.8087 -- iter: 192/448
[A[ATraining Step: 133  | total loss: [1m[32m0.42705[0m[0m | time: 9.260s
[2K
| Adam | epoch: 010 | loss: 0.42705 - acc: 0.8153 -- iter: 224/448
[A[ATraining Step: 134  | total loss: [1m[32m0.43304[0m[0m | time: 10.566s
[2K
| Adam | epoch: 010 | loss: 0.43304 - acc: 0.8119 -- iter: 256/448
[A[ATraining Step: 135  | total loss: [1m[32m0.40885[0m[0m | time: 11.980s
[2K
| Adam | epoch: 010 | loss: 0.40885 - acc: 0.8276 -- iter: 288/448
[A[ATraining Step: 136  | total loss: [1m[32m0.39037[0m[0m | time: 13.473s
[2K
| Adam | epoch: 010 | loss: 0.39037 - acc: 0.8386 -- iter: 320/448
[A[ATraining Step: 137  | total loss: [1m[32m0.38593[0m[0m | time: 14.612s
[2K
| Adam | epoch: 010 | loss: 0.38593 - acc: 0.8454 -- iter: 352/448
[A[ATraining Step: 138  | total loss: [1m[32m0.37777[0m[0m | time: 15.918s
[2K
| Adam | epoch: 010 | loss: 0.37777 - acc: 0.8546 -- iter: 384/448
[A[ATraining Step: 139  | total loss: [1m[32m0.37818[0m[0m | time: 17.260s
[2K
| Adam | epoch: 010 | loss: 0.37818 - acc: 0.8597 -- iter: 416/448
[A[ATraining Step: 140  | total loss: [1m[32m0.38937[0m[0m | time: 20.112s
[2K
| Adam | epoch: 010 | loss: 0.38937 - acc: 0.8581 | val_loss: 0.38061 - val_acc: 0.8429 -- iter: 448/448
--
Training Step: 141  | total loss: [1m[32m0.36691[0m[0m | time: 1.322s
[2K
| Adam | epoch: 011 | loss: 0.36691 - acc: 0.8692 -- iter: 032/448
[A[ATraining Step: 142  | total loss: [1m[32m0.36054[0m[0m | time: 2.818s
[2K
| Adam | epoch: 011 | loss: 0.36054 - acc: 0.8760 -- iter: 064/448
[A[ATraining Step: 143  | total loss: [1m[32m0.37355[0m[0m | time: 4.279s
[2K
| Adam | epoch: 011 | loss: 0.37355 - acc: 0.8666 -- iter: 096/448
[A[ATraining Step: 144  | total loss: [1m[32m0.40254[0m[0m | time: 5.837s
[2K
| Adam | epoch: 011 | loss: 0.40254 - acc: 0.8518 -- iter: 128/448
[A[ATraining Step: 145  | total loss: [1m[32m0.39647[0m[0m | time: 7.197s
[2K
| Adam | epoch: 011 | loss: 0.39647 - acc: 0.8541 -- iter: 160/448
[A[ATraining Step: 146  | total loss: [1m[32m0.37756[0m[0m | time: 8.571s
[2K
| Adam | epoch: 011 | loss: 0.37756 - acc: 0.8562 -- iter: 192/448
[A[ATraining Step: 147  | total loss: [1m[32m0.37072[0m[0m | time: 9.864s
[2K
| Adam | epoch: 011 | loss: 0.37072 - acc: 0.8549 -- iter: 224/448
[A[ATraining Step: 148  | total loss: [1m[32m0.35201[0m[0m | time: 11.236s
[2K
| Adam | epoch: 011 | loss: 0.35201 - acc: 0.8601 -- iter: 256/448
[A[ATraining Step: 149  | total loss: [1m[32m0.34420[0m[0m | time: 12.437s
[2K
| Adam | epoch: 011 | loss: 0.34420 - acc: 0.8616 -- iter: 288/448
[A[ATraining Step: 150  | total loss: [1m[32m0.33270[0m[0m | time: 13.796s
[2K
| Adam | epoch: 011 | loss: 0.33270 - acc: 0.8629 -- iter: 320/448
[A[ATraining Step: 151  | total loss: [1m[32m0.32172[0m[0m | time: 15.131s
[2K
| Adam | epoch: 011 | loss: 0.32172 - acc: 0.8704 -- iter: 352/448
[A[ATraining Step: 152  | total loss: [1m[32m0.32903[0m[0m | time: 16.587s
[2K
| Adam | epoch: 011 | loss: 0.32903 - acc: 0.8677 -- iter: 384/448
[A[ATraining Step: 153  | total loss: [1m[32m0.31108[0m[0m | time: 17.897s
[2K
| Adam | epoch: 011 | loss: 0.31108 - acc: 0.8778 -- iter: 416/448
[A[ATraining Step: 154  | total loss: [1m[32m0.30649[0m[0m | time: 20.343s
[2K
| Adam | epoch: 011 | loss: 0.30649 - acc: 0.8838 | val_loss: 0.34655 - val_acc: 0.8714 -- iter: 448/448
--
Training Step: 155  | total loss: [1m[32m0.30445[0m[0m | time: 1.458s
[2K
| Adam | epoch: 012 | loss: 0.30445 - acc: 0.8829 -- iter: 032/448
[A[ATraining Step: 156  | total loss: [1m[32m0.30406[0m[0m | time: 2.861s
[2K
| Adam | epoch: 012 | loss: 0.30406 - acc: 0.8821 -- iter: 064/448
[A[ATraining Step: 157  | total loss: [1m[32m0.30187[0m[0m | time: 4.287s
[2K
| Adam | epoch: 012 | loss: 0.30187 - acc: 0.8845 -- iter: 096/448
[A[ATraining Step: 158  | total loss: [1m[32m0.29895[0m[0m | time: 5.657s
[2K
| Adam | epoch: 012 | loss: 0.29895 - acc: 0.8804 -- iter: 128/448
[A[ATraining Step: 159  | total loss: [1m[32m0.29720[0m[0m | time: 7.228s
[2K
| Adam | epoch: 012 | loss: 0.29720 - acc: 0.8799 -- iter: 160/448
[A[ATraining Step: 160  | total loss: [1m[32m0.29773[0m[0m | time: 8.634s
[2K
| Adam | epoch: 012 | loss: 0.29773 - acc: 0.8825 -- iter: 192/448
[A[ATraining Step: 161  | total loss: [1m[32m0.28358[0m[0m | time: 9.842s
[2K
| Adam | epoch: 012 | loss: 0.28358 - acc: 0.8849 -- iter: 224/448
[A[ATraining Step: 162  | total loss: [1m[32m0.27275[0m[0m | time: 11.359s
[2K
| Adam | epoch: 012 | loss: 0.27275 - acc: 0.8902 -- iter: 256/448
[A[ATraining Step: 163  | total loss: [1m[32m0.25528[0m[0m | time: 12.849s
[2K
| Adam | epoch: 012 | loss: 0.25528 - acc: 0.9012 -- iter: 288/448
[A[ATraining Step: 164  | total loss: [1m[32m0.24847[0m[0m | time: 14.274s
[2K
| Adam | epoch: 012 | loss: 0.24847 - acc: 0.9048 -- iter: 320/448
[A[ATraining Step: 165  | total loss: [1m[32m0.23615[0m[0m | time: 15.657s
[2K
| Adam | epoch: 012 | loss: 0.23615 - acc: 0.9112 -- iter: 352/448
[A[ATraining Step: 166  | total loss: [1m[32m0.22107[0m[0m | time: 17.080s
[2K
| Adam | epoch: 012 | loss: 0.22107 - acc: 0.9201 -- iter: 384/448
[A[ATraining Step: 167  | total loss: [1m[32m0.20576[0m[0m | time: 18.621s
[2K
| Adam | epoch: 012 | loss: 0.20576 - acc: 0.9281 -- iter: 416/448
[A[ATraining Step: 168  | total loss: [1m[32m0.21418[0m[0m | time: 21.162s
[2K
| Adam | epoch: 012 | loss: 0.21418 - acc: 0.9196 | val_loss: 0.33434 - val_acc: 0.8500 -- iter: 448/448
--
Training Step: 169  | total loss: [1m[32m0.21496[0m[0m | time: 1.181s
[2K
| Adam | epoch: 013 | loss: 0.21496 - acc: 0.9245 -- iter: 032/448
[A[ATraining Step: 170  | total loss: [1m[32m0.21224[0m[0m | time: 2.595s
[2K
| Adam | epoch: 013 | loss: 0.21224 - acc: 0.9258 -- iter: 064/448
[A[ATraining Step: 171  | total loss: [1m[32m0.20428[0m[0m | time: 4.039s
[2K
| Adam | epoch: 013 | loss: 0.20428 - acc: 0.9301 -- iter: 096/448
[A[ATraining Step: 172  | total loss: [1m[32m0.19727[0m[0m | time: 5.438s
[2K
| Adam | epoch: 013 | loss: 0.19727 - acc: 0.9309 -- iter: 128/448
[A[ATraining Step: 173  | total loss: [1m[32m0.20411[0m[0m | time: 6.825s
[2K
| Adam | epoch: 013 | loss: 0.20411 - acc: 0.9284 -- iter: 160/448
[A[ATraining Step: 174  | total loss: [1m[32m0.21068[0m[0m | time: 8.168s
[2K
| Adam | epoch: 013 | loss: 0.21068 - acc: 0.9231 -- iter: 192/448
[A[ATraining Step: 175  | total loss: [1m[32m0.20486[0m[0m | time: 9.503s
[2K
| Adam | epoch: 013 | loss: 0.20486 - acc: 0.9245 -- iter: 224/448
[A[ATraining Step: 176  | total loss: [1m[32m0.20001[0m[0m | time: 10.794s
[2K
| Adam | epoch: 013 | loss: 0.20001 - acc: 0.9258 -- iter: 256/448
[A[ATraining Step: 177  | total loss: [1m[32m0.19257[0m[0m | time: 12.251s
[2K
| Adam | epoch: 013 | loss: 0.19257 - acc: 0.9332 -- iter: 288/448
[A[ATraining Step: 178  | total loss: [1m[32m0.33559[0m[0m | time: 13.595s
[2K
| Adam | epoch: 013 | loss: 0.33559 - acc: 0.8993 -- iter: 320/448
[A[ATraining Step: 179  | total loss: [1m[32m0.31752[0m[0m | time: 14.982s
[2K
| Adam | epoch: 013 | loss: 0.31752 - acc: 0.9000 -- iter: 352/448
[A[ATraining Step: 180  | total loss: [1m[32m0.29282[0m[0m | time: 16.146s
[2K
| Adam | epoch: 013 | loss: 0.29282 - acc: 0.9100 -- iter: 384/448
[A[ATraining Step: 181  | total loss: [1m[32m0.26922[0m[0m | time: 17.274s
[2K
| Adam | epoch: 013 | loss: 0.26922 - acc: 0.9190 -- iter: 416/448
[A[ATraining Step: 182  | total loss: [1m[32m0.26006[0m[0m | time: 19.327s
[2K
| Adam | epoch: 013 | loss: 0.26006 - acc: 0.9208 | val_loss: 0.31779 - val_acc: 0.8786 -- iter: 448/448
--
Training Step: 183  | total loss: [1m[32m0.25225[0m[0m | time: 1.297s
[2K
| Adam | epoch: 014 | loss: 0.25225 - acc: 0.9225 -- iter: 032/448
[A[ATraining Step: 184  | total loss: [1m[32m0.24183[0m[0m | time: 2.339s
[2K
| Adam | epoch: 014 | loss: 0.24183 - acc: 0.9240 -- iter: 064/448
[A[ATraining Step: 185  | total loss: [1m[32m0.22429[0m[0m | time: 3.288s
[2K
| Adam | epoch: 014 | loss: 0.22429 - acc: 0.9316 -- iter: 096/448
[A[ATraining Step: 186  | total loss: [1m[32m0.20923[0m[0m | time: 4.338s
[2K
| Adam | epoch: 014 | loss: 0.20923 - acc: 0.9384 -- iter: 128/448
[A[ATraining Step: 187  | total loss: [1m[32m0.20353[0m[0m | time: 5.364s
[2K
| Adam | epoch: 014 | loss: 0.20353 - acc: 0.9383 -- iter: 160/448
[A[ATraining Step: 188  | total loss: [1m[32m0.19822[0m[0m | time: 6.399s
[2K
| Adam | epoch: 014 | loss: 0.19822 - acc: 0.9414 -- iter: 192/448
[A[ATraining Step: 189  | total loss: [1m[32m0.18797[0m[0m | time: 7.406s
[2K
| Adam | epoch: 014 | loss: 0.18797 - acc: 0.9472 -- iter: 224/448
[A[ATraining Step: 190  | total loss: [1m[32m0.17950[0m[0m | time: 8.529s
[2K
| Adam | epoch: 014 | loss: 0.17950 - acc: 0.9494 -- iter: 256/448
[A[ATraining Step: 191  | total loss: [1m[32m0.17496[0m[0m | time: 9.575s
[2K
| Adam | epoch: 014 | loss: 0.17496 - acc: 0.9482 -- iter: 288/448
[A[ATraining Step: 192  | total loss: [1m[32m0.17174[0m[0m | time: 10.647s
[2K
| Adam | epoch: 014 | loss: 0.17174 - acc: 0.9440 -- iter: 320/448
[A[ATraining Step: 193  | total loss: [1m[32m0.29393[0m[0m | time: 11.997s
[2K
| Adam | epoch: 014 | loss: 0.29393 - acc: 0.9121 -- iter: 352/448
[A[ATraining Step: 194  | total loss: [1m[32m0.28231[0m[0m | time: 13.210s
[2K
| Adam | epoch: 014 | loss: 0.28231 - acc: 0.9115 -- iter: 384/448
[A[ATraining Step: 195  | total loss: [1m[32m0.32865[0m[0m | time: 14.347s
[2K
| Adam | epoch: 014 | loss: 0.32865 - acc: 0.8829 -- iter: 416/448
[A[ATraining Step: 196  | total loss: [1m[32m0.35955[0m[0m | time: 16.369s
[2K
| Adam | epoch: 014 | loss: 0.35955 - acc: 0.8633 | val_loss: 0.33663 - val_acc: 0.8500 -- iter: 448/448
--
Training Step: 197  | total loss: [1m[32m0.33440[0m[0m | time: 1.125s
[2K
| Adam | epoch: 015 | loss: 0.33440 - acc: 0.8739 -- iter: 032/448
[A[ATraining Step: 198  | total loss: [1m[32m0.30739[0m[0m | time: 2.021s
[2K
| Adam | epoch: 015 | loss: 0.30739 - acc: 0.8865 -- iter: 064/448
[A[ATraining Step: 199  | total loss: [1m[32m0.28873[0m[0m | time: 3.232s
[2K
| Adam | epoch: 015 | loss: 0.28873 - acc: 0.8947 -- iter: 096/448
[A[ATraining Step: 200  | total loss: [1m[32m0.26616[0m[0m | time: 5.549s
[2K
| Adam | epoch: 015 | loss: 0.26616 - acc: 0.9052 | val_loss: 0.34123 - val_acc: 0.8571 -- iter: 128/448
--
Training Step: 201  | total loss: [1m[32m0.25860[0m[0m | time: 6.791s
[2K
| Adam | epoch: 015 | loss: 0.25860 - acc: 0.9085 -- iter: 160/448
[A[ATraining Step: 202  | total loss: [1m[32m0.23975[0m[0m | time: 8.051s
[2K
| Adam | epoch: 015 | loss: 0.23975 - acc: 0.9145 -- iter: 192/448
[A[ATraining Step: 203  | total loss: [1m[32m0.23282[0m[0m | time: 9.252s
[2K
| Adam | epoch: 015 | loss: 0.23282 - acc: 0.9199 -- iter: 224/448
[A[ATraining Step: 204  | total loss: [1m[32m0.21913[0m[0m | time: 10.459s
[2K
| Adam | epoch: 015 | loss: 0.21913 - acc: 0.9248 -- iter: 256/448
[A[ATraining Step: 205  | total loss: [1m[32m0.20347[0m[0m | time: 11.756s
[2K
| Adam | epoch: 015 | loss: 0.20347 - acc: 0.9323 -- iter: 288/448
[A[ATraining Step: 206  | total loss: [1m[32m0.19755[0m[0m | time: 12.912s
[2K
| Adam | epoch: 015 | loss: 0.19755 - acc: 0.9328 -- iter: 320/448
[A[ATraining Step: 207  | total loss: [1m[32m0.18552[0m[0m | time: 14.420s
[2K
| Adam | epoch: 015 | loss: 0.18552 - acc: 0.9396 -- iter: 352/448
[A[ATraining Step: 208  | total loss: [1m[32m0.18417[0m[0m | time: 15.809s
[2K
| Adam | epoch: 015 | loss: 0.18417 - acc: 0.9394 -- iter: 384/448
[A[ATraining Step: 209  | total loss: [1m[32m0.16840[0m[0m | time: 17.384s
[2K
| Adam | epoch: 015 | loss: 0.16840 - acc: 0.9454 -- iter: 416/448
[A[ATraining Step: 210  | total loss: [1m[32m0.15547[0m[0m | time: 20.122s
[2K
| Adam | epoch: 015 | loss: 0.15547 - acc: 0.9509 | val_loss: 0.27111 - val_acc: 0.9000 -- iter: 448/448
--
Validation AUC:0.9583333333333334
Validation AUPRC:0.9656045116394134
Test AUC:0.9356617647058825
Test AUPRC:0.9433711277647443
BestTestF1Score	0.84	0.69	0.84	0.82	0.87	59	13	59	9	0.25
BestTestMCCScore	0.83	0.7	0.85	0.9	0.78	53	6	66	15	0.51
BestTestAccuracyScore	0.83	0.7	0.85	0.9	0.78	53	6	66	15	0.51
BestValidationF1Score	0.91	0.8	0.9	0.88	0.93	67	9	59	5	0.25
BestValidationMCC	0.9	0.8	0.9	0.94	0.86	62	4	64	10	0.51
BestValidationAccuracy	0.9	0.8	0.9	0.94	0.86	62	4	64	10	0.51
TestPredictions (Threshold:0.51)
CHEMBL516663,TP,ACT,0.9800000190734863	CHEMBL205888,TP,ACT,0.9800000190734863	CHEMBL2348463,TP,ACT,0.9900000095367432	CHEMBL64321,TN,INACT,0.009999999776482582	CHEMBL1201353,TN,INACT,0.20000000298023224	CHEMBL2348471,TP,ACT,0.7300000190734863	CHEMBL308924,TN,INACT,0.05000000074505806	CHEMBL34328,TN,INACT,0.019999999552965164	CHEMBL31781,FP,INACT,0.9100000262260437	CHEMBL231466,TP,ACT,0.9900000095367432	CHEMBL3633665,TN,INACT,0.09000000357627869	CHEMBL126114,TP,ACT,0.9399999976158142	CHEMBL3633663,TN,INACT,0.3400000035762787	CHEMBL351183,FP,INACT,0.550000011920929	CHEMBL31524,FP,INACT,0.6100000143051147	CHEMBL2337205,TP,ACT,0.9900000095367432	CHEMBL3416124,TP,ACT,1.0	CHEMBL460307,TP,ACT,0.9800000190734863	CHEMBL107680,TN,INACT,0.3799999952316284	CHEMBL295651,TN,INACT,0.3100000023841858	CHEMBL113230,TP,ACT,0.9800000190734863	CHEMBL291516,TN,INACT,0.009999999776482582	CHEMBL279147,TP,ACT,0.8500000238418579	CHEMBL3582027,TP,ACT,0.9700000286102295	CHEMBL197257,TP,ACT,0.8799999952316284	CHEMBL1259056,TP,ACT,0.9399999976158142	CHEMBL461088,TN,INACT,0.03999999910593033	CHEMBL1214436,TP,ACT,0.9900000095367432	CHEMBL1788276,TP,ACT,0.8999999761581421	CHEMBL1214500,TP,ACT,0.9800000190734863	CHEMBL1180343,TN,INACT,0.3799999952316284	CHEMBL64235,TN,INACT,0.029999999329447746	CHEMBL46395,TN,INACT,0.019999999552965164	CHEMBL2348464,TP,ACT,0.9800000190734863	CHEMBL3416129,TP,ACT,0.9700000286102295	CHEMBL2338183,FN,ACT,0.49000000953674316	CHEMBL386489,FN,ACT,0.3400000035762787	CHEMBL232328,FN,ACT,0.05999999865889549	CHEMBL2093084,TN,INACT,0.07000000029802322	CHEMBL3331453,TP,ACT,0.9700000286102295	CHEMBL2064056,TP,ACT,0.9800000190734863	CHEMBL2338177,FN,ACT,0.4300000071525574	CHEMBL121353,TP,ACT,0.9800000190734863	CHEMBL3416125,TP,ACT,0.9900000095367432	CHEMBL114478,TN,INACT,0.029999999329447746	CHEMBL102390,TN,INACT,0.05999999865889549	CHEMBL595022,TN,INACT,0.019999999552965164	CHEMBL3735797,TN,INACT,0.20000000298023224	CHEMBL284965,TN,INACT,0.019999999552965164	CHEMBL25688,TN,INACT,0.07999999821186066	CHEMBL465,TP,ACT,0.9900000095367432	CHEMBL105594,TN,INACT,0.07000000029802322	CHEMBL595265,TN,INACT,0.019999999552965164	CHEMBL3331447,TP,ACT,0.9900000095367432	CHEMBL418366,TP,ACT,0.9900000095367432	CHEMBL120778,TP,ACT,1.0	CHEMBL3582015,FN,ACT,0.11999999731779099	CHEMBL3331452,FN,ACT,0.44999998807907104	CHEMBL35657,TP,ACT,0.9800000190734863	CHEMBL72295,FP,INACT,0.9599999785423279	CHEMBL33438,TN,INACT,0.019999999552965164	CHEMBL474531,FN,ACT,0.12999999523162842	CHEMBL294087,TN,INACT,0.009999999776482582	CHEMBL232730,FN,ACT,0.07999999821186066	CHEMBL14359,TN,INACT,0.029999999329447746	CHEMBL170335,TN,INACT,0.12999999523162842	CHEMBL2338189,FN,ACT,0.3700000047683716	CHEMBL64124,TN,INACT,0.009999999776482582	CHEMBL2180817,TP,ACT,0.7200000286102295	CHEMBL150696,TN,INACT,0.03999999910593033	CHEMBL73164,TN,INACT,0.23000000417232513	CHEMBL462593,FN,ACT,0.23999999463558197	CHEMBL418353,TP,ACT,0.9800000190734863	CHEMBL2442641,FN,ACT,0.03999999910593033	CHEMBL9746,TN,INACT,0.029999999329447746	CHEMBL151668,TN,INACT,0.05000000074505806	CHEMBL1082036,TN,INACT,0.019999999552965164	CHEMBL370884,FN,ACT,0.38999998569488525	CHEMBL149691,TP,ACT,0.9900000095367432	CHEMBL151167,TP,ACT,1.0	CHEMBL386725,TP,ACT,0.9800000190734863	CHEMBL66789,TN,INACT,0.0	CHEMBL2442635,FN,ACT,0.03999999910593033	CHEMBL2391353,TN,INACT,0.07000000029802322	CHEMBL1214557,TP,ACT,0.949999988079071	CHEMBL2042551,TN,INACT,0.10999999940395355	CHEMBL150942,TP,ACT,1.0	CHEMBL119914,TP,ACT,1.0	CHEMBL574597,TN,INACT,0.019999999552965164	CHEMBL3633650,TN,INACT,0.3199999928474426	CHEMBL62804,TN,INACT,0.019999999552965164	CHEMBL404866,TN,INACT,0.019999999552965164	CHEMBL461709,TN,INACT,0.05000000074505806	CHEMBL3331461,TP,ACT,0.9900000095367432	CHEMBL2442643,TP,ACT,0.7300000190734863	CHEMBL356213,TN,INACT,0.28999999165534973	CHEMBL108085,TP,ACT,0.9900000095367432	CHEMBL1258371,TN,INACT,0.11999999731779099	CHEMBL168223,TN,INACT,0.03999999910593033	CHEMBL21937,TN,INACT,0.029999999329447746	CHEMBL328476,TN,INACT,0.019999999552965164	CHEMBL302196,TN,INACT,0.019999999552965164	CHEMBL1214559,TP,ACT,0.9900000095367432	CHEMBL108868,TP,ACT,1.0	CHEMBL216595,TP,ACT,1.0	CHEMBL3577345,TN,INACT,0.009999999776482582	CHEMBL1214502,TP,ACT,0.9900000095367432	CHEMBL436120,TP,ACT,0.9700000286102295	CHEMBL2064067,TP,ACT,0.9900000095367432	CHEMBL40796,TN,INACT,0.029999999329447746	CHEMBL1076,TN,INACT,0.029999999329447746	CHEMBL80945,TN,INACT,0.029999999329447746	CHEMBL241279,TN,INACT,0.03999999910593033	CHEMBL2442645,TP,ACT,0.9599999785423279	CHEMBL281232,TN,INACT,0.009999999776482582	CHEMBL303479,TP,ACT,0.9300000071525574	CHEMBL2442640,FN,ACT,0.05000000074505806	CHEMBL3290986,TN,INACT,0.019999999552965164	CHEMBL137483,TN,INACT,0.009999999776482582	CHEMBL246627,FN,ACT,0.029999999329447746	CHEMBL286139,TN,INACT,0.009999999776482582	CHEMBL353502,FP,INACT,0.8100000023841858	CHEMBL60435,TN,INACT,0.009999999776482582	CHEMBL332405,TN,INACT,0.07000000029802322	CHEMBL3577344,TN,INACT,0.019999999552965164	CHEMBL240888,TN,INACT,0.10000000149011612	CHEMBL129198,TN,INACT,0.46000000834465027	CHEMBL119385,FP,INACT,0.9300000071525574	CHEMBL2442642,TN,INACT,0.05000000074505806	CHEMBL1834525,TP,ACT,0.699999988079071	CHEMBL216659,TP,ACT,1.0	CHEMBL1214434,TP,ACT,0.9900000095367432	CHEMBL205908,TP,ACT,0.9599999785423279	CHEMBL9666,TN,INACT,0.019999999552965164	CHEMBL461502,TN,INACT,0.029999999329447746	CHEMBL257547,TN,INACT,0.03999999910593033	CHEMBL2064069,TP,ACT,0.9800000190734863	CHEMBL513277,TN,INACT,0.05000000074505806	CHEMBL3290446,TP,ACT,0.9900000095367432	CHEMBL3736248,TN,INACT,0.009999999776482582	

