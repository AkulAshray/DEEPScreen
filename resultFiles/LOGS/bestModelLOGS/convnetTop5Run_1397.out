CNNModel CHEMBL2782 adam 0.001 30 32 0 0.8 False True
Number of active compounds :	236
Number of inactive compounds :	236
---------------------------------
Run id: CNNModel_CHEMBL2782_adam_0.001_30_32_0_0.8_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL2782_adam_0.001_30_32_0.8_True/
---------------------------------
Training samples: 216
Validation samples: 68
--
Training Step: 1  | time: 0.759s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/216
[A[ATraining Step: 2  | total loss: [1m[32m0.62379[0m[0m | time: 1.405s
[2K
| Adam | epoch: 001 | loss: 0.62379 - acc: 0.4781 -- iter: 064/216
[A[ATraining Step: 3  | total loss: [1m[32m0.67699[0m[0m | time: 2.009s
[2K
| Adam | epoch: 001 | loss: 0.67699 - acc: 0.6239 -- iter: 096/216
[A[ATraining Step: 4  | total loss: [1m[32m0.68790[0m[0m | time: 2.629s
[2K
| Adam | epoch: 001 | loss: 0.68790 - acc: 0.5544 -- iter: 128/216
[A[ATraining Step: 5  | total loss: [1m[32m0.69359[0m[0m | time: 3.236s
[2K
| Adam | epoch: 001 | loss: 0.69359 - acc: 0.5384 -- iter: 160/216
[A[ATraining Step: 6  | total loss: [1m[32m0.68529[0m[0m | time: 3.840s
[2K
| Adam | epoch: 001 | loss: 0.68529 - acc: 0.5740 -- iter: 192/216
[A[ATraining Step: 7  | total loss: [1m[32m0.68019[0m[0m | time: 5.320s
[2K
| Adam | epoch: 001 | loss: 0.68019 - acc: 0.5858 | val_loss: 0.65819 - val_acc: 0.6324 -- iter: 216/216
--
Training Step: 8  | total loss: [1m[32m0.68058[0m[0m | time: 0.477s
[2K
| Adam | epoch: 002 | loss: 0.68058 - acc: 0.5844 -- iter: 032/216
[A[ATraining Step: 9  | total loss: [1m[32m0.67968[0m[0m | time: 1.085s
[2K
| Adam | epoch: 002 | loss: 0.67968 - acc: 0.5838 -- iter: 064/216
[A[ATraining Step: 10  | total loss: [1m[32m0.65608[0m[0m | time: 1.682s
[2K
| Adam | epoch: 002 | loss: 0.65608 - acc: 0.6357 -- iter: 096/216
[A[ATraining Step: 11  | total loss: [1m[32m0.67985[0m[0m | time: 2.286s
[2K
| Adam | epoch: 002 | loss: 0.67985 - acc: 0.6010 -- iter: 128/216
[A[ATraining Step: 12  | total loss: [1m[32m0.70832[0m[0m | time: 2.892s
[2K
| Adam | epoch: 002 | loss: 0.70832 - acc: 0.5415 -- iter: 160/216
[A[ATraining Step: 13  | total loss: [1m[32m0.70643[0m[0m | time: 3.521s
[2K
| Adam | epoch: 002 | loss: 0.70643 - acc: 0.5237 -- iter: 192/216
[A[ATraining Step: 14  | total loss: [1m[32m0.69437[0m[0m | time: 5.147s
[2K
| Adam | epoch: 002 | loss: 0.69437 - acc: 0.5651 | val_loss: 0.68491 - val_acc: 0.6324 -- iter: 216/216
--
Training Step: 15  | total loss: [1m[32m0.68984[0m[0m | time: 0.472s
[2K
| Adam | epoch: 003 | loss: 0.68984 - acc: 0.5886 -- iter: 032/216
[A[ATraining Step: 16  | total loss: [1m[32m0.68534[0m[0m | time: 0.935s
[2K
| Adam | epoch: 003 | loss: 0.68534 - acc: 0.6491 -- iter: 064/216
[A[ATraining Step: 17  | total loss: [1m[32m0.68260[0m[0m | time: 1.530s
[2K
| Adam | epoch: 003 | loss: 0.68260 - acc: 0.6854 -- iter: 096/216
[A[ATraining Step: 18  | total loss: [1m[32m0.68196[0m[0m | time: 2.137s
[2K
| Adam | epoch: 003 | loss: 0.68196 - acc: 0.6861 -- iter: 128/216
[A[ATraining Step: 19  | total loss: [1m[32m0.68796[0m[0m | time: 2.747s
[2K
| Adam | epoch: 003 | loss: 0.68796 - acc: 0.5928 -- iter: 160/216
[A[ATraining Step: 20  | total loss: [1m[32m0.68806[0m[0m | time: 3.373s
[2K
| Adam | epoch: 003 | loss: 0.68806 - acc: 0.5831 -- iter: 192/216
[A[ATraining Step: 21  | total loss: [1m[32m0.68890[0m[0m | time: 4.988s
[2K
| Adam | epoch: 003 | loss: 0.68890 - acc: 0.5670 | val_loss: 0.68007 - val_acc: 0.6324 -- iter: 216/216
--
Training Step: 22  | total loss: [1m[32m0.68433[0m[0m | time: 0.616s
[2K
| Adam | epoch: 004 | loss: 0.68433 - acc: 0.6125 -- iter: 032/216
[A[ATraining Step: 23  | total loss: [1m[32m0.68901[0m[0m | time: 1.078s
[2K
| Adam | epoch: 004 | loss: 0.68901 - acc: 0.5617 -- iter: 064/216
[A[ATraining Step: 24  | total loss: [1m[32m0.68995[0m[0m | time: 1.529s
[2K
| Adam | epoch: 004 | loss: 0.68995 - acc: 0.5444 -- iter: 096/216
[A[ATraining Step: 25  | total loss: [1m[32m0.69039[0m[0m | time: 2.124s
[2K
| Adam | epoch: 004 | loss: 0.69039 - acc: 0.5323 -- iter: 128/216
[A[ATraining Step: 26  | total loss: [1m[32m0.68914[0m[0m | time: 2.736s
[2K
| Adam | epoch: 004 | loss: 0.68914 - acc: 0.5403 -- iter: 160/216
[A[ATraining Step: 27  | total loss: [1m[32m0.68441[0m[0m | time: 3.347s
[2K
| Adam | epoch: 004 | loss: 0.68441 - acc: 0.5781 -- iter: 192/216
[A[ATraining Step: 28  | total loss: [1m[32m0.68180[0m[0m | time: 4.965s
[2K
| Adam | epoch: 004 | loss: 0.68180 - acc: 0.5898 | val_loss: 0.66664 - val_acc: 0.6324 -- iter: 216/216
--
Training Step: 29  | total loss: [1m[32m0.68148[0m[0m | time: 0.637s
[2K
| Adam | epoch: 005 | loss: 0.68148 - acc: 0.5832 -- iter: 032/216
[A[ATraining Step: 30  | total loss: [1m[32m0.68210[0m[0m | time: 1.241s
[2K
| Adam | epoch: 005 | loss: 0.68210 - acc: 0.5783 -- iter: 064/216
[A[ATraining Step: 31  | total loss: [1m[32m0.68302[0m[0m | time: 1.708s
[2K
| Adam | epoch: 005 | loss: 0.68302 - acc: 0.5674 -- iter: 096/216
[A[ATraining Step: 32  | total loss: [1m[32m0.68794[0m[0m | time: 2.170s
[2K
| Adam | epoch: 005 | loss: 0.68794 - acc: 0.5429 -- iter: 128/216
[A[ATraining Step: 33  | total loss: [1m[32m0.68966[0m[0m | time: 2.782s
[2K
| Adam | epoch: 005 | loss: 0.68966 - acc: 0.5243 -- iter: 160/216
[A[ATraining Step: 34  | total loss: [1m[32m0.68214[0m[0m | time: 3.377s
[2K
| Adam | epoch: 005 | loss: 0.68214 - acc: 0.5526 -- iter: 192/216
[A[ATraining Step: 35  | total loss: [1m[32m0.67943[0m[0m | time: 4.998s
[2K
| Adam | epoch: 005 | loss: 0.67943 - acc: 0.5547 | val_loss: 0.64049 - val_acc: 0.6324 -- iter: 216/216
--
Training Step: 36  | total loss: [1m[32m0.67815[0m[0m | time: 0.609s
[2K
| Adam | epoch: 006 | loss: 0.67815 - acc: 0.5499 -- iter: 032/216
[A[ATraining Step: 37  | total loss: [1m[32m0.68265[0m[0m | time: 1.229s
[2K
| Adam | epoch: 006 | loss: 0.68265 - acc: 0.5337 -- iter: 064/216
[A[ATraining Step: 38  | total loss: [1m[32m0.66700[0m[0m | time: 1.842s
[2K
| Adam | epoch: 006 | loss: 0.66700 - acc: 0.5638 -- iter: 096/216
[A[ATraining Step: 39  | total loss: [1m[32m0.65835[0m[0m | time: 2.325s
[2K
| Adam | epoch: 006 | loss: 0.65835 - acc: 0.5815 -- iter: 128/216
[A[ATraining Step: 40  | total loss: [1m[32m0.64542[0m[0m | time: 2.793s
[2K
| Adam | epoch: 006 | loss: 0.64542 - acc: 0.5974 -- iter: 160/216
[A[ATraining Step: 41  | total loss: [1m[32m0.62868[0m[0m | time: 3.403s
[2K
| Adam | epoch: 006 | loss: 0.62868 - acc: 0.6102 -- iter: 192/216
[A[ATraining Step: 42  | total loss: [1m[32m0.61623[0m[0m | time: 5.013s
[2K
| Adam | epoch: 006 | loss: 0.61623 - acc: 0.6185 | val_loss: 0.61172 - val_acc: 0.7353 -- iter: 216/216
--
Training Step: 43  | total loss: [1m[32m0.60681[0m[0m | time: 0.602s
[2K
| Adam | epoch: 007 | loss: 0.60681 - acc: 0.6251 -- iter: 032/216
[A[ATraining Step: 44  | total loss: [1m[32m0.61434[0m[0m | time: 1.207s
[2K
| Adam | epoch: 007 | loss: 0.61434 - acc: 0.6359 -- iter: 064/216
[A[ATraining Step: 45  | total loss: [1m[32m0.62132[0m[0m | time: 1.820s
[2K
| Adam | epoch: 007 | loss: 0.62132 - acc: 0.6553 -- iter: 096/216
[A[ATraining Step: 46  | total loss: [1m[32m0.63684[0m[0m | time: 2.418s
[2K
| Adam | epoch: 007 | loss: 0.63684 - acc: 0.6346 -- iter: 128/216
[A[ATraining Step: 47  | total loss: [1m[32m0.64068[0m[0m | time: 2.884s
[2K
| Adam | epoch: 007 | loss: 0.64068 - acc: 0.6535 -- iter: 160/216
[A[ATraining Step: 48  | total loss: [1m[32m0.63830[0m[0m | time: 3.334s
[2K
| Adam | epoch: 007 | loss: 0.63830 - acc: 0.6824 -- iter: 192/216
[A[ATraining Step: 49  | total loss: [1m[32m0.62559[0m[0m | time: 4.954s
[2K
| Adam | epoch: 007 | loss: 0.62559 - acc: 0.6997 | val_loss: 0.60380 - val_acc: 0.7353 -- iter: 216/216
--
Training Step: 50  | total loss: [1m[32m0.63023[0m[0m | time: 0.909s
[2K
| Adam | epoch: 008 | loss: 0.63023 - acc: 0.6929 -- iter: 032/216
[A[ATraining Step: 51  | total loss: [1m[32m0.61439[0m[0m | time: 1.851s
[2K
| Adam | epoch: 008 | loss: 0.61439 - acc: 0.7112 -- iter: 064/216
[A[ATraining Step: 52  | total loss: [1m[32m0.61729[0m[0m | time: 2.783s
[2K
| Adam | epoch: 008 | loss: 0.61729 - acc: 0.7123 -- iter: 096/216
[A[ATraining Step: 53  | total loss: [1m[32m0.61796[0m[0m | time: 3.741s
[2K
| Adam | epoch: 008 | loss: 0.61796 - acc: 0.7086 -- iter: 128/216
[A[ATraining Step: 54  | total loss: [1m[32m0.60914[0m[0m | time: 4.754s
[2K
| Adam | epoch: 008 | loss: 0.60914 - acc: 0.7283 -- iter: 160/216
[A[ATraining Step: 55  | total loss: [1m[32m0.61147[0m[0m | time: 5.583s
[2K
| Adam | epoch: 008 | loss: 0.61147 - acc: 0.7224 -- iter: 192/216
[A[ATraining Step: 56  | total loss: [1m[32m0.61048[0m[0m | time: 7.336s
[2K
| Adam | epoch: 008 | loss: 0.61048 - acc: 0.7204 | val_loss: 0.59865 - val_acc: 0.7059 -- iter: 216/216
--
Training Step: 57  | total loss: [1m[32m0.60487[0m[0m | time: 0.925s
[2K
| Adam | epoch: 009 | loss: 0.60487 - acc: 0.7188 -- iter: 032/216
[A[ATraining Step: 58  | total loss: [1m[32m0.60185[0m[0m | time: 1.902s
[2K
| Adam | epoch: 009 | loss: 0.60185 - acc: 0.7145 -- iter: 064/216
[A[ATraining Step: 59  | total loss: [1m[32m0.59562[0m[0m | time: 2.830s
[2K
| Adam | epoch: 009 | loss: 0.59562 - acc: 0.7193 -- iter: 096/216
[A[ATraining Step: 60  | total loss: [1m[32m0.59171[0m[0m | time: 3.906s
[2K
| Adam | epoch: 009 | loss: 0.59171 - acc: 0.7192 -- iter: 128/216
[A[ATraining Step: 61  | total loss: [1m[32m0.57563[0m[0m | time: 5.010s
[2K
| Adam | epoch: 009 | loss: 0.57563 - acc: 0.7314 -- iter: 160/216
[A[ATraining Step: 62  | total loss: [1m[32m0.54947[0m[0m | time: 5.873s
[2K
| Adam | epoch: 009 | loss: 0.54947 - acc: 0.7498 -- iter: 192/216
[A[ATraining Step: 63  | total loss: [1m[32m0.54696[0m[0m | time: 7.718s
[2K
| Adam | epoch: 009 | loss: 0.54696 - acc: 0.7499 | val_loss: 0.75858 - val_acc: 0.5735 -- iter: 216/216
--
Training Step: 64  | total loss: [1m[32m0.55058[0m[0m | time: 0.794s
[2K
| Adam | epoch: 010 | loss: 0.55058 - acc: 0.7499 -- iter: 032/216
[A[ATraining Step: 65  | total loss: [1m[32m0.54616[0m[0m | time: 1.773s
[2K
| Adam | epoch: 010 | loss: 0.54616 - acc: 0.7499 -- iter: 064/216
[A[ATraining Step: 66  | total loss: [1m[32m0.59606[0m[0m | time: 2.824s
[2K
| Adam | epoch: 010 | loss: 0.59606 - acc: 0.7081 -- iter: 096/216
[A[ATraining Step: 67  | total loss: [1m[32m0.58399[0m[0m | time: 3.906s
[2K
| Adam | epoch: 010 | loss: 0.58399 - acc: 0.7056 -- iter: 128/216
[A[ATraining Step: 68  | total loss: [1m[32m0.56835[0m[0m | time: 4.725s
[2K
| Adam | epoch: 010 | loss: 0.56835 - acc: 0.7220 -- iter: 160/216
[A[ATraining Step: 69  | total loss: [1m[32m0.54557[0m[0m | time: 5.811s
[2K
| Adam | epoch: 010 | loss: 0.54557 - acc: 0.7399 -- iter: 192/216
[A[ATraining Step: 70  | total loss: [1m[32m0.58031[0m[0m | time: 7.858s
[2K
| Adam | epoch: 010 | loss: 0.58031 - acc: 0.7266 | val_loss: 0.52900 - val_acc: 0.7353 -- iter: 216/216
--
Training Step: 71  | total loss: [1m[32m0.57982[0m[0m | time: 0.780s
[2K
| Adam | epoch: 011 | loss: 0.57982 - acc: 0.7257 -- iter: 032/216
[A[ATraining Step: 72  | total loss: [1m[32m0.57446[0m[0m | time: 1.533s
[2K
| Adam | epoch: 011 | loss: 0.57446 - acc: 0.7285 -- iter: 064/216
[A[ATraining Step: 73  | total loss: [1m[32m0.56776[0m[0m | time: 2.663s
[2K
| Adam | epoch: 011 | loss: 0.56776 - acc: 0.7308 -- iter: 096/216
[A[ATraining Step: 74  | total loss: [1m[32m0.56010[0m[0m | time: 3.639s
[2K
| Adam | epoch: 011 | loss: 0.56010 - acc: 0.7398 -- iter: 128/216
[A[ATraining Step: 75  | total loss: [1m[32m0.56314[0m[0m | time: 4.569s
[2K
| Adam | epoch: 011 | loss: 0.56314 - acc: 0.7274 -- iter: 160/216
[A[ATraining Step: 76  | total loss: [1m[32m0.55234[0m[0m | time: 5.616s
[2K
| Adam | epoch: 011 | loss: 0.55234 - acc: 0.7398 -- iter: 192/216
[A[ATraining Step: 77  | total loss: [1m[32m0.54385[0m[0m | time: 7.768s
[2K
| Adam | epoch: 011 | loss: 0.54385 - acc: 0.7574 | val_loss: 0.55705 - val_acc: 0.7647 -- iter: 216/216
--
Training Step: 78  | total loss: [1m[32m0.53821[0m[0m | time: 1.166s
[2K
| Adam | epoch: 012 | loss: 0.53821 - acc: 0.7599 -- iter: 032/216
[A[ATraining Step: 79  | total loss: [1m[32m0.53559[0m[0m | time: 1.885s
[2K
| Adam | epoch: 012 | loss: 0.53559 - acc: 0.7654 -- iter: 064/216
[A[ATraining Step: 80  | total loss: [1m[32m0.53487[0m[0m | time: 2.540s
[2K
| Adam | epoch: 012 | loss: 0.53487 - acc: 0.7553 -- iter: 096/216
[A[ATraining Step: 81  | total loss: [1m[32m0.53088[0m[0m | time: 3.586s
[2K
| Adam | epoch: 012 | loss: 0.53088 - acc: 0.7463 -- iter: 128/216
[A[ATraining Step: 82  | total loss: [1m[32m0.52721[0m[0m | time: 4.664s
[2K
| Adam | epoch: 012 | loss: 0.52721 - acc: 0.7467 -- iter: 160/216
[A[ATraining Step: 83  | total loss: [1m[32m0.51931[0m[0m | time: 5.920s
[2K
| Adam | epoch: 012 | loss: 0.51931 - acc: 0.7533 -- iter: 192/216
[A[ATraining Step: 84  | total loss: [1m[32m0.51500[0m[0m | time: 11.470s
[2K
| Adam | epoch: 012 | loss: 0.51500 - acc: 0.7561 | val_loss: 0.50899 - val_acc: 0.7794 -- iter: 216/216
--
Training Step: 85  | total loss: [1m[32m0.49649[0m[0m | time: 0.947s
[2K
| Adam | epoch: 013 | loss: 0.49649 - acc: 0.7711 -- iter: 032/216
[A[ATraining Step: 86  | total loss: [1m[32m0.48460[0m[0m | time: 1.730s
[2K
| Adam | epoch: 013 | loss: 0.48460 - acc: 0.7784 -- iter: 064/216
[A[ATraining Step: 87  | total loss: [1m[32m0.48622[0m[0m | time: 2.379s
[2K
| Adam | epoch: 013 | loss: 0.48622 - acc: 0.7849 -- iter: 096/216
[A[ATraining Step: 88  | total loss: [1m[32m0.47082[0m[0m | time: 3.093s
[2K
| Adam | epoch: 013 | loss: 0.47082 - acc: 0.7897 -- iter: 128/216
[A[ATraining Step: 89  | total loss: [1m[32m0.45981[0m[0m | time: 4.006s
[2K
| Adam | epoch: 013 | loss: 0.45981 - acc: 0.7983 -- iter: 160/216
[A[ATraining Step: 90  | total loss: [1m[32m0.43284[0m[0m | time: 4.623s
[2K
| Adam | epoch: 013 | loss: 0.43284 - acc: 0.8153 -- iter: 192/216
[A[ATraining Step: 91  | total loss: [1m[32m0.43180[0m[0m | time: 6.226s
[2K
| Adam | epoch: 013 | loss: 0.43180 - acc: 0.8150 | val_loss: 0.51916 - val_acc: 0.7647 -- iter: 216/216
--
Training Step: 92  | total loss: [1m[32m0.45184[0m[0m | time: 1.049s
[2K
| Adam | epoch: 014 | loss: 0.45184 - acc: 0.8085 -- iter: 032/216
[A[ATraining Step: 93  | total loss: [1m[32m0.44021[0m[0m | time: 2.093s
[2K
| Adam | epoch: 014 | loss: 0.44021 - acc: 0.8152 -- iter: 064/216
[A[ATraining Step: 94  | total loss: [1m[32m0.43173[0m[0m | time: 3.108s
[2K
| Adam | epoch: 014 | loss: 0.43173 - acc: 0.8212 -- iter: 096/216
[A[ATraining Step: 95  | total loss: [1m[32m0.42962[0m[0m | time: 3.624s
[2K
| Adam | epoch: 014 | loss: 0.42962 - acc: 0.8203 -- iter: 128/216
[A[ATraining Step: 96  | total loss: [1m[32m0.42457[0m[0m | time: 4.091s
[2K
| Adam | epoch: 014 | loss: 0.42457 - acc: 0.8216 -- iter: 160/216
[A[ATraining Step: 97  | total loss: [1m[32m0.40983[0m[0m | time: 4.700s
[2K
| Adam | epoch: 014 | loss: 0.40983 - acc: 0.8311 -- iter: 192/216
[A[ATraining Step: 98  | total loss: [1m[32m0.40170[0m[0m | time: 6.320s
[2K
| Adam | epoch: 014 | loss: 0.40170 - acc: 0.8355 | val_loss: 0.52676 - val_acc: 0.7794 -- iter: 216/216
--
Training Step: 99  | total loss: [1m[32m0.38576[0m[0m | time: 0.645s
[2K
| Adam | epoch: 015 | loss: 0.38576 - acc: 0.8426 -- iter: 032/216
[A[ATraining Step: 100  | total loss: [1m[32m0.38419[0m[0m | time: 1.250s
[2K
| Adam | epoch: 015 | loss: 0.38419 - acc: 0.8396 -- iter: 064/216
[A[ATraining Step: 101  | total loss: [1m[32m0.38031[0m[0m | time: 1.859s
[2K
| Adam | epoch: 015 | loss: 0.38031 - acc: 0.8400 -- iter: 096/216
[A[ATraining Step: 102  | total loss: [1m[32m0.37166[0m[0m | time: 2.477s
[2K
| Adam | epoch: 015 | loss: 0.37166 - acc: 0.8435 -- iter: 128/216
[A[ATraining Step: 103  | total loss: [1m[32m0.37880[0m[0m | time: 2.941s
[2K
| Adam | epoch: 015 | loss: 0.37880 - acc: 0.8404 -- iter: 160/216
[A[ATraining Step: 104  | total loss: [1m[32m0.45216[0m[0m | time: 3.398s
[2K
| Adam | epoch: 015 | loss: 0.45216 - acc: 0.8230 -- iter: 192/216
[A[ATraining Step: 105  | total loss: [1m[32m0.42452[0m[0m | time: 5.022s
[2K
| Adam | epoch: 015 | loss: 0.42452 - acc: 0.8365 | val_loss: 0.52397 - val_acc: 0.7500 -- iter: 216/216
--
Training Step: 106  | total loss: [1m[32m0.42482[0m[0m | time: 0.609s
[2K
| Adam | epoch: 016 | loss: 0.42482 - acc: 0.8341 -- iter: 032/216
[A[ATraining Step: 107  | total loss: [1m[32m0.44342[0m[0m | time: 1.229s
[2K
| Adam | epoch: 016 | loss: 0.44342 - acc: 0.8195 -- iter: 064/216
[A[ATraining Step: 108  | total loss: [1m[32m0.43554[0m[0m | time: 1.855s
[2K
| Adam | epoch: 016 | loss: 0.43554 - acc: 0.8250 -- iter: 096/216
[A[ATraining Step: 109  | total loss: [1m[32m0.42132[0m[0m | time: 2.460s
[2K
| Adam | epoch: 016 | loss: 0.42132 - acc: 0.8332 -- iter: 128/216
[A[ATraining Step: 110  | total loss: [1m[32m0.41401[0m[0m | time: 3.078s
[2K
| Adam | epoch: 016 | loss: 0.41401 - acc: 0.8373 -- iter: 160/216
[A[ATraining Step: 111  | total loss: [1m[32m0.41335[0m[0m | time: 3.545s
[2K
| Adam | epoch: 016 | loss: 0.41335 - acc: 0.8349 -- iter: 192/216
[A[ATraining Step: 112  | total loss: [1m[32m0.40231[0m[0m | time: 5.042s
[2K
| Adam | epoch: 016 | loss: 0.40231 - acc: 0.8389 | val_loss: 0.52817 - val_acc: 0.7353 -- iter: 216/216
--
Training Step: 113  | total loss: [1m[32m0.38578[0m[0m | time: 0.634s
[2K
| Adam | epoch: 017 | loss: 0.38578 - acc: 0.8508 -- iter: 032/216
[A[ATraining Step: 114  | total loss: [1m[32m0.37826[0m[0m | time: 1.251s
[2K
| Adam | epoch: 017 | loss: 0.37826 - acc: 0.8564 -- iter: 064/216
[A[ATraining Step: 115  | total loss: [1m[32m0.37172[0m[0m | time: 1.846s
[2K
| Adam | epoch: 017 | loss: 0.37172 - acc: 0.8582 -- iter: 096/216
[A[ATraining Step: 116  | total loss: [1m[32m0.35459[0m[0m | time: 2.476s
[2K
| Adam | epoch: 017 | loss: 0.35459 - acc: 0.8661 -- iter: 128/216
[A[ATraining Step: 117  | total loss: [1m[32m0.35321[0m[0m | time: 3.108s
[2K
| Adam | epoch: 017 | loss: 0.35321 - acc: 0.8639 -- iter: 160/216
[A[ATraining Step: 118  | total loss: [1m[32m0.34526[0m[0m | time: 3.760s
[2K
| Adam | epoch: 017 | loss: 0.34526 - acc: 0.8681 -- iter: 192/216
[A[ATraining Step: 119  | total loss: [1m[32m0.33261[0m[0m | time: 5.224s
[2K
| Adam | epoch: 017 | loss: 0.33261 - acc: 0.8720 | val_loss: 0.78852 - val_acc: 0.6765 -- iter: 216/216
--
Training Step: 120  | total loss: [1m[32m0.36835[0m[0m | time: 0.679s
[2K
| Adam | epoch: 018 | loss: 0.36835 - acc: 0.8723 -- iter: 032/216
[A[ATraining Step: 121  | total loss: [1m[32m0.35908[0m[0m | time: 1.564s
[2K
| Adam | epoch: 018 | loss: 0.35908 - acc: 0.8767 -- iter: 064/216
[A[ATraining Step: 122  | total loss: [1m[32m0.34155[0m[0m | time: 2.408s
[2K
| Adam | epoch: 018 | loss: 0.34155 - acc: 0.8828 -- iter: 096/216
[A[ATraining Step: 123  | total loss: [1m[32m0.32232[0m[0m | time: 3.467s
[2K
| Adam | epoch: 018 | loss: 0.32232 - acc: 0.8883 -- iter: 128/216
[A[ATraining Step: 124  | total loss: [1m[32m0.30525[0m[0m | time: 4.489s
[2K
| Adam | epoch: 018 | loss: 0.30525 - acc: 0.8901 -- iter: 160/216
[A[ATraining Step: 125  | total loss: [1m[32m0.32717[0m[0m | time: 5.717s
[2K
| Adam | epoch: 018 | loss: 0.32717 - acc: 0.8792 -- iter: 192/216
[A[ATraining Step: 126  | total loss: [1m[32m0.32244[0m[0m | time: 16.067s
[2K
| Adam | epoch: 018 | loss: 0.32244 - acc: 0.8756 | val_loss: 0.64342 - val_acc: 0.7059 -- iter: 216/216
--
Training Step: 127  | total loss: [1m[32m0.30294[0m[0m | time: 0.856s
[2K
| Adam | epoch: 019 | loss: 0.30294 - acc: 0.8849 -- iter: 032/216
[A[ATraining Step: 128  | total loss: [1m[32m0.29508[0m[0m | time: 1.583s
[2K
| Adam | epoch: 019 | loss: 0.29508 - acc: 0.8881 -- iter: 064/216
[A[ATraining Step: 129  | total loss: [1m[32m0.28298[0m[0m | time: 2.550s
[2K
| Adam | epoch: 019 | loss: 0.28298 - acc: 0.8951 -- iter: 096/216
[A[ATraining Step: 130  | total loss: [1m[32m0.27190[0m[0m | time: 3.583s
[2K
| Adam | epoch: 019 | loss: 0.27190 - acc: 0.9025 -- iter: 128/216
[A[ATraining Step: 131  | total loss: [1m[32m0.26549[0m[0m | time: 4.782s
[2K
| Adam | epoch: 019 | loss: 0.26549 - acc: 0.9060 -- iter: 160/216
[A[ATraining Step: 132  | total loss: [1m[32m0.25499[0m[0m | time: 7.119s
[2K
| Adam | epoch: 019 | loss: 0.25499 - acc: 0.9060 -- iter: 192/216
[A[ATraining Step: 133  | total loss: [1m[32m0.26504[0m[0m | time: 13.282s
[2K
| Adam | epoch: 019 | loss: 0.26504 - acc: 0.8998 | val_loss: 0.63173 - val_acc: 0.7206 -- iter: 216/216
--
Training Step: 134  | total loss: [1m[32m0.25248[0m[0m | time: 0.969s
[2K
| Adam | epoch: 020 | loss: 0.25248 - acc: 0.9067 -- iter: 032/216
[A[ATraining Step: 135  | total loss: [1m[32m0.24318[0m[0m | time: 1.832s
[2K
| Adam | epoch: 020 | loss: 0.24318 - acc: 0.9129 -- iter: 064/216
[A[ATraining Step: 136  | total loss: [1m[32m0.26307[0m[0m | time: 2.722s
[2K
| Adam | epoch: 020 | loss: 0.26307 - acc: 0.9008 -- iter: 096/216
[A[ATraining Step: 137  | total loss: [1m[32m0.26727[0m[0m | time: 3.754s
[2K
| Adam | epoch: 020 | loss: 0.26727 - acc: 0.8940 -- iter: 128/216
[A[ATraining Step: 138  | total loss: [1m[32m0.24748[0m[0m | time: 8.815s
[2K
| Adam | epoch: 020 | loss: 0.24748 - acc: 0.9046 -- iter: 160/216
[A[ATraining Step: 139  | total loss: [1m[32m0.23545[0m[0m | time: 17.000s
[2K
| Adam | epoch: 020 | loss: 0.23545 - acc: 0.9079 -- iter: 192/216
[A[ATraining Step: 140  | total loss: [1m[32m0.24356[0m[0m | time: 18.969s
[2K
| Adam | epoch: 020 | loss: 0.24356 - acc: 0.9046 | val_loss: 0.52084 - val_acc: 0.7647 -- iter: 216/216
--
Training Step: 141  | total loss: [1m[32m0.24396[0m[0m | time: 1.151s
[2K
| Adam | epoch: 021 | loss: 0.24396 - acc: 0.9048 -- iter: 032/216
[A[ATraining Step: 142  | total loss: [1m[32m0.22902[0m[0m | time: 2.378s
[2K
| Adam | epoch: 021 | loss: 0.22902 - acc: 0.9143 -- iter: 064/216
[A[ATraining Step: 143  | total loss: [1m[32m0.21990[0m[0m | time: 3.148s
[2K
| Adam | epoch: 021 | loss: 0.21990 - acc: 0.9198 -- iter: 096/216
[A[ATraining Step: 144  | total loss: [1m[32m0.20887[0m[0m | time: 3.866s
[2K
| Adam | epoch: 021 | loss: 0.20887 - acc: 0.9278 -- iter: 128/216
[A[ATraining Step: 145  | total loss: [1m[32m0.19817[0m[0m | time: 4.848s
[2K
| Adam | epoch: 021 | loss: 0.19817 - acc: 0.9350 -- iter: 160/216
[A[ATraining Step: 146  | total loss: [1m[32m0.20217[0m[0m | time: 5.839s
[2K
| Adam | epoch: 021 | loss: 0.20217 - acc: 0.9290 -- iter: 192/216
[A[ATraining Step: 147  | total loss: [1m[32m0.18945[0m[0m | time: 7.807s
[2K
| Adam | epoch: 021 | loss: 0.18945 - acc: 0.9330 | val_loss: 0.64742 - val_acc: 0.8088 -- iter: 216/216
--
Training Step: 148  | total loss: [1m[32m0.17684[0m[0m | time: 5.395s
[2K
| Adam | epoch: 022 | loss: 0.17684 - acc: 0.9366 -- iter: 032/216
[A[ATraining Step: 149  | total loss: [1m[32m0.16721[0m[0m | time: 6.376s
[2K
| Adam | epoch: 022 | loss: 0.16721 - acc: 0.9398 -- iter: 064/216
[A[ATraining Step: 150  | total loss: [1m[32m0.16145[0m[0m | time: 7.415s
[2K
| Adam | epoch: 022 | loss: 0.16145 - acc: 0.9427 -- iter: 096/216
[A[ATraining Step: 151  | total loss: [1m[32m0.16636[0m[0m | time: 8.182s
[2K
| Adam | epoch: 022 | loss: 0.16636 - acc: 0.9422 -- iter: 128/216
[A[ATraining Step: 152  | total loss: [1m[32m0.15376[0m[0m | time: 8.941s
[2K
| Adam | epoch: 022 | loss: 0.15376 - acc: 0.9479 -- iter: 160/216
[A[ATraining Step: 153  | total loss: [1m[32m0.14369[0m[0m | time: 10.057s
[2K
| Adam | epoch: 022 | loss: 0.14369 - acc: 0.9490 -- iter: 192/216
[A[ATraining Step: 154  | total loss: [1m[32m0.13626[0m[0m | time: 11.950s
[2K
| Adam | epoch: 022 | loss: 0.13626 - acc: 0.9510 | val_loss: 0.92776 - val_acc: 0.6912 -- iter: 216/216
--
Training Step: 155  | total loss: [1m[32m0.12659[0m[0m | time: 9.815s
[2K
| Adam | epoch: 023 | loss: 0.12659 - acc: 0.9559 -- iter: 032/216
[A[ATraining Step: 156  | total loss: [1m[32m0.12420[0m[0m | time: 10.830s
[2K
| Adam | epoch: 023 | loss: 0.12420 - acc: 0.9603 -- iter: 064/216
[A[ATraining Step: 157  | total loss: [1m[32m0.11661[0m[0m | time: 11.828s
[2K
| Adam | epoch: 023 | loss: 0.11661 - acc: 0.9642 -- iter: 096/216
[A[ATraining Step: 158  | total loss: [1m[32m0.10663[0m[0m | time: 12.770s
[2K
| Adam | epoch: 023 | loss: 0.10663 - acc: 0.9678 -- iter: 128/216
[A[ATraining Step: 159  | total loss: [1m[32m0.10688[0m[0m | time: 13.599s
[2K
| Adam | epoch: 023 | loss: 0.10688 - acc: 0.9679 -- iter: 160/216
[A[ATraining Step: 160  | total loss: [1m[32m0.13962[0m[0m | time: 14.436s
[2K
| Adam | epoch: 023 | loss: 0.13962 - acc: 0.9586 -- iter: 192/216
[A[ATraining Step: 161  | total loss: [1m[32m0.14428[0m[0m | time: 16.337s
[2K
| Adam | epoch: 023 | loss: 0.14428 - acc: 0.9544 | val_loss: 0.76683 - val_acc: 0.6765 -- iter: 216/216
--
Training Step: 162  | total loss: [1m[32m0.15428[0m[0m | time: 0.628s
[2K
| Adam | epoch: 024 | loss: 0.15428 - acc: 0.9465 -- iter: 032/216
[A[ATraining Step: 163  | total loss: [1m[32m0.13980[0m[0m | time: 1.245s
[2K
| Adam | epoch: 024 | loss: 0.13980 - acc: 0.9518 -- iter: 064/216
[A[ATraining Step: 164  | total loss: [1m[32m0.13966[0m[0m | time: 1.902s
[2K
| Adam | epoch: 024 | loss: 0.13966 - acc: 0.9535 -- iter: 096/216
[A[ATraining Step: 165  | total loss: [1m[32m0.16592[0m[0m | time: 2.499s
[2K
| Adam | epoch: 024 | loss: 0.16592 - acc: 0.9394 -- iter: 128/216
[A[ATraining Step: 166  | total loss: [1m[32m0.17570[0m[0m | time: 3.170s
[2K
| Adam | epoch: 024 | loss: 0.17570 - acc: 0.9330 -- iter: 160/216
[A[ATraining Step: 167  | total loss: [1m[32m0.16423[0m[0m | time: 3.633s
[2K
| Adam | epoch: 024 | loss: 0.16423 - acc: 0.9397 -- iter: 192/216
[A[ATraining Step: 168  | total loss: [1m[32m0.18924[0m[0m | time: 5.099s
[2K
| Adam | epoch: 024 | loss: 0.18924 - acc: 0.9374 | val_loss: 0.56541 - val_acc: 0.7647 -- iter: 216/216
--
Training Step: 169  | total loss: [1m[32m0.17845[0m[0m | time: 0.635s
[2K
| Adam | epoch: 025 | loss: 0.17845 - acc: 0.9436 -- iter: 032/216
[A[ATraining Step: 170  | total loss: [1m[32m0.16387[0m[0m | time: 1.248s
[2K
| Adam | epoch: 025 | loss: 0.16387 - acc: 0.9493 -- iter: 064/216
[A[ATraining Step: 171  | total loss: [1m[32m0.16123[0m[0m | time: 1.879s
[2K
| Adam | epoch: 025 | loss: 0.16123 - acc: 0.9512 -- iter: 096/216
[A[ATraining Step: 172  | total loss: [1m[32m0.15389[0m[0m | time: 2.511s
[2K
| Adam | epoch: 025 | loss: 0.15389 - acc: 0.9530 -- iter: 128/216
[A[ATraining Step: 173  | total loss: [1m[32m0.14647[0m[0m | time: 3.116s
[2K
| Adam | epoch: 025 | loss: 0.14647 - acc: 0.9546 -- iter: 160/216
[A[ATraining Step: 174  | total loss: [1m[32m0.14650[0m[0m | time: 3.754s
[2K
| Adam | epoch: 025 | loss: 0.14650 - acc: 0.9529 -- iter: 192/216
[A[ATraining Step: 175  | total loss: [1m[32m0.13965[0m[0m | time: 5.234s
[2K
| Adam | epoch: 025 | loss: 0.13965 - acc: 0.9544 | val_loss: 0.65617 - val_acc: 0.7059 -- iter: 216/216
--
Training Step: 176  | total loss: [1m[32m0.17770[0m[0m | time: 0.497s
[2K
| Adam | epoch: 026 | loss: 0.17770 - acc: 0.9507 -- iter: 032/216
[A[ATraining Step: 177  | total loss: [1m[32m0.16267[0m[0m | time: 1.379s
[2K
| Adam | epoch: 026 | loss: 0.16267 - acc: 0.9556 -- iter: 064/216
[A[ATraining Step: 178  | total loss: [1m[32m0.15299[0m[0m | time: 2.218s
[2K
| Adam | epoch: 026 | loss: 0.15299 - acc: 0.9600 -- iter: 096/216
[A[ATraining Step: 179  | total loss: [1m[32m0.14407[0m[0m | time: 3.066s
[2K
| Adam | epoch: 026 | loss: 0.14407 - acc: 0.9609 -- iter: 128/216
[A[ATraining Step: 180  | total loss: [1m[32m0.14056[0m[0m | time: 4.015s
[2K
| Adam | epoch: 026 | loss: 0.14056 - acc: 0.9617 -- iter: 160/216
[A[ATraining Step: 181  | total loss: [1m[32m0.13148[0m[0m | time: 4.899s
[2K
| Adam | epoch: 026 | loss: 0.13148 - acc: 0.9655 -- iter: 192/216
[A[ATraining Step: 182  | total loss: [1m[32m0.12105[0m[0m | time: 6.913s
[2K
| Adam | epoch: 026 | loss: 0.12105 - acc: 0.9690 | val_loss: 0.68226 - val_acc: 0.7059 -- iter: 216/216
--
Training Step: 183  | total loss: [1m[32m0.11129[0m[0m | time: 1.003s
[2K
| Adam | epoch: 027 | loss: 0.11129 - acc: 0.9721 -- iter: 032/216
[A[ATraining Step: 184  | total loss: [1m[32m0.10215[0m[0m | time: 2.086s
[2K
| Adam | epoch: 027 | loss: 0.10215 - acc: 0.9749 -- iter: 064/216
[A[ATraining Step: 185  | total loss: [1m[32m0.09341[0m[0m | time: 7.365s
[2K
| Adam | epoch: 027 | loss: 0.09341 - acc: 0.9774 -- iter: 096/216
[A[ATraining Step: 186  | total loss: [1m[32m0.08602[0m[0m | time: 8.310s
[2K
| Adam | epoch: 027 | loss: 0.08602 - acc: 0.9796 -- iter: 128/216
[A[ATraining Step: 187  | total loss: [1m[32m0.07993[0m[0m | time: 9.257s
[2K
| Adam | epoch: 027 | loss: 0.07993 - acc: 0.9817 -- iter: 160/216
[A[ATraining Step: 188  | total loss: [1m[32m0.07274[0m[0m | time: 10.201s
[2K
| Adam | epoch: 027 | loss: 0.07274 - acc: 0.9835 -- iter: 192/216
[A[ATraining Step: 189  | total loss: [1m[32m0.07141[0m[0m | time: 12.274s
[2K
| Adam | epoch: 027 | loss: 0.07141 - acc: 0.9852 | val_loss: 0.86611 - val_acc: 0.7206 -- iter: 216/216
--
Training Step: 190  | total loss: [1m[32m0.06755[0m[0m | time: 0.852s
[2K
| Adam | epoch: 028 | loss: 0.06755 - acc: 0.9835 -- iter: 032/216
[A[ATraining Step: 191  | total loss: [1m[32m0.06201[0m[0m | time: 1.578s
[2K
| Adam | epoch: 028 | loss: 0.06201 - acc: 0.9852 -- iter: 064/216
[A[ATraining Step: 192  | total loss: [1m[32m0.06165[0m[0m | time: 2.328s
[2K
| Adam | epoch: 028 | loss: 0.06165 - acc: 0.9825 -- iter: 096/216
[A[ATraining Step: 193  | total loss: [1m[32m0.05930[0m[0m | time: 3.386s
[2K
| Adam | epoch: 028 | loss: 0.05930 - acc: 0.9842 -- iter: 128/216
[A[ATraining Step: 194  | total loss: [1m[32m0.05521[0m[0m | time: 4.387s
[2K
| Adam | epoch: 028 | loss: 0.05521 - acc: 0.9858 -- iter: 160/216
[A[ATraining Step: 195  | total loss: [1m[32m0.05016[0m[0m | time: 5.517s
[2K
| Adam | epoch: 028 | loss: 0.05016 - acc: 0.9872 -- iter: 192/216
[A[ATraining Step: 196  | total loss: [1m[32m0.05216[0m[0m | time: 7.510s
[2K
| Adam | epoch: 028 | loss: 0.05216 - acc: 0.9854 | val_loss: 0.95913 - val_acc: 0.7353 -- iter: 216/216
--
Training Step: 197  | total loss: [1m[32m0.04939[0m[0m | time: 0.956s
[2K
| Adam | epoch: 029 | loss: 0.04939 - acc: 0.9868 -- iter: 032/216
[A[ATraining Step: 198  | total loss: [1m[32m0.04614[0m[0m | time: 1.909s
[2K
| Adam | epoch: 029 | loss: 0.04614 - acc: 0.9882 -- iter: 064/216
[A[ATraining Step: 199  | total loss: [1m[32m0.04378[0m[0m | time: 2.630s
[2K
| Adam | epoch: 029 | loss: 0.04378 - acc: 0.9893 -- iter: 096/216
[A[ATraining Step: 200  | total loss: [1m[32m0.05452[0m[0m | time: 4.540s
[2K
| Adam | epoch: 029 | loss: 0.05452 - acc: 0.9862 | val_loss: 1.28769 - val_acc: 0.6912 -- iter: 128/216
--
Training Step: 201  | total loss: [1m[32m0.05644[0m[0m | time: 5.775s
[2K
| Adam | epoch: 029 | loss: 0.05644 - acc: 0.9835 -- iter: 160/216
[A[ATraining Step: 202  | total loss: [1m[32m0.05632[0m[0m | time: 8.784s
[2K
| Adam | epoch: 029 | loss: 0.05632 - acc: 0.9820 -- iter: 192/216
[A[ATraining Step: 203  | total loss: [1m[32m0.05205[0m[0m | time: 18.465s
[2K
| Adam | epoch: 029 | loss: 0.05205 - acc: 0.9838 | val_loss: 0.97561 - val_acc: 0.7500 -- iter: 216/216
--
Training Step: 204  | total loss: [1m[32m0.04751[0m[0m | time: 0.957s
[2K
| Adam | epoch: 030 | loss: 0.04751 - acc: 0.9854 -- iter: 032/216
[A[ATraining Step: 205  | total loss: [1m[32m0.04588[0m[0m | time: 1.891s
[2K
| Adam | epoch: 030 | loss: 0.04588 - acc: 0.9837 -- iter: 064/216
[A[ATraining Step: 206  | total loss: [1m[32m0.05549[0m[0m | time: 2.976s
[2K
| Adam | epoch: 030 | loss: 0.05549 - acc: 0.9822 -- iter: 096/216
[A[ATraining Step: 207  | total loss: [1m[32m0.05183[0m[0m | time: 3.876s
[2K
| Adam | epoch: 030 | loss: 0.05183 - acc: 0.9840 -- iter: 128/216
[A[ATraining Step: 208  | total loss: [1m[32m0.10472[0m[0m | time: 4.763s
[2K
| Adam | epoch: 030 | loss: 0.10472 - acc: 0.9773 -- iter: 160/216
[A[ATraining Step: 209  | total loss: [1m[32m0.09505[0m[0m | time: 12.123s
[2K
| Adam | epoch: 030 | loss: 0.09505 - acc: 0.9796 -- iter: 192/216
[A[ATraining Step: 210  | total loss: [1m[32m0.09280[0m[0m | time: 14.085s
[2K
| Adam | epoch: 030 | loss: 0.09280 - acc: 0.9785 | val_loss: 0.73803 - val_acc: 0.7647 -- iter: 216/216
--
Validation AUC:0.8474418604651163
Validation AUPRC:0.8151246232015184
Test AUC:0.893719806763285
Test AUPRC:0.8934780390937365
BestTestF1Score	0.81	0.73	0.88	0.89	0.74	17	2	43	6	0.95
BestTestMCCScore	0.76	0.71	0.87	1.0	0.61	14	0	45	9	0.98
BestTestAccuracyScore	0.76	0.71	0.87	1.0	0.61	14	0	45	9	0.98
BestValidationF1Score	0.71	0.62	0.82	0.88	0.6	15	2	41	10	0.95
BestValidationMCC	0.7	0.62	0.82	0.93	0.56	14	1	42	11	0.98
BestValidationAccuracy	0.7	0.62	0.82	0.93	0.56	14	1	42	11	0.98
TestPredictions (Threshold:0.98)
CHEMBL2334211,TP,ACT,1.0	CHEMBL78291,TN,INACT,0.30000001192092896	CHEMBL283531,TN,INACT,0.009999999776482582	CHEMBL22695,TN,INACT,0.0	CHEMBL282051,TN,INACT,0.03999999910593033	CHEMBL2334199,TP,ACT,1.0	CHEMBL221907,TN,INACT,0.019999999552965164	CHEMBL1257559,TP,ACT,0.9800000190734863	CHEMBL1829786,TN,INACT,0.11999999731779099	CHEMBL2375696,TN,INACT,0.03999999910593033	CHEMBL3133094,FN,ACT,0.9700000286102295	CHEMBL2431936,TN,INACT,0.0	CHEMBL341384,TN,INACT,0.009999999776482582	CHEMBL1257678,TP,ACT,0.9800000190734863	CHEMBL2334198,TP,ACT,1.0	CHEMBL21870,TN,INACT,0.009999999776482582	CHEMBL477936,TN,INACT,0.0	CHEMBL406139,TN,INACT,0.0	CHEMBL3133006,FN,ACT,0.9700000286102295	CHEMBL2398797,TP,ACT,0.9900000095367432	CHEMBL2334197,TP,ACT,1.0	CHEMBL9416,TN,INACT,0.019999999552965164	CHEMBL49215,TN,INACT,0.05000000074505806	CHEMBL449307,FN,ACT,0.7099999785423279	CHEMBL277915,TN,INACT,0.0	CHEMBL24992,TN,INACT,0.029999999329447746	CHEMBL278161,TN,INACT,0.8799999952316284	CHEMBL280732,TN,INACT,0.8700000047683716	CHEMBL3233886,FN,ACT,0.009999999776482582	CHEMBL277674,TN,INACT,0.0	CHEMBL22525,TN,INACT,0.019999999552965164	CHEMBL2335520,TN,INACT,0.009999999776482582	CHEMBL433862,TN,INACT,0.009999999776482582	CHEMBL2335513,TN,INACT,0.9599999785423279	CHEMBL3290439,TN,INACT,0.019999999552965164	CHEMBL2333811,TP,ACT,1.0	CHEMBL375563,TN,INACT,0.029999999329447746	CHEMBL2334082,FN,ACT,0.9700000286102295	CHEMBL2431948,TN,INACT,0.18000000715255737	CHEMBL22407,TN,INACT,0.019999999552965164	CHEMBL282718,TN,INACT,0.0	CHEMBL1922685,TN,INACT,0.009999999776482582	CHEMBL269463,TN,INACT,0.009999999776482582	CHEMBL368497,TN,INACT,0.0	CHEMBL2396682,TP,ACT,1.0	CHEMBL432696,TN,INACT,0.9599999785423279	CHEMBL371024,TN,INACT,0.009999999776482582	CHEMBL135787,TN,INACT,0.0	CHEMBL282542,TN,INACT,0.0	CHEMBL1626156,TN,INACT,0.05000000074505806	CHEMBL423775,TN,INACT,0.3499999940395355	CHEMBL2170728,FN,ACT,0.029999999329447746	CHEMBL449977,FN,ACT,0.05000000074505806	CHEMBL289648,TN,INACT,0.019999999552965164	CHEMBL503766,FN,ACT,0.8899999856948853	CHEMBL454397,TP,ACT,0.9800000190734863	CHEMBL2375599,TP,ACT,1.0	CHEMBL200429,TN,INACT,0.8100000023841858	CHEMBL2398806,TP,ACT,1.0	CHEMBL22669,TN,INACT,0.009999999776482582	CHEMBL2334213,TP,ACT,1.0	CHEMBL2375698,FN,ACT,0.0	CHEMBL328544,TN,INACT,0.4099999964237213	CHEMBL1257558,TP,ACT,0.9800000190734863	CHEMBL1687979,TN,INACT,0.009999999776482582	CHEMBL295608,TN,INACT,0.029999999329447746	CHEMBL276448,TN,INACT,0.1899999976158142	CHEMBL2207798,TN,INACT,0.9399999976158142	

