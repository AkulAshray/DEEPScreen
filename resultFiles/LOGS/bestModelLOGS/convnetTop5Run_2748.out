ImageNetInceptionV2 CHEMBL5028 RMSprop 0.0005 15 0 0 0.6 False True
Number of active compounds :	139
Number of inactive compounds :	139
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL5028_RMSprop_0.0005_15_0_0_0.6_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL5028_RMSprop_0.0005_15_0.6/
---------------------------------
Training samples: 166
Validation samples: 53
--
Training Step: 1  | time: 60.609s
[2K
| RMSProp | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/166
[A[ATraining Step: 2  | total loss: [1m[32m0.64762[0m[0m | time: 68.697s
[2K
| RMSProp | epoch: 001 | loss: 0.64762 - acc: 0.4500 -- iter: 064/166
[A[ATraining Step: 3  | total loss: [1m[32m0.71139[0m[0m | time: 76.718s
[2K
| RMSProp | epoch: 001 | loss: 0.71139 - acc: 0.3886 -- iter: 096/166
[A[ATraining Step: 4  | total loss: [1m[32m0.71801[0m[0m | time: 84.656s
[2K
| RMSProp | epoch: 001 | loss: 0.71801 - acc: 0.4018 -- iter: 128/166
[A[ATraining Step: 5  | total loss: [1m[32m0.68680[0m[0m | time: 105.181s
[2K
| RMSProp | epoch: 001 | loss: 0.68680 - acc: 0.5347 -- iter: 160/166
[A[ATraining Step: 6  | total loss: [1m[32m0.69778[0m[0m | time: 124.383s
[2K
| RMSProp | epoch: 001 | loss: 0.69778 - acc: 0.4923 | val_loss: 0.68742 - val_acc: 0.5660 -- iter: 166/166
--
Training Step: 7  | total loss: [1m[32m0.71041[0m[0m | time: 2.321s
[2K
| RMSProp | epoch: 002 | loss: 0.71041 - acc: 0.4969 -- iter: 032/166
[A[ATraining Step: 8  | total loss: [1m[32m0.68205[0m[0m | time: 10.504s
[2K
| RMSProp | epoch: 002 | loss: 0.68205 - acc: 0.4987 -- iter: 064/166
[A[ATraining Step: 9  | total loss: [1m[32m0.69914[0m[0m | time: 18.633s
[2K
| RMSProp | epoch: 002 | loss: 0.69914 - acc: 0.4497 -- iter: 096/166
[A[ATraining Step: 10  | total loss: [1m[32m0.70200[0m[0m | time: 33.566s
[2K
| RMSProp | epoch: 002 | loss: 0.70200 - acc: 0.5217 -- iter: 128/166
[A[ATraining Step: 11  | total loss: [1m[32m0.68030[0m[0m | time: 68.743s
[2K
| RMSProp | epoch: 002 | loss: 0.68030 - acc: 0.5707 -- iter: 160/166
[A[ATraining Step: 12  | total loss: [1m[32m0.67844[0m[0m | time: 89.899s
[2K
| RMSProp | epoch: 002 | loss: 0.67844 - acc: 0.5810 | val_loss: 0.68601 - val_acc: 0.5660 -- iter: 166/166
--
Training Step: 13  | total loss: [1m[32m0.67752[0m[0m | time: 2.396s
[2K
| RMSProp | epoch: 003 | loss: 0.67752 - acc: 0.5463 -- iter: 032/166
[A[ATraining Step: 14  | total loss: [1m[32m0.74709[0m[0m | time: 4.631s
[2K
| RMSProp | epoch: 003 | loss: 0.74709 - acc: 0.4592 -- iter: 064/166
[A[ATraining Step: 15  | total loss: [1m[32m0.71672[0m[0m | time: 12.748s
[2K
| RMSProp | epoch: 003 | loss: 0.71672 - acc: 0.5404 -- iter: 096/166
[A[ATraining Step: 16  | total loss: [1m[32m0.72040[0m[0m | time: 20.920s
[2K
| RMSProp | epoch: 003 | loss: 0.72040 - acc: 0.4784 -- iter: 128/166
[A[ATraining Step: 17  | total loss: [1m[32m0.72526[0m[0m | time: 32.551s
[2K
| RMSProp | epoch: 003 | loss: 0.72526 - acc: 0.4861 -- iter: 160/166
[A[ATraining Step: 18  | total loss: [1m[32m0.71361[0m[0m | time: 49.101s
[2K
| RMSProp | epoch: 003 | loss: 0.71361 - acc: 0.5018 | val_loss: 0.68487 - val_acc: 0.5660 -- iter: 166/166
--
Training Step: 19  | total loss: [1m[32m0.69662[0m[0m | time: 11.045s
[2K
| RMSProp | epoch: 004 | loss: 0.69662 - acc: 0.5324 -- iter: 032/166
[A[ATraining Step: 20  | total loss: [1m[32m0.69749[0m[0m | time: 13.303s
[2K
| RMSProp | epoch: 004 | loss: 0.69749 - acc: 0.4919 -- iter: 064/166
[A[ATraining Step: 21  | total loss: [1m[32m0.71033[0m[0m | time: 15.561s
[2K
| RMSProp | epoch: 004 | loss: 0.71033 - acc: 0.4944 -- iter: 096/166
[A[ATraining Step: 22  | total loss: [1m[32m0.67394[0m[0m | time: 23.780s
[2K
| RMSProp | epoch: 004 | loss: 0.67394 - acc: 0.6461 -- iter: 128/166
[A[ATraining Step: 23  | total loss: [1m[32m0.66335[0m[0m | time: 32.098s
[2K
| RMSProp | epoch: 004 | loss: 0.66335 - acc: 0.6400 -- iter: 160/166
[A[ATraining Step: 24  | total loss: [1m[32m0.66662[0m[0m | time: 46.882s
[2K
| RMSProp | epoch: 004 | loss: 0.66662 - acc: 0.6357 | val_loss: 0.68487 - val_acc: 0.5660 -- iter: 166/166
--
Training Step: 25  | total loss: [1m[32m0.66541[0m[0m | time: 67.954s
[2K
| RMSProp | epoch: 005 | loss: 0.66541 - acc: 0.6413 -- iter: 032/166
[A[ATraining Step: 26  | total loss: [1m[32m0.66928[0m[0m | time: 96.676s
[2K
| RMSProp | epoch: 005 | loss: 0.66928 - acc: 0.6205 -- iter: 064/166
[A[ATraining Step: 27  | total loss: [1m[32m0.67479[0m[0m | time: 99.860s
[2K
| RMSProp | epoch: 005 | loss: 0.67479 - acc: 0.6216 -- iter: 096/166
[A[ATraining Step: 28  | total loss: [1m[32m0.64423[0m[0m | time: 103.158s
[2K
| RMSProp | epoch: 005 | loss: 0.64423 - acc: 0.6746 -- iter: 128/166
[A[ATraining Step: 29  | total loss: [1m[32m0.60259[0m[0m | time: 112.664s
[2K
| RMSProp | epoch: 005 | loss: 0.60259 - acc: 0.7537 -- iter: 160/166
[A[ATraining Step: 30  | total loss: [1m[32m0.62401[0m[0m | time: 123.288s
[2K
| RMSProp | epoch: 005 | loss: 0.62401 - acc: 0.7084 | val_loss: 0.70861 - val_acc: 0.4340 -- iter: 166/166
--
Training Step: 31  | total loss: [1m[32m0.64325[0m[0m | time: 10.869s
[2K
| RMSProp | epoch: 006 | loss: 0.64325 - acc: 0.6603 -- iter: 032/166
[A[ATraining Step: 32  | total loss: [1m[32m0.66267[0m[0m | time: 25.431s
[2K
| RMSProp | epoch: 006 | loss: 0.66267 - acc: 0.6032 -- iter: 064/166
[A[ATraining Step: 33  | total loss: [1m[32m0.66542[0m[0m | time: 39.141s
[2K
| RMSProp | epoch: 006 | loss: 0.66542 - acc: 0.6011 -- iter: 096/166
[A[ATraining Step: 34  | total loss: [1m[32m0.65492[0m[0m | time: 42.591s
[2K
| RMSProp | epoch: 006 | loss: 0.65492 - acc: 0.6263 -- iter: 128/166
[A[ATraining Step: 35  | total loss: [1m[32m0.63685[0m[0m | time: 46.387s
[2K
| RMSProp | epoch: 006 | loss: 0.63685 - acc: 0.6696 -- iter: 160/166
[A[ATraining Step: 36  | total loss: [1m[32m0.64523[0m[0m | time: 92.292s
[2K
| RMSProp | epoch: 006 | loss: 0.64523 - acc: 0.6690 | val_loss: 0.70335 - val_acc: 0.4340 -- iter: 166/166
--
Training Step: 37  | total loss: [1m[32m0.64459[0m[0m | time: 8.237s
[2K
| RMSProp | epoch: 007 | loss: 0.64459 - acc: 0.6852 -- iter: 032/166
[A[ATraining Step: 38  | total loss: [1m[32m0.63851[0m[0m | time: 16.859s
[2K
| RMSProp | epoch: 007 | loss: 0.63851 - acc: 0.6796 -- iter: 064/166
[A[ATraining Step: 39  | total loss: [1m[32m0.63871[0m[0m | time: 28.136s
[2K
| RMSProp | epoch: 007 | loss: 0.63871 - acc: 0.6811 -- iter: 096/166
[A[ATraining Step: 40  | total loss: [1m[32m0.63683[0m[0m | time: 40.951s
[2K
| RMSProp | epoch: 007 | loss: 0.63683 - acc: 0.6647 -- iter: 128/166
[A[ATraining Step: 41  | total loss: [1m[32m0.63513[0m[0m | time: 44.436s
[2K
| RMSProp | epoch: 007 | loss: 0.63513 - acc: 0.6746 -- iter: 160/166
[A[ATraining Step: 42  | total loss: [1m[32m0.64325[0m[0m | time: 52.236s
[2K
| RMSProp | epoch: 007 | loss: 0.64325 - acc: 0.6432 | val_loss: 0.67568 - val_acc: 0.5472 -- iter: 166/166
--
Training Step: 43  | total loss: [1m[32m0.63133[0m[0m | time: 8.172s
[2K
| RMSProp | epoch: 008 | loss: 0.63133 - acc: 0.6473 -- iter: 032/166
[A[ATraining Step: 44  | total loss: [1m[32m0.64057[0m[0m | time: 16.260s
[2K
| RMSProp | epoch: 008 | loss: 0.64057 - acc: 0.6327 -- iter: 064/166
[A[ATraining Step: 45  | total loss: [1m[32m0.64084[0m[0m | time: 24.235s
[2K
| RMSProp | epoch: 008 | loss: 0.64084 - acc: 0.6420 -- iter: 096/166
[A[ATraining Step: 46  | total loss: [1m[32m0.63231[0m[0m | time: 35.843s
[2K
| RMSProp | epoch: 008 | loss: 0.63231 - acc: 0.6496 -- iter: 128/166
[A[ATraining Step: 47  | total loss: [1m[32m0.63179[0m[0m | time: 46.785s
[2K
| RMSProp | epoch: 008 | loss: 0.63179 - acc: 0.6404 -- iter: 160/166
[A[ATraining Step: 48  | total loss: [1m[32m0.62582[0m[0m | time: 53.997s
[2K
| RMSProp | epoch: 008 | loss: 0.62582 - acc: 0.6530 | val_loss: 0.70391 - val_acc: 0.4340 -- iter: 166/166
--
Training Step: 49  | total loss: [1m[32m0.64225[0m[0m | time: 4.011s
[2K
| RMSProp | epoch: 009 | loss: 0.64225 - acc: 0.6289 -- iter: 032/166
[A[ATraining Step: 50  | total loss: [1m[32m0.63644[0m[0m | time: 13.696s
[2K
| RMSProp | epoch: 009 | loss: 0.63644 - acc: 0.6089 -- iter: 064/166
[A[ATraining Step: 51  | total loss: [1m[32m0.63997[0m[0m | time: 21.496s
[2K
| RMSProp | epoch: 009 | loss: 0.63997 - acc: 0.6209 -- iter: 096/166
[A[ATraining Step: 52  | total loss: [1m[32m0.64161[0m[0m | time: 29.799s
[2K
| RMSProp | epoch: 009 | loss: 0.64161 - acc: 0.6309 -- iter: 128/166
[A[ATraining Step: 53  | total loss: [1m[32m0.65443[0m[0m | time: 43.489s
[2K
| RMSProp | epoch: 009 | loss: 0.65443 - acc: 0.6023 -- iter: 160/166
[A[ATraining Step: 54  | total loss: [1m[32m0.64451[0m[0m | time: 59.940s
[2K
| RMSProp | epoch: 009 | loss: 0.64451 - acc: 0.6238 | val_loss: 0.68161 - val_acc: 0.6038 -- iter: 166/166
--
Training Step: 55  | total loss: [1m[32m0.64107[0m[0m | time: 2.283s
[2K
| RMSProp | epoch: 010 | loss: 0.64107 - acc: 0.6239 -- iter: 032/166
[A[ATraining Step: 56  | total loss: [1m[32m0.64680[0m[0m | time: 4.534s
[2K
| RMSProp | epoch: 010 | loss: 0.64680 - acc: 0.6065 -- iter: 064/166
[A[ATraining Step: 57  | total loss: [1m[32m0.61845[0m[0m | time: 12.529s
[2K
| RMSProp | epoch: 010 | loss: 0.61845 - acc: 0.6610 -- iter: 096/166
[A[ATraining Step: 58  | total loss: [1m[32m0.62185[0m[0m | time: 20.586s
[2K
| RMSProp | epoch: 010 | loss: 0.62185 - acc: 0.6518 -- iter: 128/166
[A[ATraining Step: 59  | total loss: [1m[32m0.61278[0m[0m | time: 28.764s
[2K
| RMSProp | epoch: 010 | loss: 0.61278 - acc: 0.6608 -- iter: 160/166
[A[ATraining Step: 60  | total loss: [1m[32m0.61843[0m[0m | time: 42.350s
[2K
| RMSProp | epoch: 010 | loss: 0.61843 - acc: 0.6437 | val_loss: 0.66097 - val_acc: 0.7358 -- iter: 166/166
--
Training Step: 61  | total loss: [1m[32m0.61227[0m[0m | time: 11.875s
[2K
| RMSProp | epoch: 011 | loss: 0.61227 - acc: 0.6616 -- iter: 032/166
[A[ATraining Step: 62  | total loss: [1m[32m0.61504[0m[0m | time: 14.117s
[2K
| RMSProp | epoch: 011 | loss: 0.61504 - acc: 0.6690 -- iter: 064/166
[A[ATraining Step: 63  | total loss: [1m[32m0.67328[0m[0m | time: 16.385s
[2K
| RMSProp | epoch: 011 | loss: 0.67328 - acc: 0.5842 -- iter: 096/166
[A[ATraining Step: 64  | total loss: [1m[32m0.67017[0m[0m | time: 24.467s
[2K
| RMSProp | epoch: 011 | loss: 0.67017 - acc: 0.5736 -- iter: 128/166
[A[ATraining Step: 65  | total loss: [1m[32m0.66967[0m[0m | time: 32.313s
[2K
| RMSProp | epoch: 011 | loss: 0.66967 - acc: 0.5607 -- iter: 160/166
[A[ATraining Step: 66  | total loss: [1m[32m0.65960[0m[0m | time: 48.839s
[2K
| RMSProp | epoch: 011 | loss: 0.65960 - acc: 0.5913 | val_loss: 0.74980 - val_acc: 0.4340 -- iter: 166/166
--
Training Step: 67  | total loss: [1m[32m0.65035[0m[0m | time: 11.367s
[2K
| RMSProp | epoch: 012 | loss: 0.65035 - acc: 0.6104 -- iter: 032/166
[A[ATraining Step: 68  | total loss: [1m[32m0.63876[0m[0m | time: 22.921s
[2K
| RMSProp | epoch: 012 | loss: 0.63876 - acc: 0.6343 -- iter: 064/166
[A[ATraining Step: 69  | total loss: [1m[32m0.63203[0m[0m | time: 25.206s
[2K
| RMSProp | epoch: 012 | loss: 0.63203 - acc: 0.6515 -- iter: 096/166
[A[ATraining Step: 70  | total loss: [1m[32m0.62894[0m[0m | time: 27.430s
[2K
| RMSProp | epoch: 012 | loss: 0.62894 - acc: 0.6532 -- iter: 128/166
[A[ATraining Step: 71  | total loss: [1m[32m0.60851[0m[0m | time: 35.501s
[2K
| RMSProp | epoch: 012 | loss: 0.60851 - acc: 0.6927 -- iter: 160/166
[A[ATraining Step: 72  | total loss: [1m[32m0.60551[0m[0m | time: 46.249s
[2K
| RMSProp | epoch: 012 | loss: 0.60551 - acc: 0.6992 | val_loss: 0.61067 - val_acc: 0.7925 -- iter: 166/166
--
Training Step: 73  | total loss: [1m[32m0.59669[0m[0m | time: 7.907s
[2K
| RMSProp | epoch: 013 | loss: 0.59669 - acc: 0.6979 -- iter: 032/166
[A[ATraining Step: 74  | total loss: [1m[32m0.59122[0m[0m | time: 15.956s
[2K
| RMSProp | epoch: 013 | loss: 0.59122 - acc: 0.7208 -- iter: 064/166
[A[ATraining Step: 75  | total loss: [1m[32m0.59035[0m[0m | time: 23.835s
[2K
| RMSProp | epoch: 013 | loss: 0.59035 - acc: 0.7205 -- iter: 096/166
[A[ATraining Step: 76  | total loss: [1m[32m0.57759[0m[0m | time: 26.094s
[2K
| RMSProp | epoch: 013 | loss: 0.57759 - acc: 0.7371 -- iter: 128/166
[A[ATraining Step: 77  | total loss: [1m[32m0.58448[0m[0m | time: 28.378s
[2K
| RMSProp | epoch: 013 | loss: 0.58448 - acc: 0.7120 -- iter: 160/166
[A[ATraining Step: 78  | total loss: [1m[32m0.56061[0m[0m | time: 38.792s
[2K
| RMSProp | epoch: 013 | loss: 0.56061 - acc: 0.7421 | val_loss: 1.31538 - val_acc: 0.4340 -- iter: 166/166
--
Training Step: 79  | total loss: [1m[32m0.59869[0m[0m | time: 7.989s
[2K
| RMSProp | epoch: 014 | loss: 0.59869 - acc: 0.7041 -- iter: 032/166
[A[ATraining Step: 80  | total loss: [1m[32m0.58905[0m[0m | time: 15.966s
[2K
| RMSProp | epoch: 014 | loss: 0.58905 - acc: 0.7184 -- iter: 064/166
[A[ATraining Step: 81  | total loss: [1m[32m0.58488[0m[0m | time: 23.861s
[2K
| RMSProp | epoch: 014 | loss: 0.58488 - acc: 0.7216 -- iter: 096/166
[A[ATraining Step: 82  | total loss: [1m[32m0.57296[0m[0m | time: 31.585s
[2K
| RMSProp | epoch: 014 | loss: 0.57296 - acc: 0.7276 -- iter: 128/166
[A[ATraining Step: 83  | total loss: [1m[32m0.56718[0m[0m | time: 33.835s
[2K
| RMSProp | epoch: 014 | loss: 0.56718 - acc: 0.7267 -- iter: 160/166
[A[ATraining Step: 84  | total loss: [1m[32m0.56444[0m[0m | time: 38.452s
[2K
| RMSProp | epoch: 014 | loss: 0.56444 - acc: 0.7540 | val_loss: 0.55526 - val_acc: 0.8868 -- iter: 166/166
--
Training Step: 85  | total loss: [1m[32m0.53872[0m[0m | time: 7.903s
[2K
| RMSProp | epoch: 015 | loss: 0.53872 - acc: 0.7786 -- iter: 032/166
[A[ATraining Step: 86  | total loss: [1m[32m0.53606[0m[0m | time: 15.828s
[2K
| RMSProp | epoch: 015 | loss: 0.53606 - acc: 0.7789 -- iter: 064/166
[A[ATraining Step: 87  | total loss: [1m[32m0.52187[0m[0m | time: 23.592s
[2K
| RMSProp | epoch: 015 | loss: 0.52187 - acc: 0.7947 -- iter: 096/166
[A[ATraining Step: 88  | total loss: [1m[32m0.51597[0m[0m | time: 31.447s
[2K
| RMSProp | epoch: 015 | loss: 0.51597 - acc: 0.8028 -- iter: 128/166
[A[ATraining Step: 89  | total loss: [1m[32m0.51321[0m[0m | time: 39.173s
[2K
| RMSProp | epoch: 015 | loss: 0.51321 - acc: 0.8069 -- iter: 160/166
[A[ATraining Step: 90  | total loss: [1m[32m0.50014[0m[0m | time: 43.769s
[2K
| RMSProp | epoch: 015 | loss: 0.50014 - acc: 0.8199 | val_loss: 0.55887 - val_acc: 0.6792 -- iter: 166/166
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.9550724637681158
Validation AUPRC:0.9480562588625585
Test AUC:0.9362318840579711
Test AUPRC:0.9605232967581063
BestTestF1Score	0.89	0.78	0.89	0.96	0.83	25	1	22	5	0.67
BestTestMCCScore	0.89	0.78	0.89	0.96	0.83	25	1	22	5	0.67
BestTestAccuracyScore	0.89	0.78	0.89	0.96	0.83	25	1	22	5	0.67
BestValidationF1Score	0.86	0.77	0.89	0.9	0.83	19	2	28	4	0.67
BestValidationMCC	0.86	0.77	0.89	0.9	0.83	19	2	28	4	0.67
BestValidationAccuracy	0.86	0.77	0.89	0.9	0.83	19	2	28	4	0.67
TestPredictions (Threshold:0.67)
CHEMBL253460,TP,ACT,0.8100000023841858	CHEMBL1077203,FN,ACT,0.550000011920929	CHEMBL434567,TP,ACT,0.8100000023841858	CHEMBL251823,TP,ACT,0.8500000238418579	CHEMBL2437169,TN,INACT,0.5600000023841858	CHEMBL237442,TN,INACT,0.3400000035762787	CHEMBL399351,TP,ACT,0.7099999785423279	CHEMBL235944,TN,INACT,0.49000000953674316	CHEMBL418639,TN,INACT,0.6600000262260437	CHEMBL491643,TP,ACT,0.75	CHEMBL568346,TP,ACT,0.7699999809265137	CHEMBL402954,TP,ACT,0.800000011920929	CHEMBL267455,TN,INACT,0.5799999833106995	CHEMBL1935291,TN,INACT,0.5600000023841858	CHEMBL1269961,TN,INACT,0.36000001430511475	CHEMBL3793635,TN,INACT,0.6000000238418579	CHEMBL394724,TN,INACT,0.5199999809265137	CHEMBL375418,TP,ACT,0.75	CHEMBL1093690,TP,ACT,0.7300000190734863	CHEMBL254715,TP,ACT,0.6800000071525574	CHEMBL295871,TN,INACT,0.6399999856948853	CHEMBL3794343,TN,INACT,0.6499999761581421	CHEMBL496018,TP,ACT,0.8500000238418579	CHEMBL585299,TP,ACT,0.8399999737739563	CHEMBL403185,FN,ACT,0.6000000238418579	CHEMBL403022,TP,ACT,0.8600000143051147	CHEMBL442432,TP,ACT,0.7699999809265137	CHEMBL398743,TP,ACT,0.8600000143051147	CHEMBL235914,TN,INACT,0.2199999988079071	CHEMBL243874,TN,INACT,0.4699999988079071	CHEMBL403380,TP,ACT,0.800000011920929	CHEMBL403161,TP,ACT,0.6800000071525574	CHEMBL252612,TP,ACT,0.7599999904632568	CHEMBL1807402,TN,INACT,0.6000000238418579	CHEMBL590343,TP,ACT,0.6700000166893005	CHEMBL3793385,TN,INACT,0.6299999952316284	CHEMBL1287851,FN,ACT,0.5400000214576721	CHEMBL404132,TP,ACT,0.8100000023841858	CHEMBL327254,TN,INACT,0.5899999737739563	CHEMBL572045,FN,ACT,0.6600000262260437	CHEMBL235681,TN,INACT,0.3400000035762787	CHEMBL372165,TN,INACT,0.2199999988079071	CHEMBL496829,TP,ACT,0.75	CHEMBL3593359,FP,INACT,0.7300000190734863	CHEMBL496212,FN,ACT,0.6399999856948853	CHEMBL149155,TN,INACT,0.5099999904632568	CHEMBL3104658,TN,INACT,0.6000000238418579	CHEMBL569166,TP,ACT,0.8500000238418579	CHEMBL1939844,TN,INACT,0.5799999833106995	CHEMBL252442,TP,ACT,0.7300000190734863	CHEMBL446457,TP,ACT,0.8799999952316284	CHEMBL33413,TN,INACT,0.5699999928474426	CHEMBL496825,TP,ACT,0.8999999761581421	

