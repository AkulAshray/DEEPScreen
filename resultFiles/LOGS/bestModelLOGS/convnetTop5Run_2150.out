CNNModel CHEMBL4892 adam 0.0005 15 128 0 0.6 False True
Number of active compounds :	218
Number of inactive compounds :	218
---------------------------------
Run id: CNNModel_CHEMBL4892_adam_0.0005_15_128_0_0.6_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL4892_adam_0.0005_15_128_0.6_True/
---------------------------------
Training samples: 275
Validation samples: 86
--
Training Step: 1  | time: 4.409s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/275
[A[ATraining Step: 2  | total loss: [1m[32m0.62379[0m[0m | time: 5.326s
[2K
| Adam | epoch: 001 | loss: 0.62379 - acc: 0.5344 -- iter: 064/275
[A[ATraining Step: 3  | total loss: [1m[32m0.68071[0m[0m | time: 6.252s
[2K
| Adam | epoch: 001 | loss: 0.68071 - acc: 0.5063 -- iter: 096/275
[A[ATraining Step: 4  | total loss: [1m[32m0.68780[0m[0m | time: 7.164s
[2K
| Adam | epoch: 001 | loss: 0.68780 - acc: 0.6187 -- iter: 128/275
[A[ATraining Step: 5  | total loss: [1m[32m0.68768[0m[0m | time: 8.251s
[2K
| Adam | epoch: 001 | loss: 0.68768 - acc: 0.6231 -- iter: 160/275
[A[ATraining Step: 6  | total loss: [1m[32m0.68771[0m[0m | time: 9.443s
[2K
| Adam | epoch: 001 | loss: 0.68771 - acc: 0.5841 -- iter: 192/275
[A[ATraining Step: 7  | total loss: [1m[32m0.68749[0m[0m | time: 10.282s
[2K
| Adam | epoch: 001 | loss: 0.68749 - acc: 0.5712 -- iter: 224/275
[A[ATraining Step: 8  | total loss: [1m[32m0.69939[0m[0m | time: 11.419s
[2K
| Adam | epoch: 001 | loss: 0.69939 - acc: 0.5311 -- iter: 256/275
[A[ATraining Step: 9  | total loss: [1m[32m0.70096[0m[0m | time: 13.321s
[2K
| Adam | epoch: 001 | loss: 0.70096 - acc: 0.5146 | val_loss: 0.71103 - val_acc: 0.4535 -- iter: 275/275
--
Training Step: 10  | total loss: [1m[32m0.66514[0m[0m | time: 5.044s
[2K
| Adam | epoch: 002 | loss: 0.66514 - acc: 0.6784 -- iter: 032/275
[A[ATraining Step: 11  | total loss: [1m[32m0.64294[0m[0m | time: 5.910s
[2K
| Adam | epoch: 002 | loss: 0.64294 - acc: 0.7559 -- iter: 064/275
[A[ATraining Step: 12  | total loss: [1m[32m0.66194[0m[0m | time: 6.809s
[2K
| Adam | epoch: 002 | loss: 0.66194 - acc: 0.6689 -- iter: 096/275
[A[ATraining Step: 13  | total loss: [1m[32m0.66200[0m[0m | time: 7.725s
[2K
| Adam | epoch: 002 | loss: 0.66200 - acc: 0.6501 -- iter: 128/275
[A[ATraining Step: 14  | total loss: [1m[32m0.68970[0m[0m | time: 8.632s
[2K
| Adam | epoch: 002 | loss: 0.68970 - acc: 0.5887 -- iter: 160/275
[A[ATraining Step: 15  | total loss: [1m[32m0.69298[0m[0m | time: 9.622s
[2K
| Adam | epoch: 002 | loss: 0.69298 - acc: 0.5784 -- iter: 192/275
[A[ATraining Step: 16  | total loss: [1m[32m0.71083[0m[0m | time: 10.610s
[2K
| Adam | epoch: 002 | loss: 0.71083 - acc: 0.5373 -- iter: 224/275
[A[ATraining Step: 17  | total loss: [1m[32m0.69789[0m[0m | time: 11.484s
[2K
| Adam | epoch: 002 | loss: 0.69789 - acc: 0.5576 -- iter: 256/275
[A[ATraining Step: 18  | total loss: [1m[32m0.69011[0m[0m | time: 13.505s
[2K
| Adam | epoch: 002 | loss: 0.69011 - acc: 0.5701 | val_loss: 0.70983 - val_acc: 0.4535 -- iter: 275/275
--
Training Step: 19  | total loss: [1m[32m0.69410[0m[0m | time: 2.449s
[2K
| Adam | epoch: 003 | loss: 0.69410 - acc: 0.5468 -- iter: 032/275
[A[ATraining Step: 20  | total loss: [1m[32m0.69392[0m[0m | time: 2.991s
[2K
| Adam | epoch: 003 | loss: 0.69392 - acc: 0.5402 -- iter: 064/275
[A[ATraining Step: 21  | total loss: [1m[32m0.69316[0m[0m | time: 3.916s
[2K
| Adam | epoch: 003 | loss: 0.69316 - acc: 0.5359 -- iter: 096/275
[A[ATraining Step: 22  | total loss: [1m[32m0.68921[0m[0m | time: 4.869s
[2K
| Adam | epoch: 003 | loss: 0.68921 - acc: 0.5532 -- iter: 128/275
[A[ATraining Step: 23  | total loss: [1m[32m0.68966[0m[0m | time: 5.836s
[2K
| Adam | epoch: 003 | loss: 0.68966 - acc: 0.5469 -- iter: 160/275
[A[ATraining Step: 24  | total loss: [1m[32m0.69089[0m[0m | time: 6.861s
[2K
| Adam | epoch: 003 | loss: 0.69089 - acc: 0.5337 -- iter: 192/275
[A[ATraining Step: 25  | total loss: [1m[32m0.68661[0m[0m | time: 7.891s
[2K
| Adam | epoch: 003 | loss: 0.68661 - acc: 0.5756 -- iter: 224/275
[A[ATraining Step: 26  | total loss: [1m[32m0.68533[0m[0m | time: 8.785s
[2K
| Adam | epoch: 003 | loss: 0.68533 - acc: 0.5887 -- iter: 256/275
[A[ATraining Step: 27  | total loss: [1m[32m0.68835[0m[0m | time: 10.860s
[2K
| Adam | epoch: 003 | loss: 0.68835 - acc: 0.5579 | val_loss: 0.69902 - val_acc: 0.4535 -- iter: 275/275
--
Training Step: 28  | total loss: [1m[32m0.68916[0m[0m | time: 0.797s
[2K
| Adam | epoch: 004 | loss: 0.68916 - acc: 0.5512 -- iter: 032/275
[A[ATraining Step: 29  | total loss: [1m[32m0.68806[0m[0m | time: 1.363s
[2K
| Adam | epoch: 004 | loss: 0.68806 - acc: 0.5616 -- iter: 064/275
[A[ATraining Step: 30  | total loss: [1m[32m0.69129[0m[0m | time: 1.985s
[2K
| Adam | epoch: 004 | loss: 0.69129 - acc: 0.5283 -- iter: 096/275
[A[ATraining Step: 31  | total loss: [1m[32m0.69362[0m[0m | time: 2.893s
[2K
| Adam | epoch: 004 | loss: 0.69362 - acc: 0.5035 -- iter: 128/275
[A[ATraining Step: 32  | total loss: [1m[32m0.69362[0m[0m | time: 3.838s
[2K
| Adam | epoch: 004 | loss: 0.69362 - acc: 0.5027 -- iter: 160/275
[A[ATraining Step: 33  | total loss: [1m[32m0.69124[0m[0m | time: 4.894s
[2K
| Adam | epoch: 004 | loss: 0.69124 - acc: 0.5296 -- iter: 192/275
[A[ATraining Step: 34  | total loss: [1m[32m0.69126[0m[0m | time: 5.929s
[2K
| Adam | epoch: 004 | loss: 0.69126 - acc: 0.5299 -- iter: 224/275
[A[ATraining Step: 35  | total loss: [1m[32m0.69074[0m[0m | time: 6.796s
[2K
| Adam | epoch: 004 | loss: 0.69074 - acc: 0.5368 -- iter: 256/275
[A[ATraining Step: 36  | total loss: [1m[32m0.68901[0m[0m | time: 8.856s
[2K
| Adam | epoch: 004 | loss: 0.68901 - acc: 0.5548 | val_loss: 0.69845 - val_acc: 0.4535 -- iter: 275/275
--
Training Step: 37  | total loss: [1m[32m0.68826[0m[0m | time: 0.894s
[2K
| Adam | epoch: 005 | loss: 0.68826 - acc: 0.5626 -- iter: 032/275
[A[ATraining Step: 38  | total loss: [1m[32m0.68986[0m[0m | time: 1.840s
[2K
| Adam | epoch: 005 | loss: 0.68986 - acc: 0.5442 -- iter: 064/275
[A[ATraining Step: 39  | total loss: [1m[32m0.69179[0m[0m | time: 2.461s
[2K
| Adam | epoch: 005 | loss: 0.69179 - acc: 0.5238 -- iter: 096/275
[A[ATraining Step: 40  | total loss: [1m[32m0.68984[0m[0m | time: 3.104s
[2K
| Adam | epoch: 005 | loss: 0.68984 - acc: 0.5440 -- iter: 128/275
[A[ATraining Step: 41  | total loss: [1m[32m0.68800[0m[0m | time: 4.116s
[2K
| Adam | epoch: 005 | loss: 0.68800 - acc: 0.5601 -- iter: 160/275
[A[ATraining Step: 42  | total loss: [1m[32m0.68669[0m[0m | time: 4.967s
[2K
| Adam | epoch: 005 | loss: 0.68669 - acc: 0.5718 -- iter: 192/275
[A[ATraining Step: 43  | total loss: [1m[32m0.68467[0m[0m | time: 5.975s
[2K
| Adam | epoch: 005 | loss: 0.68467 - acc: 0.5867 -- iter: 224/275
[A[ATraining Step: 44  | total loss: [1m[32m0.68343[0m[0m | time: 7.306s
[2K
| Adam | epoch: 005 | loss: 0.68343 - acc: 0.5933 -- iter: 256/275
[A[ATraining Step: 45  | total loss: [1m[32m0.68361[0m[0m | time: 41.072s
[2K
| Adam | epoch: 005 | loss: 0.68361 - acc: 0.5881 | val_loss: 0.70718 - val_acc: 0.4535 -- iter: 275/275
--
Training Step: 46  | total loss: [1m[32m0.68880[0m[0m | time: 0.933s
[2K
| Adam | epoch: 006 | loss: 0.68880 - acc: 0.5578 -- iter: 032/275
[A[ATraining Step: 47  | total loss: [1m[32m0.68503[0m[0m | time: 1.988s
[2K
| Adam | epoch: 006 | loss: 0.68503 - acc: 0.5739 -- iter: 064/275
[A[ATraining Step: 48  | total loss: [1m[32m0.69180[0m[0m | time: 2.997s
[2K
| Adam | epoch: 006 | loss: 0.69180 - acc: 0.5419 -- iter: 096/275
[A[ATraining Step: 49  | total loss: [1m[32m0.68849[0m[0m | time: 3.574s
[2K
| Adam | epoch: 006 | loss: 0.68849 - acc: 0.5550 -- iter: 128/275
[A[ATraining Step: 50  | total loss: [1m[32m0.68547[0m[0m | time: 4.196s
[2K
| Adam | epoch: 006 | loss: 0.68547 - acc: 0.5669 -- iter: 160/275
[A[ATraining Step: 51  | total loss: [1m[32m0.68264[0m[0m | time: 5.346s
[2K
| Adam | epoch: 006 | loss: 0.68264 - acc: 0.5768 -- iter: 192/275
[A[ATraining Step: 52  | total loss: [1m[32m0.67893[0m[0m | time: 6.649s
[2K
| Adam | epoch: 006 | loss: 0.67893 - acc: 0.5887 -- iter: 224/275
[A[ATraining Step: 53  | total loss: [1m[32m0.67949[0m[0m | time: 11.906s
[2K
| Adam | epoch: 006 | loss: 0.67949 - acc: 0.5848 -- iter: 256/275
[A[ATraining Step: 54  | total loss: [1m[32m0.68507[0m[0m | time: 13.738s
[2K
| Adam | epoch: 006 | loss: 0.68507 - acc: 0.5680 | val_loss: 0.72382 - val_acc: 0.4535 -- iter: 275/275
--
Training Step: 55  | total loss: [1m[32m0.68500[0m[0m | time: 0.922s
[2K
| Adam | epoch: 007 | loss: 0.68500 - acc: 0.5672 -- iter: 032/275
[A[ATraining Step: 56  | total loss: [1m[32m0.68200[0m[0m | time: 2.063s
[2K
| Adam | epoch: 007 | loss: 0.68200 - acc: 0.5753 -- iter: 064/275
[A[ATraining Step: 57  | total loss: [1m[32m0.68217[0m[0m | time: 3.341s
[2K
| Adam | epoch: 007 | loss: 0.68217 - acc: 0.5736 -- iter: 096/275
[A[ATraining Step: 58  | total loss: [1m[32m0.67810[0m[0m | time: 5.717s
[2K
| Adam | epoch: 007 | loss: 0.67810 - acc: 0.5848 -- iter: 128/275
[A[ATraining Step: 59  | total loss: [1m[32m0.68233[0m[0m | time: 8.881s
[2K
| Adam | epoch: 007 | loss: 0.68233 - acc: 0.5734 -- iter: 160/275
[A[ATraining Step: 60  | total loss: [1m[32m0.67376[0m[0m | time: 9.471s
[2K
| Adam | epoch: 007 | loss: 0.67376 - acc: 0.5951 -- iter: 192/275
[A[ATraining Step: 61  | total loss: [1m[32m0.66562[0m[0m | time: 10.393s
[2K
| Adam | epoch: 007 | loss: 0.66562 - acc: 0.6136 -- iter: 224/275
[A[ATraining Step: 62  | total loss: [1m[32m0.66620[0m[0m | time: 11.336s
[2K
| Adam | epoch: 007 | loss: 0.66620 - acc: 0.6110 -- iter: 256/275
[A[ATraining Step: 63  | total loss: [1m[32m0.68107[0m[0m | time: 13.227s
[2K
| Adam | epoch: 007 | loss: 0.68107 - acc: 0.5851 | val_loss: 0.73621 - val_acc: 0.4535 -- iter: 275/275
--
Training Step: 64  | total loss: [1m[32m0.68495[0m[0m | time: 6.974s
[2K
| Adam | epoch: 008 | loss: 0.68495 - acc: 0.5783 -- iter: 032/275
[A[ATraining Step: 65  | total loss: [1m[32m0.68345[0m[0m | time: 7.940s
[2K
| Adam | epoch: 008 | loss: 0.68345 - acc: 0.5802 -- iter: 064/275
[A[ATraining Step: 66  | total loss: [1m[32m0.68446[0m[0m | time: 8.913s
[2K
| Adam | epoch: 008 | loss: 0.68446 - acc: 0.5743 -- iter: 096/275
[A[ATraining Step: 67  | total loss: [1m[32m0.68392[0m[0m | time: 9.816s
[2K
| Adam | epoch: 008 | loss: 0.68392 - acc: 0.5729 -- iter: 128/275
[A[ATraining Step: 68  | total loss: [1m[32m0.68276[0m[0m | time: 10.788s
[2K
| Adam | epoch: 008 | loss: 0.68276 - acc: 0.5753 -- iter: 160/275
[A[ATraining Step: 69  | total loss: [1m[32m0.68422[0m[0m | time: 11.439s
[2K
| Adam | epoch: 008 | loss: 0.68422 - acc: 0.5665 -- iter: 192/275
[A[ATraining Step: 70  | total loss: [1m[32m0.68348[0m[0m | time: 12.067s
[2K
| Adam | epoch: 008 | loss: 0.68348 - acc: 0.5680 -- iter: 224/275
[A[ATraining Step: 71  | total loss: [1m[32m0.68286[0m[0m | time: 12.861s
[2K
| Adam | epoch: 008 | loss: 0.68286 - acc: 0.5692 -- iter: 256/275
[A[ATraining Step: 72  | total loss: [1m[32m0.68389[0m[0m | time: 14.860s
[2K
| Adam | epoch: 008 | loss: 0.68389 - acc: 0.5614 | val_loss: 0.69912 - val_acc: 0.4535 -- iter: 275/275
--
Training Step: 73  | total loss: [1m[32m0.68307[0m[0m | time: 1.791s
[2K
| Adam | epoch: 009 | loss: 0.68307 - acc: 0.5650 -- iter: 032/275
[A[ATraining Step: 74  | total loss: [1m[32m0.68176[0m[0m | time: 2.750s
[2K
| Adam | epoch: 009 | loss: 0.68176 - acc: 0.5750 -- iter: 064/275
[A[ATraining Step: 75  | total loss: [1m[32m0.68201[0m[0m | time: 3.687s
[2K
| Adam | epoch: 009 | loss: 0.68201 - acc: 0.5703 -- iter: 096/275
[A[ATraining Step: 76  | total loss: [1m[32m0.68171[0m[0m | time: 4.627s
[2K
| Adam | epoch: 009 | loss: 0.68171 - acc: 0.5694 -- iter: 128/275
[A[ATraining Step: 77  | total loss: [1m[32m0.68216[0m[0m | time: 5.667s
[2K
| Adam | epoch: 009 | loss: 0.68216 - acc: 0.5654 -- iter: 160/275
[A[ATraining Step: 78  | total loss: [1m[32m0.68047[0m[0m | time: 6.757s
[2K
| Adam | epoch: 009 | loss: 0.68047 - acc: 0.5749 -- iter: 192/275
[A[ATraining Step: 79  | total loss: [1m[32m0.68000[0m[0m | time: 7.232s
[2K
| Adam | epoch: 009 | loss: 0.68000 - acc: 0.5736 -- iter: 224/275
[A[ATraining Step: 80  | total loss: [1m[32m0.68335[0m[0m | time: 7.812s
[2K
| Adam | epoch: 009 | loss: 0.68335 - acc: 0.5580 -- iter: 256/275
[A[ATraining Step: 81  | total loss: [1m[32m0.68609[0m[0m | time: 9.817s
[2K
| Adam | epoch: 009 | loss: 0.68609 - acc: 0.5442 | val_loss: 0.70028 - val_acc: 0.4535 -- iter: 275/275
--
Training Step: 82  | total loss: [1m[32m0.68406[0m[0m | time: 1.005s
[2K
| Adam | epoch: 010 | loss: 0.68406 - acc: 0.5523 -- iter: 032/275
[A[ATraining Step: 83  | total loss: [1m[32m0.68263[0m[0m | time: 2.074s
[2K
| Adam | epoch: 010 | loss: 0.68263 - acc: 0.5564 -- iter: 064/275
[A[ATraining Step: 84  | total loss: [1m[32m0.68241[0m[0m | time: 3.106s
[2K
| Adam | epoch: 010 | loss: 0.68241 - acc: 0.5570 -- iter: 096/275
[A[ATraining Step: 85  | total loss: [1m[32m0.68282[0m[0m | time: 4.094s
[2K
| Adam | epoch: 010 | loss: 0.68282 - acc: 0.5513 -- iter: 128/275
[A[ATraining Step: 86  | total loss: [1m[32m0.68314[0m[0m | time: 5.114s
[2K
| Adam | epoch: 010 | loss: 0.68314 - acc: 0.5431 -- iter: 160/275
[A[ATraining Step: 87  | total loss: [1m[32m0.68026[0m[0m | time: 6.149s
[2K
| Adam | epoch: 010 | loss: 0.68026 - acc: 0.5481 -- iter: 192/275
[A[ATraining Step: 88  | total loss: [1m[32m0.67456[0m[0m | time: 7.160s
[2K
| Adam | epoch: 010 | loss: 0.67456 - acc: 0.5652 -- iter: 224/275
[A[ATraining Step: 89  | total loss: [1m[32m0.67273[0m[0m | time: 7.716s
[2K
| Adam | epoch: 010 | loss: 0.67273 - acc: 0.5680 -- iter: 256/275
[A[ATraining Step: 90  | total loss: [1m[32m0.67920[0m[0m | time: 9.171s
[2K
| Adam | epoch: 010 | loss: 0.67920 - acc: 0.5533 | val_loss: 0.69282 - val_acc: 0.4535 -- iter: 275/275
--
Training Step: 91  | total loss: [1m[32m0.68440[0m[0m | time: 0.625s
[2K
| Adam | epoch: 011 | loss: 0.68440 - acc: 0.5401 -- iter: 032/275
[A[ATraining Step: 92  | total loss: [1m[32m0.68481[0m[0m | time: 1.234s
[2K
| Adam | epoch: 011 | loss: 0.68481 - acc: 0.5299 -- iter: 064/275
[A[ATraining Step: 93  | total loss: [1m[32m0.68287[0m[0m | time: 1.835s
[2K
| Adam | epoch: 011 | loss: 0.68287 - acc: 0.5331 -- iter: 096/275
[A[ATraining Step: 94  | total loss: [1m[32m0.68018[0m[0m | time: 2.454s
[2K
| Adam | epoch: 011 | loss: 0.68018 - acc: 0.5454 -- iter: 128/275
[A[ATraining Step: 95  | total loss: [1m[32m0.67908[0m[0m | time: 3.093s
[2K
| Adam | epoch: 011 | loss: 0.67908 - acc: 0.5440 -- iter: 160/275
[A[ATraining Step: 96  | total loss: [1m[32m0.67353[0m[0m | time: 3.713s
[2K
| Adam | epoch: 011 | loss: 0.67353 - acc: 0.5490 -- iter: 192/275
[A[ATraining Step: 97  | total loss: [1m[32m0.67289[0m[0m | time: 4.366s
[2K
| Adam | epoch: 011 | loss: 0.67289 - acc: 0.5441 -- iter: 224/275
[A[ATraining Step: 98  | total loss: [1m[32m0.67100[0m[0m | time: 4.967s
[2K
| Adam | epoch: 011 | loss: 0.67100 - acc: 0.5428 -- iter: 256/275
[A[ATraining Step: 99  | total loss: [1m[32m0.66648[0m[0m | time: 6.343s
[2K
| Adam | epoch: 011 | loss: 0.66648 - acc: 0.5510 | val_loss: 0.66606 - val_acc: 0.5581 -- iter: 275/275
--
Training Step: 100  | total loss: [1m[32m0.66691[0m[0m | time: 0.387s
[2K
| Adam | epoch: 012 | loss: 0.66691 - acc: 0.5380 -- iter: 032/275
[A[ATraining Step: 101  | total loss: [1m[32m0.66402[0m[0m | time: 0.998s
[2K
| Adam | epoch: 012 | loss: 0.66402 - acc: 0.5474 -- iter: 064/275
[A[ATraining Step: 102  | total loss: [1m[32m0.65861[0m[0m | time: 1.612s
[2K
| Adam | epoch: 012 | loss: 0.65861 - acc: 0.5708 -- iter: 096/275
[A[ATraining Step: 103  | total loss: [1m[32m0.65586[0m[0m | time: 2.256s
[2K
| Adam | epoch: 012 | loss: 0.65586 - acc: 0.5918 -- iter: 128/275
[A[ATraining Step: 104  | total loss: [1m[32m0.64981[0m[0m | time: 2.860s
[2K
| Adam | epoch: 012 | loss: 0.64981 - acc: 0.6014 -- iter: 160/275
[A[ATraining Step: 105  | total loss: [1m[32m0.64273[0m[0m | time: 3.472s
[2K
| Adam | epoch: 012 | loss: 0.64273 - acc: 0.6131 -- iter: 192/275
[A[ATraining Step: 106  | total loss: [1m[32m0.62874[0m[0m | time: 4.082s
[2K
| Adam | epoch: 012 | loss: 0.62874 - acc: 0.6299 -- iter: 224/275
[A[ATraining Step: 107  | total loss: [1m[32m0.61662[0m[0m | time: 4.702s
[2K
| Adam | epoch: 012 | loss: 0.61662 - acc: 0.6451 -- iter: 256/275
[A[ATraining Step: 108  | total loss: [1m[32m0.62565[0m[0m | time: 6.331s
[2K
| Adam | epoch: 012 | loss: 0.62565 - acc: 0.6399 | val_loss: 0.56751 - val_acc: 0.6977 -- iter: 275/275
--
Training Step: 109  | total loss: [1m[32m0.61090[0m[0m | time: 0.592s
[2K
| Adam | epoch: 013 | loss: 0.61090 - acc: 0.6541 -- iter: 032/275
[A[ATraining Step: 110  | total loss: [1m[32m0.60454[0m[0m | time: 1.229s
[2K
| Adam | epoch: 013 | loss: 0.60454 - acc: 0.6676 -- iter: 064/275
[A[ATraining Step: 111  | total loss: [1m[32m0.60097[0m[0m | time: 2.473s
[2K
| Adam | epoch: 013 | loss: 0.60097 - acc: 0.6693 -- iter: 096/275
[A[ATraining Step: 112  | total loss: [1m[32m0.58894[0m[0m | time: 3.824s
[2K
| Adam | epoch: 013 | loss: 0.58894 - acc: 0.6867 -- iter: 128/275
[A[ATraining Step: 113  | total loss: [1m[32m0.56733[0m[0m | time: 5.568s
[2K
| Adam | epoch: 013 | loss: 0.56733 - acc: 0.7055 -- iter: 160/275
[A[ATraining Step: 114  | total loss: [1m[32m0.57639[0m[0m | time: 6.453s
[2K
| Adam | epoch: 013 | loss: 0.57639 - acc: 0.6944 -- iter: 192/275
[A[ATraining Step: 115  | total loss: [1m[32m0.55671[0m[0m | time: 7.483s
[2K
| Adam | epoch: 013 | loss: 0.55671 - acc: 0.7124 -- iter: 224/275
[A[ATraining Step: 116  | total loss: [1m[32m0.53991[0m[0m | time: 8.493s
[2K
| Adam | epoch: 013 | loss: 0.53991 - acc: 0.7224 -- iter: 256/275
[A[ATraining Step: 117  | total loss: [1m[32m0.53168[0m[0m | time: 10.490s
[2K
| Adam | epoch: 013 | loss: 0.53168 - acc: 0.7346 | val_loss: 0.53260 - val_acc: 0.6744 -- iter: 275/275
--
Training Step: 118  | total loss: [1m[32m0.52587[0m[0m | time: 0.944s
[2K
| Adam | epoch: 014 | loss: 0.52587 - acc: 0.7424 -- iter: 032/275
[A[ATraining Step: 119  | total loss: [1m[32m0.52353[0m[0m | time: 1.472s
[2K
| Adam | epoch: 014 | loss: 0.52353 - acc: 0.7369 -- iter: 064/275
[A[ATraining Step: 120  | total loss: [1m[32m0.50103[0m[0m | time: 2.052s
[2K
| Adam | epoch: 014 | loss: 0.50103 - acc: 0.7474 -- iter: 096/275
[A[ATraining Step: 121  | total loss: [1m[32m0.47945[0m[0m | time: 2.993s
[2K
| Adam | epoch: 014 | loss: 0.47945 - acc: 0.7621 -- iter: 128/275
[A[ATraining Step: 122  | total loss: [1m[32m0.53409[0m[0m | time: 3.896s
[2K
| Adam | epoch: 014 | loss: 0.53409 - acc: 0.7422 -- iter: 160/275
[A[ATraining Step: 123  | total loss: [1m[32m0.55162[0m[0m | time: 4.883s
[2K
| Adam | epoch: 014 | loss: 0.55162 - acc: 0.7273 -- iter: 192/275
[A[ATraining Step: 124  | total loss: [1m[32m0.53412[0m[0m | time: 5.862s
[2K
| Adam | epoch: 014 | loss: 0.53412 - acc: 0.7327 -- iter: 224/275
[A[ATraining Step: 125  | total loss: [1m[32m0.51676[0m[0m | time: 6.804s
[2K
| Adam | epoch: 014 | loss: 0.51676 - acc: 0.7469 -- iter: 256/275
[A[ATraining Step: 126  | total loss: [1m[32m0.51934[0m[0m | time: 8.750s
[2K
| Adam | epoch: 014 | loss: 0.51934 - acc: 0.7410 | val_loss: 0.52430 - val_acc: 0.7326 -- iter: 275/275
--
Training Step: 127  | total loss: [1m[32m0.52705[0m[0m | time: 0.917s
[2K
| Adam | epoch: 015 | loss: 0.52705 - acc: 0.7325 -- iter: 032/275
[A[ATraining Step: 128  | total loss: [1m[32m0.52081[0m[0m | time: 1.885s
[2K
| Adam | epoch: 015 | loss: 0.52081 - acc: 0.7405 -- iter: 064/275
[A[ATraining Step: 129  | total loss: [1m[32m0.51018[0m[0m | time: 2.704s
[2K
| Adam | epoch: 015 | loss: 0.51018 - acc: 0.7477 -- iter: 096/275
[A[ATraining Step: 130  | total loss: [1m[32m0.49090[0m[0m | time: 3.578s
[2K
| Adam | epoch: 015 | loss: 0.49090 - acc: 0.7677 -- iter: 128/275
[A[ATraining Step: 131  | total loss: [1m[32m0.47170[0m[0m | time: 4.733s
[2K
| Adam | epoch: 015 | loss: 0.47170 - acc: 0.7804 -- iter: 160/275
[A[ATraining Step: 132  | total loss: [1m[32m0.45366[0m[0m | time: 6.270s
[2K
| Adam | epoch: 015 | loss: 0.45366 - acc: 0.7961 -- iter: 192/275
[A[ATraining Step: 133  | total loss: [1m[32m0.43719[0m[0m | time: 7.201s
[2K
| Adam | epoch: 015 | loss: 0.43719 - acc: 0.8071 -- iter: 224/275
[A[ATraining Step: 134  | total loss: [1m[32m0.43173[0m[0m | time: 8.171s
[2K
| Adam | epoch: 015 | loss: 0.43173 - acc: 0.8077 -- iter: 256/275
[A[ATraining Step: 135  | total loss: [1m[32m0.41363[0m[0m | time: 10.137s
[2K
| Adam | epoch: 015 | loss: 0.41363 - acc: 0.8175 | val_loss: 0.52271 - val_acc: 0.7442 -- iter: 275/275
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.8308783415166394
Validation AUPRC:0.8171990834777019
Test AUC:0.8196078431372549
Test AUPRC:0.7835831572592803
BestTestF1Score	0.72	0.49	0.72	0.61	0.89	31	20	31	4	0.21
BestTestMCCScore	0.73	0.59	0.8	0.82	0.66	23	5	46	12	0.62
BestTestAccuracyScore	0.73	0.59	0.8	0.82	0.66	23	5	46	12	0.62
BestValidationF1Score	0.76	0.54	0.76	0.68	0.87	34	16	31	5	0.21
BestValidationMCC	0.72	0.56	0.78	0.86	0.62	24	4	43	15	0.62
BestValidationAccuracy	0.72	0.56	0.78	0.86	0.62	24	4	43	15	0.62
TestPredictions (Threshold:0.62)
CHEMBL1204345,FN,ACT,0.4300000071525574	CHEMBL424214,TN,INACT,0.05999999865889549	CHEMBL109783,FN,ACT,0.23000000417232513	CHEMBL285010,FP,INACT,0.9200000166893005	CHEMBL26522,TN,INACT,0.11999999731779099	CHEMBL545363,TN,INACT,0.23000000417232513	CHEMBL416747,TP,ACT,0.8500000238418579	CHEMBL421523,TN,INACT,0.10999999940395355	CHEMBL321644,TN,INACT,0.3499999940395355	CHEMBL13754,TP,ACT,0.9300000071525574	CHEMBL2325928,TP,ACT,0.7099999785423279	CHEMBL595022,TN,INACT,0.15000000596046448	CHEMBL48120,TN,INACT,0.10000000149011612	CHEMBL2111789,TN,INACT,0.07999999821186066	CHEMBL283320,TN,INACT,0.05000000074505806	CHEMBL594802,TN,INACT,0.12999999523162842	CHEMBL78080,TN,INACT,0.10000000149011612	CHEMBL3780633,TN,INACT,0.11999999731779099	CHEMBL133451,FN,ACT,0.2199999988079071	CHEMBL1259217,TP,ACT,0.9599999785423279	CHEMBL1258987,TP,ACT,0.9100000262260437	CHEMBL45665,FP,INACT,0.8199999928474426	CHEMBL112877,TN,INACT,0.07000000029802322	CHEMBL59823,TP,ACT,0.8399999737739563	CHEMBL148967,TN,INACT,0.5199999809265137	CHEMBL52785,FN,ACT,0.49000000953674316	CHEMBL1259113,TP,ACT,0.699999988079071	CHEMBL173708,TN,INACT,0.12999999523162842	CHEMBL3633650,TN,INACT,0.1599999964237213	CHEMBL217002,TN,INACT,0.07999999821186066	CHEMBL352779,TN,INACT,0.07000000029802322	CHEMBL64845,TP,ACT,0.7400000095367432	CHEMBL297215,TN,INACT,0.5400000214576721	CHEMBL329160,TP,ACT,0.8999999761581421	CHEMBL330674,TN,INACT,0.2199999988079071	CHEMBL227378,TN,INACT,0.10000000149011612	CHEMBL2326175,TP,ACT,0.7300000190734863	CHEMBL44463,TN,INACT,0.05999999865889549	CHEMBL42360,TN,INACT,0.07000000029802322	CHEMBL413040,TN,INACT,0.3799999952316284	CHEMBL48833,TP,ACT,0.9100000262260437	CHEMBL417358,FP,INACT,0.7599999904632568	CHEMBL18270,TP,ACT,0.8299999833106995	CHEMBL405355,FN,ACT,0.4099999964237213	CHEMBL2398752,TN,INACT,0.07000000029802322	CHEMBL322547,TN,INACT,0.07000000029802322	CHEMBL3633665,TN,INACT,0.5099999904632568	CHEMBL174463,TN,INACT,0.07000000029802322	CHEMBL11131,TN,INACT,0.12999999523162842	CHEMBL123099,TN,INACT,0.4699999988079071	CHEMBL279225,TN,INACT,0.550000011920929	CHEMBL1215,FN,ACT,0.07999999821186066	CHEMBL21328,TN,INACT,0.5299999713897705	CHEMBL318901,FN,ACT,0.20000000298023224	CHEMBL404557,TN,INACT,0.10999999940395355	CHEMBL336081,TN,INACT,0.07999999821186066	CHEMBL415879,TN,INACT,0.5299999713897705	CHEMBL461502,TN,INACT,0.07000000029802322	CHEMBL1259173,TP,ACT,0.949999988079071	CHEMBL1259006,TP,ACT,0.9900000095367432	CHEMBL6437,FN,ACT,0.10999999940395355	CHEMBL13735,TP,ACT,0.9100000262260437	CHEMBL1916635,TN,INACT,0.11999999731779099	CHEMBL48448,FP,INACT,0.6899999976158142	CHEMBL317333,FN,ACT,0.05999999865889549	CHEMBL162095,TN,INACT,0.07999999821186066	CHEMBL156283,TN,INACT,0.5600000023841858	CHEMBL78830,TN,INACT,0.11999999731779099	CHEMBL50188,FN,ACT,0.27000001072883606	CHEMBL553833,TP,ACT,0.8999999761581421	CHEMBL13903,TP,ACT,0.8799999952316284	CHEMBL43661,TN,INACT,0.5299999713897705	CHEMBL312935,TP,ACT,0.8700000047683716	CHEMBL611,FN,ACT,0.28999999165534973	CHEMBL191915,TN,INACT,0.4699999988079071	CHEMBL21508,TN,INACT,0.09000000357627869	CHEMBL12713,FN,ACT,0.23999999463558197	CHEMBL418499,TP,ACT,0.6299999952316284	CHEMBL51561,TP,ACT,0.9599999785423279	CHEMBL89916,TP,ACT,0.8299999833106995	CHEMBL51888,FP,INACT,0.8899999856948853	CHEMBL308243,TN,INACT,0.5	CHEMBL47404,TN,INACT,0.10000000149011612	CHEMBL64610,TP,ACT,0.6299999952316284	CHEMBL300735,TP,ACT,0.8399999737739563	CHEMBL322537,TN,INACT,0.05999999865889549	

