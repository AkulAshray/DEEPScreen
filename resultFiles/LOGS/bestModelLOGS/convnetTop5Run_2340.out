CNNModel CHEMBL215 adam 0.0005 15 32 0 0.8 False True
Number of active compounds :	501
Number of inactive compounds :	334
---------------------------------
Run id: CNNModel_CHEMBL215_adam_0.0005_15_32_0_0.8_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL215_adam_0.0005_15_32_0.8_True/
---------------------------------
Training samples: 532
Validation samples: 167
--
Training Step: 1  | time: 1.391s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/532
[A[ATraining Step: 2  | total loss: [1m[32m0.62386[0m[0m | time: 2.416s
[2K
| Adam | epoch: 001 | loss: 0.62386 - acc: 0.4500 -- iter: 064/532
[A[ATraining Step: 3  | total loss: [1m[32m0.67542[0m[0m | time: 3.423s
[2K
| Adam | epoch: 001 | loss: 0.67542 - acc: 0.6187 -- iter: 096/532
[A[ATraining Step: 4  | total loss: [1m[32m0.68191[0m[0m | time: 4.472s
[2K
| Adam | epoch: 001 | loss: 0.68191 - acc: 0.6000 -- iter: 128/532
[A[ATraining Step: 5  | total loss: [1m[32m0.69636[0m[0m | time: 5.501s
[2K
| Adam | epoch: 001 | loss: 0.69636 - acc: 0.5308 -- iter: 160/532
[A[ATraining Step: 6  | total loss: [1m[32m0.67378[0m[0m | time: 6.502s
[2K
| Adam | epoch: 001 | loss: 0.67378 - acc: 0.6114 -- iter: 192/532
[A[ATraining Step: 7  | total loss: [1m[32m0.65311[0m[0m | time: 7.495s
[2K
| Adam | epoch: 001 | loss: 0.65311 - acc: 0.6571 -- iter: 224/532
[A[ATraining Step: 8  | total loss: [1m[32m0.69001[0m[0m | time: 8.500s
[2K
| Adam | epoch: 001 | loss: 0.69001 - acc: 0.5863 -- iter: 256/532
[A[ATraining Step: 9  | total loss: [1m[32m0.72180[0m[0m | time: 9.559s
[2K
| Adam | epoch: 001 | loss: 0.72180 - acc: 0.5241 -- iter: 288/532
[A[ATraining Step: 10  | total loss: [1m[32m0.69132[0m[0m | time: 10.584s
[2K
| Adam | epoch: 001 | loss: 0.69132 - acc: 0.5745 -- iter: 320/532
[A[ATraining Step: 11  | total loss: [1m[32m0.70348[0m[0m | time: 11.667s
[2K
| Adam | epoch: 001 | loss: 0.70348 - acc: 0.5244 -- iter: 352/532
[A[ATraining Step: 12  | total loss: [1m[32m0.68821[0m[0m | time: 12.731s
[2K
| Adam | epoch: 001 | loss: 0.68821 - acc: 0.5697 -- iter: 384/532
[A[ATraining Step: 13  | total loss: [1m[32m0.67695[0m[0m | time: 13.751s
[2K
| Adam | epoch: 001 | loss: 0.67695 - acc: 0.6202 -- iter: 416/532
[A[ATraining Step: 14  | total loss: [1m[32m0.67640[0m[0m | time: 14.776s
[2K
| Adam | epoch: 001 | loss: 0.67640 - acc: 0.6221 -- iter: 448/532
[A[ATraining Step: 15  | total loss: [1m[32m0.67607[0m[0m | time: 15.877s
[2K
| Adam | epoch: 001 | loss: 0.67607 - acc: 0.6233 -- iter: 480/532
[A[ATraining Step: 16  | total loss: [1m[32m0.67424[0m[0m | time: 16.904s
[2K
| Adam | epoch: 001 | loss: 0.67424 - acc: 0.6356 -- iter: 512/532
[A[ATraining Step: 17  | total loss: [1m[32m0.66756[0m[0m | time: 18.665s
[2K
| Adam | epoch: 001 | loss: 0.66756 - acc: 0.6656 | val_loss: 0.67171 - val_acc: 0.6108 -- iter: 532/532
--
Training Step: 18  | total loss: [1m[32m0.66302[0m[0m | time: 0.713s
[2K
| Adam | epoch: 002 | loss: 0.66302 - acc: 0.6775 -- iter: 032/532
[A[ATraining Step: 19  | total loss: [1m[32m0.65811[0m[0m | time: 1.781s
[2K
| Adam | epoch: 002 | loss: 0.65811 - acc: 0.6850 -- iter: 064/532
[A[ATraining Step: 20  | total loss: [1m[32m0.65909[0m[0m | time: 2.830s
[2K
| Adam | epoch: 002 | loss: 0.65909 - acc: 0.6657 -- iter: 096/532
[A[ATraining Step: 21  | total loss: [1m[32m0.67319[0m[0m | time: 3.856s
[2K
| Adam | epoch: 002 | loss: 0.67319 - acc: 0.6240 -- iter: 128/532
[A[ATraining Step: 22  | total loss: [1m[32m0.66982[0m[0m | time: 4.861s
[2K
| Adam | epoch: 002 | loss: 0.66982 - acc: 0.6243 -- iter: 160/532
[A[ATraining Step: 23  | total loss: [1m[32m0.67308[0m[0m | time: 5.847s
[2K
| Adam | epoch: 002 | loss: 0.67308 - acc: 0.6154 -- iter: 192/532
[A[ATraining Step: 24  | total loss: [1m[32m0.67015[0m[0m | time: 6.898s
[2K
| Adam | epoch: 002 | loss: 0.67015 - acc: 0.6181 -- iter: 224/532
[A[ATraining Step: 25  | total loss: [1m[32m0.67403[0m[0m | time: 7.928s
[2K
| Adam | epoch: 002 | loss: 0.67403 - acc: 0.6115 -- iter: 256/532
[A[ATraining Step: 26  | total loss: [1m[32m0.65616[0m[0m | time: 9.030s
[2K
| Adam | epoch: 002 | loss: 0.65616 - acc: 0.6399 -- iter: 288/532
[A[ATraining Step: 27  | total loss: [1m[32m0.68105[0m[0m | time: 10.052s
[2K
| Adam | epoch: 002 | loss: 0.68105 - acc: 0.5959 -- iter: 320/532
[A[ATraining Step: 28  | total loss: [1m[32m0.68742[0m[0m | time: 11.065s
[2K
| Adam | epoch: 002 | loss: 0.68742 - acc: 0.5797 -- iter: 352/532
[A[ATraining Step: 29  | total loss: [1m[32m0.68418[0m[0m | time: 12.091s
[2K
| Adam | epoch: 002 | loss: 0.68418 - acc: 0.5831 -- iter: 384/532
[A[ATraining Step: 30  | total loss: [1m[32m0.67687[0m[0m | time: 13.167s
[2K
| Adam | epoch: 002 | loss: 0.67687 - acc: 0.6004 -- iter: 416/532
[A[ATraining Step: 31  | total loss: [1m[32m0.67656[0m[0m | time: 14.195s
[2K
| Adam | epoch: 002 | loss: 0.67656 - acc: 0.5989 -- iter: 448/532
[A[ATraining Step: 32  | total loss: [1m[32m0.67272[0m[0m | time: 15.285s
[2K
| Adam | epoch: 002 | loss: 0.67272 - acc: 0.6118 -- iter: 480/532
[A[ATraining Step: 33  | total loss: [1m[32m0.67751[0m[0m | time: 16.294s
[2K
| Adam | epoch: 002 | loss: 0.67751 - acc: 0.5941 -- iter: 512/532
[A[ATraining Step: 34  | total loss: [1m[32m0.67418[0m[0m | time: 18.313s
[2K
| Adam | epoch: 002 | loss: 0.67418 - acc: 0.6074 | val_loss: 0.67306 - val_acc: 0.6108 -- iter: 532/532
--
Training Step: 35  | total loss: [1m[32m0.67339[0m[0m | time: 0.489s
[2K
| Adam | epoch: 003 | loss: 0.67339 - acc: 0.6111 -- iter: 032/532
[A[ATraining Step: 36  | total loss: [1m[32m0.68124[0m[0m | time: 0.873s
[2K
| Adam | epoch: 003 | loss: 0.68124 - acc: 0.5782 -- iter: 064/532
[A[ATraining Step: 37  | total loss: [1m[32m0.68770[0m[0m | time: 1.464s
[2K
| Adam | epoch: 003 | loss: 0.68770 - acc: 0.5525 -- iter: 096/532
[A[ATraining Step: 38  | total loss: [1m[32m0.68314[0m[0m | time: 2.073s
[2K
| Adam | epoch: 003 | loss: 0.68314 - acc: 0.5728 -- iter: 128/532
[A[ATraining Step: 39  | total loss: [1m[32m0.67953[0m[0m | time: 2.684s
[2K
| Adam | epoch: 003 | loss: 0.67953 - acc: 0.5888 -- iter: 160/532
[A[ATraining Step: 40  | total loss: [1m[32m0.67408[0m[0m | time: 3.290s
[2K
| Adam | epoch: 003 | loss: 0.67408 - acc: 0.6132 -- iter: 192/532
[A[ATraining Step: 41  | total loss: [1m[32m0.67190[0m[0m | time: 3.923s
[2K
| Adam | epoch: 003 | loss: 0.67190 - acc: 0.6211 -- iter: 224/532
[A[ATraining Step: 42  | total loss: [1m[32m0.66840[0m[0m | time: 4.550s
[2K
| Adam | epoch: 003 | loss: 0.66840 - acc: 0.6330 -- iter: 256/532
[A[ATraining Step: 43  | total loss: [1m[32m0.67339[0m[0m | time: 5.172s
[2K
| Adam | epoch: 003 | loss: 0.67339 - acc: 0.6151 -- iter: 288/532
[A[ATraining Step: 44  | total loss: [1m[32m0.67549[0m[0m | time: 5.779s
[2K
| Adam | epoch: 003 | loss: 0.67549 - acc: 0.6060 -- iter: 320/532
[A[ATraining Step: 45  | total loss: [1m[32m0.67529[0m[0m | time: 6.374s
[2K
| Adam | epoch: 003 | loss: 0.67529 - acc: 0.6039 -- iter: 352/532
[A[ATraining Step: 46  | total loss: [1m[32m0.66642[0m[0m | time: 6.993s
[2K
| Adam | epoch: 003 | loss: 0.66642 - acc: 0.6230 -- iter: 384/532
[A[ATraining Step: 47  | total loss: [1m[32m0.66537[0m[0m | time: 7.592s
[2K
| Adam | epoch: 003 | loss: 0.66537 - acc: 0.6234 -- iter: 416/532
[A[ATraining Step: 48  | total loss: [1m[32m0.66528[0m[0m | time: 8.197s
[2K
| Adam | epoch: 003 | loss: 0.66528 - acc: 0.6236 -- iter: 448/532
[A[ATraining Step: 49  | total loss: [1m[32m0.66156[0m[0m | time: 8.837s
[2K
| Adam | epoch: 003 | loss: 0.66156 - acc: 0.6288 -- iter: 480/532
[A[ATraining Step: 50  | total loss: [1m[32m0.68869[0m[0m | time: 9.482s
[2K
| Adam | epoch: 003 | loss: 0.68869 - acc: 0.5894 -- iter: 512/532
[A[ATraining Step: 51  | total loss: [1m[32m0.68757[0m[0m | time: 11.095s
[2K
| Adam | epoch: 003 | loss: 0.68757 - acc: 0.5901 | val_loss: 0.66848 - val_acc: 0.6108 -- iter: 532/532
--
Training Step: 52  | total loss: [1m[32m0.69057[0m[0m | time: 0.622s
[2K
| Adam | epoch: 004 | loss: 0.69057 - acc: 0.5812 -- iter: 032/532
[A[ATraining Step: 53  | total loss: [1m[32m0.68985[0m[0m | time: 1.004s
[2K
| Adam | epoch: 004 | loss: 0.68985 - acc: 0.5785 -- iter: 064/532
[A[ATraining Step: 54  | total loss: [1m[32m0.68752[0m[0m | time: 1.392s
[2K
| Adam | epoch: 004 | loss: 0.68752 - acc: 0.5816 -- iter: 096/532
[A[ATraining Step: 55  | total loss: [1m[32m0.68580[0m[0m | time: 2.006s
[2K
| Adam | epoch: 004 | loss: 0.68580 - acc: 0.5842 -- iter: 128/532
[A[ATraining Step: 56  | total loss: [1m[32m0.68149[0m[0m | time: 2.617s
[2K
| Adam | epoch: 004 | loss: 0.68149 - acc: 0.5988 -- iter: 160/532
[A[ATraining Step: 57  | total loss: [1m[32m0.68188[0m[0m | time: 3.236s
[2K
| Adam | epoch: 004 | loss: 0.68188 - acc: 0.5937 -- iter: 192/532
[A[ATraining Step: 58  | total loss: [1m[32m0.68055[0m[0m | time: 3.837s
[2K
| Adam | epoch: 004 | loss: 0.68055 - acc: 0.5980 -- iter: 224/532
[A[ATraining Step: 59  | total loss: [1m[32m0.67892[0m[0m | time: 4.457s
[2K
| Adam | epoch: 004 | loss: 0.67892 - acc: 0.6058 -- iter: 256/532
[A[ATraining Step: 60  | total loss: [1m[32m0.67966[0m[0m | time: 5.074s
[2K
| Adam | epoch: 004 | loss: 0.67966 - acc: 0.6001 -- iter: 288/532
[A[ATraining Step: 61  | total loss: [1m[32m0.67717[0m[0m | time: 5.688s
[2K
| Adam | epoch: 004 | loss: 0.67717 - acc: 0.6115 -- iter: 320/532
[A[ATraining Step: 62  | total loss: [1m[32m0.67826[0m[0m | time: 6.318s
[2K
| Adam | epoch: 004 | loss: 0.67826 - acc: 0.6052 -- iter: 352/532
[A[ATraining Step: 63  | total loss: [1m[32m0.68268[0m[0m | time: 6.920s
[2K
| Adam | epoch: 004 | loss: 0.68268 - acc: 0.5839 -- iter: 384/532
[A[ATraining Step: 64  | total loss: [1m[32m0.68467[0m[0m | time: 7.536s
[2K
| Adam | epoch: 004 | loss: 0.68467 - acc: 0.5734 -- iter: 416/532
[A[ATraining Step: 65  | total loss: [1m[32m0.68159[0m[0m | time: 8.159s
[2K
| Adam | epoch: 004 | loss: 0.68159 - acc: 0.5875 -- iter: 448/532
[A[ATraining Step: 66  | total loss: [1m[32m0.67895[0m[0m | time: 8.771s
[2K
| Adam | epoch: 004 | loss: 0.67895 - acc: 0.5997 -- iter: 480/532
[A[ATraining Step: 67  | total loss: [1m[32m0.68119[0m[0m | time: 9.384s
[2K
| Adam | epoch: 004 | loss: 0.68119 - acc: 0.5877 -- iter: 512/532
[A[ATraining Step: 68  | total loss: [1m[32m0.68186[0m[0m | time: 10.989s
[2K
| Adam | epoch: 004 | loss: 0.68186 - acc: 0.5847 | val_loss: 0.67243 - val_acc: 0.6108 -- iter: 532/532
--
Training Step: 69  | total loss: [1m[32m0.67955[0m[0m | time: 0.627s
[2K
| Adam | epoch: 005 | loss: 0.67955 - acc: 0.5931 -- iter: 032/532
[A[ATraining Step: 70  | total loss: [1m[32m0.67368[0m[0m | time: 1.258s
[2K
| Adam | epoch: 005 | loss: 0.67368 - acc: 0.6148 -- iter: 064/532
[A[ATraining Step: 71  | total loss: [1m[32m0.67398[0m[0m | time: 1.859s
[2K
| Adam | epoch: 005 | loss: 0.67398 - acc: 0.6124 -- iter: 096/532
[A[ATraining Step: 72  | total loss: [1m[32m0.67538[0m[0m | time: 2.475s
[2K
| Adam | epoch: 005 | loss: 0.67538 - acc: 0.6054 -- iter: 128/532
[A[ATraining Step: 73  | total loss: [1m[32m0.67711[0m[0m | time: 3.315s
[2K
| Adam | epoch: 005 | loss: 0.67711 - acc: 0.5992 -- iter: 160/532
[A[ATraining Step: 74  | total loss: [1m[32m0.67391[0m[0m | time: 4.167s
[2K
| Adam | epoch: 005 | loss: 0.67391 - acc: 0.6055 -- iter: 192/532
[A[ATraining Step: 75  | total loss: [1m[32m0.67584[0m[0m | time: 5.044s
[2K
| Adam | epoch: 005 | loss: 0.67584 - acc: 0.6008 -- iter: 224/532
[A[ATraining Step: 76  | total loss: [1m[32m0.67435[0m[0m | time: 6.022s
[2K
| Adam | epoch: 005 | loss: 0.67435 - acc: 0.6034 -- iter: 256/532
[A[ATraining Step: 77  | total loss: [1m[32m0.67927[0m[0m | time: 7.405s
[2K
| Adam | epoch: 005 | loss: 0.67927 - acc: 0.5925 -- iter: 288/532
[A[ATraining Step: 78  | total loss: [1m[32m0.67738[0m[0m | time: 8.811s
[2K
| Adam | epoch: 005 | loss: 0.67738 - acc: 0.5959 -- iter: 320/532
[A[ATraining Step: 79  | total loss: [1m[32m0.67851[0m[0m | time: 10.119s
[2K
| Adam | epoch: 005 | loss: 0.67851 - acc: 0.5924 -- iter: 352/532
[A[ATraining Step: 80  | total loss: [1m[32m0.67937[0m[0m | time: 10.998s
[2K
| Adam | epoch: 005 | loss: 0.67937 - acc: 0.5894 -- iter: 384/532
[A[ATraining Step: 81  | total loss: [1m[32m0.67618[0m[0m | time: 11.964s
[2K
| Adam | epoch: 005 | loss: 0.67618 - acc: 0.5961 -- iter: 416/532
[A[ATraining Step: 82  | total loss: [1m[32m0.67589[0m[0m | time: 12.955s
[2K
| Adam | epoch: 005 | loss: 0.67589 - acc: 0.5959 -- iter: 448/532
[A[ATraining Step: 83  | total loss: [1m[32m0.67440[0m[0m | time: 13.937s
[2K
| Adam | epoch: 005 | loss: 0.67440 - acc: 0.5988 -- iter: 480/532
[A[ATraining Step: 84  | total loss: [1m[32m0.67331[0m[0m | time: 14.975s
[2K
| Adam | epoch: 005 | loss: 0.67331 - acc: 0.6014 -- iter: 512/532
[A[ATraining Step: 85  | total loss: [1m[32m0.67074[0m[0m | time: 16.937s
[2K
| Adam | epoch: 005 | loss: 0.67074 - acc: 0.6069 | val_loss: 0.66760 - val_acc: 0.6108 -- iter: 532/532
--
Training Step: 86  | total loss: [1m[32m0.67072[0m[0m | time: 0.911s
[2K
| Adam | epoch: 006 | loss: 0.67072 - acc: 0.6056 -- iter: 032/532
[A[ATraining Step: 87  | total loss: [1m[32m0.67099[0m[0m | time: 1.880s
[2K
| Adam | epoch: 006 | loss: 0.67099 - acc: 0.6044 -- iter: 064/532
[A[ATraining Step: 88  | total loss: [1m[32m0.67259[0m[0m | time: 2.854s
[2K
| Adam | epoch: 006 | loss: 0.67259 - acc: 0.6002 -- iter: 096/532
[A[ATraining Step: 89  | total loss: [1m[32m0.67581[0m[0m | time: 3.485s
[2K
| Adam | epoch: 006 | loss: 0.67581 - acc: 0.5933 -- iter: 128/532
[A[ATraining Step: 90  | total loss: [1m[32m0.67336[0m[0m | time: 4.181s
[2K
| Adam | epoch: 006 | loss: 0.67336 - acc: 0.5990 -- iter: 160/532
[A[ATraining Step: 91  | total loss: [1m[32m0.67087[0m[0m | time: 5.220s
[2K
| Adam | epoch: 006 | loss: 0.67087 - acc: 0.6041 -- iter: 192/532
[A[ATraining Step: 92  | total loss: [1m[32m0.66563[0m[0m | time: 6.047s
[2K
| Adam | epoch: 006 | loss: 0.66563 - acc: 0.6156 -- iter: 224/532
[A[ATraining Step: 93  | total loss: [1m[32m0.66506[0m[0m | time: 7.305s
[2K
| Adam | epoch: 006 | loss: 0.66506 - acc: 0.6165 -- iter: 256/532
[A[ATraining Step: 94  | total loss: [1m[32m0.67277[0m[0m | time: 8.677s
[2K
| Adam | epoch: 006 | loss: 0.67277 - acc: 0.6017 -- iter: 288/532
[A[ATraining Step: 95  | total loss: [1m[32m0.66846[0m[0m | time: 9.748s
[2K
| Adam | epoch: 006 | loss: 0.66846 - acc: 0.6103 -- iter: 320/532
[A[ATraining Step: 96  | total loss: [1m[32m0.66870[0m[0m | time: 10.724s
[2K
| Adam | epoch: 006 | loss: 0.66870 - acc: 0.6086 -- iter: 352/532
[A[ATraining Step: 97  | total loss: [1m[32m0.66721[0m[0m | time: 11.617s
[2K
| Adam | epoch: 006 | loss: 0.66721 - acc: 0.6103 -- iter: 384/532
[A[ATraining Step: 98  | total loss: [1m[32m0.66929[0m[0m | time: 12.545s
[2K
| Adam | epoch: 006 | loss: 0.66929 - acc: 0.6055 -- iter: 416/532
[A[ATraining Step: 99  | total loss: [1m[32m0.66770[0m[0m | time: 13.529s
[2K
| Adam | epoch: 006 | loss: 0.66770 - acc: 0.6075 -- iter: 448/532
[A[ATraining Step: 100  | total loss: [1m[32m0.67129[0m[0m | time: 14.515s
[2K
| Adam | epoch: 006 | loss: 0.67129 - acc: 0.5998 -- iter: 480/532
[A[ATraining Step: 101  | total loss: [1m[32m0.66755[0m[0m | time: 15.548s
[2K
| Adam | epoch: 006 | loss: 0.66755 - acc: 0.6086 -- iter: 512/532
[A[ATraining Step: 102  | total loss: [1m[32m0.67070[0m[0m | time: 17.419s
[2K
| Adam | epoch: 006 | loss: 0.67070 - acc: 0.6009 | val_loss: 0.66710 - val_acc: 0.6108 -- iter: 532/532
--
Training Step: 103  | total loss: [1m[32m0.67342[0m[0m | time: 0.938s
[2K
| Adam | epoch: 007 | loss: 0.67342 - acc: 0.5939 -- iter: 032/532
[A[ATraining Step: 104  | total loss: [1m[32m0.66830[0m[0m | time: 1.956s
[2K
| Adam | epoch: 007 | loss: 0.66830 - acc: 0.6064 -- iter: 064/532
[A[ATraining Step: 105  | total loss: [1m[32m0.66262[0m[0m | time: 3.080s
[2K
| Adam | epoch: 007 | loss: 0.66262 - acc: 0.6207 -- iter: 096/532
[A[ATraining Step: 106  | total loss: [1m[32m0.66215[0m[0m | time: 3.883s
[2K
| Adam | epoch: 007 | loss: 0.66215 - acc: 0.6212 -- iter: 128/532
[A[ATraining Step: 107  | total loss: [1m[32m0.66039[0m[0m | time: 4.576s
[2K
| Adam | epoch: 007 | loss: 0.66039 - acc: 0.6247 -- iter: 160/532
[A[ATraining Step: 108  | total loss: [1m[32m0.65870[0m[0m | time: 5.281s
[2K
| Adam | epoch: 007 | loss: 0.65870 - acc: 0.6272 -- iter: 192/532
[A[ATraining Step: 109  | total loss: [1m[32m0.65696[0m[0m | time: 6.600s
[2K
| Adam | epoch: 007 | loss: 0.65696 - acc: 0.6295 -- iter: 224/532
[A[ATraining Step: 110  | total loss: [1m[32m0.66287[0m[0m | time: 7.788s
[2K
| Adam | epoch: 007 | loss: 0.66287 - acc: 0.6197 -- iter: 256/532
[A[ATraining Step: 111  | total loss: [1m[32m0.66880[0m[0m | time: 8.672s
[2K
| Adam | epoch: 007 | loss: 0.66880 - acc: 0.6108 -- iter: 288/532
[A[ATraining Step: 112  | total loss: [1m[32m0.66391[0m[0m | time: 9.667s
[2K
| Adam | epoch: 007 | loss: 0.66391 - acc: 0.6185 -- iter: 320/532
[A[ATraining Step: 113  | total loss: [1m[32m0.65978[0m[0m | time: 10.659s
[2K
| Adam | epoch: 007 | loss: 0.65978 - acc: 0.6254 -- iter: 352/532
[A[ATraining Step: 114  | total loss: [1m[32m0.66512[0m[0m | time: 11.656s
[2K
| Adam | epoch: 007 | loss: 0.66512 - acc: 0.6160 -- iter: 384/532
[A[ATraining Step: 115  | total loss: [1m[32m0.66595[0m[0m | time: 12.719s
[2K
| Adam | epoch: 007 | loss: 0.66595 - acc: 0.6138 -- iter: 416/532
[A[ATraining Step: 116  | total loss: [1m[32m0.66916[0m[0m | time: 13.652s
[2K
| Adam | epoch: 007 | loss: 0.66916 - acc: 0.6055 -- iter: 448/532
[A[ATraining Step: 117  | total loss: [1m[32m0.66799[0m[0m | time: 14.586s
[2K
| Adam | epoch: 007 | loss: 0.66799 - acc: 0.6075 -- iter: 480/532
[A[ATraining Step: 118  | total loss: [1m[32m0.66610[0m[0m | time: 15.757s
[2K
| Adam | epoch: 007 | loss: 0.66610 - acc: 0.6123 -- iter: 512/532
[A[ATraining Step: 119  | total loss: [1m[32m0.67107[0m[0m | time: 18.126s
[2K
| Adam | epoch: 007 | loss: 0.67107 - acc: 0.5980 | val_loss: 0.66809 - val_acc: 0.6108 -- iter: 532/532
--
Training Step: 120  | total loss: [1m[32m0.66825[0m[0m | time: 1.116s
[2K
| Adam | epoch: 008 | loss: 0.66825 - acc: 0.6069 -- iter: 032/532
[A[ATraining Step: 121  | total loss: [1m[32m0.66558[0m[0m | time: 2.058s
[2K
| Adam | epoch: 008 | loss: 0.66558 - acc: 0.6150 -- iter: 064/532
[A[ATraining Step: 122  | total loss: [1m[32m0.66718[0m[0m | time: 3.005s
[2K
| Adam | epoch: 008 | loss: 0.66718 - acc: 0.6097 -- iter: 096/532
[A[ATraining Step: 123  | total loss: [1m[32m0.66314[0m[0m | time: 4.177s
[2K
| Adam | epoch: 008 | loss: 0.66314 - acc: 0.6206 -- iter: 128/532
[A[ATraining Step: 124  | total loss: [1m[32m0.66479[0m[0m | time: 5.575s
[2K
| Adam | epoch: 008 | loss: 0.66479 - acc: 0.6148 -- iter: 160/532
[A[ATraining Step: 125  | total loss: [1m[32m0.66777[0m[0m | time: 6.296s
[2K
| Adam | epoch: 008 | loss: 0.66777 - acc: 0.6065 -- iter: 192/532
[A[ATraining Step: 126  | total loss: [1m[32m0.66957[0m[0m | time: 6.842s
[2K
| Adam | epoch: 008 | loss: 0.66957 - acc: 0.6008 -- iter: 224/532
[A[ATraining Step: 127  | total loss: [1m[32m0.67097[0m[0m | time: 7.693s
[2K
| Adam | epoch: 008 | loss: 0.67097 - acc: 0.5957 -- iter: 256/532
[A[ATraining Step: 128  | total loss: [1m[32m0.67108[0m[0m | time: 8.649s
[2K
| Adam | epoch: 008 | loss: 0.67108 - acc: 0.5955 -- iter: 288/532
[A[ATraining Step: 129  | total loss: [1m[32m0.67098[0m[0m | time: 9.583s
[2K
| Adam | epoch: 008 | loss: 0.67098 - acc: 0.5954 -- iter: 320/532
[A[ATraining Step: 130  | total loss: [1m[32m0.66319[0m[0m | time: 10.564s
[2K
| Adam | epoch: 008 | loss: 0.66319 - acc: 0.6139 -- iter: 352/532
[A[ATraining Step: 131  | total loss: [1m[32m0.66193[0m[0m | time: 11.560s
[2K
| Adam | epoch: 008 | loss: 0.66193 - acc: 0.6151 -- iter: 384/532
[A[ATraining Step: 132  | total loss: [1m[32m0.67041[0m[0m | time: 12.517s
[2K
| Adam | epoch: 008 | loss: 0.67041 - acc: 0.5973 -- iter: 416/532
[A[ATraining Step: 133  | total loss: [1m[32m0.67531[0m[0m | time: 13.352s
[2K
| Adam | epoch: 008 | loss: 0.67531 - acc: 0.5876 -- iter: 448/532
[A[ATraining Step: 134  | total loss: [1m[32m0.67398[0m[0m | time: 14.490s
[2K
| Adam | epoch: 008 | loss: 0.67398 - acc: 0.5882 -- iter: 480/532
[A[ATraining Step: 135  | total loss: [1m[32m0.66670[0m[0m | time: 15.760s
[2K
| Adam | epoch: 008 | loss: 0.66670 - acc: 0.6044 -- iter: 512/532
[A[ATraining Step: 136  | total loss: [1m[32m0.66419[0m[0m | time: 17.995s
[2K
| Adam | epoch: 008 | loss: 0.66419 - acc: 0.6096 | val_loss: 0.66064 - val_acc: 0.6108 -- iter: 532/532
--
Training Step: 137  | total loss: [1m[32m0.65886[0m[0m | time: 1.108s
[2K
| Adam | epoch: 009 | loss: 0.65886 - acc: 0.6205 -- iter: 032/532
[A[ATraining Step: 138  | total loss: [1m[32m0.66255[0m[0m | time: 1.945s
[2K
| Adam | epoch: 009 | loss: 0.66255 - acc: 0.6116 -- iter: 064/532
[A[ATraining Step: 139  | total loss: [1m[32m0.66073[0m[0m | time: 3.004s
[2K
| Adam | epoch: 009 | loss: 0.66073 - acc: 0.6129 -- iter: 096/532
[A[ATraining Step: 140  | total loss: [1m[32m0.66217[0m[0m | time: 4.258s
[2K
| Adam | epoch: 009 | loss: 0.66217 - acc: 0.6079 -- iter: 128/532
[A[ATraining Step: 141  | total loss: [1m[32m0.66966[0m[0m | time: 5.523s
[2K
| Adam | epoch: 009 | loss: 0.66966 - acc: 0.5939 -- iter: 160/532
[A[ATraining Step: 142  | total loss: [1m[32m0.67067[0m[0m | time: 6.361s
[2K
| Adam | epoch: 009 | loss: 0.67067 - acc: 0.5908 -- iter: 192/532
[A[ATraining Step: 143  | total loss: [1m[32m0.66472[0m[0m | time: 6.972s
[2K
| Adam | epoch: 009 | loss: 0.66472 - acc: 0.6036 -- iter: 224/532
[A[ATraining Step: 144  | total loss: [1m[32m0.66212[0m[0m | time: 7.591s
[2K
| Adam | epoch: 009 | loss: 0.66212 - acc: 0.6082 -- iter: 256/532
[A[ATraining Step: 145  | total loss: [1m[32m0.65943[0m[0m | time: 8.598s
[2K
| Adam | epoch: 009 | loss: 0.65943 - acc: 0.6124 -- iter: 288/532
[A[ATraining Step: 146  | total loss: [1m[32m0.65788[0m[0m | time: 9.541s
[2K
| Adam | epoch: 009 | loss: 0.65788 - acc: 0.6137 -- iter: 320/532
[A[ATraining Step: 147  | total loss: [1m[32m0.66243[0m[0m | time: 10.611s
[2K
| Adam | epoch: 009 | loss: 0.66243 - acc: 0.6023 -- iter: 352/532
[A[ATraining Step: 148  | total loss: [1m[32m0.66030[0m[0m | time: 11.608s
[2K
| Adam | epoch: 009 | loss: 0.66030 - acc: 0.6046 -- iter: 384/532
[A[ATraining Step: 149  | total loss: [1m[32m0.65913[0m[0m | time: 12.442s
[2K
| Adam | epoch: 009 | loss: 0.65913 - acc: 0.6066 -- iter: 416/532
[A[ATraining Step: 150  | total loss: [1m[32m0.65975[0m[0m | time: 13.587s
[2K
| Adam | epoch: 009 | loss: 0.65975 - acc: 0.6022 -- iter: 448/532
[A[ATraining Step: 151  | total loss: [1m[32m0.65676[0m[0m | time: 14.981s
[2K
| Adam | epoch: 009 | loss: 0.65676 - acc: 0.6076 -- iter: 480/532
[A[ATraining Step: 152  | total loss: [1m[32m0.65181[0m[0m | time: 16.143s
[2K
| Adam | epoch: 009 | loss: 0.65181 - acc: 0.6125 -- iter: 512/532
[A[ATraining Step: 153  | total loss: [1m[32m0.64093[0m[0m | time: 18.035s
[2K
| Adam | epoch: 009 | loss: 0.64093 - acc: 0.6294 | val_loss: 0.66260 - val_acc: 0.6108 -- iter: 532/532
--
Training Step: 154  | total loss: [1m[32m0.64941[0m[0m | time: 0.922s
[2K
| Adam | epoch: 010 | loss: 0.64941 - acc: 0.6195 -- iter: 032/532
[A[ATraining Step: 155  | total loss: [1m[32m0.64183[0m[0m | time: 2.100s
[2K
| Adam | epoch: 010 | loss: 0.64183 - acc: 0.6263 -- iter: 064/532
[A[ATraining Step: 156  | total loss: [1m[32m0.64566[0m[0m | time: 3.390s
[2K
| Adam | epoch: 010 | loss: 0.64566 - acc: 0.6200 -- iter: 096/532
[A[ATraining Step: 157  | total loss: [1m[32m0.64983[0m[0m | time: 4.631s
[2K
| Adam | epoch: 010 | loss: 0.64983 - acc: 0.6142 -- iter: 128/532
[A[ATraining Step: 158  | total loss: [1m[32m0.65522[0m[0m | time: 5.454s
[2K
| Adam | epoch: 010 | loss: 0.65522 - acc: 0.6028 -- iter: 160/532
[A[ATraining Step: 159  | total loss: [1m[32m0.65459[0m[0m | time: 6.387s
[2K
| Adam | epoch: 010 | loss: 0.65459 - acc: 0.5988 -- iter: 192/532
[A[ATraining Step: 160  | total loss: [1m[32m0.65915[0m[0m | time: 7.376s
[2K
| Adam | epoch: 010 | loss: 0.65915 - acc: 0.5764 -- iter: 224/532
[A[ATraining Step: 161  | total loss: [1m[32m0.66068[0m[0m | time: 7.982s
[2K
| Adam | epoch: 010 | loss: 0.66068 - acc: 0.5781 -- iter: 256/532
[A[ATraining Step: 162  | total loss: [1m[32m0.66180[0m[0m | time: 8.612s
[2K
| Adam | epoch: 010 | loss: 0.66180 - acc: 0.5903 -- iter: 288/532
[A[ATraining Step: 163  | total loss: [1m[32m0.66296[0m[0m | time: 9.651s
[2K
| Adam | epoch: 010 | loss: 0.66296 - acc: 0.6013 -- iter: 320/532
[A[ATraining Step: 164  | total loss: [1m[32m0.66457[0m[0m | time: 10.617s
[2K
| Adam | epoch: 010 | loss: 0.66457 - acc: 0.6036 -- iter: 352/532
[A[ATraining Step: 165  | total loss: [1m[32m0.66450[0m[0m | time: 11.533s
[2K
| Adam | epoch: 010 | loss: 0.66450 - acc: 0.6152 -- iter: 384/532
[A[ATraining Step: 166  | total loss: [1m[32m0.66337[0m[0m | time: 12.659s
[2K
| Adam | epoch: 010 | loss: 0.66337 - acc: 0.6255 -- iter: 416/532
[A[ATraining Step: 167  | total loss: [1m[32m0.66002[0m[0m | time: 14.018s
[2K
| Adam | epoch: 010 | loss: 0.66002 - acc: 0.6380 -- iter: 448/532
[A[ATraining Step: 168  | total loss: [1m[32m0.66083[0m[0m | time: 15.220s
[2K
| Adam | epoch: 010 | loss: 0.66083 - acc: 0.6304 -- iter: 480/532
[A[ATraining Step: 169  | total loss: [1m[32m0.65458[0m[0m | time: 16.099s
[2K
| Adam | epoch: 010 | loss: 0.65458 - acc: 0.6361 -- iter: 512/532
[A[ATraining Step: 170  | total loss: [1m[32m0.65732[0m[0m | time: 18.045s
[2K
| Adam | epoch: 010 | loss: 0.65732 - acc: 0.6288 | val_loss: 0.65923 - val_acc: 0.6108 -- iter: 532/532
--
Training Step: 171  | total loss: [1m[32m0.65687[0m[0m | time: 0.842s
[2K
| Adam | epoch: 011 | loss: 0.65687 - acc: 0.6284 -- iter: 032/532
[A[ATraining Step: 172  | total loss: [1m[32m0.66174[0m[0m | time: 1.897s
[2K
| Adam | epoch: 011 | loss: 0.66174 - acc: 0.6187 -- iter: 064/532
[A[ATraining Step: 173  | total loss: [1m[32m0.66861[0m[0m | time: 3.200s
[2K
| Adam | epoch: 011 | loss: 0.66861 - acc: 0.6068 -- iter: 096/532
[A[ATraining Step: 174  | total loss: [1m[32m0.66621[0m[0m | time: 4.447s
[2K
| Adam | epoch: 011 | loss: 0.66621 - acc: 0.6086 -- iter: 128/532
[A[ATraining Step: 175  | total loss: [1m[32m0.65864[0m[0m | time: 5.270s
[2K
| Adam | epoch: 011 | loss: 0.65864 - acc: 0.6196 -- iter: 160/532
[A[ATraining Step: 176  | total loss: [1m[32m0.65126[0m[0m | time: 6.219s
[2K
| Adam | epoch: 011 | loss: 0.65126 - acc: 0.6295 -- iter: 192/532
[A[ATraining Step: 177  | total loss: [1m[32m0.65743[0m[0m | time: 7.168s
[2K
| Adam | epoch: 011 | loss: 0.65743 - acc: 0.6072 -- iter: 224/532
[A[ATraining Step: 178  | total loss: [1m[32m0.65928[0m[0m | time: 8.081s
[2K
| Adam | epoch: 011 | loss: 0.65928 - acc: 0.5934 -- iter: 256/532
[A[ATraining Step: 179  | total loss: [1m[32m0.65894[0m[0m | time: 8.726s
[2K
| Adam | epoch: 011 | loss: 0.65894 - acc: 0.5934 -- iter: 288/532
[A[ATraining Step: 180  | total loss: [1m[32m0.66045[0m[0m | time: 9.459s
[2K
| Adam | epoch: 011 | loss: 0.66045 - acc: 0.5841 -- iter: 320/532
[A[ATraining Step: 181  | total loss: [1m[32m0.66093[0m[0m | time: 10.507s
[2K
| Adam | epoch: 011 | loss: 0.66093 - acc: 0.5757 -- iter: 352/532
[A[ATraining Step: 182  | total loss: [1m[32m0.65880[0m[0m | time: 11.345s
[2K
| Adam | epoch: 011 | loss: 0.65880 - acc: 0.5775 -- iter: 384/532
[A[ATraining Step: 183  | total loss: [1m[32m0.65279[0m[0m | time: 12.561s
[2K
| Adam | epoch: 011 | loss: 0.65279 - acc: 0.5885 -- iter: 416/532
[A[ATraining Step: 184  | total loss: [1m[32m0.64688[0m[0m | time: 13.806s
[2K
| Adam | epoch: 011 | loss: 0.64688 - acc: 0.5984 -- iter: 448/532
[A[ATraining Step: 185  | total loss: [1m[32m0.64377[0m[0m | time: 15.118s
[2K
| Adam | epoch: 011 | loss: 0.64377 - acc: 0.6010 -- iter: 480/532
[A[ATraining Step: 186  | total loss: [1m[32m0.65081[0m[0m | time: 15.969s
[2K
| Adam | epoch: 011 | loss: 0.65081 - acc: 0.5909 -- iter: 512/532
[A[ATraining Step: 187  | total loss: [1m[32m0.64485[0m[0m | time: 17.887s
[2K
| Adam | epoch: 011 | loss: 0.64485 - acc: 0.5943 | val_loss: 0.63784 - val_acc: 0.6108 -- iter: 532/532
--
Training Step: 188  | total loss: [1m[32m0.63050[0m[0m | time: 0.849s
[2K
| Adam | epoch: 012 | loss: 0.63050 - acc: 0.6099 -- iter: 032/532
[A[ATraining Step: 189  | total loss: [1m[32m0.63066[0m[0m | time: 2.035s
[2K
| Adam | epoch: 012 | loss: 0.63066 - acc: 0.6114 -- iter: 064/532
[A[ATraining Step: 190  | total loss: [1m[32m0.63052[0m[0m | time: 3.439s
[2K
| Adam | epoch: 012 | loss: 0.63052 - acc: 0.6097 -- iter: 096/532
[A[ATraining Step: 191  | total loss: [1m[32m0.62521[0m[0m | time: 4.471s
[2K
| Adam | epoch: 012 | loss: 0.62521 - acc: 0.6143 -- iter: 128/532
[A[ATraining Step: 192  | total loss: [1m[32m0.62323[0m[0m | time: 5.355s
[2K
| Adam | epoch: 012 | loss: 0.62323 - acc: 0.6154 -- iter: 160/532
[A[ATraining Step: 193  | total loss: [1m[32m0.61813[0m[0m | time: 6.298s
[2K
| Adam | epoch: 012 | loss: 0.61813 - acc: 0.6195 -- iter: 192/532
[A[ATraining Step: 194  | total loss: [1m[32m0.60797[0m[0m | time: 7.285s
[2K
| Adam | epoch: 012 | loss: 0.60797 - acc: 0.6294 -- iter: 224/532
[A[ATraining Step: 195  | total loss: [1m[32m0.59641[0m[0m | time: 8.304s
[2K
| Adam | epoch: 012 | loss: 0.59641 - acc: 0.6415 -- iter: 256/532
[A[ATraining Step: 196  | total loss: [1m[32m0.60281[0m[0m | time: 9.462s
[2K
| Adam | epoch: 012 | loss: 0.60281 - acc: 0.6367 -- iter: 288/532
[A[ATraining Step: 197  | total loss: [1m[32m0.61551[0m[0m | time: 10.099s
[2K
| Adam | epoch: 012 | loss: 0.61551 - acc: 0.6199 -- iter: 320/532
[A[ATraining Step: 198  | total loss: [1m[32m0.61631[0m[0m | time: 10.596s
[2K
| Adam | epoch: 012 | loss: 0.61631 - acc: 0.6129 -- iter: 352/532
[A[ATraining Step: 199  | total loss: [1m[32m0.62017[0m[0m | time: 11.634s
[2K
| Adam | epoch: 012 | loss: 0.62017 - acc: 0.6066 -- iter: 384/532
[A[ATraining Step: 200  | total loss: [1m[32m0.62524[0m[0m | time: 13.988s
[2K
| Adam | epoch: 012 | loss: 0.62524 - acc: 0.5897 | val_loss: 0.66982 - val_acc: 0.6108 -- iter: 416/532
--
Training Step: 201  | total loss: [1m[32m0.63191[0m[0m | time: 14.981s
[2K
| Adam | epoch: 012 | loss: 0.63191 - acc: 0.5714 -- iter: 448/532
[A[ATraining Step: 202  | total loss: [1m[32m0.63912[0m[0m | time: 15.953s
[2K
| Adam | epoch: 012 | loss: 0.63912 - acc: 0.5673 -- iter: 480/532
[A[ATraining Step: 203  | total loss: [1m[32m0.63692[0m[0m | time: 16.878s
[2K
| Adam | epoch: 012 | loss: 0.63692 - acc: 0.5762 -- iter: 512/532
[A[ATraining Step: 204  | total loss: [1m[32m0.63952[0m[0m | time: 18.976s
[2K
| Adam | epoch: 012 | loss: 0.63952 - acc: 0.5686 | val_loss: 0.61695 - val_acc: 0.6108 -- iter: 532/532
--
Training Step: 205  | total loss: [1m[32m0.63978[0m[0m | time: 0.955s
[2K
| Adam | epoch: 013 | loss: 0.63978 - acc: 0.5649 -- iter: 032/532
[A[ATraining Step: 206  | total loss: [1m[32m0.62726[0m[0m | time: 1.864s
[2K
| Adam | epoch: 013 | loss: 0.62726 - acc: 0.5803 -- iter: 064/532
[A[ATraining Step: 207  | total loss: [1m[32m0.62028[0m[0m | time: 2.792s
[2K
| Adam | epoch: 013 | loss: 0.62028 - acc: 0.5847 -- iter: 096/532
[A[ATraining Step: 208  | total loss: [1m[32m0.62342[0m[0m | time: 3.798s
[2K
| Adam | epoch: 013 | loss: 0.62342 - acc: 0.5888 -- iter: 128/532
[A[ATraining Step: 209  | total loss: [1m[32m0.62517[0m[0m | time: 4.794s
[2K
| Adam | epoch: 013 | loss: 0.62517 - acc: 0.5861 -- iter: 160/532
[A[ATraining Step: 210  | total loss: [1m[32m0.62445[0m[0m | time: 5.825s
[2K
| Adam | epoch: 013 | loss: 0.62445 - acc: 0.5900 -- iter: 192/532
[A[ATraining Step: 211  | total loss: [1m[32m0.61977[0m[0m | time: 6.693s
[2K
| Adam | epoch: 013 | loss: 0.61977 - acc: 0.5935 -- iter: 224/532
[A[ATraining Step: 212  | total loss: [1m[32m0.61430[0m[0m | time: 7.704s
[2K
| Adam | epoch: 013 | loss: 0.61430 - acc: 0.6154 -- iter: 256/532
[A[ATraining Step: 213  | total loss: [1m[32m0.61842[0m[0m | time: 8.915s
[2K
| Adam | epoch: 013 | loss: 0.61842 - acc: 0.6164 -- iter: 288/532
[A[ATraining Step: 214  | total loss: [1m[32m0.61445[0m[0m | time: 10.139s
[2K
| Adam | epoch: 013 | loss: 0.61445 - acc: 0.6235 -- iter: 320/532
[A[ATraining Step: 215  | total loss: [1m[32m0.60624[0m[0m | time: 10.841s
[2K
| Adam | epoch: 013 | loss: 0.60624 - acc: 0.6486 -- iter: 352/532
[A[ATraining Step: 216  | total loss: [1m[32m0.59834[0m[0m | time: 11.421s
[2K
| Adam | epoch: 013 | loss: 0.59834 - acc: 0.6688 -- iter: 384/532
[A[ATraining Step: 217  | total loss: [1m[32m0.58599[0m[0m | time: 12.318s
[2K
| Adam | epoch: 013 | loss: 0.58599 - acc: 0.6769 -- iter: 416/532
[A[ATraining Step: 218  | total loss: [1m[32m0.59259[0m[0m | time: 13.221s
[2K
| Adam | epoch: 013 | loss: 0.59259 - acc: 0.6623 -- iter: 448/532
[A[ATraining Step: 219  | total loss: [1m[32m0.59490[0m[0m | time: 14.153s
[2K
| Adam | epoch: 013 | loss: 0.59490 - acc: 0.6586 -- iter: 480/532
[A[ATraining Step: 220  | total loss: [1m[32m0.58891[0m[0m | time: 15.174s
[2K
| Adam | epoch: 013 | loss: 0.58891 - acc: 0.6584 -- iter: 512/532
[A[ATraining Step: 221  | total loss: [1m[32m0.59599[0m[0m | time: 17.244s
[2K
| Adam | epoch: 013 | loss: 0.59599 - acc: 0.6550 | val_loss: 0.64226 - val_acc: 0.6287 -- iter: 532/532
--
Training Step: 222  | total loss: [1m[32m0.59521[0m[0m | time: 0.971s
[2K
| Adam | epoch: 014 | loss: 0.59521 - acc: 0.6739 -- iter: 032/532
[A[ATraining Step: 223  | total loss: [1m[32m0.59744[0m[0m | time: 1.991s
[2K
| Adam | epoch: 014 | loss: 0.59744 - acc: 0.6690 -- iter: 064/532
[A[ATraining Step: 224  | total loss: [1m[32m0.59507[0m[0m | time: 3.154s
[2K
| Adam | epoch: 014 | loss: 0.59507 - acc: 0.6802 -- iter: 096/532
[A[ATraining Step: 225  | total loss: [1m[32m0.58526[0m[0m | time: 4.039s
[2K
| Adam | epoch: 014 | loss: 0.58526 - acc: 0.6966 -- iter: 128/532
[A[ATraining Step: 226  | total loss: [1m[32m0.57829[0m[0m | time: 5.120s
[2K
| Adam | epoch: 014 | loss: 0.57829 - acc: 0.7051 -- iter: 160/532
[A[ATraining Step: 227  | total loss: [1m[32m0.57575[0m[0m | time: 6.450s
[2K
| Adam | epoch: 014 | loss: 0.57575 - acc: 0.7095 -- iter: 192/532
[A[ATraining Step: 228  | total loss: [1m[32m0.58476[0m[0m | time: 7.688s
[2K
| Adam | epoch: 014 | loss: 0.58476 - acc: 0.6948 -- iter: 224/532
[A[ATraining Step: 229  | total loss: [1m[32m0.59391[0m[0m | time: 8.549s
[2K
| Adam | epoch: 014 | loss: 0.59391 - acc: 0.6816 -- iter: 256/532
[A[ATraining Step: 230  | total loss: [1m[32m0.58000[0m[0m | time: 9.487s
[2K
| Adam | epoch: 014 | loss: 0.58000 - acc: 0.6853 -- iter: 288/532
[A[ATraining Step: 231  | total loss: [1m[32m0.56996[0m[0m | time: 10.426s
[2K
| Adam | epoch: 014 | loss: 0.56996 - acc: 0.6918 -- iter: 320/532
[A[ATraining Step: 232  | total loss: [1m[32m0.57602[0m[0m | time: 11.377s
[2K
| Adam | epoch: 014 | loss: 0.57602 - acc: 0.6757 -- iter: 352/532
[A[ATraining Step: 233  | total loss: [1m[32m0.57345[0m[0m | time: 12.067s
[2K
| Adam | epoch: 014 | loss: 0.57345 - acc: 0.6863 -- iter: 384/532
[A[ATraining Step: 234  | total loss: [1m[32m0.57328[0m[0m | time: 12.794s
[2K
| Adam | epoch: 014 | loss: 0.57328 - acc: 0.6827 -- iter: 416/532
[A[ATraining Step: 235  | total loss: [1m[32m0.57257[0m[0m | time: 13.777s
[2K
| Adam | epoch: 014 | loss: 0.57257 - acc: 0.6794 -- iter: 448/532
[A[ATraining Step: 236  | total loss: [1m[32m0.57879[0m[0m | time: 14.645s
[2K
| Adam | epoch: 014 | loss: 0.57879 - acc: 0.6740 -- iter: 480/532
[A[ATraining Step: 237  | total loss: [1m[32m0.57502[0m[0m | time: 15.791s
[2K
| Adam | epoch: 014 | loss: 0.57502 - acc: 0.6878 -- iter: 512/532
[A[ATraining Step: 238  | total loss: [1m[32m0.56981[0m[0m | time: 18.121s
[2K
| Adam | epoch: 014 | loss: 0.56981 - acc: 0.6972 | val_loss: 0.65487 - val_acc: 0.6347 -- iter: 532/532
--
Training Step: 239  | total loss: [1m[32m0.57040[0m[0m | time: 1.056s
[2K
| Adam | epoch: 015 | loss: 0.57040 - acc: 0.7056 -- iter: 032/532
[A[ATraining Step: 240  | total loss: [1m[32m0.56235[0m[0m | time: 1.867s
[2K
| Adam | epoch: 015 | loss: 0.56235 - acc: 0.7006 -- iter: 064/532
[A[ATraining Step: 241  | total loss: [1m[32m0.54907[0m[0m | time: 2.950s
[2K
| Adam | epoch: 015 | loss: 0.54907 - acc: 0.7087 -- iter: 096/532
[A[ATraining Step: 242  | total loss: [1m[32m0.54433[0m[0m | time: 4.224s
[2K
| Adam | epoch: 015 | loss: 0.54433 - acc: 0.7128 -- iter: 128/532
[A[ATraining Step: 243  | total loss: [1m[32m0.54658[0m[0m | time: 5.536s
[2K
| Adam | epoch: 015 | loss: 0.54658 - acc: 0.7134 -- iter: 160/532
[A[ATraining Step: 244  | total loss: [1m[32m0.54884[0m[0m | time: 6.379s
[2K
| Adam | epoch: 015 | loss: 0.54884 - acc: 0.7171 -- iter: 192/532
[A[ATraining Step: 245  | total loss: [1m[32m0.54497[0m[0m | time: 7.302s
[2K
| Adam | epoch: 015 | loss: 0.54497 - acc: 0.7235 -- iter: 224/532
[A[ATraining Step: 246  | total loss: [1m[32m0.53558[0m[0m | time: 8.256s
[2K
| Adam | epoch: 015 | loss: 0.53558 - acc: 0.7386 -- iter: 256/532
[A[ATraining Step: 247  | total loss: [1m[32m0.53490[0m[0m | time: 9.192s
[2K
| Adam | epoch: 015 | loss: 0.53490 - acc: 0.7398 -- iter: 288/532
[A[ATraining Step: 248  | total loss: [1m[32m0.53394[0m[0m | time: 10.192s
[2K
| Adam | epoch: 015 | loss: 0.53394 - acc: 0.7439 -- iter: 320/532
[A[ATraining Step: 249  | total loss: [1m[32m0.52361[0m[0m | time: 11.202s
[2K
| Adam | epoch: 015 | loss: 0.52361 - acc: 0.7570 -- iter: 352/532
[A[ATraining Step: 250  | total loss: [1m[32m0.51447[0m[0m | time: 12.097s
[2K
| Adam | epoch: 015 | loss: 0.51447 - acc: 0.7626 -- iter: 384/532
[A[ATraining Step: 251  | total loss: [1m[32m0.50595[0m[0m | time: 12.621s
[2K
| Adam | epoch: 015 | loss: 0.50595 - acc: 0.7613 -- iter: 416/532
[A[ATraining Step: 252  | total loss: [1m[32m0.54454[0m[0m | time: 13.332s
[2K
| Adam | epoch: 015 | loss: 0.54454 - acc: 0.7302 -- iter: 448/532
[A[ATraining Step: 253  | total loss: [1m[32m0.56780[0m[0m | time: 14.732s
[2K
| Adam | epoch: 015 | loss: 0.56780 - acc: 0.6972 -- iter: 480/532
[A[ATraining Step: 254  | total loss: [1m[32m0.57200[0m[0m | time: 16.102s
[2K
| Adam | epoch: 015 | loss: 0.57200 - acc: 0.6993 -- iter: 512/532
[A[ATraining Step: 255  | total loss: [1m[32m0.54827[0m[0m | time: 18.059s
[2K
| Adam | epoch: 015 | loss: 0.54827 - acc: 0.7231 | val_loss: 0.67931 - val_acc: 0.6168 -- iter: 532/532
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.7441930618401208
Validation AUPRC:0.8139642716813682
Test AUC:0.8165204678362573
Test AUPRC:0.8760793937983975
BestTestF1Score	0.78	0.5	0.75	0.79	0.78	74	20	52	21	0.28
BestTestMCCScore	0.79	0.6	0.78	0.9	0.69	66	7	65	29	0.33
BestTestAccuracyScore	0.77	0.51	0.75	0.82	0.73	69	15	57	26	0.3
BestValidationF1Score	0.77	0.39	0.71	0.76	0.77	79	25	40	23	0.28
BestValidationMCC	0.73	0.43	0.71	0.83	0.66	67	14	51	35	0.33
BestValidationAccuracy	0.76	0.43	0.72	0.8	0.72	73	18	47	29	0.3
TestPredictions (Threshold:0.33)
CHEMBL227190,TP,ACT,0.38999998569488525	CHEMBL3353689,TP,ACT,0.7300000190734863	CHEMBL88774,TN,INACT,0.3100000023841858	CHEMBL1164950,TN,INACT,0.20000000298023224	CHEMBL1172000,TP,ACT,0.46000000834465027	CHEMBL1929090,TN,INACT,0.25	CHEMBL3113615,TP,ACT,0.7699999809265137	CHEMBL339970,TP,ACT,0.7400000095367432	CHEMBL327452,TN,INACT,0.27000001072883606	CHEMBL1765906,TP,ACT,0.6399999856948853	CHEMBL3759314,TP,ACT,0.7099999785423279	CHEMBL25207,TN,INACT,0.30000001192092896	CHEMBL2088230,TN,INACT,0.27000001072883606	CHEMBL1089825,TP,ACT,0.4099999964237213	CHEMBL165967,TP,ACT,0.44999998807907104	CHEMBL3608376,TP,ACT,0.4099999964237213	CHEMBL274975,TN,INACT,0.23000000417232513	CHEMBL129056,TP,ACT,0.8500000238418579	CHEMBL106514,TN,INACT,0.25999999046325684	CHEMBL8562,TP,ACT,0.36000001430511475	CHEMBL16591,TP,ACT,0.6899999976158142	CHEMBL2172771,TP,ACT,0.9200000166893005	CHEMBL3422317,FN,ACT,0.25999999046325684	CHEMBL175372,TN,INACT,0.27000001072883606	CHEMBL1173121,TP,ACT,0.6499999761581421	CHEMBL2037373,FN,ACT,0.25	CHEMBL3416347,TP,ACT,0.5799999833106995	CHEMBL450838,FN,ACT,0.27000001072883606	CHEMBL2165866,FP,INACT,0.3799999952316284	CHEMBL3613038,FN,ACT,0.25999999046325684	CHEMBL520641,FN,ACT,0.27000001072883606	CHEMBL175995,FP,INACT,0.5299999713897705	CHEMBL53393,FN,ACT,0.23999999463558197	CHEMBL2062854,TN,INACT,0.30000001192092896	CHEMBL165855,FN,ACT,0.2199999988079071	CHEMBL2163377,FN,ACT,0.25	CHEMBL3416168,FN,ACT,0.25999999046325684	CHEMBL142032,TP,ACT,0.9399999976158142	CHEMBL3604183,TN,INACT,0.25999999046325684	CHEMBL440318,TP,ACT,0.4399999976158142	CHEMBL117911,FN,ACT,0.25999999046325684	CHEMBL330672,TP,ACT,0.41999998688697815	CHEMBL18435,TN,INACT,0.25	CHEMBL3608361,TP,ACT,0.3700000047683716	CHEMBL67056,FN,ACT,0.28999999165534973	CHEMBL563311,TP,ACT,0.44999998807907104	CHEMBL246361,TN,INACT,0.27000001072883606	CHEMBL87228,TN,INACT,0.23999999463558197	CHEMBL544064,TP,ACT,0.6100000143051147	CHEMBL17775,TN,INACT,0.23999999463558197	CHEMBL3094417,TP,ACT,0.49000000953674316	CHEMBL2336301,TP,ACT,0.5	CHEMBL35209,FP,INACT,0.4000000059604645	CHEMBL3099678,TN,INACT,0.2800000011920929	CHEMBL1922660,FP,INACT,0.7200000286102295	CHEMBL121928,TN,INACT,0.25	CHEMBL122649,TN,INACT,0.25999999046325684	CHEMBL185753,TN,INACT,0.27000001072883606	CHEMBL341785,TP,ACT,0.9399999976158142	CHEMBL2029843,FN,ACT,0.2800000011920929	CHEMBL129538,TN,INACT,0.27000001072883606	CHEMBL107150,TN,INACT,0.25	CHEMBL2163380,TN,INACT,0.23999999463558197	CHEMBL3759142,TP,ACT,0.7400000095367432	CHEMBL17983,TN,INACT,0.3199999928474426	CHEMBL294009,TN,INACT,0.3100000023841858	CHEMBL227139,FN,ACT,0.25	CHEMBL2172767,TP,ACT,0.7799999713897705	CHEMBL57284,TN,INACT,0.27000001072883606	CHEMBL1916435,TN,INACT,0.27000001072883606	CHEMBL181340,FP,INACT,0.6299999952316284	CHEMBL3397719,TP,ACT,0.3700000047683716	CHEMBL120362,TP,ACT,0.4099999964237213	CHEMBL390982,FN,ACT,0.25999999046325684	CHEMBL283419,TN,INACT,0.27000001072883606	CHEMBL1163243,TN,INACT,0.1899999976158142	CHEMBL52749,TP,ACT,0.33000001311302185	CHEMBL55844,TP,ACT,0.6299999952316284	CHEMBL1163875,FN,ACT,0.3199999928474426	CHEMBL1916418,TN,INACT,0.25	CHEMBL100758,TN,INACT,0.27000001072883606	CHEMBL3094415,TP,ACT,0.6600000262260437	CHEMBL152162,TN,INACT,0.25999999046325684	CHEMBL160932,TN,INACT,0.28999999165534973	CHEMBL165682,TN,INACT,0.25	CHEMBL1089317,FN,ACT,0.3199999928474426	CHEMBL3099680,FN,ACT,0.28999999165534973	CHEMBL91531,TN,INACT,0.25999999046325684	CHEMBL269576,TN,INACT,0.25	CHEMBL3349313,TN,INACT,0.25	CHEMBL287426,FN,ACT,0.25999999046325684	CHEMBL204633,TP,ACT,0.75	CHEMBL227301,FN,ACT,0.25999999046325684	CHEMBL1916437,TN,INACT,0.3100000023841858	CHEMBL3260687,TP,ACT,0.7799999713897705	CHEMBL29413,FN,ACT,0.27000001072883606	CHEMBL21987,TP,ACT,0.46000000834465027	CHEMBL3353690,TP,ACT,0.5699999928474426	CHEMBL68504,TP,ACT,0.3499999940395355	CHEMBL82171,TP,ACT,0.5	CHEMBL281259,TN,INACT,0.27000001072883606	CHEMBL122530,TP,ACT,0.4099999964237213	CHEMBL126326,TN,INACT,0.25999999046325684	CHEMBL65431,TN,INACT,0.27000001072883606	CHEMBL145,TN,INACT,0.27000001072883606	CHEMBL3318382,TP,ACT,0.6600000262260437	CHEMBL1765892,TP,ACT,0.33000001311302185	CHEMBL3759622,TP,ACT,0.7099999785423279	CHEMBL55762,FN,ACT,0.25999999046325684	CHEMBL64342,TN,INACT,0.25999999046325684	CHEMBL7505,TN,INACT,0.27000001072883606	CHEMBL115468,TN,INACT,0.27000001072883606	CHEMBL60165,FP,INACT,0.4300000071525574	CHEMBL309339,TN,INACT,0.27000001072883606	CHEMBL2164434,TN,INACT,0.2800000011920929	CHEMBL331062,TP,ACT,0.6600000262260437	CHEMBL2430041,TN,INACT,0.2800000011920929	CHEMBL1774310,TP,ACT,0.6000000238418579	CHEMBL3353702,TP,ACT,0.6499999761581421	CHEMBL105139,FN,ACT,0.28999999165534973	CHEMBL1917097,FN,ACT,0.25999999046325684	CHEMBL106771,TN,INACT,0.25999999046325684	CHEMBL459722,TP,ACT,0.6700000166893005	CHEMBL127107,TN,INACT,0.27000001072883606	CHEMBL67434,FN,ACT,0.27000001072883606	CHEMBL1830473,TP,ACT,0.7699999809265137	CHEMBL512897,TN,INACT,0.27000001072883606	CHEMBL3113613,TP,ACT,0.800000011920929	CHEMBL483269,FN,ACT,0.25999999046325684	CHEMBL88874,FN,ACT,0.25	CHEMBL231374,TN,INACT,0.3199999928474426	CHEMBL407499,FN,ACT,0.3199999928474426	CHEMBL3353724,TP,ACT,0.44999998807907104	CHEMBL344602,TN,INACT,0.23999999463558197	CHEMBL1917087,FP,INACT,0.3799999952316284	CHEMBL434057,TP,ACT,0.5199999809265137	CHEMBL416735,TP,ACT,0.5799999833106995	CHEMBL3416177,TP,ACT,0.5400000214576721	CHEMBL2165864,TN,INACT,0.27000001072883606	CHEMBL7976,TN,INACT,0.25999999046325684	CHEMBL2112567,TP,ACT,0.33000001311302185	CHEMBL550332,TP,ACT,0.6000000238418579	CHEMBL3397722,TP,ACT,0.7099999785423279	CHEMBL59356,FN,ACT,0.28999999165534973	CHEMBL452150,TN,INACT,0.25	CHEMBL1215117,TN,INACT,0.3199999928474426	CHEMBL561262,TN,INACT,0.25	CHEMBL3353716,TP,ACT,0.6800000071525574	CHEMBL3349318,TN,INACT,0.25	CHEMBL120132,TP,ACT,0.6899999976158142	CHEMBL106695,TN,INACT,0.25999999046325684	CHEMBL561284,TP,ACT,0.550000011920929	CHEMBL2336299,TP,ACT,0.7400000095367432	CHEMBL1289558,TP,ACT,0.7400000095367432	CHEMBL2336297,TP,ACT,0.5600000023841858	CHEMBL474682,TN,INACT,0.27000001072883606	CHEMBL267014,TN,INACT,0.23999999463558197	CHEMBL86676,TP,ACT,0.44999998807907104	CHEMBL1718182,TN,INACT,0.25999999046325684	CHEMBL377542,TN,INACT,0.23000000417232513	CHEMBL2336207,TP,ACT,0.550000011920929	CHEMBL508629,TP,ACT,0.33000001311302185	CHEMBL3094426,TP,ACT,0.6600000262260437	CHEMBL105777,TN,INACT,0.2800000011920929	CHEMBL508046,TP,ACT,0.4699999988079071	CHEMBL112,TN,INACT,0.25999999046325684	CHEMBL29544,FN,ACT,0.25999999046325684	

