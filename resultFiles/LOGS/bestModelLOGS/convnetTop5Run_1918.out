CNNModel CHEMBL2488 adam 0.001 15 256 0 0.6 False True
Number of active compounds :	149
Number of inactive compounds :	149
---------------------------------
Run id: CNNModel_CHEMBL2488_adam_0.001_15_256_0_0.6_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL2488_adam_0.001_15_256_0.6_True/
---------------------------------
Training samples: 190
Validation samples: 60
--
Training Step: 1  | time: 1.556s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/190
[A[ATraining Step: 2  | total loss: [1m[32m0.62363[0m[0m | time: 2.981s
[2K
| Adam | epoch: 001 | loss: 0.62363 - acc: 0.5906 -- iter: 064/190
[A[ATraining Step: 3  | total loss: [1m[32m0.68069[0m[0m | time: 4.191s
[2K
| Adam | epoch: 001 | loss: 0.68069 - acc: 0.5165 -- iter: 096/190
[A[ATraining Step: 4  | total loss: [1m[32m0.68695[0m[0m | time: 5.436s
[2K
| Adam | epoch: 001 | loss: 0.68695 - acc: 0.5744 -- iter: 128/190
[A[ATraining Step: 5  | total loss: [1m[32m0.70841[0m[0m | time: 6.377s
[2K
| Adam | epoch: 001 | loss: 0.70841 - acc: 0.4580 -- iter: 160/190
[A[ATraining Step: 6  | total loss: [1m[32m0.69793[0m[0m | time: 8.379s
[2K
| Adam | epoch: 001 | loss: 0.69793 - acc: 0.4850 | val_loss: 0.69355 - val_acc: 0.4500 -- iter: 190/190
--
Training Step: 7  | total loss: [1m[32m0.69560[0m[0m | time: 0.999s
[2K
| Adam | epoch: 002 | loss: 0.69560 - acc: 0.4540 -- iter: 032/190
[A[ATraining Step: 8  | total loss: [1m[32m0.69395[0m[0m | time: 1.979s
[2K
| Adam | epoch: 002 | loss: 0.69395 - acc: 0.4986 -- iter: 064/190
[A[ATraining Step: 9  | total loss: [1m[32m0.69509[0m[0m | time: 3.047s
[2K
| Adam | epoch: 002 | loss: 0.69509 - acc: 0.4332 -- iter: 096/190
[A[ATraining Step: 10  | total loss: [1m[32m0.69394[0m[0m | time: 4.346s
[2K
| Adam | epoch: 002 | loss: 0.69394 - acc: 0.4510 -- iter: 128/190
[A[ATraining Step: 11  | total loss: [1m[32m0.69378[0m[0m | time: 5.765s
[2K
| Adam | epoch: 002 | loss: 0.69378 - acc: 0.4594 -- iter: 160/190
[A[ATraining Step: 12  | total loss: [1m[32m0.69360[0m[0m | time: 8.323s
[2K
| Adam | epoch: 002 | loss: 0.69360 - acc: 0.4636 | val_loss: 0.69262 - val_acc: 0.5500 -- iter: 190/190
--
Training Step: 13  | total loss: [1m[32m0.69370[0m[0m | time: 1.005s
[2K
| Adam | epoch: 003 | loss: 0.69370 - acc: 0.4524 -- iter: 032/190
[A[ATraining Step: 14  | total loss: [1m[32m0.69348[0m[0m | time: 2.017s
[2K
| Adam | epoch: 003 | loss: 0.69348 - acc: 0.4719 -- iter: 064/190
[A[ATraining Step: 15  | total loss: [1m[32m0.69340[0m[0m | time: 3.201s
[2K
| Adam | epoch: 003 | loss: 0.69340 - acc: 0.4829 -- iter: 096/190
[A[ATraining Step: 16  | total loss: [1m[32m0.69330[0m[0m | time: 4.247s
[2K
| Adam | epoch: 003 | loss: 0.69330 - acc: 0.4893 -- iter: 128/190
[A[ATraining Step: 17  | total loss: [1m[32m0.69351[0m[0m | time: 5.218s
[2K
| Adam | epoch: 003 | loss: 0.69351 - acc: 0.4707 -- iter: 160/190
[A[ATraining Step: 18  | total loss: [1m[32m0.69352[0m[0m | time: 7.469s
[2K
| Adam | epoch: 003 | loss: 0.69352 - acc: 0.4700 | val_loss: 0.69258 - val_acc: 0.5500 -- iter: 190/190
--
Training Step: 19  | total loss: [1m[32m0.69327[0m[0m | time: 0.880s
[2K
| Adam | epoch: 004 | loss: 0.69327 - acc: 0.4904 -- iter: 032/190
[A[ATraining Step: 20  | total loss: [1m[32m0.69320[0m[0m | time: 1.897s
[2K
| Adam | epoch: 004 | loss: 0.69320 - acc: 0.5035 -- iter: 064/190
[A[ATraining Step: 21  | total loss: [1m[32m0.69291[0m[0m | time: 2.931s
[2K
| Adam | epoch: 004 | loss: 0.69291 - acc: 0.5231 -- iter: 096/190
[A[ATraining Step: 22  | total loss: [1m[32m0.69276[0m[0m | time: 3.873s
[2K
| Adam | epoch: 004 | loss: 0.69276 - acc: 0.5362 -- iter: 128/190
[A[ATraining Step: 23  | total loss: [1m[32m0.69253[0m[0m | time: 4.830s
[2K
| Adam | epoch: 004 | loss: 0.69253 - acc: 0.5438 -- iter: 160/190
[A[ATraining Step: 24  | total loss: [1m[32m0.69258[0m[0m | time: 6.920s
[2K
| Adam | epoch: 004 | loss: 0.69258 - acc: 0.5403 | val_loss: 0.69172 - val_acc: 0.5500 -- iter: 190/190
--
Training Step: 25  | total loss: [1m[32m0.69274[0m[0m | time: 1.293s
[2K
| Adam | epoch: 005 | loss: 0.69274 - acc: 0.5293 -- iter: 032/190
[A[ATraining Step: 26  | total loss: [1m[32m0.69236[0m[0m | time: 2.802s
[2K
| Adam | epoch: 005 | loss: 0.69236 - acc: 0.5381 -- iter: 064/190
[A[ATraining Step: 27  | total loss: [1m[32m0.69289[0m[0m | time: 4.246s
[2K
| Adam | epoch: 005 | loss: 0.69289 - acc: 0.5203 -- iter: 096/190
[A[ATraining Step: 28  | total loss: [1m[32m0.69323[0m[0m | time: 5.285s
[2K
| Adam | epoch: 005 | loss: 0.69323 - acc: 0.5069 -- iter: 128/190
[A[ATraining Step: 29  | total loss: [1m[32m0.69348[0m[0m | time: 6.488s
[2K
| Adam | epoch: 005 | loss: 0.69348 - acc: 0.4971 -- iter: 160/190
[A[ATraining Step: 30  | total loss: [1m[32m0.69281[0m[0m | time: 8.808s
[2K
| Adam | epoch: 005 | loss: 0.69281 - acc: 0.5126 | val_loss: 0.69053 - val_acc: 0.5500 -- iter: 190/190
--
Training Step: 31  | total loss: [1m[32m0.69138[0m[0m | time: 1.305s
[2K
| Adam | epoch: 006 | loss: 0.69138 - acc: 0.5457 -- iter: 032/190
[A[ATraining Step: 32  | total loss: [1m[32m0.69020[0m[0m | time: 2.515s
[2K
| Adam | epoch: 006 | loss: 0.69020 - acc: 0.5636 -- iter: 064/190
[A[ATraining Step: 33  | total loss: [1m[32m0.69131[0m[0m | time: 3.782s
[2K
| Adam | epoch: 006 | loss: 0.69131 - acc: 0.5428 -- iter: 096/190
[A[ATraining Step: 34  | total loss: [1m[32m0.69422[0m[0m | time: 5.160s
[2K
| Adam | epoch: 006 | loss: 0.69422 - acc: 0.5001 -- iter: 128/190
[A[ATraining Step: 35  | total loss: [1m[32m0.69437[0m[0m | time: 6.600s
[2K
| Adam | epoch: 006 | loss: 0.69437 - acc: 0.4931 -- iter: 160/190
[A[ATraining Step: 36  | total loss: [1m[32m0.69441[0m[0m | time: 9.038s
[2K
| Adam | epoch: 006 | loss: 0.69441 - acc: 0.4877 | val_loss: 0.69054 - val_acc: 0.5500 -- iter: 190/190
--
Training Step: 37  | total loss: [1m[32m0.69361[0m[0m | time: 1.245s
[2K
| Adam | epoch: 007 | loss: 0.69361 - acc: 0.4964 -- iter: 032/190
[A[ATraining Step: 38  | total loss: [1m[32m0.69448[0m[0m | time: 2.430s
[2K
| Adam | epoch: 007 | loss: 0.69448 - acc: 0.4727 -- iter: 064/190
[A[ATraining Step: 39  | total loss: [1m[32m0.69408[0m[0m | time: 3.744s
[2K
| Adam | epoch: 007 | loss: 0.69408 - acc: 0.4779 -- iter: 096/190
[A[ATraining Step: 40  | total loss: [1m[32m0.69299[0m[0m | time: 4.870s
[2K
| Adam | epoch: 007 | loss: 0.69299 - acc: 0.5113 -- iter: 128/190
[A[ATraining Step: 41  | total loss: [1m[32m0.69298[0m[0m | time: 6.083s
[2K
| Adam | epoch: 007 | loss: 0.69298 - acc: 0.4978 -- iter: 160/190
[A[ATraining Step: 42  | total loss: [1m[32m0.69195[0m[0m | time: 8.259s
[2K
| Adam | epoch: 007 | loss: 0.69195 - acc: 0.5102 | val_loss: 0.68569 - val_acc: 0.5500 -- iter: 190/190
--
Training Step: 43  | total loss: [1m[32m0.69071[0m[0m | time: 1.592s
[2K
| Adam | epoch: 008 | loss: 0.69071 - acc: 0.5201 -- iter: 032/190
[A[ATraining Step: 44  | total loss: [1m[32m0.68901[0m[0m | time: 9.196s
[2K
| Adam | epoch: 008 | loss: 0.68901 - acc: 0.5275 -- iter: 064/190
[A[ATraining Step: 45  | total loss: [1m[32m0.68988[0m[0m | time: 10.704s
[2K
| Adam | epoch: 008 | loss: 0.68988 - acc: 0.5175 -- iter: 096/190
[A[ATraining Step: 46  | total loss: [1m[32m0.68618[0m[0m | time: 18.160s
[2K
| Adam | epoch: 008 | loss: 0.68618 - acc: 0.5302 -- iter: 128/190
[A[ATraining Step: 47  | total loss: [1m[32m0.68243[0m[0m | time: 32.653s
[2K
| Adam | epoch: 008 | loss: 0.68243 - acc: 0.5406 -- iter: 160/190
[A[ATraining Step: 48  | total loss: [1m[32m0.68953[0m[0m | time: 51.850s
[2K
| Adam | epoch: 008 | loss: 0.68953 - acc: 0.5090 | val_loss: 0.66735 - val_acc: 0.6167 -- iter: 190/190
--
Training Step: 49  | total loss: [1m[32m0.68045[0m[0m | time: 1.318s
[2K
| Adam | epoch: 009 | loss: 0.68045 - acc: 0.5444 -- iter: 032/190
[A[ATraining Step: 50  | total loss: [1m[32m0.67204[0m[0m | time: 2.523s
[2K
| Adam | epoch: 009 | loss: 0.67204 - acc: 0.5737 -- iter: 064/190
[A[ATraining Step: 51  | total loss: [1m[32m0.67152[0m[0m | time: 3.696s
[2K
| Adam | epoch: 009 | loss: 0.67152 - acc: 0.5768 -- iter: 096/190
[A[ATraining Step: 52  | total loss: [1m[32m0.67121[0m[0m | time: 4.931s
[2K
| Adam | epoch: 009 | loss: 0.67121 - acc: 0.5699 -- iter: 128/190
[A[ATraining Step: 53  | total loss: [1m[32m0.66061[0m[0m | time: 6.245s
[2K
| Adam | epoch: 009 | loss: 0.66061 - acc: 0.6103 -- iter: 160/190
[A[ATraining Step: 54  | total loss: [1m[32m0.65410[0m[0m | time: 8.673s
[2K
| Adam | epoch: 009 | loss: 0.65410 - acc: 0.6261 | val_loss: 0.64291 - val_acc: 0.6500 -- iter: 190/190
--
Training Step: 55  | total loss: [1m[32m0.63984[0m[0m | time: 1.280s
[2K
| Adam | epoch: 010 | loss: 0.63984 - acc: 0.6482 -- iter: 032/190
[A[ATraining Step: 56  | total loss: [1m[32m0.62869[0m[0m | time: 2.612s
[2K
| Adam | epoch: 010 | loss: 0.62869 - acc: 0.6743 -- iter: 064/190
[A[ATraining Step: 57  | total loss: [1m[32m0.60940[0m[0m | time: 3.900s
[2K
| Adam | epoch: 010 | loss: 0.60940 - acc: 0.6732 -- iter: 096/190
[A[ATraining Step: 58  | total loss: [1m[32m0.61817[0m[0m | time: 5.250s
[2K
| Adam | epoch: 010 | loss: 0.61817 - acc: 0.6752 -- iter: 128/190
[A[ATraining Step: 59  | total loss: [1m[32m0.61763[0m[0m | time: 6.913s
[2K
| Adam | epoch: 010 | loss: 0.61763 - acc: 0.6852 -- iter: 160/190
[A[ATraining Step: 60  | total loss: [1m[32m0.59050[0m[0m | time: 9.345s
[2K
| Adam | epoch: 010 | loss: 0.59050 - acc: 0.7103 | val_loss: 0.57800 - val_acc: 0.6333 -- iter: 190/190
--
Training Step: 61  | total loss: [1m[32m0.60127[0m[0m | time: 0.853s
[2K
| Adam | epoch: 011 | loss: 0.60127 - acc: 0.7033 -- iter: 032/190
[A[ATraining Step: 62  | total loss: [1m[32m0.58369[0m[0m | time: 1.859s
[2K
| Adam | epoch: 011 | loss: 0.58369 - acc: 0.7133 -- iter: 064/190
[A[ATraining Step: 63  | total loss: [1m[32m0.59692[0m[0m | time: 2.764s
[2K
| Adam | epoch: 011 | loss: 0.59692 - acc: 0.7032 -- iter: 096/190
[A[ATraining Step: 64  | total loss: [1m[32m0.59973[0m[0m | time: 3.709s
[2K
| Adam | epoch: 011 | loss: 0.59973 - acc: 0.6944 -- iter: 128/190
[A[ATraining Step: 65  | total loss: [1m[32m0.59071[0m[0m | time: 4.675s
[2K
| Adam | epoch: 011 | loss: 0.59071 - acc: 0.7051 -- iter: 160/190
[A[ATraining Step: 66  | total loss: [1m[32m0.61319[0m[0m | time: 6.831s
[2K
| Adam | epoch: 011 | loss: 0.61319 - acc: 0.6764 | val_loss: 0.68769 - val_acc: 0.4667 -- iter: 190/190
--
Training Step: 67  | total loss: [1m[32m0.61794[0m[0m | time: 1.150s
[2K
| Adam | epoch: 012 | loss: 0.61794 - acc: 0.6515 -- iter: 032/190
[A[ATraining Step: 68  | total loss: [1m[32m0.61963[0m[0m | time: 1.983s
[2K
| Adam | epoch: 012 | loss: 0.61963 - acc: 0.6372 -- iter: 064/190
[A[ATraining Step: 69  | total loss: [1m[32m0.62417[0m[0m | time: 2.748s
[2K
| Adam | epoch: 012 | loss: 0.62417 - acc: 0.6285 -- iter: 096/190
[A[ATraining Step: 70  | total loss: [1m[32m0.61967[0m[0m | time: 3.645s
[2K
| Adam | epoch: 012 | loss: 0.61967 - acc: 0.6483 -- iter: 128/190
[A[ATraining Step: 71  | total loss: [1m[32m0.61487[0m[0m | time: 4.642s
[2K
| Adam | epoch: 012 | loss: 0.61487 - acc: 0.6580 -- iter: 160/190
[A[ATraining Step: 72  | total loss: [1m[32m0.61186[0m[0m | time: 6.668s
[2K
| Adam | epoch: 012 | loss: 0.61186 - acc: 0.6648 | val_loss: 0.56380 - val_acc: 0.7000 -- iter: 190/190
--
Training Step: 73  | total loss: [1m[32m0.60414[0m[0m | time: 1.056s
[2K
| Adam | epoch: 013 | loss: 0.60414 - acc: 0.6743 -- iter: 032/190
[A[ATraining Step: 74  | total loss: [1m[32m0.59027[0m[0m | time: 2.074s
[2K
| Adam | epoch: 013 | loss: 0.59027 - acc: 0.6963 -- iter: 064/190
[A[ATraining Step: 75  | total loss: [1m[32m0.58310[0m[0m | time: 3.115s
[2K
| Adam | epoch: 013 | loss: 0.58310 - acc: 0.7021 -- iter: 096/190
[A[ATraining Step: 76  | total loss: [1m[32m0.56522[0m[0m | time: 4.278s
[2K
| Adam | epoch: 013 | loss: 0.56522 - acc: 0.7207 -- iter: 128/190
[A[ATraining Step: 77  | total loss: [1m[32m0.54817[0m[0m | time: 6.023s
[2K
| Adam | epoch: 013 | loss: 0.54817 - acc: 0.7361 -- iter: 160/190
[A[ATraining Step: 78  | total loss: [1m[32m0.52766[0m[0m | time: 8.769s
[2K
| Adam | epoch: 013 | loss: 0.52766 - acc: 0.7533 | val_loss: 0.49412 - val_acc: 0.7500 -- iter: 190/190
--
Training Step: 79  | total loss: [1m[32m0.51647[0m[0m | time: 7.892s
[2K
| Adam | epoch: 014 | loss: 0.51647 - acc: 0.7594 -- iter: 032/190
[A[ATraining Step: 80  | total loss: [1m[32m0.49528[0m[0m | time: 9.100s
[2K
| Adam | epoch: 014 | loss: 0.49528 - acc: 0.7712 -- iter: 064/190
[A[ATraining Step: 81  | total loss: [1m[32m0.47674[0m[0m | time: 10.356s
[2K
| Adam | epoch: 014 | loss: 0.47674 - acc: 0.7849 -- iter: 096/190
[A[ATraining Step: 82  | total loss: [1m[32m0.46478[0m[0m | time: 11.617s
[2K
| Adam | epoch: 014 | loss: 0.46478 - acc: 0.7939 -- iter: 128/190
[A[ATraining Step: 83  | total loss: [1m[32m0.45028[0m[0m | time: 12.693s
[2K
| Adam | epoch: 014 | loss: 0.45028 - acc: 0.8051 -- iter: 160/190
[A[ATraining Step: 84  | total loss: [1m[32m0.42711[0m[0m | time: 14.944s
[2K
| Adam | epoch: 014 | loss: 0.42711 - acc: 0.8179 | val_loss: 0.49560 - val_acc: 0.7500 -- iter: 190/190
--
Training Step: 85  | total loss: [1m[32m0.39859[0m[0m | time: 1.309s
[2K
| Adam | epoch: 015 | loss: 0.39859 - acc: 0.8328 -- iter: 032/190
[A[ATraining Step: 86  | total loss: [1m[32m0.39734[0m[0m | time: 2.876s
[2K
| Adam | epoch: 015 | loss: 0.39734 - acc: 0.8339 -- iter: 064/190
[A[ATraining Step: 87  | total loss: [1m[32m0.41482[0m[0m | time: 4.199s
[2K
| Adam | epoch: 015 | loss: 0.41482 - acc: 0.8349 -- iter: 096/190
[A[ATraining Step: 88  | total loss: [1m[32m0.39879[0m[0m | time: 5.566s
[2K
| Adam | epoch: 015 | loss: 0.39879 - acc: 0.8420 -- iter: 128/190
[A[ATraining Step: 89  | total loss: [1m[32m0.39264[0m[0m | time: 7.063s
[2K
| Adam | epoch: 015 | loss: 0.39264 - acc: 0.8422 -- iter: 160/190
[A[ATraining Step: 90  | total loss: [1m[32m0.37144[0m[0m | time: 9.512s
[2K
| Adam | epoch: 015 | loss: 0.37144 - acc: 0.8455 | val_loss: 0.74236 - val_acc: 0.7167 -- iter: 190/190
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.9191919191919192
Validation AUPRC:0.9088941696919901
Test AUC:0.8754406580493538
Test AUPRC:0.9361440495723472
BestTestF1Score	0.81	0.52	0.77	0.83	0.78	29	6	17	8	0.93
BestTestMCCScore	0.81	0.52	0.77	0.83	0.78	29	6	17	8	0.93
BestTestAccuracyScore	0.82	0.61	0.8	0.9	0.76	28	3	20	9	0.98
BestValidationF1Score	0.84	0.7	0.85	0.8	0.89	24	6	27	3	0.93
BestValidationMCC	0.84	0.7	0.85	0.8	0.89	24	6	27	3	0.93
BestValidationAccuracy	0.82	0.7	0.85	0.91	0.74	20	2	31	7	0.98
TestPredictions (Threshold:0.93)
CHEMBL305568,TP,ACT,0.9900000095367432	CHEMBL293856,TP,ACT,1.0	CHEMBL2036322,TP,ACT,1.0	CHEMBL1933727,TP,ACT,1.0	CHEMBL604897,TP,ACT,0.9900000095367432	CHEMBL304887,TP,ACT,0.9900000095367432	CHEMBL47138,TN,INACT,0.44999998807907104	CHEMBL1929546,TP,ACT,1.0	CHEMBL297139,FP,INACT,0.9800000190734863	CHEMBL1765668,TN,INACT,0.5299999713897705	CHEMBL432522,TP,ACT,0.9800000190734863	CHEMBL298026,TN,INACT,0.7799999713897705	CHEMBL2036308,TP,ACT,0.9900000095367432	CHEMBL90491,TN,INACT,0.15000000596046448	CHEMBL220820,TN,INACT,0.7900000214576721	CHEMBL223744,FP,INACT,0.949999988079071	CHEMBL3586309,FN,ACT,0.5699999928474426	CHEMBL594423,FN,ACT,0.44999998807907104	CHEMBL393753,FP,INACT,0.9300000071525574	CHEMBL64542,TP,ACT,0.9900000095367432	CHEMBL1933725,TP,ACT,0.9800000190734863	CHEMBL489310,TN,INACT,0.18000000715255737	CHEMBL340501,TN,INACT,0.38999998569488525	CHEMBL359564,TP,ACT,0.9900000095367432	CHEMBL1207972,FN,ACT,0.6100000143051147	CHEMBL595632,TP,ACT,0.9900000095367432	CHEMBL83,TN,INACT,0.4300000071525574	CHEMBL603625,TP,ACT,1.0	CHEMBL128422,TN,INACT,0.8500000238418579	CHEMBL185087,FN,ACT,0.6700000166893005	CHEMBL439790,TN,INACT,0.6700000166893005	CHEMBL428794,FP,INACT,0.9800000190734863	CHEMBL362541,TP,ACT,0.9800000190734863	CHEMBL2037290,FN,ACT,0.47999998927116394	CHEMBL1957435,TP,ACT,0.9900000095367432	CHEMBL399000,FP,INACT,0.9700000286102295	CHEMBL1813116,TP,ACT,0.9900000095367432	CHEMBL565799,TN,INACT,0.7300000190734863	CHEMBL589411,FN,ACT,0.5600000023841858	CHEMBL64423,FN,ACT,0.7699999809265137	CHEMBL64072,TP,ACT,1.0	CHEMBL180343,TN,INACT,0.5400000214576721	CHEMBL185251,TP,ACT,0.9900000095367432	CHEMBL185369,TP,ACT,0.9900000095367432	CHEMBL1819611,TP,ACT,0.9900000095367432	CHEMBL590582,TN,INACT,0.4300000071525574	CHEMBL2036311,TP,ACT,0.9900000095367432	CHEMBL2036312,TP,ACT,1.0	CHEMBL363984,TP,ACT,0.9900000095367432	CHEMBL2037292,TP,ACT,1.0	CHEMBL2036319,TP,ACT,0.9900000095367432	CHEMBL548,TP,ACT,0.9700000286102295	CHEMBL235797,TN,INACT,0.11999999731779099	CHEMBL60555,TP,ACT,0.9900000095367432	CHEMBL2164609,TN,INACT,0.8899999856948853	CHEMBL127204,FP,INACT,0.9800000190734863	CHEMBL447178,TN,INACT,0.28999999165534973	CHEMBL173039,TN,INACT,0.4699999988079071	CHEMBL593260,FN,ACT,0.6700000166893005	CHEMBL365243,TP,ACT,0.9800000190734863	

