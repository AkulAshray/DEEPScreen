CNNModel CHEMBL4956 adam 0.001 30 128 0 0.6 False True
Number of active compounds :	108
Number of inactive compounds :	108
---------------------------------
Run id: CNNModel_CHEMBL4956_adam_0.001_30_128_0_0.6_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL4956_adam_0.001_30_128_0.6_True/
---------------------------------
Training samples: 86
Validation samples: 28
--
Training Step: 1  | time: 0.827s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 32/86
[A[ATraining Step: 2  | total loss: [1m[32m0.62427[0m[0m | time: 1.675s
[2K
| Adam | epoch: 001 | loss: 0.62427 - acc: 0.3937 -- iter: 64/86
[A[ATraining Step: 3  | total loss: [1m[32m0.67960[0m[0m | time: 3.246s
[2K
| Adam | epoch: 001 | loss: 0.67960 - acc: 0.5063 | val_loss: 0.69472 - val_acc: 0.5357 -- iter: 86/86
--
Training Step: 4  | total loss: [1m[32m0.68293[0m[0m | time: 0.482s
[2K
| Adam | epoch: 002 | loss: 0.68293 - acc: 0.5697 -- iter: 32/86
[A[ATraining Step: 5  | total loss: [1m[32m0.68139[0m[0m | time: 1.130s
[2K
| Adam | epoch: 002 | loss: 0.68139 - acc: 0.5844 -- iter: 64/86
[A[ATraining Step: 6  | total loss: [1m[32m0.72130[0m[0m | time: 2.775s
[2K
| Adam | epoch: 002 | loss: 0.72130 - acc: 0.5101 | val_loss: 0.68980 - val_acc: 0.5357 -- iter: 86/86
--
Training Step: 7  | total loss: [1m[32m0.69944[0m[0m | time: 0.652s
[2K
| Adam | epoch: 003 | loss: 0.69944 - acc: 0.5415 -- iter: 32/86
[A[ATraining Step: 8  | total loss: [1m[32m0.68988[0m[0m | time: 1.219s
[2K
| Adam | epoch: 003 | loss: 0.68988 - acc: 0.5693 -- iter: 64/86
[A[ATraining Step: 9  | total loss: [1m[32m0.68768[0m[0m | time: 3.083s
[2K
| Adam | epoch: 003 | loss: 0.68768 - acc: 0.5807 | val_loss: 0.69110 - val_acc: 0.5357 -- iter: 86/86
--
Training Step: 10  | total loss: [1m[32m0.69061[0m[0m | time: 0.783s
[2K
| Adam | epoch: 004 | loss: 0.69061 - acc: 0.5404 -- iter: 32/86
[A[ATraining Step: 11  | total loss: [1m[32m0.69129[0m[0m | time: 1.349s
[2K
| Adam | epoch: 004 | loss: 0.69129 - acc: 0.5361 -- iter: 64/86
[A[ATraining Step: 12  | total loss: [1m[32m0.69229[0m[0m | time: 2.932s
[2K
| Adam | epoch: 004 | loss: 0.69229 - acc: 0.5198 | val_loss: 0.69157 - val_acc: 0.5357 -- iter: 86/86
--
Training Step: 13  | total loss: [1m[32m0.69277[0m[0m | time: 0.806s
[2K
| Adam | epoch: 005 | loss: 0.69277 - acc: 0.5113 -- iter: 32/86
[A[ATraining Step: 14  | total loss: [1m[32m0.69474[0m[0m | time: 1.650s
[2K
| Adam | epoch: 005 | loss: 0.69474 - acc: 0.4683 -- iter: 64/86
[A[ATraining Step: 15  | total loss: [1m[32m0.69132[0m[0m | time: 3.303s
[2K
| Adam | epoch: 005 | loss: 0.69132 - acc: 0.5541 | val_loss: 0.69170 - val_acc: 0.5357 -- iter: 86/86
--
Training Step: 16  | total loss: [1m[32m0.69057[0m[0m | time: 0.619s
[2K
| Adam | epoch: 006 | loss: 0.69057 - acc: 0.5679 -- iter: 32/86
[A[ATraining Step: 17  | total loss: [1m[32m0.69003[0m[0m | time: 1.500s
[2K
| Adam | epoch: 006 | loss: 0.69003 - acc: 0.5762 -- iter: 64/86
[A[ATraining Step: 18  | total loss: [1m[32m0.69073[0m[0m | time: 3.378s
[2K
| Adam | epoch: 006 | loss: 0.69073 - acc: 0.5606 | val_loss: 0.69123 - val_acc: 0.5357 -- iter: 86/86
--
Training Step: 19  | total loss: [1m[32m0.69162[0m[0m | time: 0.818s
[2K
| Adam | epoch: 007 | loss: 0.69162 - acc: 0.5404 -- iter: 32/86
[A[ATraining Step: 20  | total loss: [1m[32m0.69125[0m[0m | time: 1.611s
[2K
| Adam | epoch: 007 | loss: 0.69125 - acc: 0.5420 -- iter: 64/86
[A[ATraining Step: 21  | total loss: [1m[32m0.69105[0m[0m | time: 3.721s
[2K
| Adam | epoch: 007 | loss: 0.69105 - acc: 0.5431 | val_loss: 0.69044 - val_acc: 0.5357 -- iter: 86/86
--
Training Step: 22  | total loss: [1m[32m0.68971[0m[0m | time: 0.670s
[2K
| Adam | epoch: 008 | loss: 0.68971 - acc: 0.5583 -- iter: 32/86
[A[ATraining Step: 23  | total loss: [1m[32m0.69159[0m[0m | time: 1.131s
[2K
| Adam | epoch: 008 | loss: 0.69159 - acc: 0.5323 -- iter: 64/86
[A[ATraining Step: 24  | total loss: [1m[32m0.69109[0m[0m | time: 2.733s
[2K
| Adam | epoch: 008 | loss: 0.69109 - acc: 0.5360 | val_loss: 0.68936 - val_acc: 0.5357 -- iter: 86/86
--
Training Step: 25  | total loss: [1m[32m0.69054[0m[0m | time: 1.060s
[2K
| Adam | epoch: 009 | loss: 0.69054 - acc: 0.5386 -- iter: 32/86
[A[ATraining Step: 26  | total loss: [1m[32m0.68957[0m[0m | time: 2.134s
[2K
| Adam | epoch: 009 | loss: 0.68957 - acc: 0.5449 -- iter: 64/86
[A[ATraining Step: 27  | total loss: [1m[32m0.69049[0m[0m | time: 3.748s
[2K
| Adam | epoch: 009 | loss: 0.69049 - acc: 0.5334 | val_loss: 0.68797 - val_acc: 0.5357 -- iter: 86/86
--
Training Step: 28  | total loss: [1m[32m0.69424[0m[0m | time: 0.644s
[2K
| Adam | epoch: 010 | loss: 0.69424 - acc: 0.5023 -- iter: 32/86
[A[ATraining Step: 29  | total loss: [1m[32m0.69678[0m[0m | time: 1.480s
[2K
| Adam | epoch: 010 | loss: 0.69678 - acc: 0.4796 -- iter: 64/86
[A[ATraining Step: 30  | total loss: [1m[32m0.69167[0m[0m | time: 3.463s
[2K
| Adam | epoch: 010 | loss: 0.69167 - acc: 0.5215 | val_loss: 0.68779 - val_acc: 0.5357 -- iter: 86/86
--
Training Step: 31  | total loss: [1m[32m0.69178[0m[0m | time: 0.579s
[2K
| Adam | epoch: 011 | loss: 0.69178 - acc: 0.5165 -- iter: 32/86
[A[ATraining Step: 32  | total loss: [1m[32m0.68850[0m[0m | time: 1.229s
[2K
| Adam | epoch: 011 | loss: 0.68850 - acc: 0.5435 -- iter: 64/86
[A[ATraining Step: 33  | total loss: [1m[32m0.68569[0m[0m | time: 3.115s
[2K
| Adam | epoch: 011 | loss: 0.68569 - acc: 0.5639 | val_loss: 0.68335 - val_acc: 0.5357 -- iter: 86/86
--
Training Step: 34  | total loss: [1m[32m0.68569[0m[0m | time: 0.875s
[2K
| Adam | epoch: 012 | loss: 0.68569 - acc: 0.5569 -- iter: 32/86
[A[ATraining Step: 35  | total loss: [1m[32m0.68764[0m[0m | time: 1.470s
[2K
| Adam | epoch: 012 | loss: 0.68764 - acc: 0.5384 -- iter: 64/86
[A[ATraining Step: 36  | total loss: [1m[32m0.68971[0m[0m | time: 3.057s
[2K
| Adam | epoch: 012 | loss: 0.68971 - acc: 0.5213 | val_loss: 0.67797 - val_acc: 0.5357 -- iter: 86/86
--
Training Step: 37  | total loss: [1m[32m0.69088[0m[0m | time: 1.104s
[2K
| Adam | epoch: 013 | loss: 0.69088 - acc: 0.5079 -- iter: 32/86
[A[ATraining Step: 38  | total loss: [1m[32m0.68247[0m[0m | time: 2.113s
[2K
| Adam | epoch: 013 | loss: 0.68247 - acc: 0.5492 -- iter: 64/86
[A[ATraining Step: 39  | total loss: [1m[32m0.68585[0m[0m | time: 3.837s
[2K
| Adam | epoch: 013 | loss: 0.68585 - acc: 0.5218 | val_loss: 0.66600 - val_acc: 0.5357 -- iter: 86/86
--
Training Step: 40  | total loss: [1m[32m0.68506[0m[0m | time: 0.521s
[2K
| Adam | epoch: 014 | loss: 0.68506 - acc: 0.5177 -- iter: 32/86
[A[ATraining Step: 41  | total loss: [1m[32m0.68367[0m[0m | time: 1.342s
[2K
| Adam | epoch: 014 | loss: 0.68367 - acc: 0.5145 -- iter: 64/86
[A[ATraining Step: 42  | total loss: [1m[32m0.67984[0m[0m | time: 3.160s
[2K
| Adam | epoch: 014 | loss: 0.67984 - acc: 0.5175 | val_loss: 0.63938 - val_acc: 0.5357 -- iter: 86/86
--
Training Step: 43  | total loss: [1m[32m0.67300[0m[0m | time: 0.571s
[2K
| Adam | epoch: 015 | loss: 0.67300 - acc: 0.5254 -- iter: 32/86
[A[ATraining Step: 44  | total loss: [1m[32m0.66847[0m[0m | time: 1.129s
[2K
| Adam | epoch: 015 | loss: 0.66847 - acc: 0.5289 -- iter: 64/86
[A[ATraining Step: 45  | total loss: [1m[32m0.66141[0m[0m | time: 2.986s
[2K
| Adam | epoch: 015 | loss: 0.66141 - acc: 0.5317 | val_loss: 0.60129 - val_acc: 0.5357 -- iter: 86/86
--
Training Step: 46  | total loss: [1m[32m0.65696[0m[0m | time: 1.093s
[2K
| Adam | epoch: 016 | loss: 0.65696 - acc: 0.5160 -- iter: 32/86
[A[ATraining Step: 47  | total loss: [1m[32m0.64222[0m[0m | time: 1.815s
[2K
| Adam | epoch: 016 | loss: 0.64222 - acc: 0.5338 -- iter: 64/86
[A[ATraining Step: 48  | total loss: [1m[32m0.62232[0m[0m | time: 3.553s
[2K
| Adam | epoch: 016 | loss: 0.62232 - acc: 0.5357 | val_loss: 0.50118 - val_acc: 0.6071 -- iter: 86/86
--
Training Step: 49  | total loss: [1m[32m0.60394[0m[0m | time: 0.744s
[2K
| Adam | epoch: 017 | loss: 0.60394 - acc: 0.5372 -- iter: 32/86
[A[ATraining Step: 50  | total loss: [1m[32m0.58815[0m[0m | time: 1.545s
[2K
| Adam | epoch: 017 | loss: 0.58815 - acc: 0.5606 -- iter: 64/86
[A[ATraining Step: 51  | total loss: [1m[32m0.57011[0m[0m | time: 3.274s
[2K
| Adam | epoch: 017 | loss: 0.57011 - acc: 0.5942 | val_loss: 0.45100 - val_acc: 0.8929 -- iter: 86/86
--
Training Step: 52  | total loss: [1m[32m0.56287[0m[0m | time: 0.471s
[2K
| Adam | epoch: 018 | loss: 0.56287 - acc: 0.6278 -- iter: 32/86
[A[ATraining Step: 53  | total loss: [1m[32m0.53942[0m[0m | time: 1.124s
[2K
| Adam | epoch: 018 | loss: 0.53942 - acc: 0.6626 -- iter: 64/86
[A[ATraining Step: 54  | total loss: [1m[32m0.52558[0m[0m | time: 2.801s
[2K
| Adam | epoch: 018 | loss: 0.52558 - acc: 0.6934 | val_loss: 0.39163 - val_acc: 0.8929 -- iter: 86/86
--
Training Step: 55  | total loss: [1m[32m0.50272[0m[0m | time: 0.861s
[2K
| Adam | epoch: 019 | loss: 0.50272 - acc: 0.7238 -- iter: 32/86
[A[ATraining Step: 56  | total loss: [1m[32m0.49469[0m[0m | time: 1.539s
[2K
| Adam | epoch: 019 | loss: 0.49469 - acc: 0.7307 -- iter: 64/86
[A[ATraining Step: 57  | total loss: [1m[32m0.48877[0m[0m | time: 3.548s
[2K
| Adam | epoch: 019 | loss: 0.48877 - acc: 0.7428 | val_loss: 0.65014 - val_acc: 0.7500 -- iter: 86/86
--
Training Step: 58  | total loss: [1m[32m0.46029[0m[0m | time: 0.931s
[2K
| Adam | epoch: 020 | loss: 0.46029 - acc: 0.7694 -- iter: 32/86
[A[ATraining Step: 59  | total loss: [1m[32m0.44876[0m[0m | time: 1.681s
[2K
| Adam | epoch: 020 | loss: 0.44876 - acc: 0.7836 -- iter: 64/86
[A[ATraining Step: 60  | total loss: [1m[32m0.41825[0m[0m | time: 3.375s
[2K
| Adam | epoch: 020 | loss: 0.41825 - acc: 0.8062 | val_loss: 0.32963 - val_acc: 0.8929 -- iter: 86/86
--
Training Step: 61  | total loss: [1m[32m0.40187[0m[0m | time: 0.919s
[2K
| Adam | epoch: 021 | loss: 0.40187 - acc: 0.8196 -- iter: 32/86
[A[ATraining Step: 62  | total loss: [1m[32m0.37128[0m[0m | time: 1.587s
[2K
| Adam | epoch: 021 | loss: 0.37128 - acc: 0.8388 -- iter: 64/86
[A[ATraining Step: 63  | total loss: [1m[32m0.41347[0m[0m | time: 3.042s
[2K
| Adam | epoch: 021 | loss: 0.41347 - acc: 0.8236 | val_loss: 0.40379 - val_acc: 0.8571 -- iter: 86/86
--
Training Step: 64  | total loss: [1m[32m0.39204[0m[0m | time: 0.707s
[2K
| Adam | epoch: 022 | loss: 0.39204 - acc: 0.8229 -- iter: 32/86
[A[ATraining Step: 65  | total loss: [1m[32m0.38208[0m[0m | time: 1.840s
[2K
| Adam | epoch: 022 | loss: 0.38208 - acc: 0.8335 -- iter: 64/86
[A[ATraining Step: 66  | total loss: [1m[32m0.40392[0m[0m | time: 3.678s
[2K
| Adam | epoch: 022 | loss: 0.40392 - acc: 0.8272 | val_loss: 0.40848 - val_acc: 0.8929 -- iter: 86/86
--
Training Step: 67  | total loss: [1m[32m0.38647[0m[0m | time: 0.603s
[2K
| Adam | epoch: 023 | loss: 0.38647 - acc: 0.8367 -- iter: 32/86
[A[ATraining Step: 68  | total loss: [1m[32m0.37503[0m[0m | time: 1.220s
[2K
| Adam | epoch: 023 | loss: 0.37503 - acc: 0.8399 -- iter: 64/86
[A[ATraining Step: 69  | total loss: [1m[32m0.38470[0m[0m | time: 3.316s
[2K
| Adam | epoch: 023 | loss: 0.38470 - acc: 0.8320 | val_loss: 0.28668 - val_acc: 0.8929 -- iter: 86/86
--
Training Step: 70  | total loss: [1m[32m0.37120[0m[0m | time: 0.847s
[2K
| Adam | epoch: 024 | loss: 0.37120 - acc: 0.8370 -- iter: 32/86
[A[ATraining Step: 71  | total loss: [1m[32m0.35110[0m[0m | time: 1.593s
[2K
| Adam | epoch: 024 | loss: 0.35110 - acc: 0.8449 -- iter: 64/86
[A[ATraining Step: 72  | total loss: [1m[32m0.32053[0m[0m | time: 3.367s
[2K
| Adam | epoch: 024 | loss: 0.32053 - acc: 0.8623 | val_loss: 0.34636 - val_acc: 0.8214 -- iter: 86/86
--
Training Step: 73  | total loss: [1m[32m0.30461[0m[0m | time: 0.856s
[2K
| Adam | epoch: 025 | loss: 0.30461 - acc: 0.8726 -- iter: 32/86
[A[ATraining Step: 74  | total loss: [1m[32m0.31911[0m[0m | time: 1.719s
[2K
| Adam | epoch: 025 | loss: 0.31911 - acc: 0.8694 -- iter: 64/86
[A[ATraining Step: 75  | total loss: [1m[32m0.29997[0m[0m | time: 3.499s
[2K
| Adam | epoch: 025 | loss: 0.29997 - acc: 0.8802 | val_loss: 0.54697 - val_acc: 0.8214 -- iter: 86/86
--
Training Step: 76  | total loss: [1m[32m0.29176[0m[0m | time: 0.475s
[2K
| Adam | epoch: 026 | loss: 0.29176 - acc: 0.8833 -- iter: 32/86
[A[ATraining Step: 77  | total loss: [1m[32m0.27341[0m[0m | time: 1.143s
[2K
| Adam | epoch: 026 | loss: 0.27341 - acc: 0.8908 -- iter: 64/86
[A[ATraining Step: 78  | total loss: [1m[32m0.27979[0m[0m | time: 2.822s
[2K
| Adam | epoch: 026 | loss: 0.27979 - acc: 0.8924 | val_loss: 0.29745 - val_acc: 0.8929 -- iter: 86/86
--
Training Step: 79  | total loss: [1m[32m0.27412[0m[0m | time: 0.558s
[2K
| Adam | epoch: 027 | loss: 0.27412 - acc: 0.8906 -- iter: 32/86
[A[ATraining Step: 80  | total loss: [1m[32m0.30274[0m[0m | time: 1.144s
[2K
| Adam | epoch: 027 | loss: 0.30274 - acc: 0.8879 -- iter: 64/86
[A[ATraining Step: 81  | total loss: [1m[32m0.28873[0m[0m | time: 2.815s
[2K
| Adam | epoch: 027 | loss: 0.28873 - acc: 0.8946 | val_loss: 0.52117 - val_acc: 0.7857 -- iter: 86/86
--
Training Step: 82  | total loss: [1m[32m0.32527[0m[0m | time: 0.849s
[2K
| Adam | epoch: 028 | loss: 0.32527 - acc: 0.8801 -- iter: 32/86
[A[ATraining Step: 83  | total loss: [1m[32m0.32271[0m[0m | time: 1.464s
[2K
| Adam | epoch: 028 | loss: 0.32271 - acc: 0.8828 -- iter: 64/86
[A[ATraining Step: 84  | total loss: [1m[32m0.31573[0m[0m | time: 2.977s
[2K
| Adam | epoch: 028 | loss: 0.31573 - acc: 0.8854 | val_loss: 0.44862 - val_acc: 0.8571 -- iter: 86/86
--
Training Step: 85  | total loss: [1m[32m0.29398[0m[0m | time: 0.639s
[2K
| Adam | epoch: 029 | loss: 0.29398 - acc: 0.8969 -- iter: 32/86
[A[ATraining Step: 86  | total loss: [1m[32m0.27757[0m[0m | time: 1.313s
[2K
| Adam | epoch: 029 | loss: 0.27757 - acc: 0.9072 -- iter: 64/86
[A[ATraining Step: 87  | total loss: [1m[32m0.28304[0m[0m | time: 2.830s
[2K
| Adam | epoch: 029 | loss: 0.28304 - acc: 0.9040 | val_loss: 0.49634 - val_acc: 0.8571 -- iter: 86/86
--
Training Step: 88  | total loss: [1m[32m0.32925[0m[0m | time: 0.616s
[2K
| Adam | epoch: 030 | loss: 0.32925 - acc: 0.8954 -- iter: 32/86
[A[ATraining Step: 89  | total loss: [1m[32m0.30998[0m[0m | time: 1.462s
[2K
| Adam | epoch: 030 | loss: 0.30998 - acc: 0.9058 -- iter: 64/86
[A[ATraining Step: 90  | total loss: [1m[32m0.28706[0m[0m | time: 3.304s
[2K
| Adam | epoch: 030 | loss: 0.28706 - acc: 0.9153 | val_loss: 0.25023 - val_acc: 0.8571 -- iter: 86/86
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.9538461538461539
Validation AUPRC:0.9704545454545455
Test AUC:0.8687499999999999
Test AUPRC:0.8950892857142857
BestTestF1Score	0.88	0.82	0.93	0.88	0.88	7	1	19	1	0.74
BestTestMCCScore	0.88	0.82	0.93	0.88	0.88	7	1	19	1	0.74
BestTestAccuracyScore	0.88	0.82	0.93	0.88	0.88	7	1	19	1	0.74
BestValidationF1Score	0.93	0.87	0.93	1.0	0.87	13	0	13	2	0.74
BestValidationMCC	0.93	0.87	0.93	1.0	0.87	13	0	13	2	0.74
BestValidationAccuracy	0.93	0.87	0.93	1.0	0.87	13	0	13	2	0.74
TestPredictions (Threshold:0.74)
CHEMBL3647582,TP,ACT,1.0	CHEMBL134752,TN,INACT,0.14000000059604645	CHEMBL3647534,TP,ACT,1.0	CHEMBL3274623,TN,INACT,0.14000000059604645	CHEMBL2337889,TN,INACT,0.12999999523162842	CHEMBL3274975,TN,INACT,0.10999999940395355	CHEMBL2402891,TN,INACT,0.10000000149011612	CHEMBL1947048,TN,INACT,0.12999999523162842	CHEMBL147379,FN,ACT,0.07000000029802322	CHEMBL2297684,TN,INACT,0.09000000357627869	CHEMBL445169,TN,INACT,0.12999999523162842	CHEMBL2062313,TN,INACT,0.1599999964237213	CHEMBL466398,FP,INACT,0.9599999785423279	CHEMBL3647561,TP,ACT,1.0	CHEMBL39071,TN,INACT,0.10999999940395355	CHEMBL3647578,TP,ACT,1.0	CHEMBL3647575,TP,ACT,0.9900000095367432	CHEMBL2402902,TN,INACT,0.10000000149011612	CHEMBL1669092,TN,INACT,0.07000000029802322	CHEMBL498186,TN,INACT,0.11999999731779099	CHEMBL3274624,TN,INACT,0.2199999988079071	CHEMBL2179517,TN,INACT,0.09000000357627869	CHEMBL480798,TN,INACT,0.15000000596046448	CHEMBL508949,TN,INACT,0.14000000059604645	CHEMBL3335594,TN,INACT,0.10000000149011612	CHEMBL342270,TP,ACT,1.0	CHEMBL1681894,TP,ACT,0.9100000262260437	CHEMBL58757,TN,INACT,0.6399999856948853	

