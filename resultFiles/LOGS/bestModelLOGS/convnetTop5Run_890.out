ImageNetInceptionV2 CHEMBL3369 RMSprop 0.0005 15 0 0 0.6 False True
Number of active compounds :	192
Number of inactive compounds :	128
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL3369_RMSprop_0.0005_15_0_0_0.6_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL3369_RMSprop_0.0005_15_0.6/
---------------------------------
Training samples: 188
Validation samples: 59
--
Training Step: 1  | time: 81.457s
[2K
| RMSProp | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/188
[A[ATraining Step: 2  | total loss: [1m[32m0.80054[0m[0m | time: 112.559s
[2K
| RMSProp | epoch: 001 | loss: 0.80054 - acc: 0.2250 -- iter: 064/188
[A[ATraining Step: 3  | total loss: [1m[32m0.75389[0m[0m | time: 145.583s
[2K
| RMSProp | epoch: 001 | loss: 0.75389 - acc: 0.3989 -- iter: 096/188
[A[ATraining Step: 4  | total loss: [1m[32m0.78060[0m[0m | time: 174.233s
[2K
| RMSProp | epoch: 001 | loss: 0.78060 - acc: 0.3810 -- iter: 128/188
[A[ATraining Step: 5  | total loss: [1m[32m0.77995[0m[0m | time: 196.730s
[2K
| RMSProp | epoch: 001 | loss: 0.77995 - acc: 0.3768 -- iter: 160/188
[A[ATraining Step: 6  | total loss: [1m[32m0.74575[0m[0m | time: 214.680s
[2K
| RMSProp | epoch: 001 | loss: 0.74575 - acc: 0.4560 | val_loss: 0.68415 - val_acc: 0.5763 -- iter: 188/188
--
Training Step: 7  | total loss: [1m[32m0.76348[0m[0m | time: 61.942s
[2K
| RMSProp | epoch: 002 | loss: 0.76348 - acc: 0.3753 -- iter: 032/188
[A[ATraining Step: 8  | total loss: [1m[32m0.73094[0m[0m | time: 99.383s
[2K
| RMSProp | epoch: 002 | loss: 0.73094 - acc: 0.4454 -- iter: 064/188
[A[ATraining Step: 9  | total loss: [1m[32m0.74839[0m[0m | time: 134.323s
[2K
| RMSProp | epoch: 002 | loss: 0.74839 - acc: 0.4247 -- iter: 096/188
[A[ATraining Step: 10  | total loss: [1m[32m0.73165[0m[0m | time: 189.490s
[2K
| RMSProp | epoch: 002 | loss: 0.73165 - acc: 0.4936 -- iter: 128/188
[A[ATraining Step: 11  | total loss: [1m[32m0.71408[0m[0m | time: 201.234s
[2K
| RMSProp | epoch: 002 | loss: 0.71408 - acc: 0.4966 -- iter: 160/188
[A[ATraining Step: 12  | total loss: [1m[32m0.69270[0m[0m | time: 232.742s
[2K
| RMSProp | epoch: 002 | loss: 0.69270 - acc: 0.5403 | val_loss: 0.68307 - val_acc: 0.5763 -- iter: 188/188
--
Training Step: 13  | total loss: [1m[32m0.70449[0m[0m | time: 43.866s
[2K
| RMSProp | epoch: 003 | loss: 0.70449 - acc: 0.5097 -- iter: 032/188
[A[ATraining Step: 14  | total loss: [1m[32m0.69142[0m[0m | time: 62.077s
[2K
| RMSProp | epoch: 003 | loss: 0.69142 - acc: 0.5641 -- iter: 064/188
[A[ATraining Step: 15  | total loss: [1m[32m0.70424[0m[0m | time: 84.099s
[2K
| RMSProp | epoch: 003 | loss: 0.70424 - acc: 0.5111 -- iter: 096/188
[A[ATraining Step: 16  | total loss: [1m[32m0.70206[0m[0m | time: 93.276s
[2K
| RMSProp | epoch: 003 | loss: 0.70206 - acc: 0.5069 -- iter: 128/188
[A[ATraining Step: 17  | total loss: [1m[32m0.70133[0m[0m | time: 102.734s
[2K
| RMSProp | epoch: 003 | loss: 0.70133 - acc: 0.5157 -- iter: 160/188
[A[ATraining Step: 18  | total loss: [1m[32m0.70077[0m[0m | time: 144.176s
[2K
| RMSProp | epoch: 003 | loss: 0.70077 - acc: 0.5211 | val_loss: 0.68315 - val_acc: 0.5763 -- iter: 188/188
--
Training Step: 19  | total loss: [1m[32m0.69119[0m[0m | time: 21.140s
[2K
| RMSProp | epoch: 004 | loss: 0.69119 - acc: 0.5453 -- iter: 032/188
[A[ATraining Step: 20  | total loss: [1m[32m0.70630[0m[0m | time: 40.813s
[2K
| RMSProp | epoch: 004 | loss: 0.70630 - acc: 0.5207 -- iter: 064/188
[A[ATraining Step: 21  | total loss: [1m[32m0.70271[0m[0m | time: 55.627s
[2K
| RMSProp | epoch: 004 | loss: 0.70271 - acc: 0.5254 -- iter: 096/188
[A[ATraining Step: 22  | total loss: [1m[32m0.70826[0m[0m | time: 66.527s
[2K
| RMSProp | epoch: 004 | loss: 0.70826 - acc: 0.4963 -- iter: 128/188
[A[ATraining Step: 23  | total loss: [1m[32m0.69087[0m[0m | time: 89.874s
[2K
| RMSProp | epoch: 004 | loss: 0.69087 - acc: 0.5337 -- iter: 160/188
[A[ATraining Step: 24  | total loss: [1m[32m0.68062[0m[0m | time: 117.066s
[2K
| RMSProp | epoch: 004 | loss: 0.68062 - acc: 0.5506 | val_loss: 0.68361 - val_acc: 0.5763 -- iter: 188/188
--
Training Step: 25  | total loss: [1m[32m0.68157[0m[0m | time: 20.327s
[2K
| RMSProp | epoch: 005 | loss: 0.68157 - acc: 0.5453 -- iter: 032/188
[A[ATraining Step: 26  | total loss: [1m[32m0.67974[0m[0m | time: 37.054s
[2K
| RMSProp | epoch: 005 | loss: 0.67974 - acc: 0.5168 -- iter: 064/188
[A[ATraining Step: 27  | total loss: [1m[32m0.67691[0m[0m | time: 47.364s
[2K
| RMSProp | epoch: 005 | loss: 0.67691 - acc: 0.5446 -- iter: 096/188
[A[ATraining Step: 28  | total loss: [1m[32m0.67895[0m[0m | time: 55.532s
[2K
| RMSProp | epoch: 005 | loss: 0.67895 - acc: 0.5334 -- iter: 128/188
[A[ATraining Step: 29  | total loss: [1m[32m0.65211[0m[0m | time: 80.448s
[2K
| RMSProp | epoch: 005 | loss: 0.65211 - acc: 0.5774 -- iter: 160/188
[A[ATraining Step: 30  | total loss: [1m[32m0.65651[0m[0m | time: 107.399s
[2K
| RMSProp | epoch: 005 | loss: 0.65651 - acc: 0.5813 | val_loss: 0.68831 - val_acc: 0.5763 -- iter: 188/188
--
Training Step: 31  | total loss: [1m[32m0.66991[0m[0m | time: 41.728s
[2K
| RMSProp | epoch: 006 | loss: 0.66991 - acc: 0.5625 -- iter: 032/188
[A[ATraining Step: 32  | total loss: [1m[32m0.67648[0m[0m | time: 57.547s
[2K
| RMSProp | epoch: 006 | loss: 0.67648 - acc: 0.5696 -- iter: 064/188
[A[ATraining Step: 33  | total loss: [1m[32m0.66804[0m[0m | time: 68.877s
[2K
| RMSProp | epoch: 006 | loss: 0.66804 - acc: 0.6092 -- iter: 096/188
[A[ATraining Step: 34  | total loss: [1m[32m0.65749[0m[0m | time: 77.256s
[2K
| RMSProp | epoch: 006 | loss: 0.65749 - acc: 0.6126 -- iter: 128/188
[A[ATraining Step: 35  | total loss: [1m[32m0.65161[0m[0m | time: 86.015s
[2K
| RMSProp | epoch: 006 | loss: 0.65161 - acc: 0.6189 -- iter: 160/188
[A[ATraining Step: 36  | total loss: [1m[32m0.64712[0m[0m | time: 99.700s
[2K
| RMSProp | epoch: 006 | loss: 0.64712 - acc: 0.6311 | val_loss: 0.68877 - val_acc: 0.5763 -- iter: 188/188
--
Training Step: 37  | total loss: [1m[32m0.64661[0m[0m | time: 102.694s
[2K
| RMSProp | epoch: 007 | loss: 0.64661 - acc: 0.6361 -- iter: 032/188
[A[ATraining Step: 38  | total loss: [1m[32m0.63553[0m[0m | time: 120.583s
[2K
| RMSProp | epoch: 007 | loss: 0.63553 - acc: 0.6340 -- iter: 064/188
[A[ATraining Step: 39  | total loss: [1m[32m0.64005[0m[0m | time: 159.197s
[2K
| RMSProp | epoch: 007 | loss: 0.64005 - acc: 0.6143 -- iter: 096/188
[A[ATraining Step: 40  | total loss: [1m[32m0.64323[0m[0m | time: 171.848s
[2K
| RMSProp | epoch: 007 | loss: 0.64323 - acc: 0.6163 -- iter: 128/188
[A[ATraining Step: 41  | total loss: [1m[32m0.63652[0m[0m | time: 180.341s
[2K
| RMSProp | epoch: 007 | loss: 0.63652 - acc: 0.6351 -- iter: 160/188
[A[ATraining Step: 42  | total loss: [1m[32m0.63747[0m[0m | time: 191.577s
[2K
| RMSProp | epoch: 007 | loss: 0.63747 - acc: 0.6429 | val_loss: 0.68599 - val_acc: 0.5763 -- iter: 188/188
--
Training Step: 43  | total loss: [1m[32m0.62848[0m[0m | time: 17.189s
[2K
| RMSProp | epoch: 008 | loss: 0.62848 - acc: 0.6555 -- iter: 032/188
[A[ATraining Step: 44  | total loss: [1m[32m0.63285[0m[0m | time: 36.584s
[2K
| RMSProp | epoch: 008 | loss: 0.63285 - acc: 0.6448 -- iter: 064/188
[A[ATraining Step: 45  | total loss: [1m[32m0.62766[0m[0m | time: 54.418s
[2K
| RMSProp | epoch: 008 | loss: 0.62766 - acc: 0.6680 -- iter: 096/188
[A[ATraining Step: 46  | total loss: [1m[32m0.61852[0m[0m | time: 78.391s
[2K
| RMSProp | epoch: 008 | loss: 0.61852 - acc: 0.6921 -- iter: 128/188
[A[ATraining Step: 47  | total loss: [1m[32m0.62457[0m[0m | time: 87.586s
[2K
| RMSProp | epoch: 008 | loss: 0.62457 - acc: 0.6760 -- iter: 160/188
[A[ATraining Step: 48  | total loss: [1m[32m0.62487[0m[0m | time: 98.677s
[2K
| RMSProp | epoch: 008 | loss: 0.62487 - acc: 0.6778 | val_loss: 0.68979 - val_acc: 0.5932 -- iter: 188/188
--
Training Step: 49  | total loss: [1m[32m0.62772[0m[0m | time: 49.896s
[2K
| RMSProp | epoch: 009 | loss: 0.62772 - acc: 0.6667 -- iter: 032/188
[A[ATraining Step: 50  | total loss: [1m[32m0.62792[0m[0m | time: 88.956s
[2K
| RMSProp | epoch: 009 | loss: 0.62792 - acc: 0.6464 -- iter: 064/188
[A[ATraining Step: 51  | total loss: [1m[32m0.62370[0m[0m | time: 137.487s
[2K
| RMSProp | epoch: 009 | loss: 0.62370 - acc: 0.6526 -- iter: 096/188
[A[ATraining Step: 52  | total loss: [1m[32m0.63125[0m[0m | time: 149.123s
[2K
| RMSProp | epoch: 009 | loss: 0.63125 - acc: 0.6438 -- iter: 128/188
[A[ATraining Step: 53  | total loss: [1m[32m0.62278[0m[0m | time: 158.525s
[2K
| RMSProp | epoch: 009 | loss: 0.62278 - acc: 0.6410 -- iter: 160/188
[A[ATraining Step: 54  | total loss: [1m[32m0.61915[0m[0m | time: 173.109s
[2K
| RMSProp | epoch: 009 | loss: 0.61915 - acc: 0.6523 | val_loss: 0.68768 - val_acc: 0.5763 -- iter: 188/188
--
Training Step: 55  | total loss: [1m[32m0.61886[0m[0m | time: 71.236s
[2K
| RMSProp | epoch: 010 | loss: 0.61886 - acc: 0.6439 -- iter: 032/188
[A[ATraining Step: 56  | total loss: [1m[32m0.61724[0m[0m | time: 113.407s
[2K
| RMSProp | epoch: 010 | loss: 0.61724 - acc: 0.6438 -- iter: 064/188
[A[ATraining Step: 57  | total loss: [1m[32m0.60923[0m[0m | time: 130.661s
[2K
| RMSProp | epoch: 010 | loss: 0.60923 - acc: 0.6733 -- iter: 096/188
[A[ATraining Step: 58  | total loss: [1m[32m0.61243[0m[0m | time: 143.018s
[2K
| RMSProp | epoch: 010 | loss: 0.61243 - acc: 0.6795 -- iter: 128/188
[A[ATraining Step: 59  | total loss: [1m[32m0.60837[0m[0m | time: 152.620s
[2K
| RMSProp | epoch: 010 | loss: 0.60837 - acc: 0.6806 -- iter: 160/188
[A[ATraining Step: 60  | total loss: [1m[32m0.59908[0m[0m | time: 166.769s
[2K
| RMSProp | epoch: 010 | loss: 0.59908 - acc: 0.7022 | val_loss: 0.68908 - val_acc: 0.4407 -- iter: 188/188
--
Training Step: 61  | total loss: [1m[32m0.59666[0m[0m | time: 39.811s
[2K
| RMSProp | epoch: 011 | loss: 0.59666 - acc: 0.7084 -- iter: 032/188
[A[ATraining Step: 62  | total loss: [1m[32m0.59622[0m[0m | time: 58.221s
[2K
| RMSProp | epoch: 011 | loss: 0.59622 - acc: 0.7057 -- iter: 064/188
[A[ATraining Step: 63  | total loss: [1m[32m0.59857[0m[0m | time: 93.232s
[2K
| RMSProp | epoch: 011 | loss: 0.59857 - acc: 0.6887 -- iter: 096/188
[A[ATraining Step: 64  | total loss: [1m[32m0.58797[0m[0m | time: 104.197s
[2K
| RMSProp | epoch: 011 | loss: 0.58797 - acc: 0.7053 -- iter: 128/188
[A[ATraining Step: 65  | total loss: [1m[32m0.59730[0m[0m | time: 113.239s
[2K
| RMSProp | epoch: 011 | loss: 0.59730 - acc: 0.6915 -- iter: 160/188
[A[ATraining Step: 66  | total loss: [1m[32m0.59053[0m[0m | time: 125.543s
[2K
| RMSProp | epoch: 011 | loss: 0.59053 - acc: 0.7025 | val_loss: 0.71185 - val_acc: 0.5763 -- iter: 188/188
--
Training Step: 67  | total loss: [1m[32m0.58760[0m[0m | time: 23.434s
[2K
| RMSProp | epoch: 012 | loss: 0.58760 - acc: 0.7119 -- iter: 032/188
[A[ATraining Step: 68  | total loss: [1m[32m0.59011[0m[0m | time: 42.749s
[2K
| RMSProp | epoch: 012 | loss: 0.59011 - acc: 0.7090 -- iter: 064/188
[A[ATraining Step: 69  | total loss: [1m[32m0.60587[0m[0m | time: 54.827s
[2K
| RMSProp | epoch: 012 | loss: 0.60587 - acc: 0.6846 -- iter: 096/188
[A[ATraining Step: 70  | total loss: [1m[32m0.59526[0m[0m | time: 112.159s
[2K
| RMSProp | epoch: 012 | loss: 0.59526 - acc: 0.7045 -- iter: 128/188
[A[ATraining Step: 71  | total loss: [1m[32m0.58101[0m[0m | time: 183.393s
[2K
| RMSProp | epoch: 012 | loss: 0.58101 - acc: 0.7219 -- iter: 160/188
[A[ATraining Step: 72  | total loss: [1m[32m0.56968[0m[0m | time: 436.118s
[2K
| RMSProp | epoch: 012 | loss: 0.56968 - acc: 0.7391 | val_loss: 0.61835 - val_acc: 0.7119 -- iter: 188/188
--
Training Step: 73  | total loss: [1m[32m0.56957[0m[0m | time: 51.830s
[2K
| RMSProp | epoch: 013 | loss: 0.56957 - acc: 0.7264 -- iter: 032/188
[A[ATraining Step: 74  | total loss: [1m[32m0.56349[0m[0m | time: 143.584s
[2K
| RMSProp | epoch: 013 | loss: 0.56349 - acc: 0.7256 -- iter: 064/188
[A[ATraining Step: 75  | total loss: [1m[32m0.56051[0m[0m | time: 230.469s
[2K
| RMSProp | epoch: 013 | loss: 0.56051 - acc: 0.7350 -- iter: 096/188
[A[ATraining Step: 76  | total loss: [1m[32m0.55541[0m[0m | time: 324.659s
[2K
| RMSProp | epoch: 013 | loss: 0.55541 - acc: 0.7366 -- iter: 128/188
[A[ATraining Step: 77  | total loss: [1m[32m0.55456[0m[0m | time: 457.996s
[2K
| RMSProp | epoch: 013 | loss: 0.55456 - acc: 0.7380 -- iter: 160/188
[A[ATraining Step: 78  | total loss: [1m[32m0.54637[0m[0m | time: 492.314s
[2K
| RMSProp | epoch: 013 | loss: 0.54637 - acc: 0.7505 | val_loss: 0.73832 - val_acc: 0.4407 -- iter: 188/188
--
Training Step: 79  | total loss: [1m[32m0.54450[0m[0m | time: 82.729s
[2K
| RMSProp | epoch: 014 | loss: 0.54450 - acc: 0.7569 -- iter: 032/188
[A[ATraining Step: 80  | total loss: [1m[32m0.53934[0m[0m | time: 145.513s
[2K
| RMSProp | epoch: 014 | loss: 0.53934 - acc: 0.7498 -- iter: 064/188
[A[ATraining Step: 81  | total loss: [1m[32m0.53204[0m[0m | time: 219.503s
[2K
| RMSProp | epoch: 014 | loss: 0.53204 - acc: 0.7625 -- iter: 096/188
[A[ATraining Step: 82  | total loss: [1m[32m0.53681[0m[0m | time: 255.460s
[2K
| RMSProp | epoch: 014 | loss: 0.53681 - acc: 0.7550 -- iter: 128/188
[A[ATraining Step: 83  | total loss: [1m[32m0.52792[0m[0m | time: 294.947s
[2K
| RMSProp | epoch: 014 | loss: 0.52792 - acc: 0.7670 -- iter: 160/188
[A[ATraining Step: 84  | total loss: [1m[32m0.52808[0m[0m | time: 338.693s
[2K
| RMSProp | epoch: 014 | loss: 0.52808 - acc: 0.7581 | val_loss: 0.60265 - val_acc: 0.6780 -- iter: 188/188
--
Training Step: 85  | total loss: [1m[32m0.51358[0m[0m | time: 32.394s
[2K
| RMSProp | epoch: 015 | loss: 0.51358 - acc: 0.7680 -- iter: 032/188
[A[ATraining Step: 86  | total loss: [1m[32m0.50864[0m[0m | time: 45.580s
[2K
| RMSProp | epoch: 015 | loss: 0.50864 - acc: 0.7756 -- iter: 064/188
[A[ATraining Step: 87  | total loss: [1m[32m0.50419[0m[0m | time: 58.598s
[2K
| RMSProp | epoch: 015 | loss: 0.50419 - acc: 0.7855 -- iter: 096/188
[A[ATraining Step: 88  | total loss: [1m[32m0.49496[0m[0m | time: 73.665s
[2K
| RMSProp | epoch: 015 | loss: 0.49496 - acc: 0.8007 -- iter: 128/188
[A[ATraining Step: 89  | total loss: [1m[32m0.49079[0m[0m | time: 87.214s
[2K
| RMSProp | epoch: 015 | loss: 0.49079 - acc: 0.7894 -- iter: 160/188
[A[ATraining Step: 90  | total loss: [1m[32m0.47832[0m[0m | time: 103.750s
[2K
| RMSProp | epoch: 015 | loss: 0.47832 - acc: 0.8074 | val_loss: 0.59845 - val_acc: 0.7119 -- iter: 188/188
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.7776470588235294
Validation AUPRC:0.7960311150893229
Test AUC:0.8305882352941176
Test AUPRC:0.8574654801346226
BestTestF1Score	0.86	0.63	0.81	0.77	0.97	33	10	15	1	0.62
BestTestMCCScore	0.86	0.63	0.81	0.77	0.97	33	10	15	1	0.62
BestTestAccuracyScore	0.86	0.63	0.81	0.77	0.97	33	10	15	1	0.62
BestValidationF1Score	0.84	0.59	0.78	0.72	1.0	34	13	12	0	0.62
BestValidationMCC	0.84	0.59	0.78	0.72	1.0	34	13	12	0	0.62
BestValidationAccuracy	0.84	0.59	0.78	0.72	1.0	34	13	12	0	0.62
TestPredictions (Threshold:0.62)
CHEMBL2062138,TP,ACT,0.7400000095367432	CHEMBL51428,TP,ACT,0.8100000023841858	CHEMBL296499,TP,ACT,0.8600000143051147	CHEMBL51715,TP,ACT,0.6200000047683716	CHEMBL2177696,TN,INACT,0.5099999904632568	CHEMBL80148,TN,INACT,0.4399999976158142	CHEMBL3354630,FP,INACT,0.7799999713897705	CHEMBL157694,TP,ACT,0.6700000166893005	CHEMBL290698,TP,ACT,0.800000011920929	CHEMBL37879,FP,INACT,0.800000011920929	CHEMBL305020,TP,ACT,0.7200000286102295	CHEMBL144549,TP,ACT,0.8500000238418579	CHEMBL342321,TP,ACT,0.8399999737739563	CHEMBL304623,FP,INACT,0.7900000214576721	CHEMBL3633450,FP,INACT,0.6499999761581421	CHEMBL58677,FP,INACT,0.8500000238418579	CHEMBL342077,FP,INACT,0.6399999856948853	CHEMBL1161556,TN,INACT,0.6100000143051147	CHEMBL165953,TP,ACT,0.8899999856948853	CHEMBL59179,FP,INACT,0.75	CHEMBL1230806,TN,INACT,0.5099999904632568	CHEMBL1161552,TN,INACT,0.46000000834465027	CHEMBL58315,FP,INACT,0.7200000286102295	CHEMBL287896,TP,ACT,0.7799999713897705	CHEMBL1161558,TN,INACT,0.4099999964237213	CHEMBL156757,TP,ACT,0.8199999928474426	CHEMBL289150,TP,ACT,0.699999988079071	CHEMBL307660,FP,INACT,0.699999988079071	CHEMBL1641744,TN,INACT,0.5899999737739563	CHEMBL16046,TP,ACT,0.8299999833106995	CHEMBL343904,TP,ACT,0.8700000047683716	CHEMBL37539,TP,ACT,0.7799999713897705	CHEMBL50625,TP,ACT,0.6800000071525574	CHEMBL24923,TN,INACT,0.3100000023841858	CHEMBL43370,TP,ACT,0.7099999785423279	CHEMBL47289,TP,ACT,0.8999999761581421	CHEMBL156012,TP,ACT,0.6499999761581421	CHEMBL1788203,TP,ACT,0.7799999713897705	CHEMBL431521,FP,INACT,0.7200000286102295	CHEMBL261634,TN,INACT,0.27000001072883606	CHEMBL501385,TN,INACT,0.4000000059604645	CHEMBL1939278,TN,INACT,0.1899999976158142	CHEMBL166792,FN,ACT,0.5600000023841858	CHEMBL77258,TP,ACT,0.7400000095367432	CHEMBL58812,TN,INACT,0.550000011920929	CHEMBL479579,TP,ACT,0.8799999952316284	CHEMBL19352,TP,ACT,0.6200000047683716	CHEMBL1161538,TN,INACT,0.5400000214576721	CHEMBL3633451,TN,INACT,0.3199999928474426	CHEMBL151135,TP,ACT,0.8100000023841858	CHEMBL1161575,TN,INACT,0.5899999737739563	CHEMBL366495,TP,ACT,0.7300000190734863	CHEMBL328519,TP,ACT,0.6600000262260437	CHEMBL147153,TP,ACT,0.800000011920929	CHEMBL148521,TP,ACT,0.7200000286102295	CHEMBL356084,TP,ACT,0.9399999976158142	CHEMBL50414,TP,ACT,0.7900000214576721	CHEMBL281394,TP,ACT,0.6399999856948853	CHEMBL148155,TP,ACT,0.7599999904632568	

