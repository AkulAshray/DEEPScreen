ImageNetInceptionV2 CHEMBL5247 adam 0.0005 15 0 0 0.8 False True
Number of active compounds :	162
Number of inactive compounds :	162
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL5247_adam_0.0005_15_0_0_0.8_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL5247_adam_0.0005_15_0.8/
---------------------------------
Training samples: 204
Validation samples: 65
--
Training Step: 1  | time: 67.339s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/204
[A[ATraining Step: 2  | total loss: [1m[32m0.63535[0m[0m | time: 84.399s
[2K
| Adam | epoch: 001 | loss: 0.63535 - acc: 0.4500 -- iter: 064/204
[A[ATraining Step: 3  | total loss: [1m[32m0.52566[0m[0m | time: 101.958s
[2K
| Adam | epoch: 001 | loss: 0.52566 - acc: 0.7210 -- iter: 096/204
[A[ATraining Step: 4  | total loss: [1m[32m0.66937[0m[0m | time: 119.459s
[2K
| Adam | epoch: 001 | loss: 0.66937 - acc: 0.6724 -- iter: 128/204
[A[ATraining Step: 5  | total loss: [1m[32m0.70003[0m[0m | time: 136.200s
[2K
| Adam | epoch: 001 | loss: 0.70003 - acc: 0.6829 -- iter: 160/204
[A[ATraining Step: 6  | total loss: [1m[32m0.63833[0m[0m | time: 153.262s
[2K
| Adam | epoch: 001 | loss: 0.63833 - acc: 0.6858 -- iter: 192/204
[A[ATraining Step: 7  | total loss: [1m[32m0.56168[0m[0m | time: 181.582s
[2K
| Adam | epoch: 001 | loss: 0.56168 - acc: 0.7056 | val_loss: 0.72209 - val_acc: 0.5692 -- iter: 204/204
--
Training Step: 8  | total loss: [1m[32m0.49895[0m[0m | time: 9.169s
[2K
| Adam | epoch: 002 | loss: 0.49895 - acc: 0.7774 -- iter: 032/204
[A[ATraining Step: 9  | total loss: [1m[32m0.27511[0m[0m | time: 27.129s
[2K
| Adam | epoch: 002 | loss: 0.27511 - acc: 0.8953 -- iter: 064/204
[A[ATraining Step: 10  | total loss: [1m[32m0.32941[0m[0m | time: 45.539s
[2K
| Adam | epoch: 002 | loss: 0.32941 - acc: 0.8851 -- iter: 096/204
[A[ATraining Step: 11  | total loss: [1m[32m0.40025[0m[0m | time: 62.325s
[2K
| Adam | epoch: 002 | loss: 0.40025 - acc: 0.8507 -- iter: 128/204
[A[ATraining Step: 12  | total loss: [1m[32m0.33868[0m[0m | time: 73.836s
[2K
| Adam | epoch: 002 | loss: 0.33868 - acc: 0.8476 -- iter: 160/204
[A[ATraining Step: 13  | total loss: [1m[32m0.31565[0m[0m | time: 85.317s
[2K
| Adam | epoch: 002 | loss: 0.31565 - acc: 0.8326 -- iter: 192/204
[A[ATraining Step: 14  | total loss: [1m[32m0.42669[0m[0m | time: 106.216s
[2K
| Adam | epoch: 002 | loss: 0.42669 - acc: 0.8499 | val_loss: 2.37443 - val_acc: 0.4308 -- iter: 204/204
--
Training Step: 15  | total loss: [1m[32m0.43765[0m[0m | time: 8.529s
[2K
| Adam | epoch: 003 | loss: 0.43765 - acc: 0.7864 -- iter: 032/204
[A[ATraining Step: 16  | total loss: [1m[32m0.40657[0m[0m | time: 17.234s
[2K
| Adam | epoch: 003 | loss: 0.40657 - acc: 0.8352 -- iter: 064/204
[A[ATraining Step: 17  | total loss: [1m[32m0.27847[0m[0m | time: 32.447s
[2K
| Adam | epoch: 003 | loss: 0.27847 - acc: 0.8945 -- iter: 096/204
[A[ATraining Step: 18  | total loss: [1m[32m0.30873[0m[0m | time: 44.943s
[2K
| Adam | epoch: 003 | loss: 0.30873 - acc: 0.8986 -- iter: 128/204
[A[ATraining Step: 19  | total loss: [1m[32m0.29159[0m[0m | time: 57.837s
[2K
| Adam | epoch: 003 | loss: 0.29159 - acc: 0.9011 -- iter: 160/204
[A[ATraining Step: 20  | total loss: [1m[32m0.23099[0m[0m | time: 71.885s
[2K
| Adam | epoch: 003 | loss: 0.23099 - acc: 0.9229 -- iter: 192/204
[A[ATraining Step: 21  | total loss: [1m[32m0.21653[0m[0m | time: 95.909s
[2K
| Adam | epoch: 003 | loss: 0.21653 - acc: 0.9274 | val_loss: 2.68943 - val_acc: 0.4308 -- iter: 204/204
--
Training Step: 22  | total loss: [1m[32m0.21535[0m[0m | time: 16.784s
[2K
| Adam | epoch: 004 | loss: 0.21535 - acc: 0.9304 -- iter: 032/204
[A[ATraining Step: 23  | total loss: [1m[32m0.18236[0m[0m | time: 25.123s
[2K
| Adam | epoch: 004 | loss: 0.18236 - acc: 0.9416 -- iter: 064/204
[A[ATraining Step: 24  | total loss: [1m[32m0.21840[0m[0m | time: 33.082s
[2K
| Adam | epoch: 004 | loss: 0.21840 - acc: 0.9346 -- iter: 096/204
[A[ATraining Step: 25  | total loss: [1m[32m0.16417[0m[0m | time: 49.624s
[2K
| Adam | epoch: 004 | loss: 0.16417 - acc: 0.9524 -- iter: 128/204
[A[ATraining Step: 26  | total loss: [1m[32m0.15967[0m[0m | time: 66.426s
[2K
| Adam | epoch: 004 | loss: 0.15967 - acc: 0.9485 -- iter: 160/204
[A[ATraining Step: 27  | total loss: [1m[32m0.14190[0m[0m | time: 80.493s
[2K
| Adam | epoch: 004 | loss: 0.14190 - acc: 0.9617 -- iter: 192/204
[A[ATraining Step: 28  | total loss: [1m[32m0.13063[0m[0m | time: 96.688s
[2K
| Adam | epoch: 004 | loss: 0.13063 - acc: 0.9635 | val_loss: 5.05786 - val_acc: 0.4308 -- iter: 204/204
--
Training Step: 29  | total loss: [1m[32m0.16285[0m[0m | time: 16.127s
[2K
| Adam | epoch: 005 | loss: 0.16285 - acc: 0.9648 -- iter: 032/204
[A[ATraining Step: 30  | total loss: [1m[32m0.15698[0m[0m | time: 32.056s
[2K
| Adam | epoch: 005 | loss: 0.15698 - acc: 0.9657 -- iter: 064/204
[A[ATraining Step: 31  | total loss: [1m[32m0.16820[0m[0m | time: 39.928s
[2K
| Adam | epoch: 005 | loss: 0.16820 - acc: 0.9448 -- iter: 096/204
[A[ATraining Step: 32  | total loss: [1m[32m0.20566[0m[0m | time: 48.033s
[2K
| Adam | epoch: 005 | loss: 0.20566 - acc: 0.9384 -- iter: 128/204
[A[ATraining Step: 33  | total loss: [1m[32m0.17043[0m[0m | time: 64.352s
[2K
| Adam | epoch: 005 | loss: 0.17043 - acc: 0.9520 -- iter: 160/204
[A[ATraining Step: 34  | total loss: [1m[32m0.19824[0m[0m | time: 80.845s
[2K
| Adam | epoch: 005 | loss: 0.19824 - acc: 0.9422 -- iter: 192/204
[A[ATraining Step: 35  | total loss: [1m[32m0.19300[0m[0m | time: 104.151s
[2K
| Adam | epoch: 005 | loss: 0.19300 - acc: 0.9412 | val_loss: 5.72050 - val_acc: 0.4308 -- iter: 204/204
--
Training Step: 36  | total loss: [1m[32m0.25167[0m[0m | time: 16.639s
[2K
| Adam | epoch: 006 | loss: 0.25167 - acc: 0.9276 -- iter: 032/204
[A[ATraining Step: 37  | total loss: [1m[32m0.22033[0m[0m | time: 33.468s
[2K
| Adam | epoch: 006 | loss: 0.22033 - acc: 0.9296 -- iter: 064/204
[A[ATraining Step: 38  | total loss: [1m[32m0.21811[0m[0m | time: 49.736s
[2K
| Adam | epoch: 006 | loss: 0.21811 - acc: 0.9312 -- iter: 096/204
[A[ATraining Step: 39  | total loss: [1m[32m0.19571[0m[0m | time: 57.391s
[2K
| Adam | epoch: 006 | loss: 0.19571 - acc: 0.9384 -- iter: 128/204
[A[ATraining Step: 40  | total loss: [1m[32m0.21262[0m[0m | time: 65.705s
[2K
| Adam | epoch: 006 | loss: 0.21262 - acc: 0.9343 -- iter: 160/204
[A[ATraining Step: 41  | total loss: [1m[32m0.18711[0m[0m | time: 82.492s
[2K
| Adam | epoch: 006 | loss: 0.18711 - acc: 0.9464 -- iter: 192/204
[A[ATraining Step: 42  | total loss: [1m[32m0.17828[0m[0m | time: 106.438s
[2K
| Adam | epoch: 006 | loss: 0.17828 - acc: 0.9448 | val_loss: 5.84126 - val_acc: 0.4308 -- iter: 204/204
--
Training Step: 43  | total loss: [1m[32m0.17743[0m[0m | time: 11.211s
[2K
| Adam | epoch: 007 | loss: 0.17743 - acc: 0.9435 -- iter: 032/204
[A[ATraining Step: 44  | total loss: [1m[32m0.16550[0m[0m | time: 26.543s
[2K
| Adam | epoch: 007 | loss: 0.16550 - acc: 0.9424 -- iter: 064/204
[A[ATraining Step: 45  | total loss: [1m[32m0.17187[0m[0m | time: 43.111s
[2K
| Adam | epoch: 007 | loss: 0.17187 - acc: 0.9469 -- iter: 096/204
[A[ATraining Step: 46  | total loss: [1m[32m0.17386[0m[0m | time: 59.712s
[2K
| Adam | epoch: 007 | loss: 0.17386 - acc: 0.9401 -- iter: 128/204
[A[ATraining Step: 47  | total loss: [1m[32m0.15355[0m[0m | time: 67.663s
[2K
| Adam | epoch: 007 | loss: 0.15355 - acc: 0.9499 -- iter: 160/204
[A[ATraining Step: 48  | total loss: [1m[32m0.46157[0m[0m | time: 76.499s
[2K
| Adam | epoch: 007 | loss: 0.46157 - acc: 0.8910 -- iter: 192/204
[A[ATraining Step: 49  | total loss: [1m[32m0.43653[0m[0m | time: 99.188s
[2K
| Adam | epoch: 007 | loss: 0.43653 - acc: 0.8819 | val_loss: 3.51811 - val_acc: 0.4308 -- iter: 204/204
--
Training Step: 50  | total loss: [1m[32m0.39051[0m[0m | time: 15.572s
[2K
| Adam | epoch: 008 | loss: 0.39051 - acc: 0.8954 -- iter: 032/204
[A[ATraining Step: 51  | total loss: [1m[32m0.33809[0m[0m | time: 36.392s
[2K
| Adam | epoch: 008 | loss: 0.33809 - acc: 0.9113 -- iter: 064/204
[A[ATraining Step: 52  | total loss: [1m[32m0.32188[0m[0m | time: 57.698s
[2K
| Adam | epoch: 008 | loss: 0.32188 - acc: 0.9153 -- iter: 096/204
[A[ATraining Step: 53  | total loss: [1m[32m0.28282[0m[0m | time: 70.206s
[2K
| Adam | epoch: 008 | loss: 0.28282 - acc: 0.9278 -- iter: 128/204
[A[ATraining Step: 54  | total loss: [1m[32m0.29204[0m[0m | time: 82.855s
[2K
| Adam | epoch: 008 | loss: 0.29204 - acc: 0.9292 -- iter: 160/204
[A[ATraining Step: 55  | total loss: [1m[32m0.25460[0m[0m | time: 88.776s
[2K
| Adam | epoch: 008 | loss: 0.25460 - acc: 0.9393 -- iter: 192/204
[A[ATraining Step: 56  | total loss: [1m[32m0.27163[0m[0m | time: 100.001s
[2K
| Adam | epoch: 008 | loss: 0.27163 - acc: 0.9361 | val_loss: 0.35605 - val_acc: 0.8154 -- iter: 204/204
--
Training Step: 57  | total loss: [1m[32m0.23696[0m[0m | time: 7.962s
[2K
| Adam | epoch: 009 | loss: 0.23696 - acc: 0.9450 -- iter: 032/204
[A[ATraining Step: 58  | total loss: [1m[32m0.21003[0m[0m | time: 15.933s
[2K
| Adam | epoch: 009 | loss: 0.21003 - acc: 0.9525 -- iter: 064/204
[A[ATraining Step: 59  | total loss: [1m[32m0.18491[0m[0m | time: 24.338s
[2K
| Adam | epoch: 009 | loss: 0.18491 - acc: 0.9589 -- iter: 096/204
[A[ATraining Step: 60  | total loss: [1m[32m0.16264[0m[0m | time: 36.759s
[2K
| Adam | epoch: 009 | loss: 0.16264 - acc: 0.9643 -- iter: 128/204
[A[ATraining Step: 61  | total loss: [1m[32m0.14553[0m[0m | time: 49.483s
[2K
| Adam | epoch: 009 | loss: 0.14553 - acc: 0.9690 -- iter: 160/204
[A[ATraining Step: 62  | total loss: [1m[32m0.13094[0m[0m | time: 62.135s
[2K
| Adam | epoch: 009 | loss: 0.13094 - acc: 0.9729 -- iter: 192/204
[A[ATraining Step: 63  | total loss: [1m[32m0.14806[0m[0m | time: 73.341s
[2K
| Adam | epoch: 009 | loss: 0.14806 - acc: 0.9685 | val_loss: 1.16914 - val_acc: 0.6462 -- iter: 204/204
--
Training Step: 64  | total loss: [1m[32m0.13002[0m[0m | time: 6.030s
[2K
| Adam | epoch: 010 | loss: 0.13002 - acc: 0.9724 -- iter: 032/204
[A[ATraining Step: 65  | total loss: [1m[32m0.15219[0m[0m | time: 18.831s
[2K
| Adam | epoch: 010 | loss: 0.15219 - acc: 0.9655 -- iter: 064/204
[A[ATraining Step: 66  | total loss: [1m[32m0.13460[0m[0m | time: 31.125s
[2K
| Adam | epoch: 010 | loss: 0.13460 - acc: 0.9697 -- iter: 096/204
[A[ATraining Step: 67  | total loss: [1m[32m0.12013[0m[0m | time: 43.831s
[2K
| Adam | epoch: 010 | loss: 0.12013 - acc: 0.9734 -- iter: 128/204
[A[ATraining Step: 68  | total loss: [1m[32m0.10681[0m[0m | time: 56.627s
[2K
| Adam | epoch: 010 | loss: 0.10681 - acc: 0.9765 -- iter: 160/204
[A[ATraining Step: 69  | total loss: [1m[32m0.10538[0m[0m | time: 69.212s
[2K
| Adam | epoch: 010 | loss: 0.10538 - acc: 0.9756 -- iter: 192/204
[A[ATraining Step: 70  | total loss: [1m[32m0.09720[0m[0m | time: 86.655s
[2K
| Adam | epoch: 010 | loss: 0.09720 - acc: 0.9784 | val_loss: 0.37704 - val_acc: 0.8000 -- iter: 204/204
--
Training Step: 71  | total loss: [1m[32m0.08736[0m[0m | time: 3.646s
[2K
| Adam | epoch: 011 | loss: 0.08736 - acc: 0.9809 -- iter: 032/204
[A[ATraining Step: 72  | total loss: [1m[32m0.32667[0m[0m | time: 7.200s
[2K
| Adam | epoch: 011 | loss: 0.32667 - acc: 0.9455 -- iter: 064/204
[A[ATraining Step: 73  | total loss: [1m[32m0.29363[0m[0m | time: 15.154s
[2K
| Adam | epoch: 011 | loss: 0.29363 - acc: 0.9516 -- iter: 096/204
[A[ATraining Step: 74  | total loss: [1m[32m0.26339[0m[0m | time: 24.002s
[2K
| Adam | epoch: 011 | loss: 0.26339 - acc: 0.9569 -- iter: 128/204
[A[ATraining Step: 75  | total loss: [1m[32m0.23630[0m[0m | time: 36.434s
[2K
| Adam | epoch: 011 | loss: 0.23630 - acc: 0.9616 -- iter: 160/204
[A[ATraining Step: 76  | total loss: [1m[32m0.21666[0m[0m | time: 48.841s
[2K
| Adam | epoch: 011 | loss: 0.21666 - acc: 0.9657 -- iter: 192/204
[A[ATraining Step: 77  | total loss: [1m[32m0.21309[0m[0m | time: 65.940s
[2K
| Adam | epoch: 011 | loss: 0.21309 - acc: 0.9594 | val_loss: 0.36936 - val_acc: 0.8462 -- iter: 204/204
--
Training Step: 78  | total loss: [1m[32m0.19649[0m[0m | time: 12.467s
[2K
| Adam | epoch: 012 | loss: 0.19649 - acc: 0.9604 -- iter: 032/204
[A[ATraining Step: 79  | total loss: [1m[32m0.18193[0m[0m | time: 18.644s
[2K
| Adam | epoch: 012 | loss: 0.18193 - acc: 0.9645 -- iter: 064/204
[A[ATraining Step: 80  | total loss: [1m[32m0.16620[0m[0m | time: 24.554s
[2K
| Adam | epoch: 012 | loss: 0.16620 - acc: 0.9681 -- iter: 096/204
[A[ATraining Step: 81  | total loss: [1m[32m0.15035[0m[0m | time: 37.124s
[2K
| Adam | epoch: 012 | loss: 0.15035 - acc: 0.9713 -- iter: 128/204
[A[ATraining Step: 82  | total loss: [1m[32m0.13774[0m[0m | time: 49.658s
[2K
| Adam | epoch: 012 | loss: 0.13774 - acc: 0.9742 -- iter: 160/204
[A[ATraining Step: 83  | total loss: [1m[32m0.12941[0m[0m | time: 62.152s
[2K
| Adam | epoch: 012 | loss: 0.12941 - acc: 0.9737 -- iter: 192/204
[A[ATraining Step: 84  | total loss: [1m[32m0.11740[0m[0m | time: 80.014s
[2K
| Adam | epoch: 012 | loss: 0.11740 - acc: 0.9763 | val_loss: 0.43338 - val_acc: 0.8154 -- iter: 204/204
--
Training Step: 85  | total loss: [1m[32m0.10951[0m[0m | time: 8.046s
[2K
| Adam | epoch: 013 | loss: 0.10951 - acc: 0.9787 -- iter: 032/204
[A[ATraining Step: 86  | total loss: [1m[32m0.10038[0m[0m | time: 16.172s
[2K
| Adam | epoch: 013 | loss: 0.10038 - acc: 0.9808 -- iter: 064/204
[A[ATraining Step: 87  | total loss: [1m[32m0.09341[0m[0m | time: 21.857s
[2K
| Adam | epoch: 013 | loss: 0.09341 - acc: 0.9827 -- iter: 096/204
[A[ATraining Step: 88  | total loss: [1m[32m0.11753[0m[0m | time: 27.939s
[2K
| Adam | epoch: 013 | loss: 0.11753 - acc: 0.9761 -- iter: 128/204
[A[ATraining Step: 89  | total loss: [1m[32m0.10640[0m[0m | time: 40.440s
[2K
| Adam | epoch: 013 | loss: 0.10640 - acc: 0.9785 -- iter: 160/204
[A[ATraining Step: 90  | total loss: [1m[32m0.09924[0m[0m | time: 52.892s
[2K
| Adam | epoch: 013 | loss: 0.09924 - acc: 0.9775 -- iter: 192/204
[A[ATraining Step: 91  | total loss: [1m[32m0.09125[0m[0m | time: 70.415s
[2K
| Adam | epoch: 013 | loss: 0.09125 - acc: 0.9798 | val_loss: 1.33144 - val_acc: 0.6923 -- iter: 204/204
--
Training Step: 92  | total loss: [1m[32m0.11825[0m[0m | time: 12.734s
[2K
| Adam | epoch: 014 | loss: 0.11825 - acc: 0.9755 -- iter: 032/204
[A[ATraining Step: 93  | total loss: [1m[32m0.10840[0m[0m | time: 25.065s
[2K
| Adam | epoch: 014 | loss: 0.10840 - acc: 0.9780 -- iter: 064/204
[A[ATraining Step: 94  | total loss: [1m[32m0.09876[0m[0m | time: 37.983s
[2K
| Adam | epoch: 014 | loss: 0.09876 - acc: 0.9802 -- iter: 096/204
[A[ATraining Step: 95  | total loss: [1m[32m0.09026[0m[0m | time: 43.999s
[2K
| Adam | epoch: 014 | loss: 0.09026 - acc: 0.9822 -- iter: 128/204
[A[ATraining Step: 96  | total loss: [1m[32m0.10477[0m[0m | time: 49.716s
[2K
| Adam | epoch: 014 | loss: 0.10477 - acc: 0.9756 -- iter: 160/204
[A[ATraining Step: 97  | total loss: [1m[32m0.09675[0m[0m | time: 62.019s
[2K
| Adam | epoch: 014 | loss: 0.09675 - acc: 0.9781 -- iter: 192/204
[A[ATraining Step: 98  | total loss: [1m[32m0.08823[0m[0m | time: 79.705s
[2K
| Adam | epoch: 014 | loss: 0.08823 - acc: 0.9803 | val_loss: 0.52286 - val_acc: 0.8308 -- iter: 204/204
--
Training Step: 99  | total loss: [1m[32m0.08271[0m[0m | time: 7.873s
[2K
| Adam | epoch: 015 | loss: 0.08271 - acc: 0.9822 -- iter: 032/204
[A[ATraining Step: 100  | total loss: [1m[32m0.09456[0m[0m | time: 15.651s
[2K
| Adam | epoch: 015 | loss: 0.09456 - acc: 0.9809 -- iter: 064/204
[A[ATraining Step: 101  | total loss: [1m[32m0.08595[0m[0m | time: 27.263s
[2K
| Adam | epoch: 015 | loss: 0.08595 - acc: 0.9828 -- iter: 096/204
[A[ATraining Step: 102  | total loss: [1m[32m0.07935[0m[0m | time: 39.986s
[2K
| Adam | epoch: 015 | loss: 0.07935 - acc: 0.9845 -- iter: 128/204
[A[ATraining Step: 103  | total loss: [1m[32m0.07256[0m[0m | time: 45.950s
[2K
| Adam | epoch: 015 | loss: 0.07256 - acc: 0.9861 -- iter: 160/204
[A[ATraining Step: 104  | total loss: [1m[32m0.06784[0m[0m | time: 51.999s
[2K
| Adam | epoch: 015 | loss: 0.06784 - acc: 0.9875 -- iter: 192/204
[A[ATraining Step: 105  | total loss: [1m[32m0.06240[0m[0m | time: 69.175s
[2K
| Adam | epoch: 015 | loss: 0.06240 - acc: 0.9887 | val_loss: 0.56814 - val_acc: 0.8308 -- iter: 204/204
--
Validation AUC:0.9237451737451737
Validation AUPRC:0.9141603691347542
Test AUC:0.9577167019027485
Test AUPRC:0.893804061600336
BestTestF1Score	0.88	0.81	0.91	0.81	0.95	21	5	38	1	0.23
BestTestMCCScore	0.88	0.81	0.91	0.81	0.95	21	5	38	1	0.23
BestTestAccuracyScore	0.88	0.81	0.91	0.81	0.95	21	5	38	1	0.23
BestValidationF1Score	0.82	0.72	0.86	0.91	0.75	21	2	35	7	0.23
BestValidationMCC	0.82	0.72	0.86	0.91	0.75	21	2	35	7	0.23
BestValidationAccuracy	0.82	0.72	0.86	0.91	0.75	21	2	35	7	0.23
TestPredictions (Threshold:0.23)
CHEMBL1828883,TN,INACT,0.009999999776482582	CHEMBL113996,TN,INACT,0.0	CHEMBL3629013,TN,INACT,0.0	CHEMBL3685659,TP,ACT,0.9700000286102295	CHEMBL366831,TN,INACT,0.0	CHEMBL141238,TN,INACT,0.0	CHEMBL3685640,TP,ACT,0.9599999785423279	CHEMBL1368744,TP,ACT,0.9300000071525574	CHEMBL3685675,TP,ACT,0.4699999988079071	CHEMBL3681027,TP,ACT,1.0	CHEMBL3685638,FN,ACT,0.029999999329447746	CHEMBL214949,TN,INACT,0.009999999776482582	CHEMBL3685629,TP,ACT,0.9900000095367432	CHEMBL246167,TN,INACT,0.0	CHEMBL3681010,TP,ACT,0.9800000190734863	CHEMBL113985,TN,INACT,0.07999999821186066	CHEMBL208433,TN,INACT,0.20000000298023224	CHEMBL453336,TN,INACT,0.0	CHEMBL114073,TN,INACT,0.0	CHEMBL395664,TN,INACT,0.0	CHEMBL3680998,TP,ACT,0.7799999713897705	CHEMBL3685641,TP,ACT,0.9900000095367432	CHEMBL336330,TN,INACT,0.14000000059604645	CHEMBL315701,TN,INACT,0.0	CHEMBL3639845,TP,ACT,0.6299999952316284	CHEMBL205652,TN,INACT,0.0	CHEMBL3681003,TP,ACT,0.6600000262260437	CHEMBL3799345,TN,INACT,0.0	CHEMBL3681007,TP,ACT,0.8999999761581421	CHEMBL1080271,FP,INACT,0.36000001430511475	CHEMBL104,TN,INACT,0.0	CHEMBL428647,TN,INACT,0.0	CHEMBL55979,TN,INACT,0.10000000149011612	CHEMBL3685711,TP,ACT,0.75	CHEMBL1087054,TN,INACT,0.0	CHEMBL1172602,TN,INACT,0.0	CHEMBL44,TN,INACT,0.0	CHEMBL277430,TN,INACT,0.0	CHEMBL3685666,TP,ACT,1.0	CHEMBL592240,FP,INACT,1.0	CHEMBL602471,TN,INACT,0.0	CHEMBL1241775,TN,INACT,0.0	CHEMBL488646,TN,INACT,0.0	CHEMBL373882,TN,INACT,0.0	CHEMBL3685701,TP,ACT,1.0	CHEMBL1828884,TN,INACT,0.009999999776482582	CHEMBL116423,FP,INACT,0.75	CHEMBL3685635,TP,ACT,1.0	CHEMBL1161235,FP,INACT,0.949999988079071	CHEMBL589120,TN,INACT,0.009999999776482582	CHEMBL55994,TN,INACT,0.0	CHEMBL3685642,TP,ACT,0.9800000190734863	CHEMBL3133826,TN,INACT,0.0	CHEMBL233958,TN,INACT,0.0	CHEMBL589259,TN,INACT,0.2199999988079071	CHEMBL3685693,TP,ACT,1.0	CHEMBL3685674,TP,ACT,0.9100000262260437	CHEMBL3133834,TN,INACT,0.0	CHEMBL602645,TN,INACT,0.0	CHEMBL488811,TN,INACT,0.0	CHEMBL3685709,TP,ACT,0.6000000238418579	CHEMBL113902,TN,INACT,0.0	CHEMBL308134,FP,INACT,0.4300000071525574	CHEMBL3680990,TP,ACT,0.9700000286102295	CHEMBL1828882,TN,INACT,0.009999999776482582	

