CNNModel CHEMBL4789 adam 0.0005 30 32 0 0.8 False True
Number of active compounds :	228
Number of inactive compounds :	228
---------------------------------
Run id: CNNModel_CHEMBL4789_adam_0.0005_30_32_0_0.8_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL4789_adam_0.0005_30_32_0.8_True/
---------------------------------
Training samples: 224
Validation samples: 71
--
Training Step: 1  | time: 1.951s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/224
[A[ATraining Step: 2  | total loss: [1m[32m0.62403[0m[0m | time: 3.699s
[2K
| Adam | epoch: 001 | loss: 0.62403 - acc: 0.3937 -- iter: 064/224
[A[ATraining Step: 3  | total loss: [1m[32m0.68078[0m[0m | time: 5.320s
[2K
| Adam | epoch: 001 | loss: 0.68078 - acc: 0.4807 -- iter: 096/224
[A[ATraining Step: 4  | total loss: [1m[32m0.69060[0m[0m | time: 6.810s
[2K
| Adam | epoch: 001 | loss: 0.69060 - acc: 0.4952 -- iter: 128/224
[A[ATraining Step: 5  | total loss: [1m[32m0.69144[0m[0m | time: 8.572s
[2K
| Adam | epoch: 001 | loss: 0.69144 - acc: 0.5418 -- iter: 160/224
[A[ATraining Step: 6  | total loss: [1m[32m0.69580[0m[0m | time: 10.234s
[2K
| Adam | epoch: 001 | loss: 0.69580 - acc: 0.4547 -- iter: 192/224
[A[ATraining Step: 7  | total loss: [1m[32m0.69816[0m[0m | time: 13.046s
[2K
| Adam | epoch: 001 | loss: 0.69816 - acc: 0.3506 | val_loss: 0.69396 - val_acc: 0.4085 -- iter: 224/224
--
Training Step: 8  | total loss: [1m[32m0.69493[0m[0m | time: 1.455s
[2K
| Adam | epoch: 002 | loss: 0.69493 - acc: 0.4698 -- iter: 032/224
[A[ATraining Step: 9  | total loss: [1m[32m0.69361[0m[0m | time: 3.151s
[2K
| Adam | epoch: 002 | loss: 0.69361 - acc: 0.5189 -- iter: 064/224
[A[ATraining Step: 10  | total loss: [1m[32m0.69359[0m[0m | time: 4.849s
[2K
| Adam | epoch: 002 | loss: 0.69359 - acc: 0.4938 -- iter: 096/224
[A[ATraining Step: 11  | total loss: [1m[32m0.69360[0m[0m | time: 6.549s
[2K
| Adam | epoch: 002 | loss: 0.69360 - acc: 0.4671 -- iter: 128/224
[A[ATraining Step: 12  | total loss: [1m[32m0.69326[0m[0m | time: 8.075s
[2K
| Adam | epoch: 002 | loss: 0.69326 - acc: 0.5101 -- iter: 160/224
[A[ATraining Step: 13  | total loss: [1m[32m0.69339[0m[0m | time: 9.858s
[2K
| Adam | epoch: 002 | loss: 0.69339 - acc: 0.4656 -- iter: 192/224
[A[ATraining Step: 14  | total loss: [1m[32m0.69327[0m[0m | time: 12.624s
[2K
| Adam | epoch: 002 | loss: 0.69327 - acc: 0.4797 | val_loss: 0.69230 - val_acc: 0.5915 -- iter: 224/224
--
Training Step: 15  | total loss: [1m[32m0.69309[0m[0m | time: 1.722s
[2K
| Adam | epoch: 003 | loss: 0.69309 - acc: 0.5243 -- iter: 032/224
[A[ATraining Step: 16  | total loss: [1m[32m0.69312[0m[0m | time: 3.492s
[2K
| Adam | epoch: 003 | loss: 0.69312 - acc: 0.5152 -- iter: 064/224
[A[ATraining Step: 17  | total loss: [1m[32m0.69316[0m[0m | time: 5.038s
[2K
| Adam | epoch: 003 | loss: 0.69316 - acc: 0.5097 -- iter: 096/224
[A[ATraining Step: 18  | total loss: [1m[32m0.69307[0m[0m | time: 6.385s
[2K
| Adam | epoch: 003 | loss: 0.69307 - acc: 0.5064 -- iter: 128/224
[A[ATraining Step: 19  | total loss: [1m[32m0.69280[0m[0m | time: 7.855s
[2K
| Adam | epoch: 003 | loss: 0.69280 - acc: 0.5147 -- iter: 160/224
[A[ATraining Step: 20  | total loss: [1m[32m0.69283[0m[0m | time: 9.519s
[2K
| Adam | epoch: 003 | loss: 0.69283 - acc: 0.5099 -- iter: 192/224
[A[ATraining Step: 21  | total loss: [1m[32m0.69348[0m[0m | time: 12.232s
[2K
| Adam | epoch: 003 | loss: 0.69348 - acc: 0.4972 | val_loss: 0.68849 - val_acc: 0.5915 -- iter: 224/224
--
Training Step: 22  | total loss: [1m[32m0.69134[0m[0m | time: 1.625s
[2K
| Adam | epoch: 004 | loss: 0.69134 - acc: 0.5449 -- iter: 032/224
[A[ATraining Step: 23  | total loss: [1m[32m0.69043[0m[0m | time: 3.139s
[2K
| Adam | epoch: 004 | loss: 0.69043 - acc: 0.5591 -- iter: 064/224
[A[ATraining Step: 24  | total loss: [1m[32m0.69160[0m[0m | time: 4.954s
[2K
| Adam | epoch: 004 | loss: 0.69160 - acc: 0.5425 -- iter: 096/224
[A[ATraining Step: 25  | total loss: [1m[32m0.69383[0m[0m | time: 6.587s
[2K
| Adam | epoch: 004 | loss: 0.69383 - acc: 0.5138 -- iter: 128/224
[A[ATraining Step: 26  | total loss: [1m[32m0.69399[0m[0m | time: 8.139s
[2K
| Adam | epoch: 004 | loss: 0.69399 - acc: 0.5102 -- iter: 160/224
[A[ATraining Step: 27  | total loss: [1m[32m0.69150[0m[0m | time: 9.654s
[2K
| Adam | epoch: 004 | loss: 0.69150 - acc: 0.5317 -- iter: 192/224
[A[ATraining Step: 28  | total loss: [1m[32m0.68803[0m[0m | time: 12.198s
[2K
| Adam | epoch: 004 | loss: 0.68803 - acc: 0.5628 | val_loss: 0.68051 - val_acc: 0.5915 -- iter: 224/224
--
Training Step: 29  | total loss: [1m[32m0.68739[0m[0m | time: 1.462s
[2K
| Adam | epoch: 005 | loss: 0.68739 - acc: 0.5627 -- iter: 032/224
[A[ATraining Step: 30  | total loss: [1m[32m0.68636[0m[0m | time: 2.945s
[2K
| Adam | epoch: 005 | loss: 0.68636 - acc: 0.5627 -- iter: 064/224
[A[ATraining Step: 31  | total loss: [1m[32m0.69276[0m[0m | time: 4.594s
[2K
| Adam | epoch: 005 | loss: 0.69276 - acc: 0.5338 -- iter: 096/224
[A[ATraining Step: 32  | total loss: [1m[32m0.69319[0m[0m | time: 6.215s
[2K
| Adam | epoch: 005 | loss: 0.69319 - acc: 0.5332 -- iter: 128/224
[A[ATraining Step: 33  | total loss: [1m[32m0.69317[0m[0m | time: 7.801s
[2K
| Adam | epoch: 005 | loss: 0.69317 - acc: 0.5328 -- iter: 160/224
[A[ATraining Step: 34  | total loss: [1m[32m0.69200[0m[0m | time: 9.304s
[2K
| Adam | epoch: 005 | loss: 0.69200 - acc: 0.5325 -- iter: 192/224
[A[ATraining Step: 35  | total loss: [1m[32m0.69051[0m[0m | time: 12.077s
[2K
| Adam | epoch: 005 | loss: 0.69051 - acc: 0.5387 | val_loss: 0.68190 - val_acc: 0.5915 -- iter: 224/224
--
Training Step: 36  | total loss: [1m[32m0.68878[0m[0m | time: 1.344s
[2K
| Adam | epoch: 006 | loss: 0.68878 - acc: 0.5500 -- iter: 032/224
[A[ATraining Step: 37  | total loss: [1m[32m0.68886[0m[0m | time: 2.775s
[2K
| Adam | epoch: 006 | loss: 0.68886 - acc: 0.5462 -- iter: 064/224
[A[ATraining Step: 38  | total loss: [1m[32m0.68697[0m[0m | time: 4.168s
[2K
| Adam | epoch: 006 | loss: 0.68697 - acc: 0.5555 -- iter: 096/224
[A[ATraining Step: 39  | total loss: [1m[32m0.69215[0m[0m | time: 5.115s
[2K
| Adam | epoch: 006 | loss: 0.69215 - acc: 0.5270 -- iter: 128/224
[A[ATraining Step: 40  | total loss: [1m[32m0.69059[0m[0m | time: 5.937s
[2K
| Adam | epoch: 006 | loss: 0.69059 - acc: 0.5336 -- iter: 160/224
[A[ATraining Step: 41  | total loss: [1m[32m0.68972[0m[0m | time: 6.681s
[2K
| Adam | epoch: 006 | loss: 0.68972 - acc: 0.5389 -- iter: 192/224
[A[ATraining Step: 42  | total loss: [1m[32m0.68954[0m[0m | time: 8.442s
[2K
| Adam | epoch: 006 | loss: 0.68954 - acc: 0.5375 | val_loss: 0.68189 - val_acc: 0.5915 -- iter: 224/224
--
Training Step: 43  | total loss: [1m[32m0.68898[0m[0m | time: 0.627s
[2K
| Adam | epoch: 007 | loss: 0.68898 - acc: 0.5419 -- iter: 032/224
[A[ATraining Step: 44  | total loss: [1m[32m0.68685[0m[0m | time: 1.245s
[2K
| Adam | epoch: 007 | loss: 0.68685 - acc: 0.5509 -- iter: 064/224
[A[ATraining Step: 45  | total loss: [1m[32m0.68949[0m[0m | time: 1.872s
[2K
| Adam | epoch: 007 | loss: 0.68949 - acc: 0.5370 -- iter: 096/224
[A[ATraining Step: 46  | total loss: [1m[32m0.68879[0m[0m | time: 2.560s
[2K
| Adam | epoch: 007 | loss: 0.68879 - acc: 0.5412 -- iter: 128/224
[A[ATraining Step: 47  | total loss: [1m[32m0.69046[0m[0m | time: 3.167s
[2K
| Adam | epoch: 007 | loss: 0.69046 - acc: 0.5294 -- iter: 160/224
[A[ATraining Step: 48  | total loss: [1m[32m0.69041[0m[0m | time: 3.809s
[2K
| Adam | epoch: 007 | loss: 0.69041 - acc: 0.5297 -- iter: 192/224
[A[ATraining Step: 49  | total loss: [1m[32m0.69037[0m[0m | time: 5.439s
[2K
| Adam | epoch: 007 | loss: 0.69037 - acc: 0.5299 | val_loss: 0.68113 - val_acc: 0.5915 -- iter: 224/224
--
Training Step: 50  | total loss: [1m[32m0.68585[0m[0m | time: 1.111s
[2K
| Adam | epoch: 008 | loss: 0.68585 - acc: 0.5592 -- iter: 032/224
[A[ATraining Step: 51  | total loss: [1m[32m0.68621[0m[0m | time: 2.177s
[2K
| Adam | epoch: 008 | loss: 0.68621 - acc: 0.5549 -- iter: 064/224
[A[ATraining Step: 52  | total loss: [1m[32m0.68820[0m[0m | time: 3.242s
[2K
| Adam | epoch: 008 | loss: 0.68820 - acc: 0.5420 -- iter: 096/224
[A[ATraining Step: 53  | total loss: [1m[32m0.68830[0m[0m | time: 4.051s
[2K
| Adam | epoch: 008 | loss: 0.68830 - acc: 0.5404 -- iter: 128/224
[A[ATraining Step: 54  | total loss: [1m[32m0.68924[0m[0m | time: 4.711s
[2K
| Adam | epoch: 008 | loss: 0.68924 - acc: 0.5346 -- iter: 160/224
[A[ATraining Step: 55  | total loss: [1m[32m0.69023[0m[0m | time: 5.339s
[2K
| Adam | epoch: 008 | loss: 0.69023 - acc: 0.5252 -- iter: 192/224
[A[ATraining Step: 56  | total loss: [1m[32m0.69566[0m[0m | time: 6.978s
[2K
| Adam | epoch: 008 | loss: 0.69566 - acc: 0.4909 | val_loss: 0.68741 - val_acc: 0.5915 -- iter: 224/224
--
Training Step: 57  | total loss: [1m[32m0.69815[0m[0m | time: 0.633s
[2K
| Adam | epoch: 009 | loss: 0.69815 - acc: 0.4618 -- iter: 032/224
[A[ATraining Step: 58  | total loss: [1m[32m0.69483[0m[0m | time: 1.270s
[2K
| Adam | epoch: 009 | loss: 0.69483 - acc: 0.5011 -- iter: 064/224
[A[ATraining Step: 59  | total loss: [1m[32m0.69295[0m[0m | time: 1.884s
[2K
| Adam | epoch: 009 | loss: 0.69295 - acc: 0.5304 -- iter: 096/224
[A[ATraining Step: 60  | total loss: [1m[32m0.69273[0m[0m | time: 2.513s
[2K
| Adam | epoch: 009 | loss: 0.69273 - acc: 0.5263 -- iter: 128/224
[A[ATraining Step: 61  | total loss: [1m[32m0.69213[0m[0m | time: 3.133s
[2K
| Adam | epoch: 009 | loss: 0.69213 - acc: 0.5311 -- iter: 160/224
[A[ATraining Step: 62  | total loss: [1m[32m0.69241[0m[0m | time: 3.790s
[2K
| Adam | epoch: 009 | loss: 0.69241 - acc: 0.5190 -- iter: 192/224
[A[ATraining Step: 63  | total loss: [1m[32m0.69244[0m[0m | time: 5.413s
[2K
| Adam | epoch: 009 | loss: 0.69244 - acc: 0.5166 | val_loss: 0.68960 - val_acc: 0.5915 -- iter: 224/224
--
Training Step: 64  | total loss: [1m[32m0.69253[0m[0m | time: 0.958s
[2K
| Adam | epoch: 010 | loss: 0.69253 - acc: 0.5145 -- iter: 032/224
[A[ATraining Step: 65  | total loss: [1m[32m0.69244[0m[0m | time: 1.813s
[2K
| Adam | epoch: 010 | loss: 0.69244 - acc: 0.5127 -- iter: 064/224
[A[ATraining Step: 66  | total loss: [1m[32m0.69229[0m[0m | time: 2.690s
[2K
| Adam | epoch: 010 | loss: 0.69229 - acc: 0.5112 -- iter: 096/224
[A[ATraining Step: 67  | total loss: [1m[32m0.69159[0m[0m | time: 3.963s
[2K
| Adam | epoch: 010 | loss: 0.69159 - acc: 0.5211 -- iter: 128/224
[A[ATraining Step: 68  | total loss: [1m[32m0.69170[0m[0m | time: 5.218s
[2K
| Adam | epoch: 010 | loss: 0.69170 - acc: 0.5112 -- iter: 160/224
[A[ATraining Step: 69  | total loss: [1m[32m0.69106[0m[0m | time: 6.553s
[2K
| Adam | epoch: 010 | loss: 0.69106 - acc: 0.5172 -- iter: 192/224
[A[ATraining Step: 70  | total loss: [1m[32m0.69001[0m[0m | time: 9.278s
[2K
| Adam | epoch: 010 | loss: 0.69001 - acc: 0.5260 | val_loss: 0.68584 - val_acc: 0.5915 -- iter: 224/224
--
Training Step: 71  | total loss: [1m[32m0.68981[0m[0m | time: 1.253s
[2K
| Adam | epoch: 011 | loss: 0.68981 - acc: 0.5302 -- iter: 032/224
[A[ATraining Step: 72  | total loss: [1m[32m0.69068[0m[0m | time: 3.165s
[2K
| Adam | epoch: 011 | loss: 0.69068 - acc: 0.5162 -- iter: 064/224
[A[ATraining Step: 73  | total loss: [1m[32m0.69118[0m[0m | time: 4.323s
[2K
| Adam | epoch: 011 | loss: 0.69118 - acc: 0.5040 -- iter: 096/224
[A[ATraining Step: 74  | total loss: [1m[32m0.69075[0m[0m | time: 5.694s
[2K
| Adam | epoch: 011 | loss: 0.69075 - acc: 0.5036 -- iter: 128/224
[A[ATraining Step: 75  | total loss: [1m[32m0.68969[0m[0m | time: 7.026s
[2K
| Adam | epoch: 011 | loss: 0.68969 - acc: 0.5201 -- iter: 160/224
[A[ATraining Step: 76  | total loss: [1m[32m0.68826[0m[0m | time: 8.479s
[2K
| Adam | epoch: 011 | loss: 0.68826 - acc: 0.5180 -- iter: 192/224
[A[ATraining Step: 77  | total loss: [1m[32m0.68757[0m[0m | time: 10.858s
[2K
| Adam | epoch: 011 | loss: 0.68757 - acc: 0.5161 | val_loss: 0.67801 - val_acc: 0.5915 -- iter: 224/224
--
Training Step: 78  | total loss: [1m[32m0.68458[0m[0m | time: 0.984s
[2K
| Adam | epoch: 012 | loss: 0.68458 - acc: 0.5242 -- iter: 032/224
[A[ATraining Step: 79  | total loss: [1m[32m0.68153[0m[0m | time: 2.172s
[2K
| Adam | epoch: 012 | loss: 0.68153 - acc: 0.5314 -- iter: 064/224
[A[ATraining Step: 80  | total loss: [1m[32m0.68223[0m[0m | time: 3.375s
[2K
| Adam | epoch: 012 | loss: 0.68223 - acc: 0.5250 -- iter: 096/224
[A[ATraining Step: 81  | total loss: [1m[32m0.68237[0m[0m | time: 4.596s
[2K
| Adam | epoch: 012 | loss: 0.68237 - acc: 0.5193 -- iter: 128/224
[A[ATraining Step: 82  | total loss: [1m[32m0.67896[0m[0m | time: 5.779s
[2K
| Adam | epoch: 012 | loss: 0.67896 - acc: 0.5299 -- iter: 160/224
[A[ATraining Step: 83  | total loss: [1m[32m0.67562[0m[0m | time: 6.994s
[2K
| Adam | epoch: 012 | loss: 0.67562 - acc: 0.5425 -- iter: 192/224
[A[ATraining Step: 84  | total loss: [1m[32m0.67154[0m[0m | time: 9.263s
[2K
| Adam | epoch: 012 | loss: 0.67154 - acc: 0.5476 | val_loss: 0.71357 - val_acc: 0.6197 -- iter: 224/224
--
Training Step: 85  | total loss: [1m[32m0.66589[0m[0m | time: 1.703s
[2K
| Adam | epoch: 013 | loss: 0.66589 - acc: 0.5647 -- iter: 032/224
[A[ATraining Step: 86  | total loss: [1m[32m0.66474[0m[0m | time: 4.197s
[2K
| Adam | epoch: 013 | loss: 0.66474 - acc: 0.5583 -- iter: 064/224
[A[ATraining Step: 87  | total loss: [1m[32m0.66439[0m[0m | time: 6.217s
[2K
| Adam | epoch: 013 | loss: 0.66439 - acc: 0.5587 -- iter: 096/224
[A[ATraining Step: 88  | total loss: [1m[32m0.65862[0m[0m | time: 11.563s
[2K
| Adam | epoch: 013 | loss: 0.65862 - acc: 0.5716 -- iter: 128/224
[A[ATraining Step: 89  | total loss: [1m[32m0.65128[0m[0m | time: 20.934s
[2K
| Adam | epoch: 013 | loss: 0.65128 - acc: 0.5863 -- iter: 160/224
[A[ATraining Step: 90  | total loss: [1m[32m0.64943[0m[0m | time: 35.779s
[2K
| Adam | epoch: 013 | loss: 0.64943 - acc: 0.5933 -- iter: 192/224
[A[ATraining Step: 91  | total loss: [1m[32m0.64284[0m[0m | time: 74.713s
[2K
| Adam | epoch: 013 | loss: 0.64284 - acc: 0.6058 | val_loss: 0.78896 - val_acc: 0.5493 -- iter: 224/224
--
Training Step: 92  | total loss: [1m[32m0.64295[0m[0m | time: 1.457s
[2K
| Adam | epoch: 014 | loss: 0.64295 - acc: 0.6015 -- iter: 032/224
[A[ATraining Step: 93  | total loss: [1m[32m0.63868[0m[0m | time: 3.033s
[2K
| Adam | epoch: 014 | loss: 0.63868 - acc: 0.6007 -- iter: 064/224
[A[ATraining Step: 94  | total loss: [1m[32m0.63464[0m[0m | time: 5.371s
[2K
| Adam | epoch: 014 | loss: 0.63464 - acc: 0.6032 -- iter: 096/224
[A[ATraining Step: 95  | total loss: [1m[32m0.63688[0m[0m | time: 7.929s
[2K
| Adam | epoch: 014 | loss: 0.63688 - acc: 0.5991 -- iter: 128/224
[A[ATraining Step: 96  | total loss: [1m[32m0.63807[0m[0m | time: 9.355s
[2K
| Adam | epoch: 014 | loss: 0.63807 - acc: 0.6017 -- iter: 160/224
[A[ATraining Step: 97  | total loss: [1m[32m0.63349[0m[0m | time: 11.065s
[2K
| Adam | epoch: 014 | loss: 0.63349 - acc: 0.6103 -- iter: 192/224
[A[ATraining Step: 98  | total loss: [1m[32m0.63127[0m[0m | time: 13.793s
[2K
| Adam | epoch: 014 | loss: 0.63127 - acc: 0.6149 | val_loss: 0.76737 - val_acc: 0.5070 -- iter: 224/224
--
Training Step: 99  | total loss: [1m[32m0.62671[0m[0m | time: 1.575s
[2K
| Adam | epoch: 015 | loss: 0.62671 - acc: 0.6159 -- iter: 032/224
[A[ATraining Step: 100  | total loss: [1m[32m0.61969[0m[0m | time: 3.239s
[2K
| Adam | epoch: 015 | loss: 0.61969 - acc: 0.6230 -- iter: 064/224
[A[ATraining Step: 101  | total loss: [1m[32m0.62129[0m[0m | time: 5.034s
[2K
| Adam | epoch: 015 | loss: 0.62129 - acc: 0.6170 -- iter: 096/224
[A[ATraining Step: 102  | total loss: [1m[32m0.62027[0m[0m | time: 6.743s
[2K
| Adam | epoch: 015 | loss: 0.62027 - acc: 0.6240 -- iter: 128/224
[A[ATraining Step: 103  | total loss: [1m[32m0.61152[0m[0m | time: 11.020s
[2K
| Adam | epoch: 015 | loss: 0.61152 - acc: 0.6366 -- iter: 160/224
[A[ATraining Step: 104  | total loss: [1m[32m0.62918[0m[0m | time: 23.296s
[2K
| Adam | epoch: 015 | loss: 0.62918 - acc: 0.6292 -- iter: 192/224
[A[ATraining Step: 105  | total loss: [1m[32m0.62212[0m[0m | time: 66.321s
[2K
| Adam | epoch: 015 | loss: 0.62212 - acc: 0.6444 | val_loss: 0.72209 - val_acc: 0.4648 -- iter: 224/224
--
Training Step: 106  | total loss: [1m[32m0.61929[0m[0m | time: 6.847s
[2K
| Adam | epoch: 016 | loss: 0.61929 - acc: 0.6519 -- iter: 032/224
[A[ATraining Step: 107  | total loss: [1m[32m0.61485[0m[0m | time: 8.316s
[2K
| Adam | epoch: 016 | loss: 0.61485 - acc: 0.6523 -- iter: 064/224
[A[ATraining Step: 108  | total loss: [1m[32m0.61894[0m[0m | time: 9.873s
[2K
| Adam | epoch: 016 | loss: 0.61894 - acc: 0.6464 -- iter: 096/224
[A[ATraining Step: 109  | total loss: [1m[32m0.61807[0m[0m | time: 11.375s
[2K
| Adam | epoch: 016 | loss: 0.61807 - acc: 0.6474 -- iter: 128/224
[A[ATraining Step: 110  | total loss: [1m[32m0.61680[0m[0m | time: 13.006s
[2K
| Adam | epoch: 016 | loss: 0.61680 - acc: 0.6452 -- iter: 160/224
[A[ATraining Step: 111  | total loss: [1m[32m0.61940[0m[0m | time: 14.713s
[2K
| Adam | epoch: 016 | loss: 0.61940 - acc: 0.6400 -- iter: 192/224
[A[ATraining Step: 112  | total loss: [1m[32m0.62862[0m[0m | time: 17.363s
[2K
| Adam | epoch: 016 | loss: 0.62862 - acc: 0.6292 | val_loss: 0.74845 - val_acc: 0.5211 -- iter: 224/224
--
Training Step: 113  | total loss: [1m[32m0.62620[0m[0m | time: 1.720s
[2K
| Adam | epoch: 017 | loss: 0.62620 - acc: 0.6350 -- iter: 032/224
[A[ATraining Step: 114  | total loss: [1m[32m0.62470[0m[0m | time: 3.293s
[2K
| Adam | epoch: 017 | loss: 0.62470 - acc: 0.6402 -- iter: 064/224
[A[ATraining Step: 115  | total loss: [1m[32m0.61393[0m[0m | time: 4.884s
[2K
| Adam | epoch: 017 | loss: 0.61393 - acc: 0.6543 -- iter: 096/224
[A[ATraining Step: 116  | total loss: [1m[32m0.61154[0m[0m | time: 6.317s
[2K
| Adam | epoch: 017 | loss: 0.61154 - acc: 0.6545 -- iter: 128/224
[A[ATraining Step: 117  | total loss: [1m[32m0.60759[0m[0m | time: 8.092s
[2K
| Adam | epoch: 017 | loss: 0.60759 - acc: 0.6610 -- iter: 160/224
[A[ATraining Step: 118  | total loss: [1m[32m0.60562[0m[0m | time: 9.750s
[2K
| Adam | epoch: 017 | loss: 0.60562 - acc: 0.6574 -- iter: 192/224
[A[ATraining Step: 119  | total loss: [1m[32m0.59485[0m[0m | time: 12.524s
[2K
| Adam | epoch: 017 | loss: 0.59485 - acc: 0.6572 | val_loss: 0.80903 - val_acc: 0.5352 -- iter: 224/224
--
Training Step: 120  | total loss: [1m[32m0.59682[0m[0m | time: 1.592s
[2K
| Adam | epoch: 018 | loss: 0.59682 - acc: 0.6571 -- iter: 032/224
[A[ATraining Step: 121  | total loss: [1m[32m0.59348[0m[0m | time: 2.999s
[2K
| Adam | epoch: 018 | loss: 0.59348 - acc: 0.6571 -- iter: 064/224
[A[ATraining Step: 122  | total loss: [1m[32m0.58828[0m[0m | time: 4.542s
[2K
| Adam | epoch: 018 | loss: 0.58828 - acc: 0.6601 -- iter: 096/224
[A[ATraining Step: 123  | total loss: [1m[32m0.59099[0m[0m | time: 5.953s
[2K
| Adam | epoch: 018 | loss: 0.59099 - acc: 0.6535 -- iter: 128/224
[A[ATraining Step: 124  | total loss: [1m[32m0.58699[0m[0m | time: 7.279s
[2K
| Adam | epoch: 018 | loss: 0.58699 - acc: 0.6600 -- iter: 160/224
[A[ATraining Step: 125  | total loss: [1m[32m0.58485[0m[0m | time: 8.662s
[2K
| Adam | epoch: 018 | loss: 0.58485 - acc: 0.6627 -- iter: 192/224
[A[ATraining Step: 126  | total loss: [1m[32m0.57584[0m[0m | time: 10.886s
[2K
| Adam | epoch: 018 | loss: 0.57584 - acc: 0.6715 | val_loss: 0.79081 - val_acc: 0.5211 -- iter: 224/224
--
Training Step: 127  | total loss: [1m[32m0.57230[0m[0m | time: 1.390s
[2K
| Adam | epoch: 019 | loss: 0.57230 - acc: 0.6731 -- iter: 032/224
[A[ATraining Step: 128  | total loss: [1m[32m0.61760[0m[0m | time: 2.684s
[2K
| Adam | epoch: 019 | loss: 0.61760 - acc: 0.6495 -- iter: 064/224
[A[ATraining Step: 129  | total loss: [1m[32m0.61082[0m[0m | time: 4.158s
[2K
| Adam | epoch: 019 | loss: 0.61082 - acc: 0.6564 -- iter: 096/224
[A[ATraining Step: 130  | total loss: [1m[32m0.61410[0m[0m | time: 5.585s
[2K
| Adam | epoch: 019 | loss: 0.61410 - acc: 0.6502 -- iter: 128/224
[A[ATraining Step: 131  | total loss: [1m[32m0.59760[0m[0m | time: 6.820s
[2K
| Adam | epoch: 019 | loss: 0.59760 - acc: 0.6570 -- iter: 160/224
[A[ATraining Step: 132  | total loss: [1m[32m0.59627[0m[0m | time: 8.246s
[2K
| Adam | epoch: 019 | loss: 0.59627 - acc: 0.6601 -- iter: 192/224
[A[ATraining Step: 133  | total loss: [1m[32m0.59353[0m[0m | time: 10.678s
[2K
| Adam | epoch: 019 | loss: 0.59353 - acc: 0.6628 | val_loss: 0.72745 - val_acc: 0.6197 -- iter: 224/224
--
Training Step: 134  | total loss: [1m[32m0.58183[0m[0m | time: 1.513s
[2K
| Adam | epoch: 020 | loss: 0.58183 - acc: 0.6778 -- iter: 032/224
[A[ATraining Step: 135  | total loss: [1m[32m0.57696[0m[0m | time: 2.756s
[2K
| Adam | epoch: 020 | loss: 0.57696 - acc: 0.6819 -- iter: 064/224
[A[ATraining Step: 136  | total loss: [1m[32m0.57527[0m[0m | time: 3.945s
[2K
| Adam | epoch: 020 | loss: 0.57527 - acc: 0.6824 -- iter: 096/224
[A[ATraining Step: 137  | total loss: [1m[32m0.56849[0m[0m | time: 4.968s
[2K
| Adam | epoch: 020 | loss: 0.56849 - acc: 0.6830 -- iter: 128/224
[A[ATraining Step: 138  | total loss: [1m[32m0.57362[0m[0m | time: 6.291s
[2K
| Adam | epoch: 020 | loss: 0.57362 - acc: 0.6865 -- iter: 160/224
[A[ATraining Step: 139  | total loss: [1m[32m0.56120[0m[0m | time: 7.512s
[2K
| Adam | epoch: 020 | loss: 0.56120 - acc: 0.7023 -- iter: 192/224
[A[ATraining Step: 140  | total loss: [1m[32m0.56319[0m[0m | time: 9.826s
[2K
| Adam | epoch: 020 | loss: 0.56319 - acc: 0.6914 | val_loss: 0.81182 - val_acc: 0.5352 -- iter: 224/224
--
Training Step: 141  | total loss: [1m[32m0.56014[0m[0m | time: 1.286s
[2K
| Adam | epoch: 021 | loss: 0.56014 - acc: 0.6910 -- iter: 032/224
[A[ATraining Step: 142  | total loss: [1m[32m0.54709[0m[0m | time: 2.557s
[2K
| Adam | epoch: 021 | loss: 0.54709 - acc: 0.7000 -- iter: 064/224
[A[ATraining Step: 143  | total loss: [1m[32m0.53505[0m[0m | time: 3.838s
[2K
| Adam | epoch: 021 | loss: 0.53505 - acc: 0.7082 -- iter: 096/224
[A[ATraining Step: 144  | total loss: [1m[32m0.52653[0m[0m | time: 5.140s
[2K
| Adam | epoch: 021 | loss: 0.52653 - acc: 0.7155 -- iter: 128/224
[A[ATraining Step: 145  | total loss: [1m[32m0.52937[0m[0m | time: 6.493s
[2K
| Adam | epoch: 021 | loss: 0.52937 - acc: 0.7158 -- iter: 160/224
[A[ATraining Step: 146  | total loss: [1m[32m0.51592[0m[0m | time: 7.657s
[2K
| Adam | epoch: 021 | loss: 0.51592 - acc: 0.7255 -- iter: 192/224
[A[ATraining Step: 147  | total loss: [1m[32m0.50503[0m[0m | time: 9.822s
[2K
| Adam | epoch: 021 | loss: 0.50503 - acc: 0.7373 | val_loss: 0.90532 - val_acc: 0.5634 -- iter: 224/224
--
Training Step: 148  | total loss: [1m[32m0.49778[0m[0m | time: 1.468s
[2K
| Adam | epoch: 022 | loss: 0.49778 - acc: 0.7448 -- iter: 032/224
[A[ATraining Step: 149  | total loss: [1m[32m0.50751[0m[0m | time: 2.824s
[2K
| Adam | epoch: 022 | loss: 0.50751 - acc: 0.7422 -- iter: 064/224
[A[ATraining Step: 150  | total loss: [1m[32m0.51122[0m[0m | time: 4.199s
[2K
| Adam | epoch: 022 | loss: 0.51122 - acc: 0.7367 -- iter: 096/224
[A[ATraining Step: 151  | total loss: [1m[32m0.50764[0m[0m | time: 5.528s
[2K
| Adam | epoch: 022 | loss: 0.50764 - acc: 0.7412 -- iter: 128/224
[A[ATraining Step: 152  | total loss: [1m[32m0.50366[0m[0m | time: 6.873s
[2K
| Adam | epoch: 022 | loss: 0.50366 - acc: 0.7421 -- iter: 160/224
[A[ATraining Step: 153  | total loss: [1m[32m0.49070[0m[0m | time: 8.371s
[2K
| Adam | epoch: 022 | loss: 0.49070 - acc: 0.7585 -- iter: 192/224
[A[ATraining Step: 154  | total loss: [1m[32m0.49499[0m[0m | time: 10.757s
[2K
| Adam | epoch: 022 | loss: 0.49499 - acc: 0.7483 | val_loss: 0.78887 - val_acc: 0.6197 -- iter: 224/224
--
Training Step: 155  | total loss: [1m[32m0.48960[0m[0m | time: 1.639s
[2K
| Adam | epoch: 023 | loss: 0.48960 - acc: 0.7516 -- iter: 032/224
[A[ATraining Step: 156  | total loss: [1m[32m0.48896[0m[0m | time: 3.377s
[2K
| Adam | epoch: 023 | loss: 0.48896 - acc: 0.7483 -- iter: 064/224
[A[ATraining Step: 157  | total loss: [1m[32m0.48297[0m[0m | time: 4.913s
[2K
| Adam | epoch: 023 | loss: 0.48297 - acc: 0.7547 -- iter: 096/224
[A[ATraining Step: 158  | total loss: [1m[32m0.49841[0m[0m | time: 6.623s
[2K
| Adam | epoch: 023 | loss: 0.49841 - acc: 0.7417 -- iter: 128/224
[A[ATraining Step: 159  | total loss: [1m[32m0.49342[0m[0m | time: 8.319s
[2K
| Adam | epoch: 023 | loss: 0.49342 - acc: 0.7488 -- iter: 160/224
[A[ATraining Step: 160  | total loss: [1m[32m0.50864[0m[0m | time: 10.041s
[2K
| Adam | epoch: 023 | loss: 0.50864 - acc: 0.7489 -- iter: 192/224
[A[ATraining Step: 161  | total loss: [1m[32m0.50378[0m[0m | time: 12.534s
[2K
| Adam | epoch: 023 | loss: 0.50378 - acc: 0.7490 | val_loss: 0.80392 - val_acc: 0.6620 -- iter: 224/224
--
Training Step: 162  | total loss: [1m[32m0.51419[0m[0m | time: 1.802s
[2K
| Adam | epoch: 024 | loss: 0.51419 - acc: 0.7366 -- iter: 032/224
[A[ATraining Step: 163  | total loss: [1m[32m0.50507[0m[0m | time: 3.316s
[2K
| Adam | epoch: 024 | loss: 0.50507 - acc: 0.7411 -- iter: 064/224
[A[ATraining Step: 164  | total loss: [1m[32m0.49223[0m[0m | time: 4.989s
[2K
| Adam | epoch: 024 | loss: 0.49223 - acc: 0.7545 -- iter: 096/224
[A[ATraining Step: 165  | total loss: [1m[32m0.48727[0m[0m | time: 6.500s
[2K
| Adam | epoch: 024 | loss: 0.48727 - acc: 0.7634 -- iter: 128/224
[A[ATraining Step: 166  | total loss: [1m[32m0.47975[0m[0m | time: 8.126s
[2K
| Adam | epoch: 024 | loss: 0.47975 - acc: 0.7777 -- iter: 160/224
[A[ATraining Step: 167  | total loss: [1m[32m0.47206[0m[0m | time: 9.744s
[2K
| Adam | epoch: 024 | loss: 0.47206 - acc: 0.7874 -- iter: 192/224
[A[ATraining Step: 168  | total loss: [1m[32m0.47562[0m[0m | time: 12.530s
[2K
| Adam | epoch: 024 | loss: 0.47562 - acc: 0.7868 | val_loss: 0.88585 - val_acc: 0.6620 -- iter: 224/224
--
Training Step: 169  | total loss: [1m[32m0.46331[0m[0m | time: 1.614s
[2K
| Adam | epoch: 025 | loss: 0.46331 - acc: 0.7956 -- iter: 032/224
[A[ATraining Step: 170  | total loss: [1m[32m0.46439[0m[0m | time: 3.223s
[2K
| Adam | epoch: 025 | loss: 0.46439 - acc: 0.7942 -- iter: 064/224
[A[ATraining Step: 171  | total loss: [1m[32m0.45824[0m[0m | time: 4.959s
[2K
| Adam | epoch: 025 | loss: 0.45824 - acc: 0.7898 -- iter: 096/224
[A[ATraining Step: 172  | total loss: [1m[32m0.45490[0m[0m | time: 6.520s
[2K
| Adam | epoch: 025 | loss: 0.45490 - acc: 0.7889 -- iter: 128/224
[A[ATraining Step: 173  | total loss: [1m[32m0.44908[0m[0m | time: 8.166s
[2K
| Adam | epoch: 025 | loss: 0.44908 - acc: 0.7944 -- iter: 160/224
[A[ATraining Step: 174  | total loss: [1m[32m0.42979[0m[0m | time: 9.994s
[2K
| Adam | epoch: 025 | loss: 0.42979 - acc: 0.8118 -- iter: 192/224
[A[ATraining Step: 175  | total loss: [1m[32m0.42417[0m[0m | time: 12.733s
[2K
| Adam | epoch: 025 | loss: 0.42417 - acc: 0.8119 | val_loss: 0.90017 - val_acc: 0.6761 -- iter: 224/224
--
Training Step: 176  | total loss: [1m[32m0.40659[0m[0m | time: 1.563s
[2K
| Adam | epoch: 026 | loss: 0.40659 - acc: 0.8276 -- iter: 032/224
[A[ATraining Step: 177  | total loss: [1m[32m0.38790[0m[0m | time: 2.972s
[2K
| Adam | epoch: 026 | loss: 0.38790 - acc: 0.8417 -- iter: 064/224
[A[ATraining Step: 178  | total loss: [1m[32m0.39101[0m[0m | time: 4.434s
[2K
| Adam | epoch: 026 | loss: 0.39101 - acc: 0.8388 -- iter: 096/224
[A[ATraining Step: 179  | total loss: [1m[32m0.37736[0m[0m | time: 5.527s
[2K
| Adam | epoch: 026 | loss: 0.37736 - acc: 0.8455 -- iter: 128/224
[A[ATraining Step: 180  | total loss: [1m[32m0.36918[0m[0m | time: 6.739s
[2K
| Adam | epoch: 026 | loss: 0.36918 - acc: 0.8516 -- iter: 160/224
[A[ATraining Step: 181  | total loss: [1m[32m0.36978[0m[0m | time: 7.877s
[2K
| Adam | epoch: 026 | loss: 0.36978 - acc: 0.8539 -- iter: 192/224
[A[ATraining Step: 182  | total loss: [1m[32m0.37331[0m[0m | time: 10.089s
[2K
| Adam | epoch: 026 | loss: 0.37331 - acc: 0.8498 | val_loss: 0.92889 - val_acc: 0.6620 -- iter: 224/224
--
Training Step: 183  | total loss: [1m[32m0.35691[0m[0m | time: 1.194s
[2K
| Adam | epoch: 027 | loss: 0.35691 - acc: 0.8586 -- iter: 032/224
[A[ATraining Step: 184  | total loss: [1m[32m0.37748[0m[0m | time: 2.096s
[2K
| Adam | epoch: 027 | loss: 0.37748 - acc: 0.8571 -- iter: 064/224
[A[ATraining Step: 185  | total loss: [1m[32m0.35829[0m[0m | time: 3.122s
[2K
| Adam | epoch: 027 | loss: 0.35829 - acc: 0.8651 -- iter: 096/224
[A[ATraining Step: 186  | total loss: [1m[32m0.37396[0m[0m | time: 4.254s
[2K
| Adam | epoch: 027 | loss: 0.37396 - acc: 0.8536 -- iter: 128/224
[A[ATraining Step: 187  | total loss: [1m[32m0.36480[0m[0m | time: 5.257s
[2K
| Adam | epoch: 027 | loss: 0.36480 - acc: 0.8589 -- iter: 160/224
[A[ATraining Step: 188  | total loss: [1m[32m0.36095[0m[0m | time: 6.447s
[2K
| Adam | epoch: 027 | loss: 0.36095 - acc: 0.8480 -- iter: 192/224
[A[ATraining Step: 189  | total loss: [1m[32m0.35946[0m[0m | time: 8.462s
[2K
| Adam | epoch: 027 | loss: 0.35946 - acc: 0.8444 | val_loss: 0.79458 - val_acc: 0.6197 -- iter: 224/224
--
Training Step: 190  | total loss: [1m[32m0.33770[0m[0m | time: 1.214s
[2K
| Adam | epoch: 028 | loss: 0.33770 - acc: 0.8569 -- iter: 032/224
[A[ATraining Step: 191  | total loss: [1m[32m0.33456[0m[0m | time: 2.281s
[2K
| Adam | epoch: 028 | loss: 0.33456 - acc: 0.8587 -- iter: 064/224
[A[ATraining Step: 192  | total loss: [1m[32m0.32232[0m[0m | time: 3.268s
[2K
| Adam | epoch: 028 | loss: 0.32232 - acc: 0.8666 -- iter: 096/224
[A[ATraining Step: 193  | total loss: [1m[32m0.30741[0m[0m | time: 4.266s
[2K
| Adam | epoch: 028 | loss: 0.30741 - acc: 0.8737 -- iter: 128/224
[A[ATraining Step: 194  | total loss: [1m[32m0.31150[0m[0m | time: 5.256s
[2K
| Adam | epoch: 028 | loss: 0.31150 - acc: 0.8738 -- iter: 160/224
[A[ATraining Step: 195  | total loss: [1m[32m0.31639[0m[0m | time: 6.344s
[2K
| Adam | epoch: 028 | loss: 0.31639 - acc: 0.8708 -- iter: 192/224
[A[ATraining Step: 196  | total loss: [1m[32m0.32050[0m[0m | time: 8.588s
[2K
| Adam | epoch: 028 | loss: 0.32050 - acc: 0.8618 | val_loss: 0.96784 - val_acc: 0.6620 -- iter: 224/224
--
Training Step: 197  | total loss: [1m[32m0.30508[0m[0m | time: 1.301s
[2K
| Adam | epoch: 029 | loss: 0.30508 - acc: 0.8694 -- iter: 032/224
[A[ATraining Step: 198  | total loss: [1m[32m0.31568[0m[0m | time: 2.574s
[2K
| Adam | epoch: 029 | loss: 0.31568 - acc: 0.8575 -- iter: 064/224
[A[ATraining Step: 199  | total loss: [1m[32m0.30855[0m[0m | time: 3.667s
[2K
| Adam | epoch: 029 | loss: 0.30855 - acc: 0.8623 -- iter: 096/224
[A[ATraining Step: 200  | total loss: [1m[32m0.28641[0m[0m | time: 6.069s
[2K
| Adam | epoch: 029 | loss: 0.28641 - acc: 0.8730 | val_loss: 1.02612 - val_acc: 0.7606 -- iter: 128/224
--
Training Step: 201  | total loss: [1m[32m0.27908[0m[0m | time: 7.797s
[2K
| Adam | epoch: 029 | loss: 0.27908 - acc: 0.8763 -- iter: 160/224
[A[ATraining Step: 202  | total loss: [1m[32m0.27267[0m[0m | time: 9.382s
[2K
| Adam | epoch: 029 | loss: 0.27267 - acc: 0.8793 -- iter: 192/224
[A[ATraining Step: 203  | total loss: [1m[32m0.28234[0m[0m | time: 12.032s
[2K
| Adam | epoch: 029 | loss: 0.28234 - acc: 0.8726 | val_loss: 0.92434 - val_acc: 0.6901 -- iter: 224/224
--
Training Step: 204  | total loss: [1m[32m0.27540[0m[0m | time: 1.715s
[2K
| Adam | epoch: 030 | loss: 0.27540 - acc: 0.8791 -- iter: 032/224
[A[ATraining Step: 205  | total loss: [1m[32m0.27160[0m[0m | time: 3.367s
[2K
| Adam | epoch: 030 | loss: 0.27160 - acc: 0.8787 -- iter: 064/224
[A[ATraining Step: 206  | total loss: [1m[32m0.26311[0m[0m | time: 5.027s
[2K
| Adam | epoch: 030 | loss: 0.26311 - acc: 0.8815 -- iter: 096/224
[A[ATraining Step: 207  | total loss: [1m[32m0.27481[0m[0m | time: 6.804s
[2K
| Adam | epoch: 030 | loss: 0.27481 - acc: 0.8714 -- iter: 128/224
[A[ATraining Step: 208  | total loss: [1m[32m0.38708[0m[0m | time: 8.653s
[2K
| Adam | epoch: 030 | loss: 0.38708 - acc: 0.8499 -- iter: 160/224
[A[ATraining Step: 209  | total loss: [1m[32m0.36071[0m[0m | time: 10.649s
[2K
| Adam | epoch: 030 | loss: 0.36071 - acc: 0.8587 -- iter: 192/224
[A[ATraining Step: 210  | total loss: [1m[32m0.33225[0m[0m | time: 13.131s
[2K
| Adam | epoch: 030 | loss: 0.33225 - acc: 0.8728 | val_loss: 0.71443 - val_acc: 0.7887 -- iter: 224/224
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.7512315270935961
Validation AUPRC:0.737121708707201
Test AUC:0.7382113821138211
Test AUPRC:0.7364314260171945
BestTestF1Score	0.57	0.32	0.68	0.65	0.5	15	8	33	15	0.43
BestTestMCCScore	0.56	0.35	0.69	0.7	0.47	14	6	35	16	0.5
BestTestAccuracyScore	0.56	0.35	0.69	0.7	0.47	14	6	35	16	0.5
BestValidationF1Score	0.7	0.53	0.77	0.76	0.66	19	6	36	10	0.43
BestValidationMCC	0.69	0.56	0.79	0.85	0.59	17	3	39	12	0.5
BestValidationAccuracy	0.69	0.56	0.79	0.85	0.59	17	3	39	12	0.5
TestPredictions (Threshold:0.5)
CHEMBL506166,TN,INACT,0.0	CHEMBL2042368,TN,INACT,0.019999999552965164	CHEMBL1161699,TN,INACT,0.0	CHEMBL2019155,TP,ACT,0.699999988079071	CHEMBL2323137,TP,ACT,0.8799999952316284	CHEMBL3138422,TN,INACT,0.10000000149011612	CHEMBL567544,FN,ACT,0.20000000298023224	CHEMBL568416,FN,ACT,0.23999999463558197	CHEMBL35420,TN,INACT,0.28999999165534973	CHEMBL507918,TN,INACT,0.1899999976158142	CHEMBL369953,TP,ACT,0.6399999856948853	CHEMBL432745,TN,INACT,0.09000000357627869	CHEMBL386049,FN,ACT,0.25	CHEMBL1628004,FN,ACT,0.07000000029802322	CHEMBL123233,TN,INACT,0.05999999865889549	CHEMBL331002,TN,INACT,0.23999999463558197	CHEMBL355001,TN,INACT,0.0	CHEMBL3809848,TN,INACT,0.0	CHEMBL232617,TP,ACT,0.5099999904632568	CHEMBL13541,TN,INACT,0.49000000953674316	CHEMBL24147,FN,ACT,0.009999999776482582	CHEMBL175652,TN,INACT,0.0	CHEMBL466083,TN,INACT,0.05999999865889549	CHEMBL478518,TN,INACT,0.12999999523162842	CHEMBL308819,TN,INACT,0.4000000059604645	CHEMBL3356279,TN,INACT,0.23000000417232513	CHEMBL449983,TN,INACT,0.1599999964237213	CHEMBL148680,TN,INACT,0.029999999329447746	CHEMBL2042369,TN,INACT,0.0	CHEMBL19,FN,ACT,0.05000000074505806	CHEMBL3098531,TN,INACT,0.019999999552965164	CHEMBL15844,TN,INACT,0.25999999046325684	CHEMBL99958,TN,INACT,0.029999999329447746	CHEMBL206452,TP,ACT,0.7699999809265137	CHEMBL1614844,FN,ACT,0.17000000178813934	CHEMBL1683467,TP,ACT,0.9100000262260437	CHEMBL331308,FP,INACT,0.5099999904632568	CHEMBL1822701,FN,ACT,0.019999999552965164	CHEMBL139023,FP,INACT,0.5199999809265137	CHEMBL35594,FP,INACT,0.5799999833106995	CHEMBL6724,TP,ACT,0.9200000166893005	CHEMBL3110152,TN,INACT,0.05000000074505806	CHEMBL572366,TN,INACT,0.23000000417232513	CHEMBL123064,TN,INACT,0.009999999776482582	CHEMBL499560,TP,ACT,0.8999999761581421	CHEMBL328910,TN,INACT,0.20999999344348907	CHEMBL3627998,TN,INACT,0.03999999910593033	CHEMBL2179308,FN,ACT,0.3799999952316284	CHEMBL1054,FN,ACT,0.3100000023841858	CHEMBL215548,FP,INACT,0.5799999833106995	CHEMBL121729,TN,INACT,0.07999999821186066	CHEMBL3356275,FP,INACT,0.6299999952316284	CHEMBL50,FN,ACT,0.10000000149011612	CHEMBL199464,FN,ACT,0.07000000029802322	CHEMBL18,TP,ACT,0.7799999713897705	CHEMBL1807500,TN,INACT,0.0	CHEMBL578167,FN,ACT,0.11999999731779099	CHEMBL467979,TP,ACT,0.6000000238418579	CHEMBL1951641,TN,INACT,0.23999999463558197	CHEMBL1822700,FN,ACT,0.44999998807907104	CHEMBL352612,TN,INACT,0.0	CHEMBL118,FN,ACT,0.03999999910593033	CHEMBL513184,TP,ACT,0.699999988079071	CHEMBL574782,TP,ACT,0.8399999737739563	CHEMBL2333417,FN,ACT,0.17000000178813934	CHEMBL2407318,TP,ACT,0.8899999856948853	CHEMBL141618,FP,INACT,0.6700000166893005	CHEMBL390320,TN,INACT,0.4699999988079071	CHEMBL140942,TN,INACT,0.25	CHEMBL941,TN,INACT,0.07000000029802322	CHEMBL567139,TP,ACT,0.5099999904632568	

