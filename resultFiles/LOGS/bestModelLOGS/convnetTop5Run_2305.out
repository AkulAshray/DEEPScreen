ImageNetInceptionV2 CHEMBL1841 adam 0.001 15 0 0 0.8 False True
Number of active compounds :	198
Number of inactive compounds :	198
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL1841_adam_0.001_15_0_0_0.8_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL1841_adam_0.001_15_0.8/
---------------------------------
Training samples: 243
Validation samples: 77
--
Training Step: 1  | time: 212.815s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/243
[A[ATraining Step: 2  | total loss: [1m[32m0.66428[0m[0m | time: 741.573s
[2K
| Adam | epoch: 001 | loss: 0.66428 - acc: 0.2812 -- iter: 064/243
[A[ATraining Step: 3  | total loss: [1m[32m0.88932[0m[0m | time: 1139.319s
[2K
| Adam | epoch: 001 | loss: 0.88932 - acc: 0.4602 -- iter: 096/243
[A[ATraining Step: 4  | total loss: [1m[32m0.78206[0m[0m | time: 1428.762s
[2K
| Adam | epoch: 001 | loss: 0.78206 - acc: 0.5135 -- iter: 128/243
[A[ATraining Step: 5  | total loss: [1m[32m0.81630[0m[0m | time: 1607.564s
[2K
| Adam | epoch: 001 | loss: 0.81630 - acc: 0.5474 -- iter: 160/243
[A[ATraining Step: 6  | total loss: [1m[32m0.80883[0m[0m | time: 1950.139s
[2K
| Adam | epoch: 001 | loss: 0.80883 - acc: 0.5571 -- iter: 192/243
[A[ATraining Step: 7  | total loss: [1m[32m0.69382[0m[0m | time: 2152.260s
[2K
| Adam | epoch: 001 | loss: 0.69382 - acc: 0.6166 -- iter: 224/243
[A[ATraining Step: 8  | total loss: [1m[32m0.66987[0m[0m | time: 2172.748s
[2K
| Adam | epoch: 001 | loss: 0.66987 - acc: 0.6213 | val_loss: 1.76569 - val_acc: 0.3636 -- iter: 243/243
--
Training Step: 9  | total loss: [1m[32m0.65488[0m[0m | time: 69.752s
[2K
| Adam | epoch: 002 | loss: 0.65488 - acc: 0.6546 -- iter: 032/243
[A[ATraining Step: 10  | total loss: [1m[32m0.58372[0m[0m | time: 180.775s
[2K
| Adam | epoch: 002 | loss: 0.58372 - acc: 0.7484 -- iter: 064/243
[A[ATraining Step: 11  | total loss: [1m[32m0.67762[0m[0m | time: 259.462s
[2K
| Adam | epoch: 002 | loss: 0.67762 - acc: 0.6159 -- iter: 096/243
[A[ATraining Step: 12  | total loss: [1m[32m0.66553[0m[0m | time: 573.693s
[2K
| Adam | epoch: 002 | loss: 0.66553 - acc: 0.6763 -- iter: 128/243
[A[ATraining Step: 13  | total loss: [1m[32m0.66916[0m[0m | time: 767.020s
[2K
| Adam | epoch: 002 | loss: 0.66916 - acc: 0.6945 -- iter: 160/243
[A[ATraining Step: 14  | total loss: [1m[32m0.64229[0m[0m | time: 863.167s
[2K
| Adam | epoch: 002 | loss: 0.64229 - acc: 0.7044 -- iter: 192/243
[A[ATraining Step: 15  | total loss: [1m[32m0.65192[0m[0m | time: 879.050s
[2K
| Adam | epoch: 002 | loss: 0.65192 - acc: 0.6978 -- iter: 224/243
[A[ATraining Step: 16  | total loss: [1m[32m0.63179[0m[0m | time: 1023.210s
[2K
| Adam | epoch: 002 | loss: 0.63179 - acc: 0.6939 | val_loss: 1.27932 - val_acc: 0.3636 -- iter: 243/243
--
Training Step: 17  | total loss: [1m[32m0.62822[0m[0m | time: 89.042s
[2K
| Adam | epoch: 003 | loss: 0.62822 - acc: 0.6804 -- iter: 032/243
[A[ATraining Step: 18  | total loss: [1m[32m0.56058[0m[0m | time: 144.048s
[2K
| Adam | epoch: 003 | loss: 0.56058 - acc: 0.7364 -- iter: 064/243
[A[ATraining Step: 19  | total loss: [1m[32m0.45301[0m[0m | time: 294.127s
[2K
| Adam | epoch: 003 | loss: 0.45301 - acc: 0.8242 -- iter: 096/243
[A[ATraining Step: 20  | total loss: [1m[32m0.58763[0m[0m | time: 306.961s
[2K
| Adam | epoch: 003 | loss: 0.58763 - acc: 0.7602 -- iter: 128/243
[A[ATraining Step: 21  | total loss: [1m[32m0.59288[0m[0m | time: 336.639s
[2K
| Adam | epoch: 003 | loss: 0.59288 - acc: 0.7667 -- iter: 160/243
[A[ATraining Step: 22  | total loss: [1m[32m0.53587[0m[0m | time: 562.677s
[2K
| Adam | epoch: 003 | loss: 0.53587 - acc: 0.7898 -- iter: 192/243
[A[ATraining Step: 23  | total loss: [1m[32m0.54736[0m[0m | time: 770.681s
[2K
| Adam | epoch: 003 | loss: 0.54736 - acc: 0.7601 -- iter: 224/243
[A[ATraining Step: 24  | total loss: [1m[32m0.52637[0m[0m | time: 1340.961s
[2K
| Adam | epoch: 003 | loss: 0.52637 - acc: 0.7573 | val_loss: 1.05892 - val_acc: 0.3636 -- iter: 243/243
--
Training Step: 25  | total loss: [1m[32m0.51165[0m[0m | time: 43.067s
[2K
| Adam | epoch: 004 | loss: 0.51165 - acc: 0.7809 -- iter: 032/243
[A[ATraining Step: 26  | total loss: [1m[32m0.51201[0m[0m | time: 277.839s
[2K
| Adam | epoch: 004 | loss: 0.51201 - acc: 0.7810 -- iter: 064/243
[A[ATraining Step: 27  | total loss: [1m[32m0.48739[0m[0m | time: 484.815s
[2K
| Adam | epoch: 004 | loss: 0.48739 - acc: 0.7967 -- iter: 096/243
[A[ATraining Step: 28  | total loss: [1m[32m0.42253[0m[0m | time: 667.058s
[2K
| Adam | epoch: 004 | loss: 0.42253 - acc: 0.8344 -- iter: 128/243
[A[ATraining Step: 29  | total loss: [1m[32m0.42816[0m[0m | time: 972.148s
[2K
| Adam | epoch: 004 | loss: 0.42816 - acc: 0.8518 -- iter: 160/243
[A[ATraining Step: 30  | total loss: [1m[32m0.49343[0m[0m | time: 1079.127s
[2K
| Adam | epoch: 004 | loss: 0.49343 - acc: 0.8055 -- iter: 192/243
[A[ATraining Step: 31  | total loss: [1m[32m0.60820[0m[0m | time: 1357.103s
[2K
| Adam | epoch: 004 | loss: 0.60820 - acc: 0.7566 -- iter: 224/243
[A[ATraining Step: 32  | total loss: [1m[32m0.53543[0m[0m | time: 1683.252s
[2K
| Adam | epoch: 004 | loss: 0.53543 - acc: 0.7903 | val_loss: 0.63011 - val_acc: 0.6364 -- iter: 243/243
--
Training Step: 33  | total loss: [1m[32m0.48278[0m[0m | time: 144.418s
[2K
| Adam | epoch: 005 | loss: 0.48278 - acc: 0.8158 -- iter: 032/243
[A[ATraining Step: 34  | total loss: [1m[32m0.47405[0m[0m | time: 305.804s
[2K
| Adam | epoch: 005 | loss: 0.47405 - acc: 0.8084 -- iter: 064/243
[A[ATraining Step: 35  | total loss: [1m[32m0.46115[0m[0m | time: 342.510s
[2K
| Adam | epoch: 005 | loss: 0.46115 - acc: 0.8158 -- iter: 096/243
[A[ATraining Step: 36  | total loss: [1m[32m0.43789[0m[0m | time: 365.811s
[2K
| Adam | epoch: 005 | loss: 0.43789 - acc: 0.8212 -- iter: 128/243
[A[ATraining Step: 37  | total loss: [1m[32m0.39187[0m[0m | time: 389.894s
[2K
| Adam | epoch: 005 | loss: 0.39187 - acc: 0.8569 -- iter: 160/243
[A[ATraining Step: 38  | total loss: [1m[32m0.39191[0m[0m | time: 404.167s
[2K
| Adam | epoch: 005 | loss: 0.39191 - acc: 0.8482 -- iter: 192/243
[A[ATraining Step: 39  | total loss: [1m[32m0.37101[0m[0m | time: 416.281s
[2K
| Adam | epoch: 005 | loss: 0.37101 - acc: 0.8653 -- iter: 224/243
[A[ATraining Step: 40  | total loss: [1m[32m0.36882[0m[0m | time: 436.662s
[2K
| Adam | epoch: 005 | loss: 0.36882 - acc: 0.8554 | val_loss: 1.02792 - val_acc: 0.6364 -- iter: 243/243
--
Training Step: 41  | total loss: [1m[32m0.32862[0m[0m | time: 102.454s
[2K
| Adam | epoch: 006 | loss: 0.32862 - acc: 0.8705 -- iter: 032/243
[A[ATraining Step: 42  | total loss: [1m[32m0.31333[0m[0m | time: 224.695s
[2K
| Adam | epoch: 006 | loss: 0.31333 - acc: 0.8713 -- iter: 064/243
[A[ATraining Step: 43  | total loss: [1m[32m0.33279[0m[0m | time: 303.726s
[2K
| Adam | epoch: 006 | loss: 0.33279 - acc: 0.8609 -- iter: 096/243
[A[ATraining Step: 44  | total loss: [1m[32m0.33531[0m[0m | time: 370.067s
[2K
| Adam | epoch: 006 | loss: 0.33531 - acc: 0.8634 -- iter: 128/243
[A[ATraining Step: 45  | total loss: [1m[32m0.32808[0m[0m | time: 446.822s
[2K
| Adam | epoch: 006 | loss: 0.32808 - acc: 0.8776 -- iter: 160/243
[A[ATraining Step: 46  | total loss: [1m[32m0.31132[0m[0m | time: 495.280s
[2K
| Adam | epoch: 006 | loss: 0.31132 - acc: 0.8893 -- iter: 192/243
[A[ATraining Step: 47  | total loss: [1m[32m0.34941[0m[0m | time: 612.996s
[2K
| Adam | epoch: 006 | loss: 0.34941 - acc: 0.8614 -- iter: 224/243
[A[ATraining Step: 48  | total loss: [1m[32m0.34357[0m[0m | time: 837.215s
[2K
| Adam | epoch: 006 | loss: 0.34357 - acc: 0.8535 | val_loss: 2.63844 - val_acc: 0.6364 -- iter: 243/243
--
Training Step: 49  | total loss: [1m[32m0.58330[0m[0m | time: 222.052s
[2K
| Adam | epoch: 007 | loss: 0.58330 - acc: 0.7878 -- iter: 032/243
[A[ATraining Step: 50  | total loss: [1m[32m0.56629[0m[0m | time: 349.195s
[2K
| Adam | epoch: 007 | loss: 0.56629 - acc: 0.7916 -- iter: 064/243
[A[ATraining Step: 51  | total loss: [1m[32m0.53353[0m[0m | time: 480.151s
[2K
| Adam | epoch: 007 | loss: 0.53353 - acc: 0.7948 -- iter: 096/243
[A[ATraining Step: 52  | total loss: [1m[32m0.55210[0m[0m | time: 525.074s
[2K
| Adam | epoch: 007 | loss: 0.55210 - acc: 0.7881 -- iter: 128/243
[A[ATraining Step: 53  | total loss: [1m[32m0.54712[0m[0m | time: 589.126s
[2K
| Adam | epoch: 007 | loss: 0.54712 - acc: 0.7733 -- iter: 160/243
[A[ATraining Step: 54  | total loss: [1m[32m0.53236[0m[0m | time: 632.432s
[2K
| Adam | epoch: 007 | loss: 0.53236 - acc: 0.7680 -- iter: 192/243
[A[ATraining Step: 55  | total loss: [1m[32m0.49525[0m[0m | time: 652.693s
[2K
| Adam | epoch: 007 | loss: 0.49525 - acc: 0.8011 -- iter: 224/243
[A[ATraining Step: 56  | total loss: [1m[32m0.48204[0m[0m | time: 674.713s
[2K
| Adam | epoch: 007 | loss: 0.48204 - acc: 0.7983 | val_loss: 1.33603 - val_acc: 0.6364 -- iter: 243/243
--
Training Step: 57  | total loss: [1m[32m0.48259[0m[0m | time: 81.150s
[2K
| Adam | epoch: 008 | loss: 0.48259 - acc: 0.7960 -- iter: 032/243
[A[ATraining Step: 58  | total loss: [1m[32m0.56634[0m[0m | time: 111.928s
[2K
| Adam | epoch: 008 | loss: 0.56634 - acc: 0.7513 -- iter: 064/243
[A[ATraining Step: 59  | total loss: [1m[32m0.53813[0m[0m | time: 157.878s
[2K
| Adam | epoch: 008 | loss: 0.53813 - acc: 0.7596 -- iter: 096/243
[A[ATraining Step: 60  | total loss: [1m[32m0.50465[0m[0m | time: 364.391s
[2K
| Adam | epoch: 008 | loss: 0.50465 - acc: 0.7831 -- iter: 128/243
[A[ATraining Step: 61  | total loss: [1m[32m0.48337[0m[0m | time: 572.421s
[2K
| Adam | epoch: 008 | loss: 0.48337 - acc: 0.7951 -- iter: 160/243
[A[ATraining Step: 62  | total loss: [1m[32m0.46463[0m[0m | time: 645.318s
[2K
| Adam | epoch: 008 | loss: 0.46463 - acc: 0.8014 -- iter: 192/243
[A[ATraining Step: 63  | total loss: [1m[32m0.43257[0m[0m | time: 688.461s
[2K
| Adam | epoch: 008 | loss: 0.43257 - acc: 0.8199 -- iter: 224/243
[A[ATraining Step: 64  | total loss: [1m[32m0.39751[0m[0m | time: 729.391s
[2K
| Adam | epoch: 008 | loss: 0.39751 - acc: 0.8424 | val_loss: 1.67833 - val_acc: 0.4026 -- iter: 243/243
--
Training Step: 65  | total loss: [1m[32m0.42164[0m[0m | time: 35.825s
[2K
| Adam | epoch: 009 | loss: 0.42164 - acc: 0.8271 -- iter: 032/243
[A[ATraining Step: 66  | total loss: [1m[32m0.44576[0m[0m | time: 130.586s
[2K
| Adam | epoch: 009 | loss: 0.44576 - acc: 0.8216 -- iter: 064/243
[A[ATraining Step: 67  | total loss: [1m[32m0.42693[0m[0m | time: 203.039s
[2K
| Adam | epoch: 009 | loss: 0.42693 - acc: 0.8355 -- iter: 096/243
[A[ATraining Step: 68  | total loss: [1m[32m0.42056[0m[0m | time: 227.328s
[2K
| Adam | epoch: 009 | loss: 0.42056 - acc: 0.8364 -- iter: 128/243
[A[ATraining Step: 69  | total loss: [1m[32m0.39228[0m[0m | time: 246.506s
[2K
| Adam | epoch: 009 | loss: 0.39228 - acc: 0.8483 -- iter: 160/243
[A[ATraining Step: 70  | total loss: [1m[32m0.36274[0m[0m | time: 331.150s
[2K
| Adam | epoch: 009 | loss: 0.36274 - acc: 0.8622 -- iter: 192/243
[A[ATraining Step: 71  | total loss: [1m[32m0.33704[0m[0m | time: 357.994s
[2K
| Adam | epoch: 009 | loss: 0.33704 - acc: 0.8707 -- iter: 224/243
[A[ATraining Step: 72  | total loss: [1m[32m0.32355[0m[0m | time: 391.186s
[2K
| Adam | epoch: 009 | loss: 0.32355 - acc: 0.8734 | val_loss: 4.32756 - val_acc: 0.3636 -- iter: 243/243
--
Training Step: 73  | total loss: [1m[32m0.29287[0m[0m | time: 105.925s
[2K
| Adam | epoch: 010 | loss: 0.29287 - acc: 0.8875 -- iter: 032/243
[A[ATraining Step: 74  | total loss: [1m[32m0.27833[0m[0m | time: 181.814s
[2K
| Adam | epoch: 010 | loss: 0.27833 - acc: 0.8964 -- iter: 064/243
[A[ATraining Step: 75  | total loss: [1m[32m0.28676[0m[0m | time: 210.232s
[2K
| Adam | epoch: 010 | loss: 0.28676 - acc: 0.8907 -- iter: 096/243
[A[ATraining Step: 76  | total loss: [1m[32m0.28086[0m[0m | time: 257.491s
[2K
| Adam | epoch: 010 | loss: 0.28086 - acc: 0.8924 -- iter: 128/243
[A[ATraining Step: 77  | total loss: [1m[32m0.26291[0m[0m | time: 338.398s
[2K
| Adam | epoch: 010 | loss: 0.26291 - acc: 0.8972 -- iter: 160/243
[A[ATraining Step: 78  | total loss: [1m[32m0.26267[0m[0m | time: 448.125s
[2K
| Adam | epoch: 010 | loss: 0.26267 - acc: 0.8948 -- iter: 192/243
[A[ATraining Step: 79  | total loss: [1m[32m0.26433[0m[0m | time: 489.015s
[2K
| Adam | epoch: 010 | loss: 0.26433 - acc: 0.8960 -- iter: 224/243
[A[ATraining Step: 80  | total loss: [1m[32m0.24385[0m[0m | time: 524.052s
[2K
| Adam | epoch: 010 | loss: 0.24385 - acc: 0.9067 | val_loss: 0.54347 - val_acc: 0.7792 -- iter: 243/243
--
Training Step: 81  | total loss: [1m[32m0.24591[0m[0m | time: 50.652s
[2K
| Adam | epoch: 011 | loss: 0.24591 - acc: 0.9054 -- iter: 032/243
[A[ATraining Step: 82  | total loss: [1m[32m0.22771[0m[0m | time: 82.665s
[2K
| Adam | epoch: 011 | loss: 0.22771 - acc: 0.9149 -- iter: 064/243
[A[ATraining Step: 83  | total loss: [1m[32m0.23315[0m[0m | time: 163.170s
[2K
| Adam | epoch: 011 | loss: 0.23315 - acc: 0.9109 -- iter: 096/243
[A[ATraining Step: 84  | total loss: [1m[32m0.24283[0m[0m | time: 235.897s
[2K
| Adam | epoch: 011 | loss: 0.24283 - acc: 0.9042 -- iter: 128/243
[A[ATraining Step: 85  | total loss: [1m[32m0.26603[0m[0m | time: 277.915s
[2K
| Adam | epoch: 011 | loss: 0.26603 - acc: 0.9013 -- iter: 160/243
[A[ATraining Step: 86  | total loss: [1m[32m0.25197[0m[0m | time: 322.147s
[2K
| Adam | epoch: 011 | loss: 0.25197 - acc: 0.9049 -- iter: 192/243
[A[ATraining Step: 87  | total loss: [1m[32m0.23598[0m[0m | time: 372.371s
[2K
| Adam | epoch: 011 | loss: 0.23598 - acc: 0.9144 -- iter: 224/243
[A[ATraining Step: 88  | total loss: [1m[32m0.22611[0m[0m | time: 433.106s
[2K
| Adam | epoch: 011 | loss: 0.22611 - acc: 0.9167 | val_loss: 0.58366 - val_acc: 0.7273 -- iter: 243/243
--
Training Step: 89  | total loss: [1m[32m0.22355[0m[0m | time: 22.703s
[2K
| Adam | epoch: 012 | loss: 0.22355 - acc: 0.9157 -- iter: 032/243
[A[ATraining Step: 90  | total loss: [1m[32m0.20765[0m[0m | time: 40.904s
[2K
| Adam | epoch: 012 | loss: 0.20765 - acc: 0.9241 -- iter: 064/243
[A[ATraining Step: 91  | total loss: [1m[32m0.19146[0m[0m | time: 113.063s
[2K
| Adam | epoch: 012 | loss: 0.19146 - acc: 0.9317 -- iter: 096/243
[A[ATraining Step: 92  | total loss: [1m[32m0.20837[0m[0m | time: 140.521s
[2K
| Adam | epoch: 012 | loss: 0.20837 - acc: 0.9198 -- iter: 128/243
[A[ATraining Step: 93  | total loss: [1m[32m0.19907[0m[0m | time: 186.884s
[2K
| Adam | epoch: 012 | loss: 0.19907 - acc: 0.9215 -- iter: 160/243
[A[ATraining Step: 94  | total loss: [1m[32m0.26425[0m[0m | time: 226.793s
[2K
| Adam | epoch: 012 | loss: 0.26425 - acc: 0.9138 -- iter: 192/243
[A[ATraining Step: 95  | total loss: [1m[32m0.26047[0m[0m | time: 280.917s
[2K
| Adam | epoch: 012 | loss: 0.26047 - acc: 0.9130 -- iter: 224/243
[A[ATraining Step: 96  | total loss: [1m[32m0.25147[0m[0m | time: 397.718s
[2K
| Adam | epoch: 012 | loss: 0.25147 - acc: 0.9155 | val_loss: 1.30269 - val_acc: 0.7143 -- iter: 243/243
--
Training Step: 97  | total loss: [1m[32m0.24128[0m[0m | time: 36.932s
[2K
| Adam | epoch: 013 | loss: 0.24128 - acc: 0.9208 -- iter: 032/243
[A[ATraining Step: 98  | total loss: [1m[32m0.22910[0m[0m | time: 70.042s
[2K
| Adam | epoch: 013 | loss: 0.22910 - acc: 0.9256 -- iter: 064/243
[A[ATraining Step: 99  | total loss: [1m[32m0.22347[0m[0m | time: 100.905s
[2K
| Adam | epoch: 013 | loss: 0.22347 - acc: 0.9225 -- iter: 096/243
[A[ATraining Step: 100  | total loss: [1m[32m0.20578[0m[0m | time: 157.488s
[2K
| Adam | epoch: 013 | loss: 0.20578 - acc: 0.9303 -- iter: 128/243
[A[ATraining Step: 101  | total loss: [1m[32m0.20110[0m[0m | time: 244.078s
[2K
| Adam | epoch: 013 | loss: 0.20110 - acc: 0.9279 -- iter: 160/243
[A[ATraining Step: 102  | total loss: [1m[32m0.18618[0m[0m | time: 313.510s
[2K
| Adam | epoch: 013 | loss: 0.18618 - acc: 0.9351 -- iter: 192/243
[A[ATraining Step: 103  | total loss: [1m[32m0.20478[0m[0m | time: 425.062s
[2K
| Adam | epoch: 013 | loss: 0.20478 - acc: 0.9384 -- iter: 224/243
[A[ATraining Step: 104  | total loss: [1m[32m0.19691[0m[0m | time: 449.968s
[2K
| Adam | epoch: 013 | loss: 0.19691 - acc: 0.9415 | val_loss: 0.60040 - val_acc: 0.7273 -- iter: 243/243
--
Training Step: 105  | total loss: [1m[32m0.19476[0m[0m | time: 17.614s
[2K
| Adam | epoch: 014 | loss: 0.19476 - acc: 0.9379 -- iter: 032/243
[A[ATraining Step: 106  | total loss: [1m[32m0.19168[0m[0m | time: 35.095s
[2K
| Adam | epoch: 014 | loss: 0.19168 - acc: 0.9348 -- iter: 064/243
[A[ATraining Step: 107  | total loss: [1m[32m0.17800[0m[0m | time: 47.147s
[2K
| Adam | epoch: 014 | loss: 0.17800 - acc: 0.9382 -- iter: 096/243
[A[ATraining Step: 108  | total loss: [1m[32m0.17548[0m[0m | time: 59.565s
[2K
| Adam | epoch: 014 | loss: 0.17548 - acc: 0.9338 -- iter: 128/243
[A[ATraining Step: 109  | total loss: [1m[32m0.16339[0m[0m | time: 78.261s
[2K
| Adam | epoch: 014 | loss: 0.16339 - acc: 0.9404 -- iter: 160/243
[A[ATraining Step: 110  | total loss: [1m[32m0.15417[0m[0m | time: 94.201s
[2K
| Adam | epoch: 014 | loss: 0.15417 - acc: 0.9464 -- iter: 192/243
[A[ATraining Step: 111  | total loss: [1m[32m0.14480[0m[0m | time: 102.969s
[2K
| Adam | epoch: 014 | loss: 0.14480 - acc: 0.9518 -- iter: 224/243
[A[ATraining Step: 112  | total loss: [1m[32m0.31749[0m[0m | time: 118.212s
[2K
| Adam | epoch: 014 | loss: 0.31749 - acc: 0.9222 | val_loss: 0.51312 - val_acc: 0.8052 -- iter: 243/243
--
Training Step: 113  | total loss: [1m[32m0.29660[0m[0m | time: 41.262s
[2K
| Adam | epoch: 015 | loss: 0.29660 - acc: 0.9237 -- iter: 032/243
[A[ATraining Step: 114  | total loss: [1m[32m0.27231[0m[0m | time: 58.715s
[2K
| Adam | epoch: 015 | loss: 0.27231 - acc: 0.9314 -- iter: 064/243
[A[ATraining Step: 115  | total loss: [1m[32m0.26167[0m[0m | time: 73.727s
[2K
| Adam | epoch: 015 | loss: 0.26167 - acc: 0.9351 -- iter: 096/243
[A[ATraining Step: 116  | total loss: [1m[32m0.24593[0m[0m | time: 81.912s
[2K
| Adam | epoch: 015 | loss: 0.24593 - acc: 0.9385 -- iter: 128/243
[A[ATraining Step: 117  | total loss: [1m[32m0.23585[0m[0m | time: 90.486s
[2K
| Adam | epoch: 015 | loss: 0.23585 - acc: 0.9394 -- iter: 160/243
[A[ATraining Step: 118  | total loss: [1m[32m0.21993[0m[0m | time: 107.879s
[2K
| Adam | epoch: 015 | loss: 0.21993 - acc: 0.9454 -- iter: 192/243
[A[ATraining Step: 119  | total loss: [1m[32m0.21463[0m[0m | time: 133.572s
[2K
| Adam | epoch: 015 | loss: 0.21463 - acc: 0.9446 -- iter: 224/243
[A[ATraining Step: 120  | total loss: [1m[32m0.21097[0m[0m | time: 155.372s
[2K
| Adam | epoch: 015 | loss: 0.21097 - acc: 0.9470 | val_loss: 0.85117 - val_acc: 0.6623 -- iter: 243/243
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.814139941690962
Validation AUPRC:0.9020337281694704
Test AUC:0.7831978319783197
Test AUPRC:0.7783772033814648
BestTestF1Score	0.73	0.42	0.68	0.6	0.92	33	22	19	3	0.76
BestTestMCCScore	0.7	0.45	0.73	0.73	0.67	24	9	32	12	0.98
BestTestAccuracyScore	0.7	0.45	0.73	0.73	0.67	24	9	32	12	0.98
BestValidationF1Score	0.81	0.38	0.73	0.72	0.94	46	18	10	3	0.76
BestValidationMCC	0.79	0.56	0.77	0.92	0.69	34	3	25	15	0.98
BestValidationAccuracy	0.79	0.56	0.77	0.92	0.69	34	3	25	15	0.98
TestPredictions (Threshold:0.98)
CHEMBL197603,FP,INACT,0.9900000095367432	CHEMBL296185,TP,ACT,0.9900000095367432	CHEMBL549792,TN,INACT,0.6399999856948853	CHEMBL2392385,TN,INACT,0.6200000047683716	CHEMBL456797,TN,INACT,0.7900000214576721	CHEMBL1916891,TP,ACT,1.0	CHEMBL2392223,TN,INACT,0.9700000286102295	CHEMBL75880,TP,ACT,1.0	CHEMBL456112,FP,INACT,0.9900000095367432	CHEMBL1908397,FN,ACT,0.9100000262260437	CHEMBL563948,TN,INACT,0.10000000149011612	CHEMBL521201,TN,INACT,0.949999988079071	CHEMBL373100,FP,INACT,1.0	CHEMBL2178804,TP,ACT,0.9800000190734863	CHEMBL3335362,TN,INACT,0.5199999809265137	CHEMBL90002,FN,ACT,0.9399999976158142	CHEMBL1288005,TN,INACT,0.5699999928474426	CHEMBL606964,TP,ACT,0.9800000190734863	CHEMBL31184,FP,INACT,0.9900000095367432	CHEMBL120077,FN,ACT,0.07999999821186066	CHEMBL463384,TN,INACT,0.9200000166893005	CHEMBL1436125,TN,INACT,0.46000000834465027	CHEMBL48614,TN,INACT,0.949999988079071	CHEMBL12253,TP,ACT,0.9900000095367432	CHEMBL232542,FP,INACT,1.0	CHEMBL728,FN,ACT,0.9700000286102295	CHEMBL3578213,TP,ACT,1.0	CHEMBL1922120,TN,INACT,0.8899999856948853	CHEMBL1630578,TP,ACT,0.9800000190734863	CHEMBL517154,TN,INACT,0.6299999952316284	CHEMBL1767292,FP,INACT,0.9900000095367432	CHEMBL318461,TN,INACT,0.6100000143051147	CHEMBL318188,TN,INACT,0.5799999833106995	CHEMBL282575,FP,INACT,1.0	CHEMBL558601,TN,INACT,0.10999999940395355	CHEMBL16028,TP,ACT,1.0	CHEMBL277347,TP,ACT,1.0	CHEMBL601719,FN,ACT,0.38999998569488525	CHEMBL2392378,TN,INACT,0.5299999713897705	CHEMBL3578224,FN,ACT,0.3199999928474426	CHEMBL559882,TN,INACT,0.11999999731779099	CHEMBL456796,TN,INACT,0.9100000262260437	CHEMBL3632719,TP,ACT,0.9800000190734863	CHEMBL1221699,FN,ACT,0.949999988079071	CHEMBL1784649,FP,INACT,0.9800000190734863	CHEMBL602472,TN,INACT,0.9700000286102295	CHEMBL2392239,TN,INACT,0.9700000286102295	CHEMBL1908395,TP,ACT,1.0	CHEMBL1221415,TN,INACT,0.7400000095367432	CHEMBL298566,TP,ACT,1.0	CHEMBL2041033,TN,INACT,0.8700000047683716	CHEMBL2205426,TP,ACT,1.0	CHEMBL522760,TN,INACT,0.8299999833106995	CHEMBL306380,TP,ACT,0.9900000095367432	CHEMBL3822591,TP,ACT,1.0	CHEMBL557456,TN,INACT,0.23000000417232513	CHEMBL475251,TP,ACT,1.0	CHEMBL297304,TN,INACT,0.9200000166893005	CHEMBL551936,TN,INACT,0.12999999523162842	CHEMBL3805488,TN,INACT,0.07999999821186066	CHEMBL364623,FN,ACT,0.9599999785423279	CHEMBL2042135,TP,ACT,1.0	CHEMBL3806289,TN,INACT,0.09000000357627869	CHEMBL360903,TN,INACT,0.6600000262260437	CHEMBL926,TP,ACT,0.9900000095367432	CHEMBL572881,FN,ACT,0.8600000143051147	CHEMBL565612,TP,ACT,1.0	CHEMBL3622137,TP,ACT,0.9800000190734863	CHEMBL274926,FP,INACT,0.9800000190734863	CHEMBL562198,TN,INACT,0.1599999964237213	CHEMBL31965,TP,ACT,0.9900000095367432	CHEMBL3578204,FN,ACT,0.9700000286102295	CHEMBL264406,TP,ACT,0.9900000095367432	CHEMBL525921,TN,INACT,0.9399999976158142	CHEMBL165399,TP,ACT,1.0	CHEMBL15285,FN,ACT,0.8899999856948853	CHEMBL3578215,FN,ACT,0.8899999856948853	

