CNNModel CHEMBL4203 adam 0.0005 15 32 0 0.6 False True
Number of active compounds :	155
Number of inactive compounds :	155
---------------------------------
Run id: CNNModel_CHEMBL4203_adam_0.0005_15_32_0_0.6_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL4203_adam_0.0005_15_32_0.6_True/
---------------------------------
Training samples: 190
Validation samples: 60
--
Training Step: 1  | time: 0.761s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/190
[A[ATraining Step: 2  | total loss: [1m[32m0.62410[0m[0m | time: 1.376s
[2K
| Adam | epoch: 001 | loss: 0.62410 - acc: 0.2812 -- iter: 064/190
[A[ATraining Step: 3  | total loss: [1m[32m0.68067[0m[0m | time: 1.996s
[2K
| Adam | epoch: 001 | loss: 0.68067 - acc: 0.4602 -- iter: 096/190
[A[ATraining Step: 4  | total loss: [1m[32m0.68994[0m[0m | time: 2.613s
[2K
| Adam | epoch: 001 | loss: 0.68994 - acc: 0.4901 -- iter: 128/190
[A[ATraining Step: 5  | total loss: [1m[32m0.69070[0m[0m | time: 3.228s
[2K
| Adam | epoch: 001 | loss: 0.69070 - acc: 0.5618 -- iter: 160/190
[A[ATraining Step: 6  | total loss: [1m[32m0.69403[0m[0m | time: 4.910s
[2K
| Adam | epoch: 001 | loss: 0.69403 - acc: 0.4819 | val_loss: 0.68321 - val_acc: 0.6500 -- iter: 190/190
--
Training Step: 7  | total loss: [1m[32m0.69044[0m[0m | time: 0.590s
[2K
| Adam | epoch: 002 | loss: 0.69044 - acc: 0.5528 -- iter: 032/190
[A[ATraining Step: 8  | total loss: [1m[32m0.68831[0m[0m | time: 1.211s
[2K
| Adam | epoch: 002 | loss: 0.68831 - acc: 0.5793 -- iter: 064/190
[A[ATraining Step: 9  | total loss: [1m[32m0.69758[0m[0m | time: 1.828s
[2K
| Adam | epoch: 002 | loss: 0.69758 - acc: 0.4877 -- iter: 096/190
[A[ATraining Step: 10  | total loss: [1m[32m0.69552[0m[0m | time: 2.476s
[2K
| Adam | epoch: 002 | loss: 0.69552 - acc: 0.4939 -- iter: 128/190
[A[ATraining Step: 11  | total loss: [1m[32m0.69059[0m[0m | time: 3.101s
[2K
| Adam | epoch: 002 | loss: 0.69059 - acc: 0.5412 -- iter: 160/190
[A[ATraining Step: 12  | total loss: [1m[32m0.69135[0m[0m | time: 4.716s
[2K
| Adam | epoch: 002 | loss: 0.69135 - acc: 0.5367 | val_loss: 0.67916 - val_acc: 0.6500 -- iter: 190/190
--
Training Step: 13  | total loss: [1m[32m0.69158[0m[0m | time: 0.588s
[2K
| Adam | epoch: 003 | loss: 0.69158 - acc: 0.5344 -- iter: 032/190
[A[ATraining Step: 14  | total loss: [1m[32m0.69611[0m[0m | time: 1.184s
[2K
| Adam | epoch: 003 | loss: 0.69611 - acc: 0.4930 -- iter: 064/190
[A[ATraining Step: 15  | total loss: [1m[32m0.69740[0m[0m | time: 1.805s
[2K
| Adam | epoch: 003 | loss: 0.69740 - acc: 0.4697 -- iter: 096/190
[A[ATraining Step: 16  | total loss: [1m[32m0.69740[0m[0m | time: 2.407s
[2K
| Adam | epoch: 003 | loss: 0.69740 - acc: 0.4576 -- iter: 128/190
[A[ATraining Step: 17  | total loss: [1m[32m0.69694[0m[0m | time: 3.006s
[2K
| Adam | epoch: 003 | loss: 0.69694 - acc: 0.4504 -- iter: 160/190
[A[ATraining Step: 18  | total loss: [1m[32m0.69597[0m[0m | time: 4.622s
[2K
| Adam | epoch: 003 | loss: 0.69597 - acc: 0.4567 | val_loss: 0.69030 - val_acc: 0.6500 -- iter: 190/190
--
Training Step: 19  | total loss: [1m[32m0.69535[0m[0m | time: 0.608s
[2K
| Adam | epoch: 004 | loss: 0.69535 - acc: 0.4607 -- iter: 032/190
[A[ATraining Step: 20  | total loss: [1m[32m0.69443[0m[0m | time: 1.200s
[2K
| Adam | epoch: 004 | loss: 0.69443 - acc: 0.4834 -- iter: 064/190
[A[ATraining Step: 21  | total loss: [1m[32m0.69373[0m[0m | time: 1.778s
[2K
| Adam | epoch: 004 | loss: 0.69373 - acc: 0.5092 -- iter: 096/190
[A[ATraining Step: 22  | total loss: [1m[32m0.69330[0m[0m | time: 2.399s
[2K
| Adam | epoch: 004 | loss: 0.69330 - acc: 0.5265 -- iter: 128/190
[A[ATraining Step: 23  | total loss: [1m[32m0.69304[0m[0m | time: 3.011s
[2K
| Adam | epoch: 004 | loss: 0.69304 - acc: 0.5369 -- iter: 160/190
[A[ATraining Step: 24  | total loss: [1m[32m0.69285[0m[0m | time: 4.651s
[2K
| Adam | epoch: 004 | loss: 0.69285 - acc: 0.5441 | val_loss: 0.69055 - val_acc: 0.6500 -- iter: 190/190
--
Training Step: 25  | total loss: [1m[32m0.69282[0m[0m | time: 0.613s
[2K
| Adam | epoch: 005 | loss: 0.69282 - acc: 0.5406 -- iter: 032/190
[A[ATraining Step: 26  | total loss: [1m[32m0.69291[0m[0m | time: 1.220s
[2K
| Adam | epoch: 005 | loss: 0.69291 - acc: 0.5299 -- iter: 064/190
[A[ATraining Step: 27  | total loss: [1m[32m0.69299[0m[0m | time: 1.800s
[2K
| Adam | epoch: 005 | loss: 0.69299 - acc: 0.5222 -- iter: 096/190
[A[ATraining Step: 28  | total loss: [1m[32m0.69363[0m[0m | time: 2.372s
[2K
| Adam | epoch: 005 | loss: 0.69363 - acc: 0.4833 -- iter: 128/190
[A[ATraining Step: 29  | total loss: [1m[32m0.69403[0m[0m | time: 2.987s
[2K
| Adam | epoch: 005 | loss: 0.69403 - acc: 0.4549 -- iter: 160/190
[A[ATraining Step: 30  | total loss: [1m[32m0.69363[0m[0m | time: 4.602s
[2K
| Adam | epoch: 005 | loss: 0.69363 - acc: 0.4804 | val_loss: 0.69180 - val_acc: 0.6500 -- iter: 190/190
--
Training Step: 31  | total loss: [1m[32m0.69381[0m[0m | time: 0.618s
[2K
| Adam | epoch: 006 | loss: 0.69381 - acc: 0.4561 -- iter: 032/190
[A[ATraining Step: 32  | total loss: [1m[32m0.69370[0m[0m | time: 1.228s
[2K
| Adam | epoch: 006 | loss: 0.69370 - acc: 0.4589 -- iter: 064/190
[A[ATraining Step: 33  | total loss: [1m[32m0.69355[0m[0m | time: 1.838s
[2K
| Adam | epoch: 006 | loss: 0.69355 - acc: 0.4748 -- iter: 096/190
[A[ATraining Step: 34  | total loss: [1m[32m0.69352[0m[0m | time: 2.416s
[2K
| Adam | epoch: 006 | loss: 0.69352 - acc: 0.4601 -- iter: 128/190
[A[ATraining Step: 35  | total loss: [1m[32m0.69340[0m[0m | time: 2.995s
[2K
| Adam | epoch: 006 | loss: 0.69340 - acc: 0.5033 -- iter: 160/190
[A[ATraining Step: 36  | total loss: [1m[32m0.69333[0m[0m | time: 4.618s
[2K
| Adam | epoch: 006 | loss: 0.69333 - acc: 0.5231 | val_loss: 0.69194 - val_acc: 0.6500 -- iter: 190/190
--
Training Step: 37  | total loss: [1m[32m0.69319[0m[0m | time: 0.614s
[2K
| Adam | epoch: 007 | loss: 0.69319 - acc: 0.5372 -- iter: 032/190
[A[ATraining Step: 38  | total loss: [1m[32m0.69304[0m[0m | time: 1.233s
[2K
| Adam | epoch: 007 | loss: 0.69304 - acc: 0.5483 -- iter: 064/190
[A[ATraining Step: 39  | total loss: [1m[32m0.69309[0m[0m | time: 1.860s
[2K
| Adam | epoch: 007 | loss: 0.69309 - acc: 0.5331 -- iter: 096/190
[A[ATraining Step: 40  | total loss: [1m[32m0.69293[0m[0m | time: 2.470s
[2K
| Adam | epoch: 007 | loss: 0.69293 - acc: 0.5386 -- iter: 128/190
[A[ATraining Step: 41  | total loss: [1m[32m0.69289[0m[0m | time: 3.037s
[2K
| Adam | epoch: 007 | loss: 0.69289 - acc: 0.5372 -- iter: 160/190
[A[ATraining Step: 42  | total loss: [1m[32m0.69312[0m[0m | time: 4.620s
[2K
| Adam | epoch: 007 | loss: 0.69312 - acc: 0.5185 | val_loss: 0.69045 - val_acc: 0.6500 -- iter: 190/190
--
Training Step: 43  | total loss: [1m[32m0.69327[0m[0m | time: 0.617s
[2K
| Adam | epoch: 008 | loss: 0.69327 - acc: 0.5035 -- iter: 032/190
[A[ATraining Step: 44  | total loss: [1m[32m0.69340[0m[0m | time: 1.225s
[2K
| Adam | epoch: 008 | loss: 0.69340 - acc: 0.4921 -- iter: 064/190
[A[ATraining Step: 45  | total loss: [1m[32m0.69339[0m[0m | time: 1.819s
[2K
| Adam | epoch: 008 | loss: 0.69339 - acc: 0.4881 -- iter: 096/190
[A[ATraining Step: 46  | total loss: [1m[32m0.69332[0m[0m | time: 2.433s
[2K
| Adam | epoch: 008 | loss: 0.69332 - acc: 0.4901 -- iter: 128/190
[A[ATraining Step: 47  | total loss: [1m[32m0.69347[0m[0m | time: 3.048s
[2K
| Adam | epoch: 008 | loss: 0.69347 - acc: 0.4764 -- iter: 160/190
[A[ATraining Step: 48  | total loss: [1m[32m0.69328[0m[0m | time: 4.889s
[2K
| Adam | epoch: 008 | loss: 0.69328 - acc: 0.4902 | val_loss: 0.69050 - val_acc: 0.6500 -- iter: 190/190
--
Training Step: 49  | total loss: [1m[32m0.69312[0m[0m | time: 0.564s
[2K
| Adam | epoch: 009 | loss: 0.69312 - acc: 0.4970 -- iter: 032/190
[A[ATraining Step: 50  | total loss: [1m[32m0.69293[0m[0m | time: 1.166s
[2K
| Adam | epoch: 009 | loss: 0.69293 - acc: 0.5027 -- iter: 064/190
[A[ATraining Step: 51  | total loss: [1m[32m0.69273[0m[0m | time: 1.784s
[2K
| Adam | epoch: 009 | loss: 0.69273 - acc: 0.5118 -- iter: 096/190
[A[ATraining Step: 52  | total loss: [1m[32m0.69257[0m[0m | time: 2.396s
[2K
| Adam | epoch: 009 | loss: 0.69257 - acc: 0.5147 -- iter: 128/190
[A[ATraining Step: 53  | total loss: [1m[32m0.69245[0m[0m | time: 2.994s
[2K
| Adam | epoch: 009 | loss: 0.69245 - acc: 0.5171 -- iter: 160/190
[A[ATraining Step: 54  | total loss: [1m[32m0.69209[0m[0m | time: 4.613s
[2K
| Adam | epoch: 009 | loss: 0.69209 - acc: 0.5283 | val_loss: 0.68749 - val_acc: 0.6500 -- iter: 190/190
--
Training Step: 55  | total loss: [1m[32m0.69234[0m[0m | time: 0.720s
[2K
| Adam | epoch: 010 | loss: 0.69234 - acc: 0.5153 -- iter: 032/190
[A[ATraining Step: 56  | total loss: [1m[32m0.69221[0m[0m | time: 1.338s
[2K
| Adam | epoch: 010 | loss: 0.69221 - acc: 0.5131 -- iter: 064/190
[A[ATraining Step: 57  | total loss: [1m[32m0.69204[0m[0m | time: 2.027s
[2K
| Adam | epoch: 010 | loss: 0.69204 - acc: 0.5113 -- iter: 096/190
[A[ATraining Step: 58  | total loss: [1m[32m0.69192[0m[0m | time: 2.637s
[2K
| Adam | epoch: 010 | loss: 0.69192 - acc: 0.5013 -- iter: 128/190
[A[ATraining Step: 59  | total loss: [1m[32m0.69140[0m[0m | time: 3.236s
[2K
| Adam | epoch: 010 | loss: 0.69140 - acc: 0.5095 -- iter: 160/190
[A[ATraining Step: 60  | total loss: [1m[32m0.69044[0m[0m | time: 4.872s
[2K
| Adam | epoch: 010 | loss: 0.69044 - acc: 0.5206 | val_loss: 0.67695 - val_acc: 0.6833 -- iter: 190/190
--
Training Step: 61  | total loss: [1m[32m0.69041[0m[0m | time: 0.617s
[2K
| Adam | epoch: 011 | loss: 0.69041 - acc: 0.5098 -- iter: 032/190
[A[ATraining Step: 62  | total loss: [1m[32m0.69026[0m[0m | time: 1.203s
[2K
| Adam | epoch: 011 | loss: 0.69026 - acc: 0.5005 -- iter: 064/190
[A[ATraining Step: 63  | total loss: [1m[32m0.68986[0m[0m | time: 1.810s
[2K
| Adam | epoch: 011 | loss: 0.68986 - acc: 0.5047 -- iter: 096/190
[A[ATraining Step: 64  | total loss: [1m[32m0.68923[0m[0m | time: 2.413s
[2K
| Adam | epoch: 011 | loss: 0.68923 - acc: 0.5416 -- iter: 128/190
[A[ATraining Step: 65  | total loss: [1m[32m0.68913[0m[0m | time: 3.011s
[2K
| Adam | epoch: 011 | loss: 0.68913 - acc: 0.5519 -- iter: 160/190
[A[ATraining Step: 66  | total loss: [1m[32m0.68801[0m[0m | time: 4.612s
[2K
| Adam | epoch: 011 | loss: 0.68801 - acc: 0.5760 | val_loss: 0.65114 - val_acc: 0.7500 -- iter: 190/190
--
Training Step: 67  | total loss: [1m[32m0.68660[0m[0m | time: 0.611s
[2K
| Adam | epoch: 012 | loss: 0.68660 - acc: 0.5893 -- iter: 032/190
[A[ATraining Step: 68  | total loss: [1m[32m0.68375[0m[0m | time: 1.234s
[2K
| Adam | epoch: 012 | loss: 0.68375 - acc: 0.5973 -- iter: 064/190
[A[ATraining Step: 69  | total loss: [1m[32m0.68356[0m[0m | time: 1.808s
[2K
| Adam | epoch: 012 | loss: 0.68356 - acc: 0.5822 -- iter: 096/190
[A[ATraining Step: 70  | total loss: [1m[32m0.68497[0m[0m | time: 2.402s
[2K
| Adam | epoch: 012 | loss: 0.68497 - acc: 0.5689 -- iter: 128/190
[A[ATraining Step: 71  | total loss: [1m[32m0.68400[0m[0m | time: 3.013s
[2K
| Adam | epoch: 012 | loss: 0.68400 - acc: 0.5838 -- iter: 160/190
[A[ATraining Step: 72  | total loss: [1m[32m0.68080[0m[0m | time: 4.643s
[2K
| Adam | epoch: 012 | loss: 0.68080 - acc: 0.6061 | val_loss: 0.64370 - val_acc: 0.7000 -- iter: 190/190
--
Training Step: 73  | total loss: [1m[32m0.67770[0m[0m | time: 0.638s
[2K
| Adam | epoch: 013 | loss: 0.67770 - acc: 0.6116 -- iter: 032/190
[A[ATraining Step: 74  | total loss: [1m[32m0.67086[0m[0m | time: 1.260s
[2K
| Adam | epoch: 013 | loss: 0.67086 - acc: 0.6302 -- iter: 064/190
[A[ATraining Step: 75  | total loss: [1m[32m0.66511[0m[0m | time: 1.877s
[2K
| Adam | epoch: 013 | loss: 0.66511 - acc: 0.6500 -- iter: 096/190
[A[ATraining Step: 76  | total loss: [1m[32m0.66208[0m[0m | time: 2.477s
[2K
| Adam | epoch: 013 | loss: 0.66208 - acc: 0.6440 -- iter: 128/190
[A[ATraining Step: 77  | total loss: [1m[32m0.66258[0m[0m | time: 3.076s
[2K
| Adam | epoch: 013 | loss: 0.66258 - acc: 0.6393 -- iter: 160/190
[A[ATraining Step: 78  | total loss: [1m[32m0.66016[0m[0m | time: 4.710s
[2K
| Adam | epoch: 013 | loss: 0.66016 - acc: 0.6457 | val_loss: 0.57448 - val_acc: 0.7000 -- iter: 190/190
--
Training Step: 79  | total loss: [1m[32m0.65164[0m[0m | time: 0.617s
[2K
| Adam | epoch: 014 | loss: 0.65164 - acc: 0.6597 -- iter: 032/190
[A[ATraining Step: 80  | total loss: [1m[32m0.64300[0m[0m | time: 1.217s
[2K
| Adam | epoch: 014 | loss: 0.64300 - acc: 0.6689 -- iter: 064/190
[A[ATraining Step: 81  | total loss: [1m[32m0.63697[0m[0m | time: 1.824s
[2K
| Adam | epoch: 014 | loss: 0.63697 - acc: 0.6677 -- iter: 096/190
[A[ATraining Step: 82  | total loss: [1m[32m0.62993[0m[0m | time: 2.487s
[2K
| Adam | epoch: 014 | loss: 0.62993 - acc: 0.6759 -- iter: 128/190
[A[ATraining Step: 83  | total loss: [1m[32m0.62788[0m[0m | time: 3.044s
[2K
| Adam | epoch: 014 | loss: 0.62788 - acc: 0.6739 -- iter: 160/190
[A[ATraining Step: 84  | total loss: [1m[32m0.61068[0m[0m | time: 4.616s
[2K
| Adam | epoch: 014 | loss: 0.61068 - acc: 0.6932 | val_loss: 0.54127 - val_acc: 0.7667 -- iter: 190/190
--
Training Step: 85  | total loss: [1m[32m0.59614[0m[0m | time: 0.609s
[2K
| Adam | epoch: 015 | loss: 0.59614 - acc: 0.7005 -- iter: 032/190
[A[ATraining Step: 86  | total loss: [1m[32m0.60970[0m[0m | time: 1.220s
[2K
| Adam | epoch: 015 | loss: 0.60970 - acc: 0.6930 -- iter: 064/190
[A[ATraining Step: 87  | total loss: [1m[32m0.61048[0m[0m | time: 1.821s
[2K
| Adam | epoch: 015 | loss: 0.61048 - acc: 0.6987 -- iter: 096/190
[A[ATraining Step: 88  | total loss: [1m[32m0.60432[0m[0m | time: 2.441s
[2K
| Adam | epoch: 015 | loss: 0.60432 - acc: 0.7007 -- iter: 128/190
[A[ATraining Step: 89  | total loss: [1m[32m0.59463[0m[0m | time: 3.064s
[2K
| Adam | epoch: 015 | loss: 0.59463 - acc: 0.6994 -- iter: 160/190
[A[ATraining Step: 90  | total loss: [1m[32m0.58593[0m[0m | time: 4.663s
[2K
| Adam | epoch: 015 | loss: 0.58593 - acc: 0.7044 | val_loss: 0.54903 - val_acc: 0.7500 -- iter: 190/190
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.8241758241758241
Validation AUPRC:0.78797034446014
Test AUC:0.7652947719688542
Test AUPRC:0.8127310037781036
BestTestF1Score	0.68	0.48	0.73	0.81	0.59	17	4	27	12	0.73
BestTestMCCScore	0.57	0.46	0.7	0.92	0.41	12	1	30	17	0.81
BestTestAccuracyScore	0.57	0.46	0.7	0.92	0.41	12	1	30	17	0.81
BestValidationF1Score	0.67	0.51	0.78	0.72	0.62	13	5	34	8	0.73
BestValidationMCC	0.65	0.61	0.82	1.0	0.48	10	0	39	11	0.81
BestValidationAccuracy	0.65	0.61	0.82	1.0	0.48	10	0	39	11	0.81
TestPredictions (Threshold:0.81)
CHEMBL515674,TN,INACT,0.38999998569488525	CHEMBL1885536,FN,ACT,0.23000000417232513	CHEMBL578373,FN,ACT,0.800000011920929	CHEMBL288441,FN,ACT,0.2800000011920929	CHEMBL456113,TN,INACT,0.23000000417232513	CHEMBL1882945,TP,ACT,0.8199999928474426	CHEMBL579001,TP,ACT,0.8299999833106995	CHEMBL566687,TP,ACT,0.8500000238418579	CHEMBL1288069,TN,INACT,0.3700000047683716	CHEMBL1725279,FN,ACT,0.6000000238418579	CHEMBL1230609,FN,ACT,0.25999999046325684	CHEMBL1473127,TP,ACT,0.8600000143051147	CHEMBL1908395,FN,ACT,0.36000001430511475	CHEMBL101868,TN,INACT,0.7799999713897705	CHEMBL2392247,TN,INACT,0.6200000047683716	CHEMBL1898211,FN,ACT,0.7300000190734863	CHEMBL1357429,FN,ACT,0.4000000059604645	CHEMBL101779,TN,INACT,0.7400000095367432	CHEMBL2392225,FN,ACT,0.7300000190734863	CHEMBL1087421,TN,INACT,0.4699999988079071	CHEMBL566293,TP,ACT,0.8399999737739563	CHEMBL408982,FN,ACT,0.5099999904632568	CHEMBL102622,TN,INACT,0.3100000023841858	CHEMBL1734733,TP,ACT,0.8500000238418579	CHEMBL1704879,TP,ACT,0.8500000238418579	CHEMBL1868723,FN,ACT,0.7200000286102295	CHEMBL3774448,FN,ACT,0.20999999344348907	CHEMBL334248,TN,INACT,0.23000000417232513	CHEMBL1922122,FP,INACT,0.8100000023841858	CHEMBL1802358,TP,ACT,0.8299999833106995	CHEMBL456378,TN,INACT,0.25	CHEMBL1396483,TP,ACT,0.8100000023841858	CHEMBL1397997,TP,ACT,0.8500000238418579	CHEMBL498705,TN,INACT,0.1899999976158142	CHEMBL509435,TN,INACT,0.7099999785423279	CHEMBL488646,TN,INACT,0.5400000214576721	CHEMBL2392383,TN,INACT,0.7099999785423279	CHEMBL1287975,TN,INACT,0.33000001311302185	CHEMBL1770624,FN,ACT,0.47999998927116394	CHEMBL379218,FN,ACT,0.3100000023841858	CHEMBL1933801,TN,INACT,0.5299999713897705	CHEMBL1329627,TP,ACT,0.8100000023841858	CHEMBL2392392,TN,INACT,0.49000000953674316	CHEMBL567332,FN,ACT,0.7699999809265137	CHEMBL1767294,TN,INACT,0.20000000298023224	CHEMBL1894742,FN,ACT,0.800000011920929	CHEMBL490241,TN,INACT,0.25999999046325684	CHEMBL223360,FN,ACT,0.20999999344348907	CHEMBL2392366,TN,INACT,0.30000001192092896	CHEMBL1707127,TP,ACT,0.8399999737739563	CHEMBL1910754,TN,INACT,0.7300000190734863	CHEMBL2392378,TN,INACT,0.5600000023841858	CHEMBL525538,TN,INACT,0.20999999344348907	CHEMBL486487,TN,INACT,0.6399999856948853	CHEMBL557321,TN,INACT,0.25	CHEMBL470851,TN,INACT,0.23999999463558197	CHEMBL487526,TN,INACT,0.38999998569488525	CHEMBL1734241,TN,INACT,0.1899999976158142	CHEMBL557456,TN,INACT,0.20000000298023224	CHEMBL490053,TN,INACT,0.2199999988079071	

