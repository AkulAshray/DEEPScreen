CNNModel CHEMBL3238 adam 0.001 30 32 0 0.6 False True
Number of active compounds :	115
Number of inactive compounds :	115
---------------------------------
Run id: CNNModel_CHEMBL3238_adam_0.001_30_32_0_0.6_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL3238_adam_0.001_30_32_0.6_True/
---------------------------------
Training samples: 140
Validation samples: 44
--
Training Step: 1  | time: 0.772s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/140
[A[ATraining Step: 2  | total loss: [1m[32m0.62372[0m[0m | time: 1.411s
[2K
| Adam | epoch: 001 | loss: 0.62372 - acc: 0.5062 -- iter: 064/140
[A[ATraining Step: 3  | total loss: [1m[32m0.68128[0m[0m | time: 2.033s
[2K
| Adam | epoch: 001 | loss: 0.68128 - acc: 0.5011 -- iter: 096/140
[A[ATraining Step: 4  | total loss: [1m[32m0.69492[0m[0m | time: 2.638s
[2K
| Adam | epoch: 001 | loss: 0.69492 - acc: 0.4300 -- iter: 128/140
[A[ATraining Step: 5  | total loss: [1m[32m0.69246[0m[0m | time: 3.912s
[2K
| Adam | epoch: 001 | loss: 0.69246 - acc: 0.5434 | val_loss: 0.69569 - val_acc: 0.4545 -- iter: 140/140
--
Training Step: 6  | total loss: [1m[32m0.69025[0m[0m | time: 0.275s
[2K
| Adam | epoch: 002 | loss: 0.69025 - acc: 0.6226 -- iter: 032/140
[A[ATraining Step: 7  | total loss: [1m[32m0.68710[0m[0m | time: 0.900s
[2K
| Adam | epoch: 002 | loss: 0.68710 - acc: 0.6491 -- iter: 064/140
[A[ATraining Step: 8  | total loss: [1m[32m0.68717[0m[0m | time: 1.505s
[2K
| Adam | epoch: 002 | loss: 0.68717 - acc: 0.6004 -- iter: 096/140
[A[ATraining Step: 9  | total loss: [1m[32m0.70199[0m[0m | time: 2.121s
[2K
| Adam | epoch: 002 | loss: 0.70199 - acc: 0.5141 -- iter: 128/140
[A[ATraining Step: 10  | total loss: [1m[32m0.69647[0m[0m | time: 3.722s
[2K
| Adam | epoch: 002 | loss: 0.69647 - acc: 0.5227 | val_loss: 0.69932 - val_acc: 0.4545 -- iter: 140/140
--
Training Step: 11  | total loss: [1m[32m0.69599[0m[0m | time: 0.432s
[2K
| Adam | epoch: 003 | loss: 0.69599 - acc: 0.5119 -- iter: 032/140
[A[ATraining Step: 12  | total loss: [1m[32m0.70253[0m[0m | time: 0.825s
[2K
| Adam | epoch: 003 | loss: 0.70253 - acc: 0.4316 -- iter: 064/140
[A[ATraining Step: 13  | total loss: [1m[32m0.70401[0m[0m | time: 1.877s
[2K
| Adam | epoch: 003 | loss: 0.70401 - acc: 0.3895 -- iter: 096/140
[A[ATraining Step: 14  | total loss: [1m[32m0.69906[0m[0m | time: 3.083s
[2K
| Adam | epoch: 003 | loss: 0.69906 - acc: 0.4475 -- iter: 128/140
[A[ATraining Step: 15  | total loss: [1m[32m0.69612[0m[0m | time: 5.315s
[2K
| Adam | epoch: 003 | loss: 0.69612 - acc: 0.4925 | val_loss: 0.69437 - val_acc: 0.4545 -- iter: 140/140
--
Training Step: 16  | total loss: [1m[32m0.69440[0m[0m | time: 1.146s
[2K
| Adam | epoch: 004 | loss: 0.69440 - acc: 0.5187 -- iter: 032/140
[A[ATraining Step: 17  | total loss: [1m[32m0.69426[0m[0m | time: 1.607s
[2K
| Adam | epoch: 004 | loss: 0.69426 - acc: 0.5007 -- iter: 064/140
[A[ATraining Step: 18  | total loss: [1m[32m0.69325[0m[0m | time: 2.085s
[2K
| Adam | epoch: 004 | loss: 0.69325 - acc: 0.5293 -- iter: 096/140
[A[ATraining Step: 19  | total loss: [1m[32m0.69261[0m[0m | time: 3.283s
[2K
| Adam | epoch: 004 | loss: 0.69261 - acc: 0.5473 -- iter: 128/140
[A[ATraining Step: 20  | total loss: [1m[32m0.69353[0m[0m | time: 5.331s
[2K
| Adam | epoch: 004 | loss: 0.69353 - acc: 0.5020 | val_loss: 0.69432 - val_acc: 0.4545 -- iter: 140/140
--
Training Step: 21  | total loss: [1m[32m0.69274[0m[0m | time: 0.620s
[2K
| Adam | epoch: 005 | loss: 0.69274 - acc: 0.5305 -- iter: 032/140
[A[ATraining Step: 22  | total loss: [1m[32m0.69217[0m[0m | time: 1.224s
[2K
| Adam | epoch: 005 | loss: 0.69217 - acc: 0.5494 -- iter: 064/140
[A[ATraining Step: 23  | total loss: [1m[32m0.69219[0m[0m | time: 1.471s
[2K
| Adam | epoch: 005 | loss: 0.69219 - acc: 0.5442 -- iter: 096/140
[A[ATraining Step: 24  | total loss: [1m[32m0.69249[0m[0m | time: 1.729s
[2K
| Adam | epoch: 005 | loss: 0.69249 - acc: 0.5317 -- iter: 128/140
[A[ATraining Step: 25  | total loss: [1m[32m0.69266[0m[0m | time: 3.339s
[2K
| Adam | epoch: 005 | loss: 0.69266 - acc: 0.5231 | val_loss: 0.69471 - val_acc: 0.4545 -- iter: 140/140
--
Training Step: 26  | total loss: [1m[32m0.69359[0m[0m | time: 0.624s
[2K
| Adam | epoch: 006 | loss: 0.69359 - acc: 0.4922 -- iter: 032/140
[A[ATraining Step: 27  | total loss: [1m[32m0.69325[0m[0m | time: 1.213s
[2K
| Adam | epoch: 006 | loss: 0.69325 - acc: 0.5022 -- iter: 064/140
[A[ATraining Step: 28  | total loss: [1m[32m0.69350[0m[0m | time: 1.838s
[2K
| Adam | epoch: 006 | loss: 0.69350 - acc: 0.4938 -- iter: 096/140
[A[ATraining Step: 29  | total loss: [1m[32m0.69321[0m[0m | time: 2.094s
[2K
| Adam | epoch: 006 | loss: 0.69321 - acc: 0.5029 -- iter: 128/140
[A[ATraining Step: 30  | total loss: [1m[32m0.69447[0m[0m | time: 3.346s
[2K
| Adam | epoch: 006 | loss: 0.69447 - acc: 0.4628 | val_loss: 0.69421 - val_acc: 0.4545 -- iter: 140/140
--
Training Step: 31  | total loss: [1m[32m0.69525[0m[0m | time: 0.599s
[2K
| Adam | epoch: 007 | loss: 0.69525 - acc: 0.4329 -- iter: 032/140
[A[ATraining Step: 32  | total loss: [1m[32m0.69464[0m[0m | time: 1.204s
[2K
| Adam | epoch: 007 | loss: 0.69464 - acc: 0.4550 -- iter: 064/140
[A[ATraining Step: 33  | total loss: [1m[32m0.69395[0m[0m | time: 1.807s
[2K
| Adam | epoch: 007 | loss: 0.69395 - acc: 0.4855 -- iter: 096/140
[A[ATraining Step: 34  | total loss: [1m[32m0.69387[0m[0m | time: 2.421s
[2K
| Adam | epoch: 007 | loss: 0.69387 - acc: 0.4819 -- iter: 128/140
[A[ATraining Step: 35  | total loss: [1m[32m0.69364[0m[0m | time: 3.679s
[2K
| Adam | epoch: 007 | loss: 0.69364 - acc: 0.4922 | val_loss: 0.69371 - val_acc: 0.4545 -- iter: 140/140
--
Training Step: 36  | total loss: [1m[32m0.69383[0m[0m | time: 0.247s
[2K
| Adam | epoch: 008 | loss: 0.69383 - acc: 0.4768 -- iter: 032/140
[A[ATraining Step: 37  | total loss: [1m[32m0.69396[0m[0m | time: 0.853s
[2K
| Adam | epoch: 008 | loss: 0.69396 - acc: 0.4647 -- iter: 064/140
[A[ATraining Step: 38  | total loss: [1m[32m0.69392[0m[0m | time: 1.458s
[2K
| Adam | epoch: 008 | loss: 0.69392 - acc: 0.4533 -- iter: 096/140
[A[ATraining Step: 39  | total loss: [1m[32m0.69371[0m[0m | time: 2.071s
[2K
| Adam | epoch: 008 | loss: 0.69371 - acc: 0.4922 -- iter: 128/140
[A[ATraining Step: 40  | total loss: [1m[32m0.69364[0m[0m | time: 3.686s
[2K
| Adam | epoch: 008 | loss: 0.69364 - acc: 0.4819 | val_loss: 0.69312 - val_acc: 0.5455 -- iter: 140/140
--
Training Step: 41  | total loss: [1m[32m0.69355[0m[0m | time: 0.275s
[2K
| Adam | epoch: 009 | loss: 0.69355 - acc: 0.4967 -- iter: 032/140
[A[ATraining Step: 42  | total loss: [1m[32m0.69344[0m[0m | time: 0.521s
[2K
| Adam | epoch: 009 | loss: 0.69344 - acc: 0.5273 -- iter: 064/140
[A[ATraining Step: 43  | total loss: [1m[32m0.69316[0m[0m | time: 1.138s
[2K
| Adam | epoch: 009 | loss: 0.69316 - acc: 0.5666 -- iter: 096/140
[A[ATraining Step: 44  | total loss: [1m[32m0.69312[0m[0m | time: 1.736s
[2K
| Adam | epoch: 009 | loss: 0.69312 - acc: 0.5605 -- iter: 128/140
[A[ATraining Step: 45  | total loss: [1m[32m0.69364[0m[0m | time: 3.335s
[2K
| Adam | epoch: 009 | loss: 0.69364 - acc: 0.5237 | val_loss: 0.69214 - val_acc: 0.5455 -- iter: 140/140
--
Training Step: 46  | total loss: [1m[32m0.69371[0m[0m | time: 0.614s
[2K
| Adam | epoch: 010 | loss: 0.69371 - acc: 0.5145 -- iter: 032/140
[A[ATraining Step: 47  | total loss: [1m[32m0.69372[0m[0m | time: 0.861s
[2K
| Adam | epoch: 010 | loss: 0.69372 - acc: 0.5070 -- iter: 064/140
[A[ATraining Step: 48  | total loss: [1m[32m0.69424[0m[0m | time: 1.116s
[2K
| Adam | epoch: 010 | loss: 0.69424 - acc: 0.4791 -- iter: 096/140
[A[ATraining Step: 49  | total loss: [1m[32m0.69453[0m[0m | time: 1.741s
[2K
| Adam | epoch: 010 | loss: 0.69453 - acc: 0.4561 -- iter: 128/140
[A[ATraining Step: 50  | total loss: [1m[32m0.69432[0m[0m | time: 3.355s
[2K
| Adam | epoch: 010 | loss: 0.69432 - acc: 0.4629 | val_loss: 0.69278 - val_acc: 0.5455 -- iter: 140/140
--
Training Step: 51  | total loss: [1m[32m0.69404[0m[0m | time: 0.622s
[2K
| Adam | epoch: 011 | loss: 0.69404 - acc: 0.4781 -- iter: 032/140
[A[ATraining Step: 52  | total loss: [1m[32m0.69387[0m[0m | time: 1.232s
[2K
| Adam | epoch: 011 | loss: 0.69387 - acc: 0.4814 -- iter: 064/140
[A[ATraining Step: 53  | total loss: [1m[32m0.69386[0m[0m | time: 1.475s
[2K
| Adam | epoch: 011 | loss: 0.69386 - acc: 0.4703 -- iter: 096/140
[A[ATraining Step: 54  | total loss: [1m[32m0.69374[0m[0m | time: 1.738s
[2K
| Adam | epoch: 011 | loss: 0.69374 - acc: 0.4625 -- iter: 128/140
[A[ATraining Step: 55  | total loss: [1m[32m0.69364[0m[0m | time: 3.342s
[2K
| Adam | epoch: 011 | loss: 0.69364 - acc: 0.4798 | val_loss: 0.69380 - val_acc: 0.4545 -- iter: 140/140
--
Training Step: 56  | total loss: [1m[32m0.69373[0m[0m | time: 0.613s
[2K
| Adam | epoch: 012 | loss: 0.69373 - acc: 0.4694 -- iter: 032/140
[A[ATraining Step: 57  | total loss: [1m[32m0.69359[0m[0m | time: 1.213s
[2K
| Adam | epoch: 012 | loss: 0.69359 - acc: 0.4780 -- iter: 064/140
[A[ATraining Step: 58  | total loss: [1m[32m0.69366[0m[0m | time: 1.825s
[2K
| Adam | epoch: 012 | loss: 0.69366 - acc: 0.4725 -- iter: 096/140
[A[ATraining Step: 59  | total loss: [1m[32m0.69378[0m[0m | time: 2.066s
[2K
| Adam | epoch: 012 | loss: 0.69378 - acc: 0.4636 -- iter: 128/140
[A[ATraining Step: 60  | total loss: [1m[32m0.69340[0m[0m | time: 3.316s
[2K
| Adam | epoch: 012 | loss: 0.69340 - acc: 0.4905 | val_loss: 0.69421 - val_acc: 0.4545 -- iter: 140/140
--
Training Step: 61  | total loss: [1m[32m0.69297[0m[0m | time: 0.611s
[2K
| Adam | epoch: 013 | loss: 0.69297 - acc: 0.5134 -- iter: 032/140
[A[ATraining Step: 62  | total loss: [1m[32m0.69319[0m[0m | time: 1.255s
[2K
| Adam | epoch: 013 | loss: 0.69319 - acc: 0.5037 -- iter: 064/140
[A[ATraining Step: 63  | total loss: [1m[32m0.69239[0m[0m | time: 1.864s
[2K
| Adam | epoch: 013 | loss: 0.69239 - acc: 0.5309 -- iter: 096/140
[A[ATraining Step: 64  | total loss: [1m[32m0.69163[0m[0m | time: 2.476s
[2K
| Adam | epoch: 013 | loss: 0.69163 - acc: 0.5466 -- iter: 128/140
[A[ATraining Step: 65  | total loss: [1m[32m0.69226[0m[0m | time: 3.731s
[2K
| Adam | epoch: 013 | loss: 0.69226 - acc: 0.5370 | val_loss: 0.70026 - val_acc: 0.4545 -- iter: 140/140
--
Training Step: 66  | total loss: [1m[32m0.69243[0m[0m | time: 0.275s
[2K
| Adam | epoch: 014 | loss: 0.69243 - acc: 0.5325 -- iter: 032/140
[A[ATraining Step: 67  | total loss: [1m[32m0.69286[0m[0m | time: 0.890s
[2K
| Adam | epoch: 014 | loss: 0.69286 - acc: 0.5286 -- iter: 064/140
[A[ATraining Step: 68  | total loss: [1m[32m0.69342[0m[0m | time: 1.494s
[2K
| Adam | epoch: 014 | loss: 0.69342 - acc: 0.5215 -- iter: 096/140
[A[ATraining Step: 69  | total loss: [1m[32m0.69392[0m[0m | time: 2.101s
[2K
| Adam | epoch: 014 | loss: 0.69392 - acc: 0.5153 -- iter: 128/140
[A[ATraining Step: 70  | total loss: [1m[32m0.69341[0m[0m | time: 3.706s
[2K
| Adam | epoch: 014 | loss: 0.69341 - acc: 0.5172 | val_loss: 0.69648 - val_acc: 0.4545 -- iter: 140/140
--
Training Step: 71  | total loss: [1m[32m0.69366[0m[0m | time: 0.252s
[2K
| Adam | epoch: 015 | loss: 0.69366 - acc: 0.5117 -- iter: 032/140
[A[ATraining Step: 72  | total loss: [1m[32m0.69174[0m[0m | time: 0.500s
[2K
| Adam | epoch: 015 | loss: 0.69174 - acc: 0.5385 -- iter: 064/140
[A[ATraining Step: 73  | total loss: [1m[32m0.68977[0m[0m | time: 1.115s
[2K
| Adam | epoch: 015 | loss: 0.68977 - acc: 0.5620 -- iter: 096/140
[A[ATraining Step: 74  | total loss: [1m[32m0.69087[0m[0m | time: 1.713s
[2K
| Adam | epoch: 015 | loss: 0.69087 - acc: 0.5483 -- iter: 128/140
[A[ATraining Step: 75  | total loss: [1m[32m0.69086[0m[0m | time: 3.313s
[2K
| Adam | epoch: 015 | loss: 0.69086 - acc: 0.5465 | val_loss: 0.70369 - val_acc: 0.4545 -- iter: 140/140
--
Training Step: 76  | total loss: [1m[32m0.69018[0m[0m | time: 0.616s
[2K
| Adam | epoch: 016 | loss: 0.69018 - acc: 0.5482 -- iter: 032/140
[A[ATraining Step: 77  | total loss: [1m[32m0.69068[0m[0m | time: 0.858s
[2K
| Adam | epoch: 016 | loss: 0.69068 - acc: 0.5431 -- iter: 064/140
[A[ATraining Step: 78  | total loss: [1m[32m0.69277[0m[0m | time: 1.105s
[2K
| Adam | epoch: 016 | loss: 0.69277 - acc: 0.5299 -- iter: 096/140
[A[ATraining Step: 79  | total loss: [1m[32m0.69438[0m[0m | time: 1.701s
[2K
| Adam | epoch: 016 | loss: 0.69438 - acc: 0.5181 -- iter: 128/140
[A[ATraining Step: 80  | total loss: [1m[32m0.69327[0m[0m | time: 3.300s
[2K
| Adam | epoch: 016 | loss: 0.69327 - acc: 0.5227 | val_loss: 0.69750 - val_acc: 0.4545 -- iter: 140/140
--
Training Step: 81  | total loss: [1m[32m0.69334[0m[0m | time: 0.605s
[2K
| Adam | epoch: 017 | loss: 0.69334 - acc: 0.5172 -- iter: 032/140
[A[ATraining Step: 82  | total loss: [1m[32m0.69459[0m[0m | time: 1.224s
[2K
| Adam | epoch: 017 | loss: 0.69459 - acc: 0.4999 -- iter: 064/140
[A[ATraining Step: 83  | total loss: [1m[32m0.69283[0m[0m | time: 1.463s
[2K
| Adam | epoch: 017 | loss: 0.69283 - acc: 0.5218 -- iter: 096/140
[A[ATraining Step: 84  | total loss: [1m[32m0.69288[0m[0m | time: 1.722s
[2K
| Adam | epoch: 017 | loss: 0.69288 - acc: 0.5196 -- iter: 128/140
[A[ATraining Step: 85  | total loss: [1m[32m0.69294[0m[0m | time: 3.333s
[2K
| Adam | epoch: 017 | loss: 0.69294 - acc: 0.5176 | val_loss: 0.69554 - val_acc: 0.4545 -- iter: 140/140
--
Training Step: 86  | total loss: [1m[32m0.69229[0m[0m | time: 0.627s
[2K
| Adam | epoch: 018 | loss: 0.69229 - acc: 0.5252 -- iter: 032/140
[A[ATraining Step: 87  | total loss: [1m[32m0.69286[0m[0m | time: 1.229s
[2K
| Adam | epoch: 018 | loss: 0.69286 - acc: 0.5133 -- iter: 064/140
[A[ATraining Step: 88  | total loss: [1m[32m0.69253[0m[0m | time: 1.838s
[2K
| Adam | epoch: 018 | loss: 0.69253 - acc: 0.5151 -- iter: 096/140
[A[ATraining Step: 89  | total loss: [1m[32m0.69252[0m[0m | time: 2.082s
[2K
| Adam | epoch: 018 | loss: 0.69252 - acc: 0.5136 -- iter: 128/140
[A[ATraining Step: 90  | total loss: [1m[32m0.69289[0m[0m | time: 3.327s
[2K
| Adam | epoch: 018 | loss: 0.69289 - acc: 0.5039 | val_loss: 0.69483 - val_acc: 0.4545 -- iter: 140/140
--
Training Step: 91  | total loss: [1m[32m0.69313[0m[0m | time: 0.616s
[2K
| Adam | epoch: 019 | loss: 0.69313 - acc: 0.4952 -- iter: 032/140
[A[ATraining Step: 92  | total loss: [1m[32m0.69290[0m[0m | time: 1.221s
[2K
| Adam | epoch: 019 | loss: 0.69290 - acc: 0.4988 -- iter: 064/140
[A[ATraining Step: 93  | total loss: [1m[32m0.69253[0m[0m | time: 1.842s
[2K
| Adam | epoch: 019 | loss: 0.69253 - acc: 0.5020 -- iter: 096/140
[A[ATraining Step: 94  | total loss: [1m[32m0.69251[0m[0m | time: 2.461s
[2K
| Adam | epoch: 019 | loss: 0.69251 - acc: 0.4987 -- iter: 128/140
[A[ATraining Step: 95  | total loss: [1m[32m0.69223[0m[0m | time: 3.707s
[2K
| Adam | epoch: 019 | loss: 0.69223 - acc: 0.4988 | val_loss: 0.69384 - val_acc: 0.4545 -- iter: 140/140
--
Training Step: 96  | total loss: [1m[32m0.69244[0m[0m | time: 0.247s
[2K
| Adam | epoch: 020 | loss: 0.69244 - acc: 0.4906 -- iter: 032/140
[A[ATraining Step: 97  | total loss: [1m[32m0.69258[0m[0m | time: 0.861s
[2K
| Adam | epoch: 020 | loss: 0.69258 - acc: 0.4832 -- iter: 064/140
[A[ATraining Step: 98  | total loss: [1m[32m0.69227[0m[0m | time: 1.469s
[2K
| Adam | epoch: 020 | loss: 0.69227 - acc: 0.4818 -- iter: 096/140
[A[ATraining Step: 99  | total loss: [1m[32m0.69133[0m[0m | time: 2.149s
[2K
| Adam | epoch: 020 | loss: 0.69133 - acc: 0.4992 -- iter: 128/140
[A[ATraining Step: 100  | total loss: [1m[32m0.69031[0m[0m | time: 3.755s
[2K
| Adam | epoch: 020 | loss: 0.69031 - acc: 0.5087 | val_loss: 0.69374 - val_acc: 0.4545 -- iter: 140/140
--
Training Step: 101  | total loss: [1m[32m0.69015[0m[0m | time: 0.247s
[2K
| Adam | epoch: 021 | loss: 0.69015 - acc: 0.5047 -- iter: 032/140
[A[ATraining Step: 102  | total loss: [1m[32m0.69055[0m[0m | time: 0.498s
[2K
| Adam | epoch: 021 | loss: 0.69055 - acc: 0.5042 -- iter: 064/140
[A[ATraining Step: 103  | total loss: [1m[32m0.69048[0m[0m | time: 1.103s
[2K
| Adam | epoch: 021 | loss: 0.69048 - acc: 0.5038 -- iter: 096/140
[A[ATraining Step: 104  | total loss: [1m[32m0.68944[0m[0m | time: 1.715s
[2K
| Adam | epoch: 021 | loss: 0.68944 - acc: 0.5034 -- iter: 128/140
[A[ATraining Step: 105  | total loss: [1m[32m0.68765[0m[0m | time: 3.333s
[2K
| Adam | epoch: 021 | loss: 0.68765 - acc: 0.5031 | val_loss: 0.69052 - val_acc: 0.4091 -- iter: 140/140
--
Training Step: 106  | total loss: [1m[32m0.68594[0m[0m | time: 0.616s
[2K
| Adam | epoch: 022 | loss: 0.68594 - acc: 0.5090 -- iter: 032/140
[A[ATraining Step: 107  | total loss: [1m[32m0.68412[0m[0m | time: 0.873s
[2K
| Adam | epoch: 022 | loss: 0.68412 - acc: 0.5144 -- iter: 064/140
[A[ATraining Step: 108  | total loss: [1m[32m0.68085[0m[0m | time: 1.125s
[2K
| Adam | epoch: 022 | loss: 0.68085 - acc: 0.5296 -- iter: 096/140
[A[ATraining Step: 109  | total loss: [1m[32m0.67479[0m[0m | time: 1.741s
[2K
| Adam | epoch: 022 | loss: 0.67479 - acc: 0.5433 -- iter: 128/140
[A[ATraining Step: 110  | total loss: [1m[32m0.66813[0m[0m | time: 3.362s
[2K
| Adam | epoch: 022 | loss: 0.66813 - acc: 0.5515 | val_loss: 0.69974 - val_acc: 0.4318 -- iter: 140/140
--
Training Step: 111  | total loss: [1m[32m0.68007[0m[0m | time: 0.611s
[2K
| Adam | epoch: 023 | loss: 0.68007 - acc: 0.5338 -- iter: 032/140
[A[ATraining Step: 112  | total loss: [1m[32m0.67885[0m[0m | time: 1.212s
[2K
| Adam | epoch: 023 | loss: 0.67885 - acc: 0.5336 -- iter: 064/140
[A[ATraining Step: 113  | total loss: [1m[32m0.67496[0m[0m | time: 1.470s
[2K
| Adam | epoch: 023 | loss: 0.67496 - acc: 0.5646 -- iter: 096/140
[A[ATraining Step: 114  | total loss: [1m[32m0.67467[0m[0m | time: 1.713s
[2K
| Adam | epoch: 023 | loss: 0.67467 - acc: 0.5831 -- iter: 128/140
[A[ATraining Step: 115  | total loss: [1m[32m0.67348[0m[0m | time: 3.320s
[2K
| Adam | epoch: 023 | loss: 0.67348 - acc: 0.6081 | val_loss: 0.67010 - val_acc: 0.6591 -- iter: 140/140
--
Training Step: 116  | total loss: [1m[32m0.66858[0m[0m | time: 0.611s
[2K
| Adam | epoch: 024 | loss: 0.66858 - acc: 0.6348 -- iter: 032/140
[A[ATraining Step: 117  | total loss: [1m[32m0.66658[0m[0m | time: 1.238s
[2K
| Adam | epoch: 024 | loss: 0.66658 - acc: 0.6339 -- iter: 064/140
[A[ATraining Step: 118  | total loss: [1m[32m0.65629[0m[0m | time: 1.846s
[2K
| Adam | epoch: 024 | loss: 0.65629 - acc: 0.6423 -- iter: 096/140
[A[ATraining Step: 119  | total loss: [1m[32m0.65763[0m[0m | time: 2.106s
[2K
| Adam | epoch: 024 | loss: 0.65763 - acc: 0.6312 -- iter: 128/140
[A[ATraining Step: 120  | total loss: [1m[32m0.64626[0m[0m | time: 3.350s
[2K
| Adam | epoch: 024 | loss: 0.64626 - acc: 0.6431 | val_loss: 0.65012 - val_acc: 0.7045 -- iter: 140/140
--
Training Step: 121  | total loss: [1m[32m0.63210[0m[0m | time: 0.610s
[2K
| Adam | epoch: 025 | loss: 0.63210 - acc: 0.6705 -- iter: 032/140
[A[ATraining Step: 122  | total loss: [1m[32m0.62296[0m[0m | time: 1.220s
[2K
| Adam | epoch: 025 | loss: 0.62296 - acc: 0.6784 -- iter: 064/140
[A[ATraining Step: 123  | total loss: [1m[32m0.60963[0m[0m | time: 1.826s
[2K
| Adam | epoch: 025 | loss: 0.60963 - acc: 0.6918 -- iter: 096/140
[A[ATraining Step: 124  | total loss: [1m[32m0.59776[0m[0m | time: 2.437s
[2K
| Adam | epoch: 025 | loss: 0.59776 - acc: 0.7070 -- iter: 128/140
[A[ATraining Step: 125  | total loss: [1m[32m0.58232[0m[0m | time: 3.677s
[2K
| Adam | epoch: 025 | loss: 0.58232 - acc: 0.7207 | val_loss: 0.72326 - val_acc: 0.7500 -- iter: 140/140
--
Training Step: 126  | total loss: [1m[32m0.59757[0m[0m | time: 0.265s
[2K
| Adam | epoch: 026 | loss: 0.59757 - acc: 0.7070 -- iter: 032/140
[A[ATraining Step: 127  | total loss: [1m[32m0.60303[0m[0m | time: 0.887s
[2K
| Adam | epoch: 026 | loss: 0.60303 - acc: 0.7113 -- iter: 064/140
[A[ATraining Step: 128  | total loss: [1m[32m0.57152[0m[0m | time: 1.508s
[2K
| Adam | epoch: 026 | loss: 0.57152 - acc: 0.7276 -- iter: 096/140
[A[ATraining Step: 129  | total loss: [1m[32m0.56257[0m[0m | time: 2.121s
[2K
| Adam | epoch: 026 | loss: 0.56257 - acc: 0.7330 -- iter: 128/140
[A[ATraining Step: 130  | total loss: [1m[32m0.54358[0m[0m | time: 3.747s
[2K
| Adam | epoch: 026 | loss: 0.54358 - acc: 0.7441 | val_loss: 0.68139 - val_acc: 0.7955 -- iter: 140/140
--
Training Step: 131  | total loss: [1m[32m0.53613[0m[0m | time: 0.256s
[2K
| Adam | epoch: 027 | loss: 0.53613 - acc: 0.7447 -- iter: 032/140
[A[ATraining Step: 132  | total loss: [1m[32m0.51077[0m[0m | time: 0.504s
[2K
| Adam | epoch: 027 | loss: 0.51077 - acc: 0.7619 -- iter: 064/140
[A[ATraining Step: 133  | total loss: [1m[32m0.48429[0m[0m | time: 1.117s
[2K
| Adam | epoch: 027 | loss: 0.48429 - acc: 0.7773 -- iter: 096/140
[A[ATraining Step: 134  | total loss: [1m[32m0.46487[0m[0m | time: 1.737s
[2K
| Adam | epoch: 027 | loss: 0.46487 - acc: 0.7840 -- iter: 128/140
[A[ATraining Step: 135  | total loss: [1m[32m0.47348[0m[0m | time: 3.347s
[2K
| Adam | epoch: 027 | loss: 0.47348 - acc: 0.7806 | val_loss: 0.90371 - val_acc: 0.7273 -- iter: 140/140
--
Training Step: 136  | total loss: [1m[32m0.44490[0m[0m | time: 0.616s
[2K
| Adam | epoch: 028 | loss: 0.44490 - acc: 0.7963 -- iter: 032/140
[A[ATraining Step: 137  | total loss: [1m[32m0.45044[0m[0m | time: 0.863s
[2K
| Adam | epoch: 028 | loss: 0.45044 - acc: 0.8010 -- iter: 064/140
[A[ATraining Step: 138  | total loss: [1m[32m0.45112[0m[0m | time: 1.110s
[2K
| Adam | epoch: 028 | loss: 0.45112 - acc: 0.8043 -- iter: 096/140
[A[ATraining Step: 139  | total loss: [1m[32m0.44423[0m[0m | time: 1.712s
[2K
| Adam | epoch: 028 | loss: 0.44423 - acc: 0.8155 -- iter: 128/140
[A[ATraining Step: 140  | total loss: [1m[32m0.43736[0m[0m | time: 3.321s
[2K
| Adam | epoch: 028 | loss: 0.43736 - acc: 0.8183 | val_loss: 0.86411 - val_acc: 0.7500 -- iter: 140/140
--
Training Step: 141  | total loss: [1m[32m0.42608[0m[0m | time: 0.624s
[2K
| Adam | epoch: 029 | loss: 0.42608 - acc: 0.8177 -- iter: 032/140
[A[ATraining Step: 142  | total loss: [1m[32m0.40529[0m[0m | time: 1.238s
[2K
| Adam | epoch: 029 | loss: 0.40529 - acc: 0.8297 -- iter: 064/140
[A[ATraining Step: 143  | total loss: [1m[32m0.40360[0m[0m | time: 1.489s
[2K
| Adam | epoch: 029 | loss: 0.40360 - acc: 0.8342 -- iter: 096/140
[A[ATraining Step: 144  | total loss: [1m[32m0.39079[0m[0m | time: 1.730s
[2K
| Adam | epoch: 029 | loss: 0.39079 - acc: 0.8425 -- iter: 128/140
[A[ATraining Step: 145  | total loss: [1m[32m0.37283[0m[0m | time: 3.366s
[2K
| Adam | epoch: 029 | loss: 0.37283 - acc: 0.8499 | val_loss: 0.93814 - val_acc: 0.7500 -- iter: 140/140
--
Training Step: 146  | total loss: [1m[32m0.35607[0m[0m | time: 0.607s
[2K
| Adam | epoch: 030 | loss: 0.35607 - acc: 0.8587 -- iter: 032/140
[A[ATraining Step: 147  | total loss: [1m[32m0.35880[0m[0m | time: 1.200s
[2K
| Adam | epoch: 030 | loss: 0.35880 - acc: 0.8509 -- iter: 064/140
[A[ATraining Step: 148  | total loss: [1m[32m0.36567[0m[0m | time: 1.800s
[2K
| Adam | epoch: 030 | loss: 0.36567 - acc: 0.8502 -- iter: 096/140
[A[ATraining Step: 149  | total loss: [1m[32m0.38496[0m[0m | time: 2.044s
[2K
| Adam | epoch: 030 | loss: 0.38496 - acc: 0.8402 -- iter: 128/140
[A[ATraining Step: 150  | total loss: [1m[32m0.36203[0m[0m | time: 3.305s
[2K
| Adam | epoch: 030 | loss: 0.36203 - acc: 0.8562 | val_loss: 0.99066 - val_acc: 0.7500 -- iter: 140/140
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.7083333333333334
Validation AUPRC:0.6756401076555427
Test AUC:0.8476190476190476
Test AUPRC:0.6403098514213257
BestTestF1Score	0.72	0.6	0.75	0.56	1.0	14	11	19	0	0.2
BestTestMCCScore	0.72	0.6	0.75	0.56	1.0	14	11	19	0	0.2
BestTestAccuracyScore	0.72	0.6	0.75	0.56	1.0	14	11	19	0	0.2
BestValidationF1Score	0.82	0.59	0.8	0.8	0.83	20	5	15	4	0.2
BestValidationMCC	0.82	0.59	0.8	0.8	0.83	20	5	15	4	0.2
BestValidationAccuracy	0.82	0.59	0.8	0.8	0.83	20	5	15	4	0.2
TestPredictions (Threshold:0.2)
CHEMBL3431741,TN,INACT,0.0	CHEMBL3431641,TP,ACT,0.8999999761581421	CHEMBL3431709,FP,INACT,0.7599999904632568	CHEMBL3431670,TN,INACT,0.009999999776482582	CHEMBL3431830,TN,INACT,0.07000000029802322	CHEMBL3431663,TN,INACT,0.009999999776482582	CHEMBL3431502,TN,INACT,0.009999999776482582	CHEMBL3431763,TP,ACT,0.9200000166893005	CHEMBL3431761,TN,INACT,0.0	CHEMBL3431915,TP,ACT,0.8999999761581421	CHEMBL3431866,TN,INACT,0.0	CHEMBL3431595,TP,ACT,0.8799999952316284	CHEMBL3431527,FP,INACT,0.5400000214576721	CHEMBL3431630,TP,ACT,0.5699999928474426	CHEMBL3431801,TP,ACT,0.8600000143051147	CHEMBL3431520,TP,ACT,0.3499999940395355	CHEMBL3431510,TN,INACT,0.029999999329447746	CHEMBL3431909,TP,ACT,0.8799999952316284	CHEMBL3431868,TN,INACT,0.0	CHEMBL3431920,FP,INACT,0.8199999928474426	CHEMBL3431918,TP,ACT,0.8799999952316284	CHEMBL3431819,FP,INACT,0.8899999856948853	CHEMBL2216779,TP,ACT,0.8199999928474426	CHEMBL3431854,TN,INACT,0.029999999329447746	CHEMBL3431885,FP,INACT,0.8700000047683716	CHEMBL3431826,FP,INACT,0.8399999737739563	CHEMBL3431755,TN,INACT,0.029999999329447746	CHEMBL3431807,TN,INACT,0.019999999552965164	CHEMBL3431825,FP,INACT,0.8100000023841858	CHEMBL3431659,TP,ACT,0.8700000047683716	CHEMBL3431509,TN,INACT,0.019999999552965164	CHEMBL3431857,TN,INACT,0.029999999329447746	CHEMBL3431554,TP,ACT,0.5899999737739563	CHEMBL3431893,FP,INACT,0.8799999952316284	CHEMBL3431481,FP,INACT,0.5299999713897705	CHEMBL3431492,TN,INACT,0.05999999865889549	CHEMBL3431536,FP,INACT,0.9200000166893005	CHEMBL3431774,TP,ACT,0.20999999344348907	CHEMBL3431805,TN,INACT,0.03999999910593033	CHEMBL3431620,TN,INACT,0.029999999329447746	CHEMBL3431476,FP,INACT,0.3199999928474426	CHEMBL3431856,TN,INACT,0.009999999776482582	CHEMBL3431516,TN,INACT,0.009999999776482582	CHEMBL3431929,TP,ACT,0.8299999833106995	

