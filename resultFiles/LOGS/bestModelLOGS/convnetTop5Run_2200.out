CNNModel CHEMBL4142 adam 0.0005 15 128 0 0.8 False True
Number of active compounds :	279
Number of inactive compounds :	279
---------------------------------
Run id: CNNModel_CHEMBL4142_adam_0.0005_15_128_0_0.8_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL4142_adam_0.0005_15_128_0.8_True/
---------------------------------
Training samples: 344
Validation samples: 108
--
Training Step: 1  | time: 0.785s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/344
[A[ATraining Step: 2  | total loss: [1m[32m0.62358[0m[0m | time: 1.413s
[2K
| Adam | epoch: 001 | loss: 0.62358 - acc: 0.5625 -- iter: 064/344
[A[ATraining Step: 3  | total loss: [1m[32m0.68125[0m[0m | time: 2.016s
[2K
| Adam | epoch: 001 | loss: 0.68125 - acc: 0.4858 -- iter: 096/344
[A[ATraining Step: 4  | total loss: [1m[32m0.68697[0m[0m | time: 2.625s
[2K
| Adam | epoch: 001 | loss: 0.68697 - acc: 0.5668 -- iter: 128/344
[A[ATraining Step: 5  | total loss: [1m[32m0.69776[0m[0m | time: 3.211s
[2K
| Adam | epoch: 001 | loss: 0.69776 - acc: 0.4556 -- iter: 160/344
[A[ATraining Step: 6  | total loss: [1m[32m0.69008[0m[0m | time: 3.821s
[2K
| Adam | epoch: 001 | loss: 0.69008 - acc: 0.5645 -- iter: 192/344
[A[ATraining Step: 7  | total loss: [1m[32m0.68850[0m[0m | time: 4.425s
[2K
| Adam | epoch: 001 | loss: 0.68850 - acc: 0.5821 -- iter: 224/344
[A[ATraining Step: 8  | total loss: [1m[32m0.70356[0m[0m | time: 5.038s
[2K
| Adam | epoch: 001 | loss: 0.70356 - acc: 0.4129 -- iter: 256/344
[A[ATraining Step: 9  | total loss: [1m[32m0.69564[0m[0m | time: 5.647s
[2K
| Adam | epoch: 001 | loss: 0.69564 - acc: 0.5086 -- iter: 288/344
[A[ATraining Step: 10  | total loss: [1m[32m0.69805[0m[0m | time: 6.266s
[2K
| Adam | epoch: 001 | loss: 0.69805 - acc: 0.4418 -- iter: 320/344
[A[ATraining Step: 11  | total loss: [1m[32m0.69891[0m[0m | time: 7.757s
[2K
| Adam | epoch: 001 | loss: 0.69891 - acc: 0.3954 | val_loss: 0.69332 - val_acc: 0.4907 -- iter: 344/344
--
Training Step: 12  | total loss: [1m[32m0.69686[0m[0m | time: 0.490s
[2K
| Adam | epoch: 002 | loss: 0.69686 - acc: 0.4237 -- iter: 032/344
[A[ATraining Step: 13  | total loss: [1m[32m0.69583[0m[0m | time: 1.099s
[2K
| Adam | epoch: 002 | loss: 0.69583 - acc: 0.4385 -- iter: 064/344
[A[ATraining Step: 14  | total loss: [1m[32m0.69448[0m[0m | time: 1.715s
[2K
| Adam | epoch: 002 | loss: 0.69448 - acc: 0.4893 -- iter: 096/344
[A[ATraining Step: 15  | total loss: [1m[32m0.69381[0m[0m | time: 2.332s
[2K
| Adam | epoch: 002 | loss: 0.69381 - acc: 0.5057 -- iter: 128/344
[A[ATraining Step: 16  | total loss: [1m[32m0.69360[0m[0m | time: 2.976s
[2K
| Adam | epoch: 002 | loss: 0.69360 - acc: 0.4918 -- iter: 160/344
[A[ATraining Step: 17  | total loss: [1m[32m0.69350[0m[0m | time: 3.596s
[2K
| Adam | epoch: 002 | loss: 0.69350 - acc: 0.4835 -- iter: 192/344
[A[ATraining Step: 18  | total loss: [1m[32m0.69347[0m[0m | time: 4.206s
[2K
| Adam | epoch: 002 | loss: 0.69347 - acc: 0.4568 -- iter: 224/344
[A[ATraining Step: 19  | total loss: [1m[32m0.69325[0m[0m | time: 4.838s
[2K
| Adam | epoch: 002 | loss: 0.69325 - acc: 0.4920 -- iter: 256/344
[A[ATraining Step: 20  | total loss: [1m[32m0.69303[0m[0m | time: 5.463s
[2K
| Adam | epoch: 002 | loss: 0.69303 - acc: 0.5147 -- iter: 288/344
[A[ATraining Step: 21  | total loss: [1m[32m0.69318[0m[0m | time: 6.070s
[2K
| Adam | epoch: 002 | loss: 0.69318 - acc: 0.5004 -- iter: 320/344
[A[ATraining Step: 22  | total loss: [1m[32m0.69325[0m[0m | time: 7.696s
[2K
| Adam | epoch: 002 | loss: 0.69325 - acc: 0.5003 | val_loss: 0.69303 - val_acc: 0.5093 -- iter: 344/344
--
Training Step: 23  | total loss: [1m[32m0.69302[0m[0m | time: 0.472s
[2K
| Adam | epoch: 003 | loss: 0.69302 - acc: 0.5184 -- iter: 032/344
[A[ATraining Step: 24  | total loss: [1m[32m0.69367[0m[0m | time: 0.940s
[2K
| Adam | epoch: 003 | loss: 0.69367 - acc: 0.4780 -- iter: 064/344
[A[ATraining Step: 25  | total loss: [1m[32m0.69408[0m[0m | time: 1.558s
[2K
| Adam | epoch: 003 | loss: 0.69408 - acc: 0.4499 -- iter: 096/344
[A[ATraining Step: 26  | total loss: [1m[32m0.69404[0m[0m | time: 2.174s
[2K
| Adam | epoch: 003 | loss: 0.69404 - acc: 0.4466 -- iter: 128/344
[A[ATraining Step: 27  | total loss: [1m[32m0.69386[0m[0m | time: 2.782s
[2K
| Adam | epoch: 003 | loss: 0.69386 - acc: 0.4604 -- iter: 160/344
[A[ATraining Step: 28  | total loss: [1m[32m0.69359[0m[0m | time: 3.399s
[2K
| Adam | epoch: 003 | loss: 0.69359 - acc: 0.4781 -- iter: 192/344
[A[ATraining Step: 29  | total loss: [1m[32m0.69342[0m[0m | time: 4.015s
[2K
| Adam | epoch: 003 | loss: 0.69342 - acc: 0.4910 -- iter: 224/344
[A[ATraining Step: 30  | total loss: [1m[32m0.69318[0m[0m | time: 4.631s
[2K
| Adam | epoch: 003 | loss: 0.69318 - acc: 0.5153 -- iter: 256/344
[A[ATraining Step: 31  | total loss: [1m[32m0.69313[0m[0m | time: 5.253s
[2K
| Adam | epoch: 003 | loss: 0.69313 - acc: 0.5190 -- iter: 288/344
[A[ATraining Step: 32  | total loss: [1m[32m0.69330[0m[0m | time: 5.859s
[2K
| Adam | epoch: 003 | loss: 0.69330 - acc: 0.5007 -- iter: 320/344
[A[ATraining Step: 33  | total loss: [1m[32m0.69351[0m[0m | time: 7.465s
[2K
| Adam | epoch: 003 | loss: 0.69351 - acc: 0.4799 | val_loss: 0.69307 - val_acc: 0.5093 -- iter: 344/344
--
Training Step: 34  | total loss: [1m[32m0.69352[0m[0m | time: 0.637s
[2K
| Adam | epoch: 004 | loss: 0.69352 - acc: 0.4775 -- iter: 032/344
[A[ATraining Step: 35  | total loss: [1m[32m0.69339[0m[0m | time: 1.111s
[2K
| Adam | epoch: 004 | loss: 0.69339 - acc: 0.4888 -- iter: 064/344
[A[ATraining Step: 36  | total loss: [1m[32m0.69327[0m[0m | time: 1.576s
[2K
| Adam | epoch: 004 | loss: 0.69327 - acc: 0.4996 -- iter: 096/344
[A[ATraining Step: 37  | total loss: [1m[32m0.69318[0m[0m | time: 2.193s
[2K
| Adam | epoch: 004 | loss: 0.69318 - acc: 0.5080 -- iter: 128/344
[A[ATraining Step: 38  | total loss: [1m[32m0.69314[0m[0m | time: 2.803s
[2K
| Adam | epoch: 004 | loss: 0.69314 - acc: 0.5126 -- iter: 160/344
[A[ATraining Step: 39  | total loss: [1m[32m0.69308[0m[0m | time: 3.416s
[2K
| Adam | epoch: 004 | loss: 0.69308 - acc: 0.5161 -- iter: 192/344
[A[ATraining Step: 40  | total loss: [1m[32m0.69320[0m[0m | time: 4.031s
[2K
| Adam | epoch: 004 | loss: 0.69320 - acc: 0.5014 -- iter: 224/344
[A[ATraining Step: 41  | total loss: [1m[32m0.69314[0m[0m | time: 4.639s
[2K
| Adam | epoch: 004 | loss: 0.69314 - acc: 0.5069 -- iter: 256/344
[A[ATraining Step: 42  | total loss: [1m[32m0.69308[0m[0m | time: 5.232s
[2K
| Adam | epoch: 004 | loss: 0.69308 - acc: 0.5113 -- iter: 288/344
[A[ATraining Step: 43  | total loss: [1m[32m0.69304[0m[0m | time: 5.844s
[2K
| Adam | epoch: 004 | loss: 0.69304 - acc: 0.5148 -- iter: 320/344
[A[ATraining Step: 44  | total loss: [1m[32m0.69319[0m[0m | time: 7.465s
[2K
| Adam | epoch: 004 | loss: 0.69319 - acc: 0.5014 | val_loss: 0.69303 - val_acc: 0.5093 -- iter: 344/344
--
Training Step: 45  | total loss: [1m[32m0.69308[0m[0m | time: 0.632s
[2K
| Adam | epoch: 005 | loss: 0.69308 - acc: 0.5118 -- iter: 032/344
[A[ATraining Step: 46  | total loss: [1m[32m0.69302[0m[0m | time: 1.233s
[2K
| Adam | epoch: 005 | loss: 0.69302 - acc: 0.5150 -- iter: 064/344
[A[ATraining Step: 47  | total loss: [1m[32m0.69303[0m[0m | time: 1.706s
[2K
| Adam | epoch: 005 | loss: 0.69303 - acc: 0.5126 -- iter: 096/344
[A[ATraining Step: 48  | total loss: [1m[32m0.69313[0m[0m | time: 2.157s
[2K
| Adam | epoch: 005 | loss: 0.69313 - acc: 0.5039 -- iter: 128/344
[A[ATraining Step: 49  | total loss: [1m[32m0.69321[0m[0m | time: 2.770s
[2K
| Adam | epoch: 005 | loss: 0.69321 - acc: 0.4967 -- iter: 160/344
[A[ATraining Step: 50  | total loss: [1m[32m0.69302[0m[0m | time: 3.378s
[2K
| Adam | epoch: 005 | loss: 0.69302 - acc: 0.5117 -- iter: 192/344
[A[ATraining Step: 51  | total loss: [1m[32m0.69303[0m[0m | time: 4.010s
[2K
| Adam | epoch: 005 | loss: 0.69303 - acc: 0.5099 -- iter: 224/344
[A[ATraining Step: 52  | total loss: [1m[32m0.69290[0m[0m | time: 4.643s
[2K
| Adam | epoch: 005 | loss: 0.69290 - acc: 0.5178 -- iter: 256/344
[A[ATraining Step: 53  | total loss: [1m[32m0.69285[0m[0m | time: 5.250s
[2K
| Adam | epoch: 005 | loss: 0.69285 - acc: 0.5198 -- iter: 288/344
[A[ATraining Step: 54  | total loss: [1m[32m0.69280[0m[0m | time: 5.853s
[2K
| Adam | epoch: 005 | loss: 0.69280 - acc: 0.5215 -- iter: 320/344
[A[ATraining Step: 55  | total loss: [1m[32m0.69296[0m[0m | time: 7.468s
[2K
| Adam | epoch: 005 | loss: 0.69296 - acc: 0.5139 | val_loss: 0.69292 - val_acc: 0.5093 -- iter: 344/344
--
Training Step: 56  | total loss: [1m[32m0.69200[0m[0m | time: 0.610s
[2K
| Adam | epoch: 006 | loss: 0.69200 - acc: 0.5559 -- iter: 032/344
[A[ATraining Step: 57  | total loss: [1m[32m0.69240[0m[0m | time: 1.223s
[2K
| Adam | epoch: 006 | loss: 0.69240 - acc: 0.5395 -- iter: 064/344
[A[ATraining Step: 58  | total loss: [1m[32m0.69355[0m[0m | time: 1.836s
[2K
| Adam | epoch: 006 | loss: 0.69355 - acc: 0.5043 -- iter: 096/344
[A[ATraining Step: 59  | total loss: [1m[32m0.69400[0m[0m | time: 2.292s
[2K
| Adam | epoch: 006 | loss: 0.69400 - acc: 0.4869 -- iter: 128/344
[A[ATraining Step: 60  | total loss: [1m[32m0.69392[0m[0m | time: 2.750s
[2K
| Adam | epoch: 006 | loss: 0.69392 - acc: 0.4887 -- iter: 160/344
[A[ATraining Step: 61  | total loss: [1m[32m0.69376[0m[0m | time: 3.353s
[2K
| Adam | epoch: 006 | loss: 0.69376 - acc: 0.4901 -- iter: 192/344
[A[ATraining Step: 62  | total loss: [1m[32m0.69344[0m[0m | time: 3.955s
[2K
| Adam | epoch: 006 | loss: 0.69344 - acc: 0.5035 -- iter: 224/344
[A[ATraining Step: 63  | total loss: [1m[32m0.69330[0m[0m | time: 4.558s
[2K
| Adam | epoch: 006 | loss: 0.69330 - acc: 0.5070 -- iter: 256/344
[A[ATraining Step: 64  | total loss: [1m[32m0.69336[0m[0m | time: 5.164s
[2K
| Adam | epoch: 006 | loss: 0.69336 - acc: 0.5022 -- iter: 288/344
[A[ATraining Step: 65  | total loss: [1m[32m0.69320[0m[0m | time: 5.791s
[2K
| Adam | epoch: 006 | loss: 0.69320 - acc: 0.5096 -- iter: 320/344
[A[ATraining Step: 66  | total loss: [1m[32m0.69336[0m[0m | time: 7.404s
[2K
| Adam | epoch: 006 | loss: 0.69336 - acc: 0.5009 | val_loss: 0.69297 - val_acc: 0.5093 -- iter: 344/344
--
Training Step: 67  | total loss: [1m[32m0.69334[0m[0m | time: 0.628s
[2K
| Adam | epoch: 007 | loss: 0.69334 - acc: 0.5008 -- iter: 032/344
[A[ATraining Step: 68  | total loss: [1m[32m0.69347[0m[0m | time: 1.232s
[2K
| Adam | epoch: 007 | loss: 0.69347 - acc: 0.4933 -- iter: 064/344
[A[ATraining Step: 69  | total loss: [1m[32m0.69322[0m[0m | time: 1.836s
[2K
| Adam | epoch: 007 | loss: 0.69322 - acc: 0.5050 -- iter: 096/344
[A[ATraining Step: 70  | total loss: [1m[32m0.69310[0m[0m | time: 2.463s
[2K
| Adam | epoch: 007 | loss: 0.69310 - acc: 0.5116 -- iter: 128/344
[A[ATraining Step: 71  | total loss: [1m[32m0.69317[0m[0m | time: 2.920s
[2K
| Adam | epoch: 007 | loss: 0.69317 - acc: 0.5068 -- iter: 160/344
[A[ATraining Step: 72  | total loss: [1m[32m0.69290[0m[0m | time: 3.372s
[2K
| Adam | epoch: 007 | loss: 0.69290 - acc: 0.5201 -- iter: 192/344
[A[ATraining Step: 73  | total loss: [1m[32m0.69265[0m[0m | time: 3.972s
[2K
| Adam | epoch: 007 | loss: 0.69265 - acc: 0.5317 -- iter: 224/344
[A[ATraining Step: 74  | total loss: [1m[32m0.69301[0m[0m | time: 4.607s
[2K
| Adam | epoch: 007 | loss: 0.69301 - acc: 0.5145 -- iter: 256/344
[A[ATraining Step: 75  | total loss: [1m[32m0.69272[0m[0m | time: 5.215s
[2K
| Adam | epoch: 007 | loss: 0.69272 - acc: 0.5265 -- iter: 288/344
[A[ATraining Step: 76  | total loss: [1m[32m0.69270[0m[0m | time: 5.841s
[2K
| Adam | epoch: 007 | loss: 0.69270 - acc: 0.5270 -- iter: 320/344
[A[ATraining Step: 77  | total loss: [1m[32m0.69255[0m[0m | time: 7.456s
[2K
| Adam | epoch: 007 | loss: 0.69255 - acc: 0.5308 | val_loss: 0.69287 - val_acc: 0.5093 -- iter: 344/344
--
Training Step: 78  | total loss: [1m[32m0.69279[0m[0m | time: 0.614s
[2K
| Adam | epoch: 008 | loss: 0.69279 - acc: 0.5210 -- iter: 032/344
[A[ATraining Step: 79  | total loss: [1m[32m0.69295[0m[0m | time: 1.245s
[2K
| Adam | epoch: 008 | loss: 0.69295 - acc: 0.5156 -- iter: 064/344
[A[ATraining Step: 80  | total loss: [1m[32m0.69308[0m[0m | time: 1.855s
[2K
| Adam | epoch: 008 | loss: 0.69308 - acc: 0.5108 -- iter: 096/344
[A[ATraining Step: 81  | total loss: [1m[32m0.69318[0m[0m | time: 2.489s
[2K
| Adam | epoch: 008 | loss: 0.69318 - acc: 0.5066 -- iter: 128/344
[A[ATraining Step: 82  | total loss: [1m[32m0.69315[0m[0m | time: 3.096s
[2K
| Adam | epoch: 008 | loss: 0.69315 - acc: 0.5059 -- iter: 160/344
[A[ATraining Step: 83  | total loss: [1m[32m0.69286[0m[0m | time: 3.562s
[2K
| Adam | epoch: 008 | loss: 0.69286 - acc: 0.5147 -- iter: 192/344
[A[ATraining Step: 84  | total loss: [1m[32m0.69279[0m[0m | time: 4.044s
[2K
| Adam | epoch: 008 | loss: 0.69279 - acc: 0.5174 -- iter: 224/344
[A[ATraining Step: 85  | total loss: [1m[32m0.69268[0m[0m | time: 4.660s
[2K
| Adam | epoch: 008 | loss: 0.69268 - acc: 0.5198 -- iter: 256/344
[A[ATraining Step: 86  | total loss: [1m[32m0.69296[0m[0m | time: 5.280s
[2K
| Adam | epoch: 008 | loss: 0.69296 - acc: 0.5116 -- iter: 288/344
[A[ATraining Step: 87  | total loss: [1m[32m0.69253[0m[0m | time: 5.889s
[2K
| Adam | epoch: 008 | loss: 0.69253 - acc: 0.5229 -- iter: 320/344
[A[ATraining Step: 88  | total loss: [1m[32m0.69233[0m[0m | time: 7.518s
[2K
| Adam | epoch: 008 | loss: 0.69233 - acc: 0.5269 | val_loss: 0.69268 - val_acc: 0.5093 -- iter: 344/344
--
Training Step: 89  | total loss: [1m[32m0.69214[0m[0m | time: 0.615s
[2K
| Adam | epoch: 009 | loss: 0.69214 - acc: 0.5304 -- iter: 032/344
[A[ATraining Step: 90  | total loss: [1m[32m0.69240[0m[0m | time: 1.239s
[2K
| Adam | epoch: 009 | loss: 0.69240 - acc: 0.5243 -- iter: 064/344
[A[ATraining Step: 91  | total loss: [1m[32m0.69269[0m[0m | time: 1.860s
[2K
| Adam | epoch: 009 | loss: 0.69269 - acc: 0.5187 -- iter: 096/344
[A[ATraining Step: 92  | total loss: [1m[32m0.69271[0m[0m | time: 2.471s
[2K
| Adam | epoch: 009 | loss: 0.69271 - acc: 0.5168 -- iter: 128/344
[A[ATraining Step: 93  | total loss: [1m[32m0.69312[0m[0m | time: 3.107s
[2K
| Adam | epoch: 009 | loss: 0.69312 - acc: 0.5089 -- iter: 160/344
[A[ATraining Step: 94  | total loss: [1m[32m0.69295[0m[0m | time: 3.710s
[2K
| Adam | epoch: 009 | loss: 0.69295 - acc: 0.5111 -- iter: 192/344
[A[ATraining Step: 95  | total loss: [1m[32m0.69267[0m[0m | time: 4.191s
[2K
| Adam | epoch: 009 | loss: 0.69267 - acc: 0.5163 -- iter: 224/344
[A[ATraining Step: 96  | total loss: [1m[32m0.69249[0m[0m | time: 4.652s
[2K
| Adam | epoch: 009 | loss: 0.69249 - acc: 0.5188 -- iter: 256/344
[A[ATraining Step: 97  | total loss: [1m[32m0.69226[0m[0m | time: 5.247s
[2K
| Adam | epoch: 009 | loss: 0.69226 - acc: 0.5211 -- iter: 288/344
[A[ATraining Step: 98  | total loss: [1m[32m0.69214[0m[0m | time: 5.865s
[2K
| Adam | epoch: 009 | loss: 0.69214 - acc: 0.5221 -- iter: 320/344
[A[ATraining Step: 99  | total loss: [1m[32m0.69240[0m[0m | time: 7.481s
[2K
| Adam | epoch: 009 | loss: 0.69240 - acc: 0.5168 | val_loss: 0.69178 - val_acc: 0.5093 -- iter: 344/344
--
Training Step: 100  | total loss: [1m[32m0.69322[0m[0m | time: 0.611s
[2K
| Adam | epoch: 010 | loss: 0.69322 - acc: 0.5026 -- iter: 032/344
[A[ATraining Step: 101  | total loss: [1m[32m0.69280[0m[0m | time: 1.219s
[2K
| Adam | epoch: 010 | loss: 0.69280 - acc: 0.5086 -- iter: 064/344
[A[ATraining Step: 102  | total loss: [1m[32m0.69208[0m[0m | time: 1.815s
[2K
| Adam | epoch: 010 | loss: 0.69208 - acc: 0.5202 -- iter: 096/344
[A[ATraining Step: 103  | total loss: [1m[32m0.69169[0m[0m | time: 2.460s
[2K
| Adam | epoch: 010 | loss: 0.69169 - acc: 0.5245 -- iter: 128/344
[A[ATraining Step: 104  | total loss: [1m[32m0.69250[0m[0m | time: 3.064s
[2K
| Adam | epoch: 010 | loss: 0.69250 - acc: 0.5126 -- iter: 160/344
[A[ATraining Step: 105  | total loss: [1m[32m0.69355[0m[0m | time: 3.667s
[2K
| Adam | epoch: 010 | loss: 0.69355 - acc: 0.4958 -- iter: 192/344
[A[ATraining Step: 106  | total loss: [1m[32m0.69347[0m[0m | time: 4.286s
[2K
| Adam | epoch: 010 | loss: 0.69347 - acc: 0.4931 -- iter: 224/344
[A[ATraining Step: 107  | total loss: [1m[32m0.69235[0m[0m | time: 4.745s
[2K
| Adam | epoch: 010 | loss: 0.69235 - acc: 0.5125 -- iter: 256/344
[A[ATraining Step: 108  | total loss: [1m[32m0.69288[0m[0m | time: 5.209s
[2K
| Adam | epoch: 010 | loss: 0.69288 - acc: 0.4987 -- iter: 288/344
[A[ATraining Step: 109  | total loss: [1m[32m0.69329[0m[0m | time: 5.831s
[2K
| Adam | epoch: 010 | loss: 0.69329 - acc: 0.4864 -- iter: 320/344
[A[ATraining Step: 110  | total loss: [1m[32m0.69239[0m[0m | time: 7.477s
[2K
| Adam | epoch: 010 | loss: 0.69239 - acc: 0.5065 | val_loss: 0.68880 - val_acc: 0.5093 -- iter: 344/344
--
Training Step: 111  | total loss: [1m[32m0.69223[0m[0m | time: 0.613s
[2K
| Adam | epoch: 011 | loss: 0.69223 - acc: 0.5058 -- iter: 032/344
[A[ATraining Step: 112  | total loss: [1m[32m0.69142[0m[0m | time: 1.236s
[2K
| Adam | epoch: 011 | loss: 0.69142 - acc: 0.5178 -- iter: 064/344
[A[ATraining Step: 113  | total loss: [1m[32m0.68980[0m[0m | time: 1.852s
[2K
| Adam | epoch: 011 | loss: 0.68980 - acc: 0.5347 -- iter: 096/344
[A[ATraining Step: 114  | total loss: [1m[32m0.69051[0m[0m | time: 2.456s
[2K
| Adam | epoch: 011 | loss: 0.69051 - acc: 0.5250 -- iter: 128/344
[A[ATraining Step: 115  | total loss: [1m[32m0.68949[0m[0m | time: 3.088s
[2K
| Adam | epoch: 011 | loss: 0.68949 - acc: 0.5288 -- iter: 160/344
[A[ATraining Step: 116  | total loss: [1m[32m0.68920[0m[0m | time: 3.699s
[2K
| Adam | epoch: 011 | loss: 0.68920 - acc: 0.5290 -- iter: 192/344
[A[ATraining Step: 117  | total loss: [1m[32m0.68828[0m[0m | time: 4.312s
[2K
| Adam | epoch: 011 | loss: 0.68828 - acc: 0.5292 -- iter: 224/344
[A[ATraining Step: 118  | total loss: [1m[32m0.69169[0m[0m | time: 4.915s
[2K
| Adam | epoch: 011 | loss: 0.69169 - acc: 0.5107 -- iter: 256/344
[A[ATraining Step: 119  | total loss: [1m[32m0.68994[0m[0m | time: 5.400s
[2K
| Adam | epoch: 011 | loss: 0.68994 - acc: 0.5159 -- iter: 288/344
[A[ATraining Step: 120  | total loss: [1m[32m0.68949[0m[0m | time: 5.859s
[2K
| Adam | epoch: 011 | loss: 0.68949 - acc: 0.5101 -- iter: 320/344
[A[ATraining Step: 121  | total loss: [1m[32m0.68852[0m[0m | time: 7.457s
[2K
| Adam | epoch: 011 | loss: 0.68852 - acc: 0.5049 | val_loss: 0.67605 - val_acc: 0.5093 -- iter: 344/344
--
Training Step: 122  | total loss: [1m[32m0.68800[0m[0m | time: 0.660s
[2K
| Adam | epoch: 012 | loss: 0.68800 - acc: 0.5013 -- iter: 032/344
[A[ATraining Step: 123  | total loss: [1m[32m0.68744[0m[0m | time: 1.269s
[2K
| Adam | epoch: 012 | loss: 0.68744 - acc: 0.4918 -- iter: 064/344
[A[ATraining Step: 124  | total loss: [1m[32m0.68565[0m[0m | time: 1.870s
[2K
| Adam | epoch: 012 | loss: 0.68565 - acc: 0.4989 -- iter: 096/344
[A[ATraining Step: 125  | total loss: [1m[32m0.68581[0m[0m | time: 2.464s
[2K
| Adam | epoch: 012 | loss: 0.68581 - acc: 0.4927 -- iter: 128/344
[A[ATraining Step: 126  | total loss: [1m[32m0.68087[0m[0m | time: 3.079s
[2K
| Adam | epoch: 012 | loss: 0.68087 - acc: 0.5028 -- iter: 160/344
[A[ATraining Step: 127  | total loss: [1m[32m0.67967[0m[0m | time: 3.682s
[2K
| Adam | epoch: 012 | loss: 0.67967 - acc: 0.5057 -- iter: 192/344
[A[ATraining Step: 128  | total loss: [1m[32m0.67998[0m[0m | time: 4.287s
[2K
| Adam | epoch: 012 | loss: 0.67998 - acc: 0.4989 -- iter: 224/344
[A[ATraining Step: 129  | total loss: [1m[32m0.67488[0m[0m | time: 4.901s
[2K
| Adam | epoch: 012 | loss: 0.67488 - acc: 0.5084 -- iter: 256/344
[A[ATraining Step: 130  | total loss: [1m[32m0.67117[0m[0m | time: 5.500s
[2K
| Adam | epoch: 012 | loss: 0.67117 - acc: 0.5325 -- iter: 288/344
[A[ATraining Step: 131  | total loss: [1m[32m0.66863[0m[0m | time: 5.972s
[2K
| Adam | epoch: 012 | loss: 0.66863 - acc: 0.5449 -- iter: 320/344
[A[ATraining Step: 132  | total loss: [1m[32m0.65938[0m[0m | time: 7.426s
[2K
| Adam | epoch: 012 | loss: 0.65938 - acc: 0.5696 | val_loss: 0.58985 - val_acc: 0.5648 -- iter: 344/344
--
Training Step: 133  | total loss: [1m[32m0.64832[0m[0m | time: 0.630s
[2K
| Adam | epoch: 013 | loss: 0.64832 - acc: 0.5793 -- iter: 032/344
[A[ATraining Step: 134  | total loss: [1m[32m0.64027[0m[0m | time: 1.258s
[2K
| Adam | epoch: 013 | loss: 0.64027 - acc: 0.5870 -- iter: 064/344
[A[ATraining Step: 135  | total loss: [1m[32m0.63511[0m[0m | time: 1.880s
[2K
| Adam | epoch: 013 | loss: 0.63511 - acc: 0.6064 -- iter: 096/344
[A[ATraining Step: 136  | total loss: [1m[32m0.62601[0m[0m | time: 2.493s
[2K
| Adam | epoch: 013 | loss: 0.62601 - acc: 0.6333 -- iter: 128/344
[A[ATraining Step: 137  | total loss: [1m[32m0.62293[0m[0m | time: 3.106s
[2K
| Adam | epoch: 013 | loss: 0.62293 - acc: 0.6356 -- iter: 160/344
[A[ATraining Step: 138  | total loss: [1m[32m0.61023[0m[0m | time: 3.719s
[2K
| Adam | epoch: 013 | loss: 0.61023 - acc: 0.6564 -- iter: 192/344
[A[ATraining Step: 139  | total loss: [1m[32m0.59590[0m[0m | time: 4.325s
[2K
| Adam | epoch: 013 | loss: 0.59590 - acc: 0.6782 -- iter: 224/344
[A[ATraining Step: 140  | total loss: [1m[32m0.59510[0m[0m | time: 4.933s
[2K
| Adam | epoch: 013 | loss: 0.59510 - acc: 0.6729 -- iter: 256/344
[A[ATraining Step: 141  | total loss: [1m[32m0.58221[0m[0m | time: 5.559s
[2K
| Adam | epoch: 013 | loss: 0.58221 - acc: 0.6838 -- iter: 288/344
[A[ATraining Step: 142  | total loss: [1m[32m0.59487[0m[0m | time: 6.167s
[2K
| Adam | epoch: 013 | loss: 0.59487 - acc: 0.6841 -- iter: 320/344
[A[ATraining Step: 143  | total loss: [1m[32m0.58178[0m[0m | time: 7.622s
[2K
| Adam | epoch: 013 | loss: 0.58178 - acc: 0.7001 | val_loss: 0.44647 - val_acc: 0.7870 -- iter: 344/344
--
Training Step: 144  | total loss: [1m[32m0.57463[0m[0m | time: 0.474s
[2K
| Adam | epoch: 014 | loss: 0.57463 - acc: 0.7051 -- iter: 032/344
[A[ATraining Step: 145  | total loss: [1m[32m0.57093[0m[0m | time: 1.091s
[2K
| Adam | epoch: 014 | loss: 0.57093 - acc: 0.7096 -- iter: 064/344
[A[ATraining Step: 146  | total loss: [1m[32m0.57037[0m[0m | time: 1.737s
[2K
| Adam | epoch: 014 | loss: 0.57037 - acc: 0.7074 -- iter: 096/344
[A[ATraining Step: 147  | total loss: [1m[32m0.55922[0m[0m | time: 2.378s
[2K
| Adam | epoch: 014 | loss: 0.55922 - acc: 0.7179 -- iter: 128/344
[A[ATraining Step: 148  | total loss: [1m[32m0.54034[0m[0m | time: 3.002s
[2K
| Adam | epoch: 014 | loss: 0.54034 - acc: 0.7336 -- iter: 160/344
[A[ATraining Step: 149  | total loss: [1m[32m0.53996[0m[0m | time: 3.607s
[2K
| Adam | epoch: 014 | loss: 0.53996 - acc: 0.7321 -- iter: 192/344
[A[ATraining Step: 150  | total loss: [1m[32m0.51724[0m[0m | time: 4.240s
[2K
| Adam | epoch: 014 | loss: 0.51724 - acc: 0.7495 -- iter: 224/344
[A[ATraining Step: 151  | total loss: [1m[32m0.51037[0m[0m | time: 4.840s
[2K
| Adam | epoch: 014 | loss: 0.51037 - acc: 0.7589 -- iter: 256/344
[A[ATraining Step: 152  | total loss: [1m[32m0.50394[0m[0m | time: 5.447s
[2K
| Adam | epoch: 014 | loss: 0.50394 - acc: 0.7643 -- iter: 288/344
[A[ATraining Step: 153  | total loss: [1m[32m0.50081[0m[0m | time: 6.061s
[2K
| Adam | epoch: 014 | loss: 0.50081 - acc: 0.7660 -- iter: 320/344
[A[ATraining Step: 154  | total loss: [1m[32m0.48093[0m[0m | time: 7.679s
[2K
| Adam | epoch: 014 | loss: 0.48093 - acc: 0.7769 | val_loss: 0.33090 - val_acc: 0.8333 -- iter: 344/344
--
Training Step: 155  | total loss: [1m[32m0.47192[0m[0m | time: 0.470s
[2K
| Adam | epoch: 015 | loss: 0.47192 - acc: 0.7805 -- iter: 032/344
[A[ATraining Step: 156  | total loss: [1m[32m0.47980[0m[0m | time: 0.932s
[2K
| Adam | epoch: 015 | loss: 0.47980 - acc: 0.7774 -- iter: 064/344
[A[ATraining Step: 157  | total loss: [1m[32m0.47272[0m[0m | time: 1.563s
[2K
| Adam | epoch: 015 | loss: 0.47272 - acc: 0.7788 -- iter: 096/344
[A[ATraining Step: 158  | total loss: [1m[32m0.47378[0m[0m | time: 2.159s
[2K
| Adam | epoch: 015 | loss: 0.47378 - acc: 0.7791 -- iter: 128/344
[A[ATraining Step: 159  | total loss: [1m[32m0.47552[0m[0m | time: 2.784s
[2K
| Adam | epoch: 015 | loss: 0.47552 - acc: 0.7730 -- iter: 160/344
[A[ATraining Step: 160  | total loss: [1m[32m0.46190[0m[0m | time: 3.404s
[2K
| Adam | epoch: 015 | loss: 0.46190 - acc: 0.7801 -- iter: 192/344
[A[ATraining Step: 161  | total loss: [1m[32m0.44697[0m[0m | time: 4.012s
[2K
| Adam | epoch: 015 | loss: 0.44697 - acc: 0.7927 -- iter: 224/344
[A[ATraining Step: 162  | total loss: [1m[32m0.43834[0m[0m | time: 4.614s
[2K
| Adam | epoch: 015 | loss: 0.43834 - acc: 0.7947 -- iter: 256/344
[A[ATraining Step: 163  | total loss: [1m[32m0.46722[0m[0m | time: 5.217s
[2K
| Adam | epoch: 015 | loss: 0.46722 - acc: 0.7777 -- iter: 288/344
[A[ATraining Step: 164  | total loss: [1m[32m0.47465[0m[0m | time: 5.829s
[2K
| Adam | epoch: 015 | loss: 0.47465 - acc: 0.7625 -- iter: 320/344
[A[ATraining Step: 165  | total loss: [1m[32m0.45649[0m[0m | time: 7.438s
[2K
| Adam | epoch: 015 | loss: 0.45649 - acc: 0.7737 | val_loss: 0.47401 - val_acc: 0.7685 -- iter: 344/344
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.9670668953687822
Validation AUPRC:0.9708085643941347
Test AUC:0.9642242862057103
Test AUPRC:0.9738316721637899
BestTestF1Score	0.9	0.8	0.9	0.94	0.86	49	3	48	8	0.83
BestTestMCCScore	0.87	0.78	0.88	0.98	0.79	45	1	50	12	0.89
BestTestAccuracyScore	0.87	0.78	0.88	0.98	0.79	45	1	50	12	0.89
BestValidationF1Score	0.92	0.83	0.92	0.91	0.93	51	5	48	4	0.83
BestValidationMCC	0.91	0.84	0.92	0.96	0.87	48	2	51	7	0.89
BestValidationAccuracy	0.91	0.84	0.92	0.96	0.87	48	2	51	7	0.89
TestPredictions (Threshold:0.89)
CHEMBL170519,TP,ACT,0.9800000190734863	CHEMBL1910753,TN,INACT,0.8399999737739563	CHEMBL573578,TN,INACT,0.4000000059604645	CHEMBL3609567,TN,INACT,0.46000000834465027	CHEMBL1910602,TN,INACT,0.4099999964237213	CHEMBL169397,TP,ACT,0.9900000095367432	CHEMBL561136,TN,INACT,0.41999998688697815	CHEMBL3800352,TP,ACT,0.9900000095367432	CHEMBL3676333,FN,ACT,0.7200000286102295	CHEMBL509435,TN,INACT,0.3400000035762787	CHEMBL557050,TN,INACT,0.23999999463558197	CHEMBL2392227,TN,INACT,0.49000000953674316	CHEMBL169837,TP,ACT,0.9700000286102295	CHEMBL1801932,TN,INACT,0.6800000071525574	CHEMBL1784660,TN,INACT,0.1899999976158142	CHEMBL3681307,TP,ACT,0.9900000095367432	CHEMBL3681308,TP,ACT,1.0	CHEMBL3681281,TP,ACT,0.9200000166893005	CHEMBL170907,TP,ACT,0.9700000286102295	CHEMBL491473,TP,ACT,0.9800000190734863	CHEMBL3701273,TP,ACT,0.9900000095367432	CHEMBL490241,TN,INACT,0.33000001311302185	CHEMBL3797998,TP,ACT,1.0	CHEMBL498248,TN,INACT,0.6399999856948853	CHEMBL3681255,TP,ACT,1.0	CHEMBL3681297,TP,ACT,0.9900000095367432	CHEMBL3798532,TP,ACT,0.9800000190734863	CHEMBL1809197,TN,INACT,0.6100000143051147	CHEMBL602471,FN,ACT,0.800000011920929	CHEMBL3701246,TP,ACT,1.0	CHEMBL3681247,TP,ACT,0.9900000095367432	CHEMBL560278,TN,INACT,0.7699999809265137	CHEMBL3681316,TP,ACT,1.0	CHEMBL2392379,TN,INACT,0.6899999976158142	CHEMBL3798997,TP,ACT,0.9599999785423279	CHEMBL486302,TN,INACT,0.800000011920929	CHEMBL456965,TN,INACT,0.18000000715255737	CHEMBL132948,TN,INACT,0.30000001192092896	CHEMBL3701237,TP,ACT,0.9900000095367432	CHEMBL457390,TN,INACT,0.25	CHEMBL120317,TN,INACT,0.7400000095367432	CHEMBL363607,FN,ACT,0.8399999737739563	CHEMBL3676325,TP,ACT,1.0	CHEMBL3681270,TP,ACT,0.9900000095367432	CHEMBL3799426,TP,ACT,1.0	CHEMBL3701277,TP,ACT,0.9900000095367432	CHEMBL230686,TP,ACT,0.9900000095367432	CHEMBL2392242,TN,INACT,0.5299999713897705	CHEMBL453737,TP,ACT,0.8999999761581421	CHEMBL569880,FN,ACT,0.7699999809265137	CHEMBL174634,TN,INACT,0.47999998927116394	CHEMBL197077,TN,INACT,0.3499999940395355	CHEMBL3800526,TP,ACT,0.9900000095367432	CHEMBL74799,TN,INACT,0.38999998569488525	CHEMBL3681305,FN,ACT,0.8799999952316284	CHEMBL337454,TN,INACT,0.2800000011920929	CHEMBL1910606,TN,INACT,0.5199999809265137	CHEMBL489344,TN,INACT,0.3100000023841858	CHEMBL3681306,TP,ACT,0.9100000262260437	CHEMBL77155,TN,INACT,0.5899999737739563	CHEMBL603469,FN,ACT,0.30000001192092896	CHEMBL524820,TN,INACT,0.6100000143051147	CHEMBL3676327,TP,ACT,0.9800000190734863	CHEMBL1822792,TP,ACT,0.8999999761581421	CHEMBL1784637,TP,ACT,0.9599999785423279	CHEMBL3335362,TN,INACT,0.3199999928474426	CHEMBL3701259,TP,ACT,1.0	CHEMBL3676324,TP,ACT,0.9900000095367432	CHEMBL574738,TP,ACT,0.9700000286102295	CHEMBL3401368,FN,ACT,0.5899999737739563	CHEMBL101868,FP,INACT,0.9300000071525574	CHEMBL169757,TN,INACT,0.5799999833106995	CHEMBL1910754,TN,INACT,0.5600000023841858	CHEMBL489627,TN,INACT,0.18000000715255737	CHEMBL2164716,TN,INACT,0.8299999833106995	CHEMBL457191,TN,INACT,0.46000000834465027	CHEMBL77262,TN,INACT,0.49000000953674316	CHEMBL3701248,TP,ACT,1.0	CHEMBL3701272,FN,ACT,0.8799999952316284	CHEMBL429743,TP,ACT,0.9900000095367432	CHEMBL95477,TN,INACT,0.5	CHEMBL456796,TN,INACT,0.25999999046325684	CHEMBL188386,FN,ACT,0.7599999904632568	CHEMBL191632,TN,INACT,0.7200000286102295	CHEMBL559882,TN,INACT,0.4099999964237213	CHEMBL171509,TP,ACT,0.9900000095367432	CHEMBL267118,TN,INACT,0.6399999856948853	CHEMBL3681301,TP,ACT,0.9800000190734863	CHEMBL3701280,TP,ACT,0.9399999976158142	CHEMBL377300,FN,ACT,0.8500000238418579	CHEMBL3701244,TP,ACT,1.0	CHEMBL3681294,TP,ACT,1.0	CHEMBL176815,TN,INACT,0.8600000143051147	CHEMBL1287945,TN,INACT,0.2800000011920929	CHEMBL1767275,TN,INACT,0.20000000298023224	CHEMBL3681245,TP,ACT,1.0	CHEMBL469770,TN,INACT,0.6200000047683716	CHEMBL3701239,TP,ACT,0.9900000095367432	CHEMBL2392236,TN,INACT,0.5199999809265137	CHEMBL3681309,TP,ACT,1.0	CHEMBL469346,TN,INACT,0.4000000059604645	CHEMBL3681272,FN,ACT,0.7699999809265137	CHEMBL3701251,TP,ACT,0.9900000095367432	CHEMBL3799140,TP,ACT,0.9800000190734863	CHEMBL1767294,TN,INACT,0.4699999988079071	CHEMBL557456,TN,INACT,0.28999999165534973	CHEMBL551936,TN,INACT,0.23000000417232513	CHEMBL57366,FN,ACT,0.6700000166893005	

