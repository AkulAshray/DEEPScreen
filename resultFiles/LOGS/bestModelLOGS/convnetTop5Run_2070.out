ImageNetInceptionV2 CHEMBL1741195 adam 0.0005 15 0 0 0.6 False True
Number of active compounds :	210
Number of inactive compounds :	210
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL1741195_adam_0.0005_15_0_0_0.6_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL1741195_adam_0.0005_15_0.6/
---------------------------------
Training samples: 268
Validation samples: 84
--
Training Step: 1  | time: 205.877s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/268
[A[ATraining Step: 2  | total loss: [1m[32m0.70902[0m[0m | time: 239.650s
[2K
| Adam | epoch: 001 | loss: 0.70902 - acc: 0.4500 -- iter: 064/268
[A[ATraining Step: 3  | total loss: [1m[32m0.64883[0m[0m | time: 419.285s
[2K
| Adam | epoch: 001 | loss: 0.64883 - acc: 0.6187 -- iter: 096/268
[A[ATraining Step: 4  | total loss: [1m[32m0.63231[0m[0m | time: 487.362s
[2K
| Adam | epoch: 001 | loss: 0.63231 - acc: 0.6234 -- iter: 128/268
[A[ATraining Step: 5  | total loss: [1m[32m0.57354[0m[0m | time: 524.280s
[2K
| Adam | epoch: 001 | loss: 0.57354 - acc: 0.6678 -- iter: 160/268
[A[ATraining Step: 6  | total loss: [1m[32m0.52697[0m[0m | time: 532.149s
[2K
| Adam | epoch: 001 | loss: 0.52697 - acc: 0.7407 -- iter: 192/268
[A[ATraining Step: 7  | total loss: [1m[32m0.37894[0m[0m | time: 542.724s
[2K
| Adam | epoch: 001 | loss: 0.37894 - acc: 0.8213 -- iter: 224/268
[A[ATraining Step: 8  | total loss: [1m[32m0.33472[0m[0m | time: 596.885s
[2K
| Adam | epoch: 001 | loss: 0.33472 - acc: 0.8691 -- iter: 256/268
[A[ATraining Step: 9  | total loss: [1m[32m0.35554[0m[0m | time: 618.481s
[2K
| Adam | epoch: 001 | loss: 0.35554 - acc: 0.8391 | val_loss: 3.65499 - val_acc: 0.3929 -- iter: 268/268
--
Training Step: 10  | total loss: [1m[32m0.29938[0m[0m | time: 3.762s
[2K
| Adam | epoch: 002 | loss: 0.29938 - acc: 0.8779 -- iter: 032/268
[A[ATraining Step: 11  | total loss: [1m[32m0.19952[0m[0m | time: 20.448s
[2K
| Adam | epoch: 002 | loss: 0.19952 - acc: 0.9357 -- iter: 064/268
[A[ATraining Step: 12  | total loss: [1m[32m0.29133[0m[0m | time: 226.236s
[2K
| Adam | epoch: 002 | loss: 0.29133 - acc: 0.8803 -- iter: 096/268
[A[ATraining Step: 13  | total loss: [1m[32m0.24298[0m[0m | time: 363.379s
[2K
| Adam | epoch: 002 | loss: 0.24298 - acc: 0.8914 -- iter: 128/268
[A[ATraining Step: 14  | total loss: [1m[32m0.21364[0m[0m | time: 533.131s
[2K
| Adam | epoch: 002 | loss: 0.21364 - acc: 0.9103 -- iter: 160/268
[A[ATraining Step: 15  | total loss: [1m[32m0.15989[0m[0m | time: 543.976s
[2K
| Adam | epoch: 002 | loss: 0.15989 - acc: 0.9332 -- iter: 192/268
[A[ATraining Step: 16  | total loss: [1m[32m0.16474[0m[0m | time: 571.163s
[2K
| Adam | epoch: 002 | loss: 0.16474 - acc: 0.9465 -- iter: 224/268
[A[ATraining Step: 17  | total loss: [1m[32m0.18162[0m[0m | time: 580.143s
[2K
| Adam | epoch: 002 | loss: 0.18162 - acc: 0.9320 -- iter: 256/268
[A[ATraining Step: 18  | total loss: [1m[32m0.16270[0m[0m | time: 592.122s
[2K
| Adam | epoch: 002 | loss: 0.16270 - acc: 0.9447 | val_loss: 2.32393 - val_acc: 0.3929 -- iter: 268/268
--
Training Step: 19  | total loss: [1m[32m0.12912[0m[0m | time: 6.436s
[2K
| Adam | epoch: 003 | loss: 0.12912 - acc: 0.9632 -- iter: 032/268
[A[ATraining Step: 20  | total loss: [1m[32m0.09541[0m[0m | time: 36.415s
[2K
| Adam | epoch: 003 | loss: 0.09541 - acc: 0.9750 -- iter: 064/268
[A[ATraining Step: 21  | total loss: [1m[32m0.07180[0m[0m | time: 58.072s
[2K
| Adam | epoch: 003 | loss: 0.07180 - acc: 0.9828 -- iter: 096/268
[A[ATraining Step: 22  | total loss: [1m[32m0.10550[0m[0m | time: 67.731s
[2K
| Adam | epoch: 003 | loss: 0.10550 - acc: 0.9598 -- iter: 128/268
[A[ATraining Step: 23  | total loss: [1m[32m0.09412[0m[0m | time: 75.953s
[2K
| Adam | epoch: 003 | loss: 0.09412 - acc: 0.9624 -- iter: 160/268
[A[ATraining Step: 24  | total loss: [1m[32m0.07743[0m[0m | time: 84.216s
[2K
| Adam | epoch: 003 | loss: 0.07743 - acc: 0.9730 -- iter: 192/268
[A[ATraining Step: 25  | total loss: [1m[32m0.06161[0m[0m | time: 92.755s
[2K
| Adam | epoch: 003 | loss: 0.06161 - acc: 0.9803 -- iter: 224/268
[A[ATraining Step: 26  | total loss: [1m[32m0.16067[0m[0m | time: 102.552s
[2K
| Adam | epoch: 003 | loss: 0.16067 - acc: 0.9525 -- iter: 256/268
[A[ATraining Step: 27  | total loss: [1m[32m0.12384[0m[0m | time: 132.370s
[2K
| Adam | epoch: 003 | loss: 0.12384 - acc: 0.9647 | val_loss: 0.78395 - val_acc: 0.4048 -- iter: 268/268
--
Training Step: 28  | total loss: [1m[32m0.16523[0m[0m | time: 8.243s
[2K
| Adam | epoch: 004 | loss: 0.16523 - acc: 0.9657 -- iter: 032/268
[A[ATraining Step: 29  | total loss: [1m[32m0.12900[0m[0m | time: 11.881s
[2K
| Adam | epoch: 004 | loss: 0.12900 - acc: 0.9740 -- iter: 064/268
[A[ATraining Step: 30  | total loss: [1m[32m0.15566[0m[0m | time: 15.532s
[2K
| Adam | epoch: 004 | loss: 0.15566 - acc: 0.9605 -- iter: 096/268
[A[ATraining Step: 31  | total loss: [1m[32m0.12893[0m[0m | time: 23.690s
[2K
| Adam | epoch: 004 | loss: 0.12893 - acc: 0.9696 -- iter: 128/268
[A[ATraining Step: 32  | total loss: [1m[32m0.14841[0m[0m | time: 32.472s
[2K
| Adam | epoch: 004 | loss: 0.14841 - acc: 0.9553 -- iter: 160/268
[A[ATraining Step: 33  | total loss: [1m[32m0.12414[0m[0m | time: 78.383s
[2K
| Adam | epoch: 004 | loss: 0.12414 - acc: 0.9651 -- iter: 192/268
[A[ATraining Step: 34  | total loss: [1m[32m0.10957[0m[0m | time: 285.468s
[2K
| Adam | epoch: 004 | loss: 0.10957 - acc: 0.9659 -- iter: 224/268
[A[ATraining Step: 35  | total loss: [1m[32m0.09441[0m[0m | time: 393.518s
[2K
| Adam | epoch: 004 | loss: 0.09441 - acc: 0.9665 -- iter: 256/268
[A[ATraining Step: 36  | total loss: [1m[32m0.08933[0m[0m | time: 499.642s
[2K
| Adam | epoch: 004 | loss: 0.08933 - acc: 0.9670 | val_loss: 3.79764 - val_acc: 0.3929 -- iter: 268/268
--
Training Step: 37  | total loss: [1m[32m0.14911[0m[0m | time: 18.914s
[2K
| Adam | epoch: 005 | loss: 0.14911 - acc: 0.9548 -- iter: 032/268
[A[ATraining Step: 38  | total loss: [1m[32m0.12483[0m[0m | time: 65.040s
[2K
| Adam | epoch: 005 | loss: 0.12483 - acc: 0.9637 -- iter: 064/268
[A[ATraining Step: 39  | total loss: [1m[32m0.10465[0m[0m | time: 98.987s
[2K
| Adam | epoch: 005 | loss: 0.10465 - acc: 0.9706 -- iter: 096/268
[A[ATraining Step: 40  | total loss: [1m[32m0.09147[0m[0m | time: 107.465s
[2K
| Adam | epoch: 005 | loss: 0.09147 - acc: 0.9761 -- iter: 128/268
[A[ATraining Step: 41  | total loss: [1m[32m0.07848[0m[0m | time: 249.394s
[2K
| Adam | epoch: 005 | loss: 0.07848 - acc: 0.9805 -- iter: 160/268
[A[ATraining Step: 42  | total loss: [1m[32m0.07011[0m[0m | time: 353.057s
[2K
| Adam | epoch: 005 | loss: 0.07011 - acc: 0.9840 -- iter: 192/268
[A[ATraining Step: 43  | total loss: [1m[32m0.09832[0m[0m | time: 471.103s
[2K
| Adam | epoch: 005 | loss: 0.09832 - acc: 0.9813 -- iter: 224/268
[A[ATraining Step: 44  | total loss: [1m[32m0.09682[0m[0m | time: 486.877s
[2K
| Adam | epoch: 005 | loss: 0.09682 - acc: 0.9846 -- iter: 256/268
[A[ATraining Step: 45  | total loss: [1m[32m0.13745[0m[0m | time: 525.569s
[2K
| Adam | epoch: 005 | loss: 0.13745 - acc: 0.9660 | val_loss: 0.48062 - val_acc: 0.7976 -- iter: 268/268
--
Training Step: 46  | total loss: [1m[32m0.12153[0m[0m | time: 129.269s
[2K
| Adam | epoch: 006 | loss: 0.12153 - acc: 0.9716 -- iter: 032/268
[A[ATraining Step: 47  | total loss: [1m[32m0.10412[0m[0m | time: 158.129s
[2K
| Adam | epoch: 006 | loss: 0.10412 - acc: 0.9763 -- iter: 064/268
[A[ATraining Step: 48  | total loss: [1m[32m0.11267[0m[0m | time: 173.586s
[2K
| Adam | epoch: 006 | loss: 0.11267 - acc: 0.9751 -- iter: 096/268
[A[ATraining Step: 49  | total loss: [1m[32m0.09804[0m[0m | time: 179.737s
[2K
| Adam | epoch: 006 | loss: 0.09804 - acc: 0.9790 -- iter: 128/268
[A[ATraining Step: 50  | total loss: [1m[32m0.08440[0m[0m | time: 185.080s
[2K
| Adam | epoch: 006 | loss: 0.08440 - acc: 0.9823 -- iter: 160/268
[A[ATraining Step: 51  | total loss: [1m[32m0.07340[0m[0m | time: 200.584s
[2K
| Adam | epoch: 006 | loss: 0.07340 - acc: 0.9850 -- iter: 192/268
[A[ATraining Step: 52  | total loss: [1m[32m0.08193[0m[0m | time: 222.628s
[2K
| Adam | epoch: 006 | loss: 0.08193 - acc: 0.9825 -- iter: 224/268
[A[ATraining Step: 53  | total loss: [1m[32m0.08385[0m[0m | time: 253.580s
[2K
| Adam | epoch: 006 | loss: 0.08385 - acc: 0.9805 -- iter: 256/268
[A[ATraining Step: 54  | total loss: [1m[32m0.08459[0m[0m | time: 356.027s
[2K
| Adam | epoch: 006 | loss: 0.08459 - acc: 0.9788 | val_loss: 1.57435 - val_acc: 0.6071 -- iter: 268/268
--
Training Step: 55  | total loss: [1m[32m0.07765[0m[0m | time: 13.132s
[2K
| Adam | epoch: 007 | loss: 0.07765 - acc: 0.9774 -- iter: 032/268
[A[ATraining Step: 56  | total loss: [1m[32m0.07510[0m[0m | time: 27.362s
[2K
| Adam | epoch: 007 | loss: 0.07510 - acc: 0.9761 -- iter: 064/268
[A[ATraining Step: 57  | total loss: [1m[32m0.06601[0m[0m | time: 45.931s
[2K
| Adam | epoch: 007 | loss: 0.06601 - acc: 0.9795 -- iter: 096/268
[A[ATraining Step: 58  | total loss: [1m[32m0.05797[0m[0m | time: 66.021s
[2K
| Adam | epoch: 007 | loss: 0.05797 - acc: 0.9823 -- iter: 128/268
[A[ATraining Step: 59  | total loss: [1m[32m0.05099[0m[0m | time: 74.514s
[2K
| Adam | epoch: 007 | loss: 0.05099 - acc: 0.9846 -- iter: 160/268
[A[ATraining Step: 60  | total loss: [1m[32m0.04470[0m[0m | time: 81.972s
[2K
| Adam | epoch: 007 | loss: 0.04470 - acc: 0.9867 -- iter: 192/268
[A[ATraining Step: 61  | total loss: [1m[32m0.03917[0m[0m | time: 99.973s
[2K
| Adam | epoch: 007 | loss: 0.03917 - acc: 0.9884 -- iter: 224/268
[A[ATraining Step: 62  | total loss: [1m[32m0.05934[0m[0m | time: 117.327s
[2K
| Adam | epoch: 007 | loss: 0.05934 - acc: 0.9738 -- iter: 256/268
[A[ATraining Step: 63  | total loss: [1m[32m0.05319[0m[0m | time: 136.493s
[2K
| Adam | epoch: 007 | loss: 0.05319 - acc: 0.9771 | val_loss: 0.24688 - val_acc: 0.8929 -- iter: 268/268
--
Training Step: 64  | total loss: [1m[32m0.06263[0m[0m | time: 14.910s
[2K
| Adam | epoch: 008 | loss: 0.06263 - acc: 0.9761 -- iter: 032/268
[A[ATraining Step: 65  | total loss: [1m[32m0.05638[0m[0m | time: 27.898s
[2K
| Adam | epoch: 008 | loss: 0.05638 - acc: 0.9790 -- iter: 064/268
[A[ATraining Step: 66  | total loss: [1m[32m0.05535[0m[0m | time: 38.261s
[2K
| Adam | epoch: 008 | loss: 0.05535 - acc: 0.9816 -- iter: 096/268
[A[ATraining Step: 67  | total loss: [1m[32m0.04932[0m[0m | time: 49.739s
[2K
| Adam | epoch: 008 | loss: 0.04932 - acc: 0.9838 -- iter: 128/268
[A[ATraining Step: 68  | total loss: [1m[32m0.04388[0m[0m | time: 64.586s
[2K
| Adam | epoch: 008 | loss: 0.04388 - acc: 0.9857 -- iter: 160/268
[A[ATraining Step: 69  | total loss: [1m[32m0.03914[0m[0m | time: 71.466s
[2K
| Adam | epoch: 008 | loss: 0.03914 - acc: 0.9874 -- iter: 192/268
[A[ATraining Step: 70  | total loss: [1m[32m0.03826[0m[0m | time: 78.877s
[2K
| Adam | epoch: 008 | loss: 0.03826 - acc: 0.9888 -- iter: 224/268
[A[ATraining Step: 71  | total loss: [1m[32m0.03483[0m[0m | time: 93.676s
[2K
| Adam | epoch: 008 | loss: 0.03483 - acc: 0.9901 -- iter: 256/268
[A[ATraining Step: 72  | total loss: [1m[32m0.03911[0m[0m | time: 116.969s
[2K
| Adam | epoch: 008 | loss: 0.03911 - acc: 0.9842 | val_loss: 9.19158 - val_acc: 0.4048 -- iter: 268/268
--
Training Step: 73  | total loss: [1m[32m0.03641[0m[0m | time: 10.615s
[2K
| Adam | epoch: 009 | loss: 0.03641 - acc: 0.9860 -- iter: 032/268
[A[ATraining Step: 74  | total loss: [1m[32m0.05444[0m[0m | time: 20.779s
[2K
| Adam | epoch: 009 | loss: 0.05444 - acc: 0.9841 -- iter: 064/268
[A[ATraining Step: 75  | total loss: [1m[32m0.05653[0m[0m | time: 34.612s
[2K
| Adam | epoch: 009 | loss: 0.05653 - acc: 0.9824 -- iter: 096/268
[A[ATraining Step: 76  | total loss: [1m[32m0.06286[0m[0m | time: 45.741s
[2K
| Adam | epoch: 009 | loss: 0.06286 - acc: 0.9742 -- iter: 128/268
[A[ATraining Step: 77  | total loss: [1m[32m0.05821[0m[0m | time: 56.118s
[2K
| Adam | epoch: 009 | loss: 0.05821 - acc: 0.9770 -- iter: 160/268
[A[ATraining Step: 78  | total loss: [1m[32m0.07101[0m[0m | time: 66.369s
[2K
| Adam | epoch: 009 | loss: 0.07101 - acc: 0.9761 -- iter: 192/268
[A[ATraining Step: 79  | total loss: [1m[32m0.06455[0m[0m | time: 70.893s
[2K
| Adam | epoch: 009 | loss: 0.06455 - acc: 0.9786 -- iter: 224/268
[A[ATraining Step: 80  | total loss: [1m[32m0.06131[0m[0m | time: 75.590s
[2K
| Adam | epoch: 009 | loss: 0.06131 - acc: 0.9808 -- iter: 256/268
[A[ATraining Step: 81  | total loss: [1m[32m0.05561[0m[0m | time: 90.600s
[2K
| Adam | epoch: 009 | loss: 0.05561 - acc: 0.9827 | val_loss: 5.22271 - val_acc: 0.6071 -- iter: 268/268
--
Training Step: 82  | total loss: [1m[32m0.05081[0m[0m | time: 11.240s
[2K
| Adam | epoch: 010 | loss: 0.05081 - acc: 0.9844 -- iter: 032/268
[A[ATraining Step: 83  | total loss: [1m[32m0.05941[0m[0m | time: 21.924s
[2K
| Adam | epoch: 010 | loss: 0.05941 - acc: 0.9829 -- iter: 064/268
[A[ATraining Step: 84  | total loss: [1m[32m0.05586[0m[0m | time: 31.695s
[2K
| Adam | epoch: 010 | loss: 0.05586 - acc: 0.9846 -- iter: 096/268
[A[ATraining Step: 85  | total loss: [1m[32m0.05598[0m[0m | time: 42.135s
[2K
| Adam | epoch: 010 | loss: 0.05598 - acc: 0.9830 -- iter: 128/268
[A[ATraining Step: 86  | total loss: [1m[32m0.05121[0m[0m | time: 52.275s
[2K
| Adam | epoch: 010 | loss: 0.05121 - acc: 0.9847 -- iter: 160/268
[A[ATraining Step: 87  | total loss: [1m[32m0.04698[0m[0m | time: 62.347s
[2K
| Adam | epoch: 010 | loss: 0.04698 - acc: 0.9862 -- iter: 192/268
[A[ATraining Step: 88  | total loss: [1m[32m0.06588[0m[0m | time: 72.112s
[2K
| Adam | epoch: 010 | loss: 0.06588 - acc: 0.9845 -- iter: 224/268
[A[ATraining Step: 89  | total loss: [1m[32m0.06047[0m[0m | time: 76.632s
[2K
| Adam | epoch: 010 | loss: 0.06047 - acc: 0.9860 -- iter: 256/268
[A[ATraining Step: 90  | total loss: [1m[32m0.06134[0m[0m | time: 86.072s
[2K
| Adam | epoch: 010 | loss: 0.06134 - acc: 0.9874 | val_loss: 0.10888 - val_acc: 0.9524 -- iter: 268/268
--
Training Step: 91  | total loss: [1m[32m0.05752[0m[0m | time: 10.092s
[2K
| Adam | epoch: 011 | loss: 0.05752 - acc: 0.9887 -- iter: 032/268
[A[ATraining Step: 92  | total loss: [1m[32m0.05843[0m[0m | time: 20.780s
[2K
| Adam | epoch: 011 | loss: 0.05843 - acc: 0.9867 -- iter: 064/268
[A[ATraining Step: 93  | total loss: [1m[32m0.05377[0m[0m | time: 30.955s
[2K
| Adam | epoch: 011 | loss: 0.05377 - acc: 0.9880 -- iter: 096/268
[A[ATraining Step: 94  | total loss: [1m[32m0.05049[0m[0m | time: 41.609s
[2K
| Adam | epoch: 011 | loss: 0.05049 - acc: 0.9892 -- iter: 128/268
[A[ATraining Step: 95  | total loss: [1m[32m0.04578[0m[0m | time: 51.625s
[2K
| Adam | epoch: 011 | loss: 0.04578 - acc: 0.9903 -- iter: 160/268
[A[ATraining Step: 96  | total loss: [1m[32m0.04248[0m[0m | time: 61.857s
[2K
| Adam | epoch: 011 | loss: 0.04248 - acc: 0.9913 -- iter: 192/268
[A[ATraining Step: 97  | total loss: [1m[32m0.03847[0m[0m | time: 72.081s
[2K
| Adam | epoch: 011 | loss: 0.03847 - acc: 0.9921 -- iter: 224/268
[A[ATraining Step: 98  | total loss: [1m[32m0.03492[0m[0m | time: 82.149s
[2K
| Adam | epoch: 011 | loss: 0.03492 - acc: 0.9929 -- iter: 256/268
[A[ATraining Step: 99  | total loss: [1m[32m0.03727[0m[0m | time: 91.484s
[2K
| Adam | epoch: 011 | loss: 0.03727 - acc: 0.9905 | val_loss: 1.01759 - val_acc: 0.7381 -- iter: 268/268
--
Training Step: 100  | total loss: [1m[32m0.03466[0m[0m | time: 4.371s
[2K
| Adam | epoch: 012 | loss: 0.03466 - acc: 0.9915 -- iter: 032/268
[A[ATraining Step: 101  | total loss: [1m[32m0.03171[0m[0m | time: 14.184s
[2K
| Adam | epoch: 012 | loss: 0.03171 - acc: 0.9923 -- iter: 064/268
[A[ATraining Step: 102  | total loss: [1m[32m0.02965[0m[0m | time: 23.968s
[2K
| Adam | epoch: 012 | loss: 0.02965 - acc: 0.9931 -- iter: 096/268
[A[ATraining Step: 103  | total loss: [1m[32m0.02697[0m[0m | time: 34.242s
[2K
| Adam | epoch: 012 | loss: 0.02697 - acc: 0.9938 -- iter: 128/268
[A[ATraining Step: 104  | total loss: [1m[32m0.02477[0m[0m | time: 44.207s
[2K
| Adam | epoch: 012 | loss: 0.02477 - acc: 0.9944 -- iter: 160/268
[A[ATraining Step: 105  | total loss: [1m[32m0.03134[0m[0m | time: 54.145s
[2K
| Adam | epoch: 012 | loss: 0.03134 - acc: 0.9918 -- iter: 192/268
[A[ATraining Step: 106  | total loss: [1m[32m0.02837[0m[0m | time: 63.962s
[2K
| Adam | epoch: 012 | loss: 0.02837 - acc: 0.9926 -- iter: 224/268
[A[ATraining Step: 107  | total loss: [1m[32m0.02595[0m[0m | time: 74.923s
[2K
| Adam | epoch: 012 | loss: 0.02595 - acc: 0.9934 -- iter: 256/268
[A[ATraining Step: 108  | total loss: [1m[32m0.07596[0m[0m | time: 89.999s
[2K
| Adam | epoch: 012 | loss: 0.07596 - acc: 0.9878 | val_loss: 0.32636 - val_acc: 0.9286 -- iter: 268/268
--
Training Step: 109  | total loss: [1m[32m0.06902[0m[0m | time: 4.321s
[2K
| Adam | epoch: 013 | loss: 0.06902 - acc: 0.9890 -- iter: 032/268
[A[ATraining Step: 110  | total loss: [1m[32m0.06332[0m[0m | time: 8.909s
[2K
| Adam | epoch: 013 | loss: 0.06332 - acc: 0.9901 -- iter: 064/268
[A[ATraining Step: 111  | total loss: [1m[32m0.05799[0m[0m | time: 18.901s
[2K
| Adam | epoch: 013 | loss: 0.05799 - acc: 0.9911 -- iter: 096/268
[A[ATraining Step: 112  | total loss: [1m[32m0.05271[0m[0m | time: 29.329s
[2K
| Adam | epoch: 013 | loss: 0.05271 - acc: 0.9920 -- iter: 128/268
[A[ATraining Step: 113  | total loss: [1m[32m0.04953[0m[0m | time: 39.658s
[2K
| Adam | epoch: 013 | loss: 0.04953 - acc: 0.9928 -- iter: 160/268
[A[ATraining Step: 114  | total loss: [1m[32m0.04510[0m[0m | time: 53.261s
[2K
| Adam | epoch: 013 | loss: 0.04510 - acc: 0.9935 -- iter: 192/268
[A[ATraining Step: 115  | total loss: [1m[32m0.04863[0m[0m | time: 61.713s
[2K
| Adam | epoch: 013 | loss: 0.04863 - acc: 0.9910 -- iter: 224/268
[A[ATraining Step: 116  | total loss: [1m[32m0.04572[0m[0m | time: 69.866s
[2K
| Adam | epoch: 013 | loss: 0.04572 - acc: 0.9919 -- iter: 256/268
[A[ATraining Step: 117  | total loss: [1m[32m0.04160[0m[0m | time: 82.093s
[2K
| Adam | epoch: 013 | loss: 0.04160 - acc: 0.9927 | val_loss: 0.21411 - val_acc: 0.9048 -- iter: 268/268
--
Training Step: 118  | total loss: [1m[32m0.08458[0m[0m | time: 8.230s
[2K
| Adam | epoch: 014 | loss: 0.08458 - acc: 0.9903 -- iter: 032/268
[A[ATraining Step: 119  | total loss: [1m[32m0.07644[0m[0m | time: 11.893s
[2K
| Adam | epoch: 014 | loss: 0.07644 - acc: 0.9913 -- iter: 064/268
[A[ATraining Step: 120  | total loss: [1m[32m0.07101[0m[0m | time: 15.616s
[2K
| Adam | epoch: 014 | loss: 0.07101 - acc: 0.9922 -- iter: 096/268
[A[ATraining Step: 121  | total loss: [1m[32m0.06523[0m[0m | time: 24.062s
[2K
| Adam | epoch: 014 | loss: 0.06523 - acc: 0.9930 -- iter: 128/268
[A[ATraining Step: 122  | total loss: [1m[32m0.05935[0m[0m | time: 32.235s
[2K
| Adam | epoch: 014 | loss: 0.05935 - acc: 0.9937 -- iter: 160/268
[A[ATraining Step: 123  | total loss: [1m[32m0.05388[0m[0m | time: 40.255s
[2K
| Adam | epoch: 014 | loss: 0.05388 - acc: 0.9943 -- iter: 192/268
[A[ATraining Step: 124  | total loss: [1m[32m0.04872[0m[0m | time: 48.346s
[2K
| Adam | epoch: 014 | loss: 0.04872 - acc: 0.9949 -- iter: 224/268
[A[ATraining Step: 125  | total loss: [1m[32m0.05597[0m[0m | time: 56.490s
[2K
| Adam | epoch: 014 | loss: 0.05597 - acc: 0.9891 -- iter: 256/268
[A[ATraining Step: 126  | total loss: [1m[32m0.05149[0m[0m | time: 68.508s
[2K
| Adam | epoch: 014 | loss: 0.05149 - acc: 0.9902 | val_loss: 0.17162 - val_acc: 0.9405 -- iter: 268/268
--
Training Step: 127  | total loss: [1m[32m0.04735[0m[0m | time: 8.293s
[2K
| Adam | epoch: 015 | loss: 0.04735 - acc: 0.9912 -- iter: 032/268
[A[ATraining Step: 128  | total loss: [1m[32m0.05097[0m[0m | time: 16.434s
[2K
| Adam | epoch: 015 | loss: 0.05097 - acc: 0.9890 -- iter: 064/268
[A[ATraining Step: 129  | total loss: [1m[32m0.07067[0m[0m | time: 20.071s
[2K
| Adam | epoch: 015 | loss: 0.07067 - acc: 0.9807 -- iter: 096/268
[A[ATraining Step: 130  | total loss: [1m[32m0.06400[0m[0m | time: 23.864s
[2K
| Adam | epoch: 015 | loss: 0.06400 - acc: 0.9826 -- iter: 128/268
[A[ATraining Step: 131  | total loss: [1m[32m0.05788[0m[0m | time: 31.864s
[2K
| Adam | epoch: 015 | loss: 0.05788 - acc: 0.9844 -- iter: 160/268
[A[ATraining Step: 132  | total loss: [1m[32m0.05445[0m[0m | time: 40.143s
[2K
| Adam | epoch: 015 | loss: 0.05445 - acc: 0.9859 -- iter: 192/268
[A[ATraining Step: 133  | total loss: [1m[32m0.06908[0m[0m | time: 48.420s
[2K
| Adam | epoch: 015 | loss: 0.06908 - acc: 0.9842 -- iter: 224/268
[A[ATraining Step: 134  | total loss: [1m[32m0.06481[0m[0m | time: 56.505s
[2K
| Adam | epoch: 015 | loss: 0.06481 - acc: 0.9858 -- iter: 256/268
[A[ATraining Step: 135  | total loss: [1m[32m0.06049[0m[0m | time: 68.557s
[2K
| Adam | epoch: 015 | loss: 0.06049 - acc: 0.9872 | val_loss: 1.17246 - val_acc: 0.6310 -- iter: 268/268
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.9976232917409389
Validation AUPRC:0.9967239967239967
Test AUC:1.0
Test AUPRC:1.0
BestTestF1Score	0.95	0.89	0.94	0.9	1.0	43	5	36	0	1.0
BestTestMCCScore	0.95	0.89	0.94	0.9	1.0	43	5	36	0	1.0
BestTestAccuracyScore	0.95	0.89	0.94	0.9	1.0	43	5	36	0	1.0
BestValidationF1Score	0.93	0.89	0.94	0.87	1.0	33	5	46	0	1.0
BestValidationMCC	0.93	0.89	0.94	0.87	1.0	33	5	46	0	1.0
BestValidationAccuracy	0.93	0.89	0.94	0.87	1.0	33	5	46	0	1.0
TestPredictions (Threshold:1.0)
CHEMBL1833982,TN,INACT,0.019999999552965164	CHEMBL3642922,TP,ACT,1.0	CHEMBL2372731,TN,INACT,0.2199999988079071	CHEMBL228612,FP,INACT,1.0	CHEMBL3642923,TP,ACT,1.0	CHEMBL406121,FP,INACT,1.0	CHEMBL348963,TN,INACT,0.9900000095367432	CHEMBL418839,FP,INACT,1.0	CHEMBL263977,TN,INACT,0.1599999964237213	CHEMBL3642938,TP,ACT,1.0	CHEMBL40380,TN,INACT,0.949999988079071	CHEMBL3672888,TP,ACT,1.0	CHEMBL3642873,TP,ACT,1.0	CHEMBL3672890,TP,ACT,1.0	CHEMBL3672851,TP,ACT,1.0	CHEMBL3672859,TP,ACT,1.0	CHEMBL3642862,TP,ACT,1.0	CHEMBL3667911,TP,ACT,1.0	CHEMBL3642902,TP,ACT,1.0	CHEMBL3642933,TP,ACT,1.0	CHEMBL3667896,TP,ACT,1.0	CHEMBL2367632,TN,INACT,0.9700000286102295	CHEMBL3642920,TP,ACT,1.0	CHEMBL105920,TN,INACT,0.9900000095367432	CHEMBL1462676,TN,INACT,0.9900000095367432	CHEMBL3642908,TP,ACT,1.0	CHEMBL254439,TN,INACT,0.49000000953674316	CHEMBL1432215,TN,INACT,0.8500000238418579	CHEMBL3642864,TP,ACT,1.0	CHEMBL213927,TN,INACT,0.9900000095367432	CHEMBL332426,TN,INACT,0.6000000238418579	CHEMBL3642924,TP,ACT,1.0	CHEMBL3642891,TP,ACT,1.0	CHEMBL3642916,TP,ACT,1.0	CHEMBL3642877,TP,ACT,1.0	CHEMBL3667904,TP,ACT,1.0	CHEMBL3143648,TN,INACT,0.03999999910593033	CHEMBL3642939,TP,ACT,1.0	CHEMBL1907779,TN,INACT,0.05999999865889549	CHEMBL3408420,TN,INACT,0.4399999976158142	CHEMBL3672892,TP,ACT,1.0	CHEMBL507020,TN,INACT,0.4300000071525574	CHEMBL231087,TN,INACT,0.9200000166893005	CHEMBL3099878,TN,INACT,0.9599999785423279	CHEMBL379194,TN,INACT,0.5799999833106995	CHEMBL3667900,TP,ACT,1.0	CHEMBL59998,TN,INACT,0.23999999463558197	CHEMBL290025,TN,INACT,0.3199999928474426	CHEMBL3672853,TP,ACT,1.0	CHEMBL3409890,TN,INACT,0.9900000095367432	CHEMBL3667905,TP,ACT,1.0	CHEMBL342036,TN,INACT,0.9900000095367432	CHEMBL3667879,TP,ACT,1.0	CHEMBL3667884,TP,ACT,1.0	CHEMBL3672872,TP,ACT,1.0	CHEMBL3642963,TP,ACT,1.0	CHEMBL3109060,TN,INACT,0.7900000214576721	CHEMBL64708,TN,INACT,0.2800000011920929	CHEMBL1413681,TN,INACT,0.3499999940395355	CHEMBL527118,TN,INACT,0.6299999952316284	CHEMBL3642881,TP,ACT,1.0	CHEMBL463385,TN,INACT,0.7599999904632568	CHEMBL238708,TN,INACT,0.7799999713897705	CHEMBL3642889,TP,ACT,1.0	CHEMBL3642911,TP,ACT,1.0	CHEMBL85164,FP,INACT,1.0	CHEMBL3642951,TP,ACT,1.0	CHEMBL3408419,TN,INACT,0.7200000286102295	CHEMBL3667906,TP,ACT,1.0	CHEMBL3642955,TP,ACT,1.0	CHEMBL3642887,TP,ACT,1.0	CHEMBL2159388,TN,INACT,0.9900000095367432	CHEMBL1437349,FP,INACT,1.0	CHEMBL3642953,TP,ACT,1.0	CHEMBL3125572,TN,INACT,0.8600000143051147	CHEMBL3672862,TP,ACT,1.0	CHEMBL3667891,TP,ACT,1.0	CHEMBL2371642,TN,INACT,0.36000001430511475	CHEMBL3642861,TP,ACT,1.0	CHEMBL3672877,TP,ACT,1.0	CHEMBL3408416,TN,INACT,0.6100000143051147	CHEMBL3672867,TP,ACT,1.0	CHEMBL2159294,TN,INACT,0.9700000286102295	CHEMBL109044,TN,INACT,0.019999999552965164	

