ImageNetInceptionV2 CHEMBL3267 adam 0.001 5 0 0 0.8 False True
Number of active compounds :	333
Number of inactive compounds :	222
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL3267_adam_0.001_5_0_0_0.8_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL3267_adam_0.001_5_0.8/
---------------------------------
Training samples: 355
Validation samples: 111
--
Training Step: 1  | time: 257.217s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/355
[A[ATraining Step: 2  | total loss: [1m[32m0.56993[0m[0m | time: 452.977s
[2K
| Adam | epoch: 001 | loss: 0.56993 - acc: 0.5625 -- iter: 064/355
[A[ATraining Step: 3  | total loss: [1m[32m0.74208[0m[0m | time: 640.986s
[2K
| Adam | epoch: 001 | loss: 0.74208 - acc: 0.5625 -- iter: 096/355
[A[ATraining Step: 4  | total loss: [1m[32m0.90671[0m[0m | time: 738.671s
[2K
| Adam | epoch: 001 | loss: 0.90671 - acc: 0.5391 -- iter: 128/355
[A[ATraining Step: 5  | total loss: [1m[32m0.77654[0m[0m | time: 891.544s
[2K
| Adam | epoch: 001 | loss: 0.77654 - acc: 0.5553 -- iter: 160/355
[A[ATraining Step: 6  | total loss: [1m[32m0.80139[0m[0m | time: 1008.911s
[2K
| Adam | epoch: 001 | loss: 0.80139 - acc: 0.5599 -- iter: 192/355
[A[ATraining Step: 7  | total loss: [1m[32m0.73071[0m[0m | time: 1147.655s
[2K
| Adam | epoch: 001 | loss: 0.73071 - acc: 0.5615 -- iter: 224/355
[A[ATraining Step: 8  | total loss: [1m[32m0.64671[0m[0m | time: 1193.801s
[2K
| Adam | epoch: 001 | loss: 0.64671 - acc: 0.6499 -- iter: 256/355
[A[ATraining Step: 9  | total loss: [1m[32m0.63851[0m[0m | time: 1238.017s
[2K
| Adam | epoch: 001 | loss: 0.63851 - acc: 0.6533 -- iter: 288/355
[A[ATraining Step: 10  | total loss: [1m[32m0.65922[0m[0m | time: 1286.606s
[2K
| Adam | epoch: 001 | loss: 0.65922 - acc: 0.6235 -- iter: 320/355
[A[ATraining Step: 11  | total loss: [1m[32m0.66022[0m[0m | time: 1376.683s
[2K
| Adam | epoch: 001 | loss: 0.66022 - acc: 0.6538 -- iter: 352/355
[A[ATraining Step: 12  | total loss: [1m[32m0.65549[0m[0m | time: 1407.153s
[2K
| Adam | epoch: 001 | loss: 0.65549 - acc: 0.6409 | val_loss: 0.71852 - val_acc: 0.6036 -- iter: 355/355
--
Training Step: 13  | total loss: [1m[32m0.65021[0m[0m | time: 2.749s
[2K
| Adam | epoch: 002 | loss: 0.65021 - acc: 0.6519 -- iter: 032/355
[A[ATraining Step: 14  | total loss: [1m[32m0.45143[0m[0m | time: 63.165s
[2K
| Adam | epoch: 002 | loss: 0.45143 - acc: 0.7943 -- iter: 064/355
[A[ATraining Step: 15  | total loss: [1m[32m0.66810[0m[0m | time: 99.498s
[2K
| Adam | epoch: 002 | loss: 0.66810 - acc: 0.6180 -- iter: 096/355
[A[ATraining Step: 16  | total loss: [1m[32m0.61449[0m[0m | time: 150.994s
[2K
| Adam | epoch: 002 | loss: 0.61449 - acc: 0.6909 -- iter: 128/355
[A[ATraining Step: 17  | total loss: [1m[32m0.65593[0m[0m | time: 173.736s
[2K
| Adam | epoch: 002 | loss: 0.65593 - acc: 0.6560 -- iter: 160/355
[A[ATraining Step: 18  | total loss: [1m[32m0.68507[0m[0m | time: 196.459s
[2K
| Adam | epoch: 002 | loss: 0.68507 - acc: 0.6128 -- iter: 192/355
[A[ATraining Step: 19  | total loss: [1m[32m0.65534[0m[0m | time: 217.530s
[2K
| Adam | epoch: 002 | loss: 0.65534 - acc: 0.6481 -- iter: 224/355
[A[ATraining Step: 20  | total loss: [1m[32m0.64453[0m[0m | time: 226.804s
[2K
| Adam | epoch: 002 | loss: 0.64453 - acc: 0.6507 -- iter: 256/355
[A[ATraining Step: 21  | total loss: [1m[32m0.61414[0m[0m | time: 236.937s
[2K
| Adam | epoch: 002 | loss: 0.61414 - acc: 0.6815 -- iter: 288/355
[A[ATraining Step: 22  | total loss: [1m[32m0.60645[0m[0m | time: 255.712s
[2K
| Adam | epoch: 002 | loss: 0.60645 - acc: 0.6739 -- iter: 320/355
[A[ATraining Step: 23  | total loss: [1m[32m0.57485[0m[0m | time: 274.443s
[2K
| Adam | epoch: 002 | loss: 0.57485 - acc: 0.6779 -- iter: 352/355
[A[ATraining Step: 24  | total loss: [1m[32m0.60706[0m[0m | time: 309.716s
[2K
| Adam | epoch: 002 | loss: 0.60706 - acc: 0.6542 | val_loss: 0.84236 - val_acc: 0.6036 -- iter: 355/355
--
Training Step: 25  | total loss: [1m[32m0.60543[0m[0m | time: 4.051s
[2K
| Adam | epoch: 003 | loss: 0.60543 - acc: 0.6803 -- iter: 032/355
[A[ATraining Step: 26  | total loss: [1m[32m0.56950[0m[0m | time: 7.057s
[2K
| Adam | epoch: 003 | loss: 0.56950 - acc: 0.6767 -- iter: 064/355
[A[ATraining Step: 27  | total loss: [1m[32m0.48625[0m[0m | time: 20.583s
[2K
| Adam | epoch: 003 | loss: 0.48625 - acc: 0.7599 -- iter: 096/355
[A[ATraining Step: 28  | total loss: [1m[32m0.49990[0m[0m | time: 36.007s
[2K
| Adam | epoch: 003 | loss: 0.49990 - acc: 0.7496 -- iter: 128/355
[A[ATraining Step: 29  | total loss: [1m[32m0.50657[0m[0m | time: 55.081s
[2K
| Adam | epoch: 003 | loss: 0.50657 - acc: 0.7421 -- iter: 160/355
[A[ATraining Step: 30  | total loss: [1m[32m0.52449[0m[0m | time: 75.127s
[2K
| Adam | epoch: 003 | loss: 0.52449 - acc: 0.7366 -- iter: 192/355
[A[ATraining Step: 31  | total loss: [1m[32m0.52535[0m[0m | time: 100.426s
[2K
| Adam | epoch: 003 | loss: 0.52535 - acc: 0.7324 -- iter: 224/355
[A[ATraining Step: 32  | total loss: [1m[32m0.53720[0m[0m | time: 120.214s
[2K
| Adam | epoch: 003 | loss: 0.53720 - acc: 0.7223 -- iter: 256/355
[A[ATraining Step: 33  | total loss: [1m[32m0.64246[0m[0m | time: 139.060s
[2K
| Adam | epoch: 003 | loss: 0.64246 - acc: 0.6804 -- iter: 288/355
[A[ATraining Step: 34  | total loss: [1m[32m0.64299[0m[0m | time: 149.189s
[2K
| Adam | epoch: 003 | loss: 0.64299 - acc: 0.7020 -- iter: 320/355
[A[ATraining Step: 35  | total loss: [1m[32m0.60339[0m[0m | time: 158.229s
[2K
| Adam | epoch: 003 | loss: 0.60339 - acc: 0.7055 -- iter: 352/355
[A[ATraining Step: 36  | total loss: [1m[32m0.64256[0m[0m | time: 192.493s
[2K
| Adam | epoch: 003 | loss: 0.64256 - acc: 0.6763 | val_loss: 3.42872 - val_acc: 0.6036 -- iter: 355/355
--
Training Step: 37  | total loss: [1m[32m0.60874[0m[0m | time: 28.919s
[2K
| Adam | epoch: 004 | loss: 0.60874 - acc: 0.6910 -- iter: 032/355
[A[ATraining Step: 38  | total loss: [1m[32m0.57647[0m[0m | time: 33.737s
[2K
| Adam | epoch: 004 | loss: 0.57647 - acc: 0.7270 -- iter: 064/355
[A[ATraining Step: 39  | total loss: [1m[32m0.56987[0m[0m | time: 38.354s
[2K
| Adam | epoch: 004 | loss: 0.56987 - acc: 0.7154 -- iter: 096/355
[A[ATraining Step: 40  | total loss: [1m[32m0.55081[0m[0m | time: 56.762s
[2K
| Adam | epoch: 004 | loss: 0.55081 - acc: 0.7688 -- iter: 128/355
[A[ATraining Step: 41  | total loss: [1m[32m0.55016[0m[0m | time: 75.622s
[2K
| Adam | epoch: 004 | loss: 0.55016 - acc: 0.7539 -- iter: 160/355
[A[ATraining Step: 42  | total loss: [1m[32m0.56279[0m[0m | time: 109.627s
[2K
| Adam | epoch: 004 | loss: 0.56279 - acc: 0.7194 -- iter: 192/355
[A[ATraining Step: 43  | total loss: [1m[32m0.57817[0m[0m | time: 186.591s
[2K
| Adam | epoch: 004 | loss: 0.57817 - acc: 0.7083 -- iter: 224/355
[A[ATraining Step: 44  | total loss: [1m[32m0.56148[0m[0m | time: 227.407s
[2K
| Adam | epoch: 004 | loss: 0.56148 - acc: 0.7047 -- iter: 256/355
[A[ATraining Step: 45  | total loss: [1m[32m0.56120[0m[0m | time: 269.454s
[2K
| Adam | epoch: 004 | loss: 0.56120 - acc: 0.7124 -- iter: 288/355
[A[ATraining Step: 46  | total loss: [1m[32m0.55384[0m[0m | time: 308.543s
[2K
| Adam | epoch: 004 | loss: 0.55384 - acc: 0.7186 -- iter: 320/355
[A[ATraining Step: 47  | total loss: [1m[32m0.54156[0m[0m | time: 323.359s
[2K
| Adam | epoch: 004 | loss: 0.54156 - acc: 0.7289 -- iter: 352/355
[A[ATraining Step: 48  | total loss: [1m[32m0.51086[0m[0m | time: 350.878s
[2K
| Adam | epoch: 004 | loss: 0.51086 - acc: 0.7423 | val_loss: 1.76831 - val_acc: 0.6126 -- iter: 355/355
--
Training Step: 49  | total loss: [1m[32m0.51170[0m[0m | time: 62.186s
[2K
| Adam | epoch: 005 | loss: 0.51170 - acc: 0.7435 -- iter: 032/355
[A[ATraining Step: 50  | total loss: [1m[32m0.49014[0m[0m | time: 85.368s
[2K
| Adam | epoch: 005 | loss: 0.49014 - acc: 0.7445 -- iter: 064/355
[A[ATraining Step: 51  | total loss: [1m[32m0.46061[0m[0m | time: 89.744s
[2K
| Adam | epoch: 005 | loss: 0.46061 - acc: 0.7692 -- iter: 096/355
[A[ATraining Step: 52  | total loss: [1m[32m0.46762[0m[0m | time: 93.830s
[2K
| Adam | epoch: 005 | loss: 0.46762 - acc: 0.7538 -- iter: 128/355
[A[ATraining Step: 53  | total loss: [1m[32m0.41490[0m[0m | time: 112.180s
[2K
| Adam | epoch: 005 | loss: 0.41490 - acc: 0.7901 -- iter: 160/355
[A[ATraining Step: 54  | total loss: [1m[32m0.43785[0m[0m | time: 144.336s
[2K
| Adam | epoch: 005 | loss: 0.43785 - acc: 0.7843 -- iter: 192/355
[A[ATraining Step: 55  | total loss: [1m[32m0.43930[0m[0m | time: 168.566s
[2K
| Adam | epoch: 005 | loss: 0.43930 - acc: 0.7928 -- iter: 224/355
[A[ATraining Step: 56  | total loss: [1m[32m0.42048[0m[0m | time: 192.238s
[2K
| Adam | epoch: 005 | loss: 0.42048 - acc: 0.7956 -- iter: 256/355
[A[ATraining Step: 57  | total loss: [1m[32m0.40929[0m[0m | time: 223.126s
[2K
| Adam | epoch: 005 | loss: 0.40929 - acc: 0.7979 -- iter: 288/355
[A[ATraining Step: 58  | total loss: [1m[32m0.40979[0m[0m | time: 250.494s
[2K
| Adam | epoch: 005 | loss: 0.40979 - acc: 0.7999 -- iter: 320/355
[A[ATraining Step: 59  | total loss: [1m[32m0.40662[0m[0m | time: 265.246s
[2K
| Adam | epoch: 005 | loss: 0.40662 - acc: 0.8016 -- iter: 352/355
[A[ATraining Step: 60  | total loss: [1m[32m0.39156[0m[0m | time: 283.657s
[2K
| Adam | epoch: 005 | loss: 0.39156 - acc: 0.8072 | val_loss: 2.49584 - val_acc: 0.4505 -- iter: 355/355
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.7320217096336499
Validation AUPRC:0.8316058106868467
Test AUC:0.6614906832298137
Test AUPRC:0.767201594998146
BestTestF1Score	0.53	0.14	0.53	0.71	0.42	29	12	30	40	0.01
BestTestMCCScore	0.36	0.17	0.49	0.8	0.23	16	4	38	53	0.09
BestTestAccuracyScore	0.53	0.14	0.53	0.71	0.42	29	12	30	40	0.01
BestValidationF1Score	0.62	0.34	0.63	0.82	0.49	33	7	37	34	0.01
BestValidationMCC	0.46	0.38	0.58	1.0	0.3	20	0	44	47	0.09
BestValidationAccuracy	0.62	0.34	0.63	0.82	0.49	33	7	37	34	0.01
TestPredictions (Threshold:0.09)
CHEMBL3600769,FN,ACT,0.009999999776482582	CHEMBL2058569,FN,ACT,0.009999999776482582	CHEMBL574207,FN,ACT,0.009999999776482582	CHEMBL1097942,TP,ACT,0.1899999976158142	CHEMBL476953,TP,ACT,0.18000000715255737	CHEMBL1241242,FN,ACT,0.0	CHEMBL422,TN,INACT,0.009999999776482582	CHEMBL3769747,TP,ACT,0.09000000357627869	CHEMBL2322333,FN,ACT,0.009999999776482582	CHEMBL1088834,FN,ACT,0.009999999776482582	CHEMBL478980,TN,INACT,0.0	CHEMBL3318715,FN,ACT,0.05999999865889549	CHEMBL1240545,FN,ACT,0.0	CHEMBL2089109,FN,ACT,0.0	CHEMBL244770,FP,INACT,0.3400000035762787	CHEMBL1242665,TN,INACT,0.0	CHEMBL1631892,TN,INACT,0.0	CHEMBL378107,FN,ACT,0.0	CHEMBL589412,FN,ACT,0.05000000074505806	CHEMBL2165051,FN,ACT,0.0	CHEMBL1957870,FN,ACT,0.0	CHEMBL1644558,FN,ACT,0.009999999776482582	CHEMBL2171932,FP,INACT,0.09000000357627869	CHEMBL1091978,TP,ACT,0.3499999940395355	CHEMBL2030446,TN,INACT,0.0	CHEMBL1645109,TN,INACT,0.0	CHEMBL2170083,TP,ACT,0.20000000298023224	CHEMBL3652164,FN,ACT,0.03999999910593033	CHEMBL2158444,TN,INACT,0.0	CHEMBL1241948,FN,ACT,0.009999999776482582	CHEMBL3589373,TN,INACT,0.0	CHEMBL2314282,TN,INACT,0.019999999552965164	CHEMBL2165270,FN,ACT,0.0	CHEMBL1889525,TN,INACT,0.0	CHEMBL591101,FN,ACT,0.0	CHEMBL2087492,TP,ACT,0.2199999988079071	CHEMBL1287853,FN,ACT,0.0	CHEMBL1242659,TP,ACT,0.09000000357627869	CHEMBL1916042,TN,INACT,0.0	CHEMBL1645107,FN,ACT,0.0	CHEMBL3360219,TN,INACT,0.029999999329447746	CHEMBL3652157,TP,ACT,0.5400000214576721	CHEMBL2165046,FN,ACT,0.07000000029802322	CHEMBL2165026,FN,ACT,0.0	CHEMBL244552,TN,INACT,0.0	CHEMBL2326953,FN,ACT,0.05000000074505806	CHEMBL2059913,TP,ACT,0.9599999785423279	CHEMBL1645097,TN,INACT,0.0	CHEMBL2064504,FN,ACT,0.009999999776482582	CHEMBL1808978,TP,ACT,0.09000000357627869	CHEMBL3589323,TN,INACT,0.019999999552965164	CHEMBL496331,FN,ACT,0.0	CHEMBL3134602,FN,ACT,0.0	CHEMBL1090478,TP,ACT,0.6000000238418579	CHEMBL3765523,TN,INACT,0.009999999776482582	CHEMBL2165179,TN,INACT,0.03999999910593033	CHEMBL1090153,FN,ACT,0.05000000074505806	CHEMBL605121,FN,ACT,0.009999999776482582	CHEMBL2058567,FN,ACT,0.0	CHEMBL3785694,TN,INACT,0.0	CHEMBL3360231,TN,INACT,0.029999999329447746	CHEMBL3112856,TP,ACT,0.25	CHEMBL3797839,TN,INACT,0.0	CHEMBL2204785,FN,ACT,0.009999999776482582	CHEMBL3600782,TN,INACT,0.029999999329447746	CHEMBL2165193,TN,INACT,0.0	CHEMBL72683,FP,INACT,0.11999999731779099	CHEMBL3318711,FN,ACT,0.009999999776482582	CHEMBL1241390,FN,ACT,0.0	CHEMBL3586667,FN,ACT,0.009999999776482582	CHEMBL3218572,TN,INACT,0.0	CHEMBL3112851,FN,ACT,0.029999999329447746	CHEMBL2064508,FN,ACT,0.009999999776482582	CHEMBL1242750,FN,ACT,0.0	CHEMBL3325458,FN,ACT,0.019999999552965164	CHEMBL190,TN,INACT,0.0	CHEMBL3403674,FN,ACT,0.0	CHEMBL3652212,TP,ACT,0.949999988079071	CHEMBL1242379,FN,ACT,0.0	CHEMBL1241481,FN,ACT,0.0	CHEMBL2158858,FN,ACT,0.03999999910593033	CHEMBL449172,TN,INACT,0.009999999776482582	CHEMBL450243,TN,INACT,0.0	CHEMBL2152138,TP,ACT,0.18000000715255737	CHEMBL3401990,FN,ACT,0.0	CHEMBL3360227,FP,INACT,0.18000000715255737	CHEMBL392349,FN,ACT,0.0	CHEMBL2030438,TN,INACT,0.019999999552965164	CHEMBL3600686,TN,INACT,0.009999999776482582	CHEMBL3765681,TN,INACT,0.0	CHEMBL1762244,TN,INACT,0.009999999776482582	CHEMBL1242663,TN,INACT,0.0	CHEMBL3600688,FN,ACT,0.009999999776482582	CHEMBL2089099,FN,ACT,0.019999999552965164	CHEMBL460137,TN,INACT,0.009999999776482582	CHEMBL2087493,TP,ACT,0.7400000095367432	CHEMBL1241863,TN,INACT,0.009999999776482582	CHEMBL2064572,TN,INACT,0.0	CHEMBL566895,TN,INACT,0.009999999776482582	CHEMBL1630545,FN,ACT,0.0	CHEMBL211819,FN,ACT,0.009999999776482582	CHEMBL3652142,FN,ACT,0.029999999329447746	CHEMBL3814969,FN,ACT,0.009999999776482582	CHEMBL2165004,FN,ACT,0.0	CHEMBL1093446,FN,ACT,0.009999999776482582	CHEMBL3652155,FN,ACT,0.029999999329447746	CHEMBL2158432,TN,INACT,0.0	CHEMBL591759,TP,ACT,0.15000000596046448	CHEMBL1450008,TN,INACT,0.0	CHEMBL477787,TN,INACT,0.029999999329447746	CHEMBL3600684,FN,ACT,0.029999999329447746	

