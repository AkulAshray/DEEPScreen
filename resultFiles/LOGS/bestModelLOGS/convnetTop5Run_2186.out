CNNModel CHEMBL1795186 adam 0.001 15 128 0 0.6 False True
Number of active compounds :	288
Number of inactive compounds :	272
---------------------------------
Run id: CNNModel_CHEMBL1795186_adam_0.001_15_128_0_0.6_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL1795186_adam_0.001_15_128_0.6_True/
---------------------------------
Training samples: 339
Validation samples: 107
--
Training Step: 1  | time: 1.174s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/339
[A[ATraining Step: 2  | total loss: [1m[32m0.62404[0m[0m | time: 2.166s
[2K
| Adam | epoch: 001 | loss: 0.62404 - acc: 0.3375 -- iter: 064/339
[A[ATraining Step: 3  | total loss: [1m[32m0.68176[0m[0m | time: 3.247s
[2K
| Adam | epoch: 001 | loss: 0.68176 - acc: 0.3938 -- iter: 096/339
[A[ATraining Step: 4  | total loss: [1m[32m0.68982[0m[0m | time: 4.043s
[2K
| Adam | epoch: 001 | loss: 0.68982 - acc: 0.4969 -- iter: 128/339
[A[ATraining Step: 5  | total loss: [1m[32m0.68855[0m[0m | time: 4.821s
[2K
| Adam | epoch: 001 | loss: 0.68855 - acc: 0.5423 -- iter: 160/339
[A[ATraining Step: 6  | total loss: [1m[32m0.70251[0m[0m | time: 5.654s
[2K
| Adam | epoch: 001 | loss: 0.70251 - acc: 0.4749 -- iter: 192/339
[A[ATraining Step: 7  | total loss: [1m[32m0.70396[0m[0m | time: 6.492s
[2K
| Adam | epoch: 001 | loss: 0.70396 - acc: 0.4337 -- iter: 224/339
[A[ATraining Step: 8  | total loss: [1m[32m0.69606[0m[0m | time: 7.327s
[2K
| Adam | epoch: 001 | loss: 0.69606 - acc: 0.5062 -- iter: 256/339
[A[ATraining Step: 9  | total loss: [1m[32m0.69447[0m[0m | time: 8.185s
[2K
| Adam | epoch: 001 | loss: 0.69447 - acc: 0.5029 -- iter: 288/339
[A[ATraining Step: 10  | total loss: [1m[32m0.69390[0m[0m | time: 9.027s
[2K
| Adam | epoch: 001 | loss: 0.69390 - acc: 0.4702 -- iter: 320/339
[A[ATraining Step: 11  | total loss: [1m[32m0.69335[0m[0m | time: 10.638s
[2K
| Adam | epoch: 001 | loss: 0.69335 - acc: 0.5583 | val_loss: 0.69362 - val_acc: 0.4393 -- iter: 339/339
--
Training Step: 12  | total loss: [1m[32m0.69395[0m[0m | time: 0.430s
[2K
| Adam | epoch: 002 | loss: 0.69395 - acc: 0.4492 -- iter: 032/339
[A[ATraining Step: 13  | total loss: [1m[32m0.69415[0m[0m | time: 1.228s
[2K
| Adam | epoch: 002 | loss: 0.69415 - acc: 0.3920 -- iter: 064/339
[A[ATraining Step: 14  | total loss: [1m[32m0.69375[0m[0m | time: 2.068s
[2K
| Adam | epoch: 002 | loss: 0.69375 - acc: 0.4362 -- iter: 096/339
[A[ATraining Step: 15  | total loss: [1m[32m0.69349[0m[0m | time: 2.929s
[2K
| Adam | epoch: 002 | loss: 0.69349 - acc: 0.4734 -- iter: 128/339
[A[ATraining Step: 16  | total loss: [1m[32m0.69342[0m[0m | time: 3.741s
[2K
| Adam | epoch: 002 | loss: 0.69342 - acc: 0.4716 -- iter: 160/339
[A[ATraining Step: 17  | total loss: [1m[32m0.69328[0m[0m | time: 4.564s
[2K
| Adam | epoch: 002 | loss: 0.69328 - acc: 0.4931 -- iter: 192/339
[A[ATraining Step: 18  | total loss: [1m[32m0.69306[0m[0m | time: 5.424s
[2K
| Adam | epoch: 002 | loss: 0.69306 - acc: 0.5496 -- iter: 224/339
[A[ATraining Step: 19  | total loss: [1m[32m0.69335[0m[0m | time: 6.310s
[2K
| Adam | epoch: 002 | loss: 0.69335 - acc: 0.5018 -- iter: 256/339
[A[ATraining Step: 20  | total loss: [1m[32m0.69361[0m[0m | time: 7.168s
[2K
| Adam | epoch: 002 | loss: 0.69361 - acc: 0.4711 -- iter: 288/339
[A[ATraining Step: 21  | total loss: [1m[32m0.69365[0m[0m | time: 7.974s
[2K
| Adam | epoch: 002 | loss: 0.69365 - acc: 0.4607 -- iter: 320/339
[A[ATraining Step: 22  | total loss: [1m[32m0.69327[0m[0m | time: 9.841s
[2K
| Adam | epoch: 002 | loss: 0.69327 - acc: 0.5006 | val_loss: 0.69249 - val_acc: 0.5607 -- iter: 339/339
--
Training Step: 23  | total loss: [1m[32m0.69308[0m[0m | time: 0.546s
[2K
| Adam | epoch: 003 | loss: 0.69308 - acc: 0.5186 -- iter: 032/339
[A[ATraining Step: 24  | total loss: [1m[32m0.69329[0m[0m | time: 1.085s
[2K
| Adam | epoch: 003 | loss: 0.69329 - acc: 0.4911 -- iter: 064/339
[A[ATraining Step: 25  | total loss: [1m[32m0.69354[0m[0m | time: 2.036s
[2K
| Adam | epoch: 003 | loss: 0.69354 - acc: 0.4720 -- iter: 096/339
[A[ATraining Step: 26  | total loss: [1m[32m0.69350[0m[0m | time: 2.916s
[2K
| Adam | epoch: 003 | loss: 0.69350 - acc: 0.4712 -- iter: 128/339
[A[ATraining Step: 27  | total loss: [1m[32m0.69362[0m[0m | time: 3.795s
[2K
| Adam | epoch: 003 | loss: 0.69362 - acc: 0.4545 -- iter: 160/339
[A[ATraining Step: 28  | total loss: [1m[32m0.69354[0m[0m | time: 4.647s
[2K
| Adam | epoch: 003 | loss: 0.69354 - acc: 0.4580 -- iter: 192/339
[A[ATraining Step: 29  | total loss: [1m[32m0.69343[0m[0m | time: 5.531s
[2K
| Adam | epoch: 003 | loss: 0.69343 - acc: 0.4682 -- iter: 224/339
[A[ATraining Step: 30  | total loss: [1m[32m0.69329[0m[0m | time: 6.503s
[2K
| Adam | epoch: 003 | loss: 0.69329 - acc: 0.5202 -- iter: 256/339
[A[ATraining Step: 31  | total loss: [1m[32m0.69325[0m[0m | time: 7.410s
[2K
| Adam | epoch: 003 | loss: 0.69325 - acc: 0.5083 -- iter: 288/339
[A[ATraining Step: 32  | total loss: [1m[32m0.69326[0m[0m | time: 8.116s
[2K
| Adam | epoch: 003 | loss: 0.69326 - acc: 0.4853 -- iter: 320/339
[A[ATraining Step: 33  | total loss: [1m[32m0.69324[0m[0m | time: 9.942s
[2K
| Adam | epoch: 003 | loss: 0.69324 - acc: 0.4886 | val_loss: 0.69327 - val_acc: 0.4393 -- iter: 339/339
--
Training Step: 34  | total loss: [1m[32m0.69328[0m[0m | time: 0.841s
[2K
| Adam | epoch: 004 | loss: 0.69328 - acc: 0.4575 -- iter: 032/339
[A[ATraining Step: 35  | total loss: [1m[32m0.69327[0m[0m | time: 1.358s
[2K
| Adam | epoch: 004 | loss: 0.69327 - acc: 0.4533 -- iter: 064/339
[A[ATraining Step: 36  | total loss: [1m[32m0.69319[0m[0m | time: 1.888s
[2K
| Adam | epoch: 004 | loss: 0.69319 - acc: 0.4790 -- iter: 096/339
[A[ATraining Step: 37  | total loss: [1m[32m0.69318[0m[0m | time: 2.741s
[2K
| Adam | epoch: 004 | loss: 0.69318 - acc: 0.4990 -- iter: 128/339
[A[ATraining Step: 38  | total loss: [1m[32m0.69315[0m[0m | time: 3.520s
[2K
| Adam | epoch: 004 | loss: 0.69315 - acc: 0.5053 -- iter: 160/339
[A[ATraining Step: 39  | total loss: [1m[32m0.69299[0m[0m | time: 4.579s
[2K
| Adam | epoch: 004 | loss: 0.69299 - acc: 0.5223 -- iter: 192/339
[A[ATraining Step: 40  | total loss: [1m[32m0.69279[0m[0m | time: 5.576s
[2K
| Adam | epoch: 004 | loss: 0.69279 - acc: 0.5474 -- iter: 224/339
[A[ATraining Step: 41  | total loss: [1m[32m0.69279[0m[0m | time: 6.368s
[2K
| Adam | epoch: 004 | loss: 0.69279 - acc: 0.5444 -- iter: 256/339
[A[ATraining Step: 42  | total loss: [1m[32m0.69283[0m[0m | time: 7.099s
[2K
| Adam | epoch: 004 | loss: 0.69283 - acc: 0.5364 -- iter: 288/339
[A[ATraining Step: 43  | total loss: [1m[32m0.69234[0m[0m | time: 7.995s
[2K
| Adam | epoch: 004 | loss: 0.69234 - acc: 0.5576 -- iter: 320/339
[A[ATraining Step: 44  | total loss: [1m[32m0.69223[0m[0m | time: 9.867s
[2K
| Adam | epoch: 004 | loss: 0.69223 - acc: 0.5584 | val_loss: 0.69511 - val_acc: 0.4393 -- iter: 339/339
--
Training Step: 45  | total loss: [1m[32m0.69239[0m[0m | time: 0.893s
[2K
| Adam | epoch: 005 | loss: 0.69239 - acc: 0.5485 -- iter: 032/339
[A[ATraining Step: 46  | total loss: [1m[32m0.69352[0m[0m | time: 1.706s
[2K
| Adam | epoch: 005 | loss: 0.69352 - acc: 0.5092 -- iter: 064/339
[A[ATraining Step: 47  | total loss: [1m[32m0.69376[0m[0m | time: 2.226s
[2K
| Adam | epoch: 005 | loss: 0.69376 - acc: 0.4974 -- iter: 096/339
[A[ATraining Step: 48  | total loss: [1m[32m0.69480[0m[0m | time: 2.857s
[2K
| Adam | epoch: 005 | loss: 0.69480 - acc: 0.4598 -- iter: 128/339
[A[ATraining Step: 49  | total loss: [1m[32m0.69538[0m[0m | time: 3.858s
[2K
| Adam | epoch: 005 | loss: 0.69538 - acc: 0.4287 -- iter: 160/339
[A[ATraining Step: 50  | total loss: [1m[32m0.69505[0m[0m | time: 4.698s
[2K
| Adam | epoch: 005 | loss: 0.69505 - acc: 0.4349 -- iter: 192/339
[A[ATraining Step: 51  | total loss: [1m[32m0.69467[0m[0m | time: 5.427s
[2K
| Adam | epoch: 005 | loss: 0.69467 - acc: 0.4639 -- iter: 224/339
[A[ATraining Step: 52  | total loss: [1m[32m0.69438[0m[0m | time: 6.304s
[2K
| Adam | epoch: 005 | loss: 0.69438 - acc: 0.4834 -- iter: 256/339
[A[ATraining Step: 53  | total loss: [1m[32m0.69411[0m[0m | time: 7.137s
[2K
| Adam | epoch: 005 | loss: 0.69411 - acc: 0.4905 -- iter: 288/339
[A[ATraining Step: 54  | total loss: [1m[32m0.69344[0m[0m | time: 8.013s
[2K
| Adam | epoch: 005 | loss: 0.69344 - acc: 0.5009 -- iter: 320/339
[A[ATraining Step: 55  | total loss: [1m[32m0.69342[0m[0m | time: 9.896s
[2K
| Adam | epoch: 005 | loss: 0.69342 - acc: 0.5008 | val_loss: 0.68323 - val_acc: 0.5607 -- iter: 339/339
--
Training Step: 56  | total loss: [1m[32m0.69270[0m[0m | time: 0.877s
[2K
| Adam | epoch: 006 | loss: 0.69270 - acc: 0.5051 -- iter: 032/339
[A[ATraining Step: 57  | total loss: [1m[32m0.69282[0m[0m | time: 1.571s
[2K
| Adam | epoch: 006 | loss: 0.69282 - acc: 0.5044 -- iter: 064/339
[A[ATraining Step: 58  | total loss: [1m[32m0.69648[0m[0m | time: 2.406s
[2K
| Adam | epoch: 006 | loss: 0.69648 - acc: 0.4867 -- iter: 096/339
[A[ATraining Step: 59  | total loss: [1m[32m0.69798[0m[0m | time: 2.968s
[2K
| Adam | epoch: 006 | loss: 0.69798 - acc: 0.4759 -- iter: 128/339
[A[ATraining Step: 60  | total loss: [1m[32m0.69803[0m[0m | time: 3.505s
[2K
| Adam | epoch: 006 | loss: 0.69803 - acc: 0.4687 -- iter: 160/339
[A[ATraining Step: 61  | total loss: [1m[32m0.69761[0m[0m | time: 4.331s
[2K
| Adam | epoch: 006 | loss: 0.69761 - acc: 0.4624 -- iter: 192/339
[A[ATraining Step: 62  | total loss: [1m[32m0.69751[0m[0m | time: 5.203s
[2K
| Adam | epoch: 006 | loss: 0.69751 - acc: 0.4432 -- iter: 224/339
[A[ATraining Step: 63  | total loss: [1m[32m0.69678[0m[0m | time: 6.125s
[2K
| Adam | epoch: 006 | loss: 0.69678 - acc: 0.4741 -- iter: 256/339
[A[ATraining Step: 64  | total loss: [1m[32m0.69618[0m[0m | time: 7.032s
[2K
| Adam | epoch: 006 | loss: 0.69618 - acc: 0.4891 -- iter: 288/339
[A[ATraining Step: 65  | total loss: [1m[32m0.69599[0m[0m | time: 7.891s
[2K
| Adam | epoch: 006 | loss: 0.69599 - acc: 0.4712 -- iter: 320/339
[A[ATraining Step: 66  | total loss: [1m[32m0.69571[0m[0m | time: 9.725s
[2K
| Adam | epoch: 006 | loss: 0.69571 - acc: 0.4671 | val_loss: 0.69345 - val_acc: 0.4393 -- iter: 339/339
--
Training Step: 67  | total loss: [1m[32m0.69521[0m[0m | time: 0.817s
[2K
| Adam | epoch: 007 | loss: 0.69521 - acc: 0.4785 -- iter: 032/339
[A[ATraining Step: 68  | total loss: [1m[32m0.69487[0m[0m | time: 1.631s
[2K
| Adam | epoch: 007 | loss: 0.69487 - acc: 0.4774 -- iter: 064/339
[A[ATraining Step: 69  | total loss: [1m[32m0.69454[0m[0m | time: 2.457s
[2K
| Adam | epoch: 007 | loss: 0.69454 - acc: 0.4800 -- iter: 096/339
[A[ATraining Step: 70  | total loss: [1m[32m0.69421[0m[0m | time: 3.265s
[2K
| Adam | epoch: 007 | loss: 0.69421 - acc: 0.4859 -- iter: 128/339
[A[ATraining Step: 71  | total loss: [1m[32m0.69397[0m[0m | time: 3.861s
[2K
| Adam | epoch: 007 | loss: 0.69397 - acc: 0.4875 -- iter: 160/339
[A[ATraining Step: 72  | total loss: [1m[32m0.69363[0m[0m | time: 4.500s
[2K
| Adam | epoch: 007 | loss: 0.69363 - acc: 0.4919 -- iter: 192/339
[A[ATraining Step: 73  | total loss: [1m[32m0.69331[0m[0m | time: 5.406s
[2K
| Adam | epoch: 007 | loss: 0.69331 - acc: 0.4957 -- iter: 224/339
[A[ATraining Step: 74  | total loss: [1m[32m0.69310[0m[0m | time: 6.119s
[2K
| Adam | epoch: 007 | loss: 0.69310 - acc: 0.4962 -- iter: 256/339
[A[ATraining Step: 75  | total loss: [1m[32m0.69231[0m[0m | time: 6.958s
[2K
| Adam | epoch: 007 | loss: 0.69231 - acc: 0.5102 -- iter: 288/339
[A[ATraining Step: 76  | total loss: [1m[32m0.69216[0m[0m | time: 7.819s
[2K
| Adam | epoch: 007 | loss: 0.69216 - acc: 0.5091 -- iter: 320/339
[A[ATraining Step: 77  | total loss: [1m[32m0.69122[0m[0m | time: 9.682s
[2K
| Adam | epoch: 007 | loss: 0.69122 - acc: 0.5180 | val_loss: 0.69044 - val_acc: 0.4393 -- iter: 339/339
--
Training Step: 78  | total loss: [1m[32m0.69031[0m[0m | time: 1.034s
[2K
| Adam | epoch: 008 | loss: 0.69031 - acc: 0.5260 -- iter: 032/339
[A[ATraining Step: 79  | total loss: [1m[32m0.68896[0m[0m | time: 1.985s
[2K
| Adam | epoch: 008 | loss: 0.68896 - acc: 0.5330 -- iter: 064/339
[A[ATraining Step: 80  | total loss: [1m[32m0.68711[0m[0m | time: 2.717s
[2K
| Adam | epoch: 008 | loss: 0.68711 - acc: 0.5328 -- iter: 096/339
[A[ATraining Step: 81  | total loss: [1m[32m0.68417[0m[0m | time: 3.541s
[2K
| Adam | epoch: 008 | loss: 0.68417 - acc: 0.5358 -- iter: 128/339
[A[ATraining Step: 82  | total loss: [1m[32m0.68649[0m[0m | time: 4.372s
[2K
| Adam | epoch: 008 | loss: 0.68649 - acc: 0.5197 -- iter: 160/339
[A[ATraining Step: 83  | total loss: [1m[32m0.68681[0m[0m | time: 4.894s
[2K
| Adam | epoch: 008 | loss: 0.68681 - acc: 0.5115 -- iter: 192/339
[A[ATraining Step: 84  | total loss: [1m[32m0.68697[0m[0m | time: 5.437s
[2K
| Adam | epoch: 008 | loss: 0.68697 - acc: 0.5025 -- iter: 224/339
[A[ATraining Step: 85  | total loss: [1m[32m0.68669[0m[0m | time: 6.262s
[2K
| Adam | epoch: 008 | loss: 0.68669 - acc: 0.4943 -- iter: 256/339
[A[ATraining Step: 86  | total loss: [1m[32m0.68256[0m[0m | time: 7.100s
[2K
| Adam | epoch: 008 | loss: 0.68256 - acc: 0.4918 -- iter: 288/339
[A[ATraining Step: 87  | total loss: [1m[32m0.68036[0m[0m | time: 7.992s
[2K
| Adam | epoch: 008 | loss: 0.68036 - acc: 0.4926 -- iter: 320/339
[A[ATraining Step: 88  | total loss: [1m[32m0.68180[0m[0m | time: 9.880s
[2K
| Adam | epoch: 008 | loss: 0.68180 - acc: 0.4871 | val_loss: 0.64672 - val_acc: 0.7570 -- iter: 339/339
--
Training Step: 89  | total loss: [1m[32m0.67988[0m[0m | time: 0.904s
[2K
| Adam | epoch: 009 | loss: 0.67988 - acc: 0.4977 -- iter: 032/339
[A[ATraining Step: 90  | total loss: [1m[32m0.67452[0m[0m | time: 1.811s
[2K
| Adam | epoch: 009 | loss: 0.67452 - acc: 0.5167 -- iter: 064/339
[A[ATraining Step: 91  | total loss: [1m[32m0.66980[0m[0m | time: 2.695s
[2K
| Adam | epoch: 009 | loss: 0.66980 - acc: 0.5463 -- iter: 096/339
[A[ATraining Step: 92  | total loss: [1m[32m0.66157[0m[0m | time: 3.554s
[2K
| Adam | epoch: 009 | loss: 0.66157 - acc: 0.5792 -- iter: 128/339
[A[ATraining Step: 93  | total loss: [1m[32m0.66174[0m[0m | time: 4.420s
[2K
| Adam | epoch: 009 | loss: 0.66174 - acc: 0.5931 -- iter: 160/339
[A[ATraining Step: 94  | total loss: [1m[32m0.65192[0m[0m | time: 5.259s
[2K
| Adam | epoch: 009 | loss: 0.65192 - acc: 0.6088 -- iter: 192/339
[A[ATraining Step: 95  | total loss: [1m[32m0.63720[0m[0m | time: 5.767s
[2K
| Adam | epoch: 009 | loss: 0.63720 - acc: 0.6198 -- iter: 224/339
[A[ATraining Step: 96  | total loss: [1m[32m0.63038[0m[0m | time: 6.370s
[2K
| Adam | epoch: 009 | loss: 0.63038 - acc: 0.6315 -- iter: 256/339
[A[ATraining Step: 97  | total loss: [1m[32m0.61543[0m[0m | time: 7.359s
[2K
| Adam | epoch: 009 | loss: 0.61543 - acc: 0.6631 -- iter: 288/339
[A[ATraining Step: 98  | total loss: [1m[32m0.61054[0m[0m | time: 8.222s
[2K
| Adam | epoch: 009 | loss: 0.61054 - acc: 0.6655 -- iter: 320/339
[A[ATraining Step: 99  | total loss: [1m[32m0.60334[0m[0m | time: 9.928s
[2K
| Adam | epoch: 009 | loss: 0.60334 - acc: 0.6677 | val_loss: 0.53413 - val_acc: 0.7570 -- iter: 339/339
--
Training Step: 100  | total loss: [1m[32m0.59250[0m[0m | time: 0.895s
[2K
| Adam | epoch: 010 | loss: 0.59250 - acc: 0.6791 -- iter: 032/339
[A[ATraining Step: 101  | total loss: [1m[32m0.58225[0m[0m | time: 1.709s
[2K
| Adam | epoch: 010 | loss: 0.58225 - acc: 0.6862 -- iter: 064/339
[A[ATraining Step: 102  | total loss: [1m[32m0.56193[0m[0m | time: 2.568s
[2K
| Adam | epoch: 010 | loss: 0.56193 - acc: 0.6988 -- iter: 096/339
[A[ATraining Step: 103  | total loss: [1m[32m0.57899[0m[0m | time: 3.538s
[2K
| Adam | epoch: 010 | loss: 0.57899 - acc: 0.7039 -- iter: 128/339
[A[ATraining Step: 104  | total loss: [1m[32m0.58045[0m[0m | time: 4.454s
[2K
| Adam | epoch: 010 | loss: 0.58045 - acc: 0.7054 -- iter: 160/339
[A[ATraining Step: 105  | total loss: [1m[32m0.58646[0m[0m | time: 5.145s
[2K
| Adam | epoch: 010 | loss: 0.58646 - acc: 0.6974 -- iter: 192/339
[A[ATraining Step: 106  | total loss: [1m[32m0.57696[0m[0m | time: 5.950s
[2K
| Adam | epoch: 010 | loss: 0.57696 - acc: 0.7026 -- iter: 224/339
[A[ATraining Step: 107  | total loss: [1m[32m0.59248[0m[0m | time: 6.452s
[2K
| Adam | epoch: 010 | loss: 0.59248 - acc: 0.6886 -- iter: 256/339
[A[ATraining Step: 108  | total loss: [1m[32m0.59763[0m[0m | time: 6.947s
[2K
| Adam | epoch: 010 | loss: 0.59763 - acc: 0.6777 -- iter: 288/339
[A[ATraining Step: 109  | total loss: [1m[32m0.59059[0m[0m | time: 7.801s
[2K
| Adam | epoch: 010 | loss: 0.59059 - acc: 0.6888 -- iter: 320/339
[A[ATraining Step: 110  | total loss: [1m[32m0.59262[0m[0m | time: 9.688s
[2K
| Adam | epoch: 010 | loss: 0.59262 - acc: 0.6887 | val_loss: 0.68880 - val_acc: 0.5047 -- iter: 339/339
--
Training Step: 111  | total loss: [1m[32m0.58728[0m[0m | time: 1.028s
[2K
| Adam | epoch: 011 | loss: 0.58728 - acc: 0.6948 -- iter: 032/339
[A[ATraining Step: 112  | total loss: [1m[32m0.58501[0m[0m | time: 1.849s
[2K
| Adam | epoch: 011 | loss: 0.58501 - acc: 0.6941 -- iter: 064/339
[A[ATraining Step: 113  | total loss: [1m[32m0.58491[0m[0m | time: 2.625s
[2K
| Adam | epoch: 011 | loss: 0.58491 - acc: 0.6934 -- iter: 096/339
[A[ATraining Step: 114  | total loss: [1m[32m0.58185[0m[0m | time: 3.507s
[2K
| Adam | epoch: 011 | loss: 0.58185 - acc: 0.7053 -- iter: 128/339
[A[ATraining Step: 115  | total loss: [1m[32m0.57526[0m[0m | time: 4.376s
[2K
| Adam | epoch: 011 | loss: 0.57526 - acc: 0.7098 -- iter: 160/339
[A[ATraining Step: 116  | total loss: [1m[32m0.58036[0m[0m | time: 5.179s
[2K
| Adam | epoch: 011 | loss: 0.58036 - acc: 0.6982 -- iter: 192/339
[A[ATraining Step: 117  | total loss: [1m[32m0.57420[0m[0m | time: 6.042s
[2K
| Adam | epoch: 011 | loss: 0.57420 - acc: 0.6940 -- iter: 224/339
[A[ATraining Step: 118  | total loss: [1m[32m0.57423[0m[0m | time: 6.901s
[2K
| Adam | epoch: 011 | loss: 0.57423 - acc: 0.6934 -- iter: 256/339
[A[ATraining Step: 119  | total loss: [1m[32m0.57653[0m[0m | time: 7.428s
[2K
| Adam | epoch: 011 | loss: 0.57653 - acc: 0.6959 -- iter: 288/339
[A[ATraining Step: 120  | total loss: [1m[32m0.55956[0m[0m | time: 7.941s
[2K
| Adam | epoch: 011 | loss: 0.55956 - acc: 0.7105 -- iter: 320/339
[A[ATraining Step: 121  | total loss: [1m[32m0.53498[0m[0m | time: 9.826s
[2K
| Adam | epoch: 011 | loss: 0.53498 - acc: 0.7395 | val_loss: 0.48613 - val_acc: 0.7664 -- iter: 339/339
--
Training Step: 122  | total loss: [1m[32m0.50966[0m[0m | time: 0.831s
[2K
| Adam | epoch: 012 | loss: 0.50966 - acc: 0.7593 -- iter: 032/339
[A[ATraining Step: 123  | total loss: [1m[32m0.49376[0m[0m | time: 1.749s
[2K
| Adam | epoch: 012 | loss: 0.49376 - acc: 0.7677 -- iter: 064/339
[A[ATraining Step: 124  | total loss: [1m[32m0.47771[0m[0m | time: 2.607s
[2K
| Adam | epoch: 012 | loss: 0.47771 - acc: 0.7784 -- iter: 096/339
[A[ATraining Step: 125  | total loss: [1m[32m0.47291[0m[0m | time: 3.482s
[2K
| Adam | epoch: 012 | loss: 0.47291 - acc: 0.7819 -- iter: 128/339
[A[ATraining Step: 126  | total loss: [1m[32m0.46531[0m[0m | time: 4.301s
[2K
| Adam | epoch: 012 | loss: 0.46531 - acc: 0.7912 -- iter: 160/339
[A[ATraining Step: 127  | total loss: [1m[32m0.46654[0m[0m | time: 5.348s
[2K
| Adam | epoch: 012 | loss: 0.46654 - acc: 0.7995 -- iter: 192/339
[A[ATraining Step: 128  | total loss: [1m[32m0.45953[0m[0m | time: 6.371s
[2K
| Adam | epoch: 012 | loss: 0.45953 - acc: 0.7977 -- iter: 224/339
[A[ATraining Step: 129  | total loss: [1m[32m0.45036[0m[0m | time: 7.120s
[2K
| Adam | epoch: 012 | loss: 0.45036 - acc: 0.8023 -- iter: 256/339
[A[ATraining Step: 130  | total loss: [1m[32m0.48898[0m[0m | time: 7.930s
[2K
| Adam | epoch: 012 | loss: 0.48898 - acc: 0.7877 -- iter: 288/339
[A[ATraining Step: 131  | total loss: [1m[32m0.47000[0m[0m | time: 8.450s
[2K
| Adam | epoch: 012 | loss: 0.47000 - acc: 0.7996 -- iter: 320/339
[A[ATraining Step: 132  | total loss: [1m[32m0.45519[0m[0m | time: 9.957s
[2K
| Adam | epoch: 012 | loss: 0.45519 - acc: 0.8038 | val_loss: 0.42053 - val_acc: 0.8037 -- iter: 339/339
--
Training Step: 133  | total loss: [1m[32m0.44480[0m[0m | time: 0.715s
[2K
| Adam | epoch: 013 | loss: 0.44480 - acc: 0.8077 -- iter: 032/339
[A[ATraining Step: 134  | total loss: [1m[32m0.42728[0m[0m | time: 1.555s
[2K
| Adam | epoch: 013 | loss: 0.42728 - acc: 0.8175 -- iter: 064/339
[A[ATraining Step: 135  | total loss: [1m[32m0.43119[0m[0m | time: 2.420s
[2K
| Adam | epoch: 013 | loss: 0.43119 - acc: 0.8201 -- iter: 096/339
[A[ATraining Step: 136  | total loss: [1m[32m0.42352[0m[0m | time: 3.264s
[2K
| Adam | epoch: 013 | loss: 0.42352 - acc: 0.8256 -- iter: 128/339
[A[ATraining Step: 137  | total loss: [1m[32m0.41494[0m[0m | time: 4.076s
[2K
| Adam | epoch: 013 | loss: 0.41494 - acc: 0.8306 -- iter: 160/339
[A[ATraining Step: 138  | total loss: [1m[32m0.41282[0m[0m | time: 4.943s
[2K
| Adam | epoch: 013 | loss: 0.41282 - acc: 0.8256 -- iter: 192/339
[A[ATraining Step: 139  | total loss: [1m[32m0.43165[0m[0m | time: 5.777s
[2K
| Adam | epoch: 013 | loss: 0.43165 - acc: 0.8118 -- iter: 224/339
[A[ATraining Step: 140  | total loss: [1m[32m0.41373[0m[0m | time: 6.586s
[2K
| Adam | epoch: 013 | loss: 0.41373 - acc: 0.8244 -- iter: 256/339
[A[ATraining Step: 141  | total loss: [1m[32m0.40582[0m[0m | time: 7.410s
[2K
| Adam | epoch: 013 | loss: 0.40582 - acc: 0.8326 -- iter: 288/339
[A[ATraining Step: 142  | total loss: [1m[32m0.39551[0m[0m | time: 8.235s
[2K
| Adam | epoch: 013 | loss: 0.39551 - acc: 0.8337 -- iter: 320/339
[A[ATraining Step: 143  | total loss: [1m[32m0.38765[0m[0m | time: 9.858s
[2K
| Adam | epoch: 013 | loss: 0.38765 - acc: 0.8378 | val_loss: 0.36494 - val_acc: 0.8318 -- iter: 339/339
--
Training Step: 144  | total loss: [1m[32m0.39020[0m[0m | time: 0.553s
[2K
| Adam | epoch: 014 | loss: 0.39020 - acc: 0.8382 -- iter: 032/339
[A[ATraining Step: 145  | total loss: [1m[32m0.38141[0m[0m | time: 1.414s
[2K
| Adam | epoch: 014 | loss: 0.38141 - acc: 0.8386 -- iter: 064/339
[A[ATraining Step: 146  | total loss: [1m[32m0.36386[0m[0m | time: 2.265s
[2K
| Adam | epoch: 014 | loss: 0.36386 - acc: 0.8485 -- iter: 096/339
[A[ATraining Step: 147  | total loss: [1m[32m0.35405[0m[0m | time: 3.071s
[2K
| Adam | epoch: 014 | loss: 0.35405 - acc: 0.8512 -- iter: 128/339
[A[ATraining Step: 148  | total loss: [1m[32m0.35228[0m[0m | time: 3.872s
[2K
| Adam | epoch: 014 | loss: 0.35228 - acc: 0.8567 -- iter: 160/339
[A[ATraining Step: 149  | total loss: [1m[32m0.33691[0m[0m | time: 4.882s
[2K
| Adam | epoch: 014 | loss: 0.33691 - acc: 0.8616 -- iter: 192/339
[A[ATraining Step: 150  | total loss: [1m[32m0.32237[0m[0m | time: 5.881s
[2K
| Adam | epoch: 014 | loss: 0.32237 - acc: 0.8661 -- iter: 224/339
[A[ATraining Step: 151  | total loss: [1m[32m0.32005[0m[0m | time: 6.679s
[2K
| Adam | epoch: 014 | loss: 0.32005 - acc: 0.8639 -- iter: 256/339
[A[ATraining Step: 152  | total loss: [1m[32m0.29868[0m[0m | time: 7.486s
[2K
| Adam | epoch: 014 | loss: 0.29868 - acc: 0.8743 -- iter: 288/339
[A[ATraining Step: 153  | total loss: [1m[32m0.30178[0m[0m | time: 8.305s
[2K
| Adam | epoch: 014 | loss: 0.30178 - acc: 0.8775 -- iter: 320/339
[A[ATraining Step: 154  | total loss: [1m[32m0.32048[0m[0m | time: 10.123s
[2K
| Adam | epoch: 014 | loss: 0.32048 - acc: 0.8742 | val_loss: 0.78498 - val_acc: 0.6822 -- iter: 339/339
--
Training Step: 155  | total loss: [1m[32m0.32309[0m[0m | time: 0.577s
[2K
| Adam | epoch: 015 | loss: 0.32309 - acc: 0.8711 -- iter: 032/339
[A[ATraining Step: 156  | total loss: [1m[32m0.33094[0m[0m | time: 1.094s
[2K
| Adam | epoch: 015 | loss: 0.33094 - acc: 0.8630 -- iter: 064/339
[A[ATraining Step: 157  | total loss: [1m[32m0.33199[0m[0m | time: 1.880s
[2K
| Adam | epoch: 015 | loss: 0.33199 - acc: 0.8556 -- iter: 096/339
[A[ATraining Step: 158  | total loss: [1m[32m0.33517[0m[0m | time: 2.770s
[2K
| Adam | epoch: 015 | loss: 0.33517 - acc: 0.8607 -- iter: 128/339
[A[ATraining Step: 159  | total loss: [1m[32m0.31899[0m[0m | time: 3.718s
[2K
| Adam | epoch: 015 | loss: 0.31899 - acc: 0.8684 -- iter: 160/339
[A[ATraining Step: 160  | total loss: [1m[32m0.31481[0m[0m | time: 4.649s
[2K
| Adam | epoch: 015 | loss: 0.31481 - acc: 0.8721 -- iter: 192/339
[A[ATraining Step: 161  | total loss: [1m[32m0.29745[0m[0m | time: 5.333s
[2K
| Adam | epoch: 015 | loss: 0.29745 - acc: 0.8818 -- iter: 224/339
[A[ATraining Step: 162  | total loss: [1m[32m0.29973[0m[0m | time: 6.118s
[2K
| Adam | epoch: 015 | loss: 0.29973 - acc: 0.8842 -- iter: 256/339
[A[ATraining Step: 163  | total loss: [1m[32m0.28497[0m[0m | time: 6.959s
[2K
| Adam | epoch: 015 | loss: 0.28497 - acc: 0.8927 -- iter: 288/339
[A[ATraining Step: 164  | total loss: [1m[32m0.27582[0m[0m | time: 7.767s
[2K
| Adam | epoch: 015 | loss: 0.27582 - acc: 0.8972 -- iter: 320/339
[A[ATraining Step: 165  | total loss: [1m[32m0.26980[0m[0m | time: 9.603s
[2K
| Adam | epoch: 015 | loss: 0.26980 - acc: 0.9012 | val_loss: 0.38308 - val_acc: 0.8318 -- iter: 339/339
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.9351063829787234
Validation AUPRC:0.9497028846133453
Test AUC:0.9524807826694619
Test AUPRC:0.9583850448860349
BestTestF1Score	0.91	0.84	0.92	0.96	0.87	46	2	52	7	0.3
BestTestMCCScore	0.91	0.84	0.92	0.96	0.87	46	2	52	7	0.3
BestTestAccuracyScore	0.91	0.84	0.92	0.96	0.87	46	2	52	7	0.3
BestValidationF1Score	0.89	0.76	0.88	0.91	0.87	52	5	42	8	0.3
BestValidationMCC	0.89	0.76	0.88	0.91	0.87	52	5	42	8	0.3
BestValidationAccuracy	0.89	0.76	0.88	0.91	0.87	52	5	42	8	0.3
TestPredictions (Threshold:0.3)
CHEMBL3654263,TP,ACT,0.6700000166893005	CHEMBL3356566,TN,INACT,0.07999999821186066	CHEMBL1795619,TN,INACT,0.019999999552965164	CHEMBL3650907,TP,ACT,0.8999999761581421	CHEMBL3775772,TN,INACT,0.05000000074505806	CHEMBL3654295,TP,ACT,0.9399999976158142	CHEMBL3621673,TN,INACT,0.029999999329447746	CHEMBL2335381,TN,INACT,0.20000000298023224	CHEMBL3654287,TP,ACT,0.9100000262260437	CHEMBL3654277,TP,ACT,0.9200000166893005	CHEMBL2181715,TP,ACT,0.6000000238418579	CHEMBL3650903,TP,ACT,0.9599999785423279	CHEMBL487269,TN,INACT,0.029999999329447746	CHEMBL3356576,TN,INACT,0.03999999910593033	CHEMBL3814920,TN,INACT,0.07000000029802322	CHEMBL3423100,TN,INACT,0.029999999329447746	CHEMBL3650864,TP,ACT,0.8899999856948853	CHEMBL2017288,TP,ACT,0.9399999976158142	CHEMBL3650895,TP,ACT,0.550000011920929	CHEMBL2181739,FN,ACT,0.10000000149011612	CHEMBL2443262,TN,INACT,0.09000000357627869	CHEMBL3590383,TN,INACT,0.1899999976158142	CHEMBL3654288,TP,ACT,0.949999988079071	CHEMBL3220851,TN,INACT,0.09000000357627869	CHEMBL1732009,TN,INACT,0.029999999329447746	CHEMBL3110251,FP,INACT,0.41999998688697815	CHEMBL2181741,TP,ACT,0.9200000166893005	CHEMBL3110248,TN,INACT,0.05999999865889549	CHEMBL3585450,TN,INACT,0.14000000059604645	CHEMBL3654312,TP,ACT,0.9800000190734863	CHEMBL3590401,TN,INACT,0.1899999976158142	CHEMBL3650878,TP,ACT,0.9399999976158142	CHEMBL558153,TN,INACT,0.03999999910593033	CHEMBL1734009,TN,INACT,0.029999999329447746	CHEMBL3650952,TP,ACT,0.8600000143051147	CHEMBL3654322,TP,ACT,0.8600000143051147	CHEMBL3650909,TP,ACT,0.9700000286102295	CHEMBL2346691,TN,INACT,0.029999999329447746	CHEMBL486569,TN,INACT,0.009999999776482582	CHEMBL3650926,TP,ACT,0.8999999761581421	CHEMBL3770112,TN,INACT,0.2800000011920929	CHEMBL2181740,FN,ACT,0.07999999821186066	CHEMBL2142704,TN,INACT,0.07000000029802322	CHEMBL3654292,TP,ACT,0.949999988079071	CHEMBL3650958,TP,ACT,0.8899999856948853	CHEMBL3654311,TP,ACT,0.9399999976158142	CHEMBL2393130,FN,ACT,0.05000000074505806	CHEMBL2349345,TN,INACT,0.07000000029802322	CHEMBL2017286,TP,ACT,0.8799999952316284	CHEMBL2436003,FP,INACT,0.9200000166893005	CHEMBL3590400,TN,INACT,0.12999999523162842	CHEMBL3650896,TP,ACT,0.5699999928474426	CHEMBL3650956,TP,ACT,0.9200000166893005	CHEMBL3654264,TP,ACT,0.9100000262260437	CHEMBL2181706,TP,ACT,0.9399999976158142	CHEMBL2430869,TP,ACT,0.9700000286102295	CHEMBL3356579,TN,INACT,0.10000000149011612	CHEMBL1777855,TN,INACT,0.07000000029802322	CHEMBL3221338,TN,INACT,0.05000000074505806	CHEMBL2349342,TN,INACT,0.05999999865889549	CHEMBL2430889,TP,ACT,0.9399999976158142	CHEMBL295316,TN,INACT,0.07999999821186066	CHEMBL1568712,TN,INACT,0.03999999910593033	CHEMBL3654316,TP,ACT,0.9100000262260437	CHEMBL3654306,TP,ACT,0.9399999976158142	CHEMBL2017264,TP,ACT,0.7900000214576721	CHEMBL3770026,TN,INACT,0.03999999910593033	CHEMBL3753648,TN,INACT,0.029999999329447746	CHEMBL3650911,TP,ACT,0.9200000166893005	CHEMBL3650954,TP,ACT,0.9100000262260437	CHEMBL3775671,TN,INACT,0.029999999329447746	CHEMBL3110252,TN,INACT,0.14000000059604645	CHEMBL3354003,FN,ACT,0.2800000011920929	CHEMBL3650945,TP,ACT,0.9700000286102295	CHEMBL3650941,TP,ACT,0.8899999856948853	CHEMBL2349341,TN,INACT,0.05999999865889549	CHEMBL2181721,TP,ACT,0.9800000190734863	CHEMBL3775750,TN,INACT,0.10000000149011612	CHEMBL3823551,TN,INACT,0.05999999865889549	CHEMBL3824133,TN,INACT,0.05999999865889549	CHEMBL2282031,TN,INACT,0.07000000029802322	CHEMBL3769417,TN,INACT,0.09000000357627869	CHEMBL3770598,TN,INACT,0.10999999940395355	CHEMBL3621675,TN,INACT,0.029999999329447746	CHEMBL3353989,TP,ACT,0.8600000143051147	CHEMBL1738777,TN,INACT,0.03999999910593033	CHEMBL2436007,TN,INACT,0.05000000074505806	CHEMBL3650880,TP,ACT,0.8100000023841858	CHEMBL3654323,TP,ACT,0.9300000071525574	CHEMBL2181818,TP,ACT,0.7900000214576721	CHEMBL3590405,FN,ACT,0.1899999976158142	CHEMBL3650975,TP,ACT,0.8899999856948853	CHEMBL3650874,TP,ACT,0.8899999856948853	CHEMBL2017285,TP,ACT,0.9300000071525574	CHEMBL3824146,TN,INACT,0.10999999940395355	CHEMBL2436009,TN,INACT,0.029999999329447746	CHEMBL1957266,FN,ACT,0.25	CHEMBL3589485,TN,INACT,0.20000000298023224	CHEMBL3353985,TP,ACT,0.9100000262260437	CHEMBL2430894,TP,ACT,0.9599999785423279	CHEMBL3221336,TN,INACT,0.14000000059604645	CHEMBL1382445,TN,INACT,0.019999999552965164	CHEMBL3087052,FN,ACT,0.05000000074505806	CHEMBL3110253,TN,INACT,0.2800000011920929	CHEMBL522211,TN,INACT,0.029999999329447746	CHEMBL3586689,TN,INACT,0.05000000074505806	CHEMBL3650929,TP,ACT,0.7699999809265137	

