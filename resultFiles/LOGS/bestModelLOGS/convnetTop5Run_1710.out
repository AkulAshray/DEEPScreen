ImageNetInceptionV2 CHEMBL4329 adam 0.001 15 0 0 0.8 False True
Number of active compounds :	178
Number of inactive compounds :	178
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL4329_adam_0.001_15_0_0_0.8_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL4329_adam_0.001_15_0.8/
---------------------------------
Training samples: 224
Validation samples: 70
--
Training Step: 1  | time: 153.946s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/224
[A[ATraining Step: 2  | total loss: [1m[32m0.69047[0m[0m | time: 230.675s
[2K
| Adam | epoch: 001 | loss: 0.69047 - acc: 0.4219 -- iter: 064/224
[A[ATraining Step: 3  | total loss: [1m[32m1.06927[0m[0m | time: 317.144s
[2K
| Adam | epoch: 001 | loss: 1.06927 - acc: 0.4858 -- iter: 096/224
[A[ATraining Step: 4  | total loss: [1m[32m0.81787[0m[0m | time: 384.021s
[2K
| Adam | epoch: 001 | loss: 0.81787 - acc: 0.5902 -- iter: 128/224
[A[ATraining Step: 5  | total loss: [1m[32m0.58063[0m[0m | time: 446.851s
[2K
| Adam | epoch: 001 | loss: 0.58063 - acc: 0.6359 -- iter: 160/224
[A[ATraining Step: 6  | total loss: [1m[32m0.48178[0m[0m | time: 474.016s
[2K
| Adam | epoch: 001 | loss: 0.48178 - acc: 0.7695 -- iter: 192/224
[A[ATraining Step: 7  | total loss: [1m[32m0.56614[0m[0m | time: 560.096s
[2K
| Adam | epoch: 001 | loss: 0.56614 - acc: 0.8141 | val_loss: 1.49978 - val_acc: 0.5143 -- iter: 224/224
--
Training Step: 8  | total loss: [1m[32m0.51338[0m[0m | time: 55.059s
[2K
| Adam | epoch: 002 | loss: 0.51338 - acc: 0.7956 -- iter: 032/224
[A[ATraining Step: 9  | total loss: [1m[32m0.36136[0m[0m | time: 73.950s
[2K
| Adam | epoch: 002 | loss: 0.36136 - acc: 0.8542 -- iter: 064/224
[A[ATraining Step: 10  | total loss: [1m[32m0.34835[0m[0m | time: 157.701s
[2K
| Adam | epoch: 002 | loss: 0.34835 - acc: 0.8490 -- iter: 096/224
[A[ATraining Step: 11  | total loss: [1m[32m0.78880[0m[0m | time: 210.855s
[2K
| Adam | epoch: 002 | loss: 0.78880 - acc: 0.8465 -- iter: 128/224
[A[ATraining Step: 12  | total loss: [1m[32m0.58019[0m[0m | time: 294.685s
[2K
| Adam | epoch: 002 | loss: 0.58019 - acc: 0.8312 -- iter: 160/224
[A[ATraining Step: 13  | total loss: [1m[32m0.54437[0m[0m | time: 359.601s
[2K
| Adam | epoch: 002 | loss: 0.54437 - acc: 0.7964 -- iter: 192/224
[A[ATraining Step: 14  | total loss: [1m[32m0.44927[0m[0m | time: 405.274s
[2K
| Adam | epoch: 002 | loss: 0.44927 - acc: 0.8286 | val_loss: 0.84440 - val_acc: 0.5143 -- iter: 224/224
--
Training Step: 15  | total loss: [1m[32m0.43646[0m[0m | time: 15.039s
[2K
| Adam | epoch: 003 | loss: 0.43646 - acc: 0.7978 -- iter: 032/224
[A[ATraining Step: 16  | total loss: [1m[32m0.37496[0m[0m | time: 30.422s
[2K
| Adam | epoch: 003 | loss: 0.37496 - acc: 0.8268 -- iter: 064/224
[A[ATraining Step: 17  | total loss: [1m[32m0.33148[0m[0m | time: 63.955s
[2K
| Adam | epoch: 003 | loss: 0.33148 - acc: 0.8554 -- iter: 096/224
[A[ATraining Step: 18  | total loss: [1m[32m0.48091[0m[0m | time: 79.757s
[2K
| Adam | epoch: 003 | loss: 0.48091 - acc: 0.8838 -- iter: 128/224
[A[ATraining Step: 19  | total loss: [1m[32m0.41460[0m[0m | time: 93.009s
[2K
| Adam | epoch: 003 | loss: 0.41460 - acc: 0.8705 -- iter: 160/224
[A[ATraining Step: 20  | total loss: [1m[32m0.34080[0m[0m | time: 104.209s
[2K
| Adam | epoch: 003 | loss: 0.34080 - acc: 0.8920 -- iter: 192/224
[A[ATraining Step: 21  | total loss: [1m[32m0.28881[0m[0m | time: 125.335s
[2K
| Adam | epoch: 003 | loss: 0.28881 - acc: 0.9061 | val_loss: 1.43339 - val_acc: 0.5143 -- iter: 224/224
--
Training Step: 22  | total loss: [1m[32m0.25207[0m[0m | time: 15.974s
[2K
| Adam | epoch: 004 | loss: 0.25207 - acc: 0.9249 -- iter: 032/224
[A[ATraining Step: 23  | total loss: [1m[32m0.21893[0m[0m | time: 31.084s
[2K
| Adam | epoch: 004 | loss: 0.21893 - acc: 0.9376 -- iter: 064/224
[A[ATraining Step: 24  | total loss: [1m[32m0.19082[0m[0m | time: 42.361s
[2K
| Adam | epoch: 004 | loss: 0.19082 - acc: 0.9464 -- iter: 096/224
[A[ATraining Step: 25  | total loss: [1m[32m0.15864[0m[0m | time: 78.636s
[2K
| Adam | epoch: 004 | loss: 0.15864 - acc: 0.9610 -- iter: 128/224
[A[ATraining Step: 26  | total loss: [1m[32m0.15274[0m[0m | time: 97.110s
[2K
| Adam | epoch: 004 | loss: 0.15274 - acc: 0.9548 -- iter: 160/224
[A[ATraining Step: 27  | total loss: [1m[32m0.13997[0m[0m | time: 113.392s
[2K
| Adam | epoch: 004 | loss: 0.13997 - acc: 0.9503 -- iter: 192/224
[A[ATraining Step: 28  | total loss: [1m[32m0.14794[0m[0m | time: 136.189s
[2K
| Adam | epoch: 004 | loss: 0.14794 - acc: 0.9549 | val_loss: 0.78985 - val_acc: 0.5143 -- iter: 224/224
--
Training Step: 29  | total loss: [1m[32m0.12949[0m[0m | time: 11.405s
[2K
| Adam | epoch: 005 | loss: 0.12949 - acc: 0.9583 -- iter: 032/224
[A[ATraining Step: 30  | total loss: [1m[32m0.10532[0m[0m | time: 24.730s
[2K
| Adam | epoch: 005 | loss: 0.10532 - acc: 0.9682 -- iter: 064/224
[A[ATraining Step: 31  | total loss: [1m[32m0.13578[0m[0m | time: 49.037s
[2K
| Adam | epoch: 005 | loss: 0.13578 - acc: 0.9539 -- iter: 096/224
[A[ATraining Step: 32  | total loss: [1m[32m0.31111[0m[0m | time: 65.035s
[2K
| Adam | epoch: 005 | loss: 0.31111 - acc: 0.9150 -- iter: 128/224
[A[ATraining Step: 33  | total loss: [1m[32m0.25161[0m[0m | time: 86.008s
[2K
| Adam | epoch: 005 | loss: 0.25161 - acc: 0.9337 -- iter: 160/224
[A[ATraining Step: 34  | total loss: [1m[32m0.30678[0m[0m | time: 104.126s
[2K
| Adam | epoch: 005 | loss: 0.30678 - acc: 0.9144 -- iter: 192/224
[A[ATraining Step: 35  | total loss: [1m[32m0.28455[0m[0m | time: 123.108s
[2K
| Adam | epoch: 005 | loss: 0.28455 - acc: 0.9062 | val_loss: 2.71637 - val_acc: 0.4857 -- iter: 224/224
--
Training Step: 36  | total loss: [1m[32m0.23713[0m[0m | time: 13.189s
[2K
| Adam | epoch: 006 | loss: 0.23713 - acc: 0.9254 -- iter: 032/224
[A[ATraining Step: 37  | total loss: [1m[32m0.19868[0m[0m | time: 24.101s
[2K
| Adam | epoch: 006 | loss: 0.19868 - acc: 0.9403 -- iter: 064/224
[A[ATraining Step: 38  | total loss: [1m[32m0.20249[0m[0m | time: 34.995s
[2K
| Adam | epoch: 006 | loss: 0.20249 - acc: 0.9336 -- iter: 096/224
[A[ATraining Step: 39  | total loss: [1m[32m0.17891[0m[0m | time: 43.747s
[2K
| Adam | epoch: 006 | loss: 0.17891 - acc: 0.9463 -- iter: 128/224
[A[ATraining Step: 40  | total loss: [1m[32m0.16900[0m[0m | time: 52.352s
[2K
| Adam | epoch: 006 | loss: 0.16900 - acc: 0.9505 -- iter: 160/224
[A[ATraining Step: 41  | total loss: [1m[32m0.14360[0m[0m | time: 60.708s
[2K
| Adam | epoch: 006 | loss: 0.14360 - acc: 0.9596 -- iter: 192/224
[A[ATraining Step: 42  | total loss: [1m[32m0.12374[0m[0m | time: 72.550s
[2K
| Adam | epoch: 006 | loss: 0.12374 - acc: 0.9669 | val_loss: 1.85078 - val_acc: 0.4857 -- iter: 224/224
--
Training Step: 43  | total loss: [1m[32m0.14627[0m[0m | time: 8.502s
[2K
| Adam | epoch: 007 | loss: 0.14627 - acc: 0.9562 -- iter: 032/224
[A[ATraining Step: 44  | total loss: [1m[32m0.15042[0m[0m | time: 17.094s
[2K
| Adam | epoch: 007 | loss: 0.15042 - acc: 0.9421 -- iter: 064/224
[A[ATraining Step: 45  | total loss: [1m[32m0.15744[0m[0m | time: 26.016s
[2K
| Adam | epoch: 007 | loss: 0.15744 - acc: 0.9467 -- iter: 096/224
[A[ATraining Step: 46  | total loss: [1m[32m0.14416[0m[0m | time: 34.433s
[2K
| Adam | epoch: 007 | loss: 0.14416 - acc: 0.9555 -- iter: 128/224
[A[ATraining Step: 47  | total loss: [1m[32m0.14042[0m[0m | time: 43.295s
[2K
| Adam | epoch: 007 | loss: 0.14042 - acc: 0.9577 -- iter: 160/224
[A[ATraining Step: 48  | total loss: [1m[32m0.16221[0m[0m | time: 51.462s
[2K
| Adam | epoch: 007 | loss: 0.16221 - acc: 0.9545 -- iter: 192/224
[A[ATraining Step: 49  | total loss: [1m[32m0.14660[0m[0m | time: 63.458s
[2K
| Adam | epoch: 007 | loss: 0.14660 - acc: 0.9567 | val_loss: 1.75237 - val_acc: 0.4857 -- iter: 224/224
--
Training Step: 50  | total loss: [1m[32m0.14681[0m[0m | time: 7.886s
[2K
| Adam | epoch: 008 | loss: 0.14681 - acc: 0.9537 -- iter: 032/224
[A[ATraining Step: 51  | total loss: [1m[32m0.18346[0m[0m | time: 16.165s
[2K
| Adam | epoch: 008 | loss: 0.18346 - acc: 0.9322 -- iter: 064/224
[A[ATraining Step: 52  | total loss: [1m[32m0.17359[0m[0m | time: 24.456s
[2K
| Adam | epoch: 008 | loss: 0.17359 - acc: 0.9330 -- iter: 096/224
[A[ATraining Step: 53  | total loss: [1m[32m0.15206[0m[0m | time: 33.029s
[2K
| Adam | epoch: 008 | loss: 0.15206 - acc: 0.9429 -- iter: 128/224
[A[ATraining Step: 54  | total loss: [1m[32m0.13781[0m[0m | time: 41.474s
[2K
| Adam | epoch: 008 | loss: 0.13781 - acc: 0.9512 -- iter: 160/224
[A[ATraining Step: 55  | total loss: [1m[32m0.12400[0m[0m | time: 49.780s
[2K
| Adam | epoch: 008 | loss: 0.12400 - acc: 0.9581 -- iter: 192/224
[A[ATraining Step: 56  | total loss: [1m[32m0.12889[0m[0m | time: 61.722s
[2K
| Adam | epoch: 008 | loss: 0.12889 - acc: 0.9552 | val_loss: 1.27361 - val_acc: 0.6000 -- iter: 224/224
--
Training Step: 57  | total loss: [1m[32m0.11927[0m[0m | time: 8.345s
[2K
| Adam | epoch: 009 | loss: 0.11927 - acc: 0.9614 -- iter: 032/224
[A[ATraining Step: 58  | total loss: [1m[32m0.11272[0m[0m | time: 17.073s
[2K
| Adam | epoch: 009 | loss: 0.11272 - acc: 0.9624 -- iter: 064/224
[A[ATraining Step: 59  | total loss: [1m[32m0.12699[0m[0m | time: 25.579s
[2K
| Adam | epoch: 009 | loss: 0.12699 - acc: 0.9591 -- iter: 096/224
[A[ATraining Step: 60  | total loss: [1m[32m0.11513[0m[0m | time: 33.695s
[2K
| Adam | epoch: 009 | loss: 0.11513 - acc: 0.9645 -- iter: 128/224
[A[ATraining Step: 61  | total loss: [1m[32m0.12147[0m[0m | time: 42.008s
[2K
| Adam | epoch: 009 | loss: 0.12147 - acc: 0.9610 -- iter: 160/224
[A[ATraining Step: 62  | total loss: [1m[32m0.12605[0m[0m | time: 50.617s
[2K
| Adam | epoch: 009 | loss: 0.12605 - acc: 0.9620 -- iter: 192/224
[A[ATraining Step: 63  | total loss: [1m[32m0.11200[0m[0m | time: 62.500s
[2K
| Adam | epoch: 009 | loss: 0.11200 - acc: 0.9668 | val_loss: 0.18532 - val_acc: 0.9286 -- iter: 224/224
--
Training Step: 64  | total loss: [1m[32m0.31370[0m[0m | time: 8.603s
[2K
| Adam | epoch: 010 | loss: 0.31370 - acc: 0.9319 -- iter: 032/224
[A[ATraining Step: 65  | total loss: [1m[32m0.28648[0m[0m | time: 17.459s
[2K
| Adam | epoch: 010 | loss: 0.28648 - acc: 0.9364 -- iter: 064/224
[A[ATraining Step: 66  | total loss: [1m[32m0.27349[0m[0m | time: 26.118s
[2K
| Adam | epoch: 010 | loss: 0.27349 - acc: 0.9366 -- iter: 096/224
[A[ATraining Step: 67  | total loss: [1m[32m0.26292[0m[0m | time: 34.513s
[2K
| Adam | epoch: 010 | loss: 0.26292 - acc: 0.9292 -- iter: 128/224
[A[ATraining Step: 68  | total loss: [1m[32m0.24251[0m[0m | time: 43.186s
[2K
| Adam | epoch: 010 | loss: 0.24251 - acc: 0.9339 -- iter: 160/224
[A[ATraining Step: 69  | total loss: [1m[32m0.22725[0m[0m | time: 51.690s
[2K
| Adam | epoch: 010 | loss: 0.22725 - acc: 0.9416 -- iter: 192/224
[A[ATraining Step: 70  | total loss: [1m[32m0.23050[0m[0m | time: 63.453s
[2K
| Adam | epoch: 010 | loss: 0.23050 - acc: 0.9375 | val_loss: 0.89346 - val_acc: 0.7000 -- iter: 224/224
--
Training Step: 71  | total loss: [1m[32m0.21847[0m[0m | time: 8.652s
[2K
| Adam | epoch: 011 | loss: 0.21847 - acc: 0.9411 -- iter: 032/224
[A[ATraining Step: 72  | total loss: [1m[32m0.20979[0m[0m | time: 17.133s
[2K
| Adam | epoch: 011 | loss: 0.20979 - acc: 0.9442 -- iter: 064/224
[A[ATraining Step: 73  | total loss: [1m[32m0.19535[0m[0m | time: 25.212s
[2K
| Adam | epoch: 011 | loss: 0.19535 - acc: 0.9469 -- iter: 096/224
[A[ATraining Step: 74  | total loss: [1m[32m0.18014[0m[0m | time: 33.800s
[2K
| Adam | epoch: 011 | loss: 0.18014 - acc: 0.9527 -- iter: 128/224
[A[ATraining Step: 75  | total loss: [1m[32m0.17380[0m[0m | time: 42.650s
[2K
| Adam | epoch: 011 | loss: 0.17380 - acc: 0.9511 -- iter: 160/224
[A[ATraining Step: 76  | total loss: [1m[32m0.17239[0m[0m | time: 51.453s
[2K
| Adam | epoch: 011 | loss: 0.17239 - acc: 0.9530 -- iter: 192/224
[A[ATraining Step: 77  | total loss: [1m[32m0.16421[0m[0m | time: 63.128s
[2K
| Adam | epoch: 011 | loss: 0.16421 - acc: 0.9547 | val_loss: 0.69512 - val_acc: 0.7571 -- iter: 224/224
--
Training Step: 78  | total loss: [1m[32m0.14901[0m[0m | time: 8.466s
[2K
| Adam | epoch: 012 | loss: 0.14901 - acc: 0.9594 -- iter: 032/224
[A[ATraining Step: 79  | total loss: [1m[32m0.14379[0m[0m | time: 17.091s
[2K
| Adam | epoch: 012 | loss: 0.14379 - acc: 0.9571 -- iter: 064/224
[A[ATraining Step: 80  | total loss: [1m[32m0.16613[0m[0m | time: 25.872s
[2K
| Adam | epoch: 012 | loss: 0.16613 - acc: 0.9551 -- iter: 096/224
[A[ATraining Step: 81  | total loss: [1m[32m0.15242[0m[0m | time: 34.247s
[2K
| Adam | epoch: 012 | loss: 0.15242 - acc: 0.9597 -- iter: 128/224
[A[ATraining Step: 82  | total loss: [1m[32m0.13842[0m[0m | time: 42.911s
[2K
| Adam | epoch: 012 | loss: 0.13842 - acc: 0.9637 -- iter: 160/224
[A[ATraining Step: 83  | total loss: [1m[32m0.12681[0m[0m | time: 51.678s
[2K
| Adam | epoch: 012 | loss: 0.12681 - acc: 0.9673 -- iter: 192/224
[A[ATraining Step: 84  | total loss: [1m[32m0.11780[0m[0m | time: 63.609s
[2K
| Adam | epoch: 012 | loss: 0.11780 - acc: 0.9706 | val_loss: 0.30163 - val_acc: 0.8857 -- iter: 224/224
--
Training Step: 85  | total loss: [1m[32m0.11507[0m[0m | time: 8.475s
[2K
| Adam | epoch: 013 | loss: 0.11507 - acc: 0.9673 -- iter: 032/224
[A[ATraining Step: 86  | total loss: [1m[32m0.10625[0m[0m | time: 16.883s
[2K
| Adam | epoch: 013 | loss: 0.10625 - acc: 0.9706 -- iter: 064/224
[A[ATraining Step: 87  | total loss: [1m[32m0.10723[0m[0m | time: 25.325s
[2K
| Adam | epoch: 013 | loss: 0.10723 - acc: 0.9704 -- iter: 096/224
[A[ATraining Step: 88  | total loss: [1m[32m0.09756[0m[0m | time: 33.876s
[2K
| Adam | epoch: 013 | loss: 0.09756 - acc: 0.9733 -- iter: 128/224
[A[ATraining Step: 89  | total loss: [1m[32m0.08866[0m[0m | time: 42.379s
[2K
| Adam | epoch: 013 | loss: 0.08866 - acc: 0.9760 -- iter: 160/224
[A[ATraining Step: 90  | total loss: [1m[32m0.08242[0m[0m | time: 50.719s
[2K
| Adam | epoch: 013 | loss: 0.08242 - acc: 0.9784 -- iter: 192/224
[A[ATraining Step: 91  | total loss: [1m[32m0.07648[0m[0m | time: 62.828s
[2K
| Adam | epoch: 013 | loss: 0.07648 - acc: 0.9806 | val_loss: 0.59534 - val_acc: 0.8143 -- iter: 224/224
--
Training Step: 92  | total loss: [1m[32m0.07102[0m[0m | time: 8.360s
[2K
| Adam | epoch: 014 | loss: 0.07102 - acc: 0.9825 -- iter: 032/224
[A[ATraining Step: 93  | total loss: [1m[32m0.06495[0m[0m | time: 16.795s
[2K
| Adam | epoch: 014 | loss: 0.06495 - acc: 0.9843 -- iter: 064/224
[A[ATraining Step: 94  | total loss: [1m[32m0.05931[0m[0m | time: 25.107s
[2K
| Adam | epoch: 014 | loss: 0.05931 - acc: 0.9858 -- iter: 096/224
[A[ATraining Step: 95  | total loss: [1m[32m0.05487[0m[0m | time: 33.434s
[2K
| Adam | epoch: 014 | loss: 0.05487 - acc: 0.9872 -- iter: 128/224
[A[ATraining Step: 96  | total loss: [1m[32m0.04988[0m[0m | time: 41.622s
[2K
| Adam | epoch: 014 | loss: 0.04988 - acc: 0.9885 -- iter: 160/224
[A[ATraining Step: 97  | total loss: [1m[32m0.04536[0m[0m | time: 49.875s
[2K
| Adam | epoch: 014 | loss: 0.04536 - acc: 0.9897 -- iter: 192/224
[A[ATraining Step: 98  | total loss: [1m[32m0.04129[0m[0m | time: 61.922s
[2K
| Adam | epoch: 014 | loss: 0.04129 - acc: 0.9907 | val_loss: 0.13949 - val_acc: 0.9571 -- iter: 224/224
--
Training Step: 99  | total loss: [1m[32m0.03805[0m[0m | time: 8.148s
[2K
| Adam | epoch: 015 | loss: 0.03805 - acc: 0.9916 -- iter: 032/224
[A[ATraining Step: 100  | total loss: [1m[32m0.03556[0m[0m | time: 16.530s
[2K
| Adam | epoch: 015 | loss: 0.03556 - acc: 0.9925 -- iter: 064/224
[A[ATraining Step: 101  | total loss: [1m[32m0.03264[0m[0m | time: 25.036s
[2K
| Adam | epoch: 015 | loss: 0.03264 - acc: 0.9932 -- iter: 096/224
[A[ATraining Step: 102  | total loss: [1m[32m0.02960[0m[0m | time: 33.771s
[2K
| Adam | epoch: 015 | loss: 0.02960 - acc: 0.9939 -- iter: 128/224
[A[ATraining Step: 103  | total loss: [1m[32m0.02698[0m[0m | time: 42.078s
[2K
| Adam | epoch: 015 | loss: 0.02698 - acc: 0.9945 -- iter: 160/224
[A[ATraining Step: 104  | total loss: [1m[32m0.02481[0m[0m | time: 50.445s
[2K
| Adam | epoch: 015 | loss: 0.02481 - acc: 0.9951 -- iter: 192/224
[A[ATraining Step: 105  | total loss: [1m[32m0.02264[0m[0m | time: 62.092s
[2K
| Adam | epoch: 015 | loss: 0.02264 - acc: 0.9956 | val_loss: 0.24924 - val_acc: 0.9429 -- iter: 224/224
--
Validation AUC:0.9812091503267972
Validation AUPRC:0.9864375593542261
Test AUC:0.9819078947368421
Test AUPRC:0.9809105144371115
BestTestF1Score	0.9	0.81	0.9	0.97	0.84	32	1	31	6	0.28
BestTestMCCScore	0.9	0.81	0.9	0.97	0.84	32	1	31	6	0.28
BestTestAccuracyScore	0.9	0.81	0.9	0.97	0.84	32	1	31	6	0.28
BestValidationF1Score	0.96	0.92	0.96	1.0	0.92	33	0	34	3	0.28
BestValidationMCC	0.96	0.92	0.96	1.0	0.92	33	0	34	3	0.28
BestValidationAccuracy	0.96	0.92	0.96	1.0	0.92	33	0	34	3	0.28
TestPredictions (Threshold:0.28)
CHEMBL310427,TN,INACT,0.0	CHEMBL355851,TN,INACT,0.009999999776482582	CHEMBL2164295,TP,ACT,0.8199999928474426	CHEMBL3613167,TP,ACT,1.0	CHEMBL1983100,TN,INACT,0.0	CHEMBL3613281,TP,ACT,1.0	CHEMBL2163542,TP,ACT,0.8600000143051147	CHEMBL351183,FP,INACT,1.0	CHEMBL40986,TN,INACT,0.0	CHEMBL1795711,TP,ACT,1.0	CHEMBL101516,TP,ACT,0.9200000166893005	CHEMBL172788,TN,INACT,0.0	CHEMBL536800,TN,INACT,0.0	CHEMBL2042403,TN,INACT,0.019999999552965164	CHEMBL116630,TP,ACT,1.0	CHEMBL2436046,TP,ACT,1.0	CHEMBL331716,TP,ACT,1.0	CHEMBL602269,TN,INACT,0.0	CHEMBL1907786,FN,ACT,0.019999999552965164	CHEMBL2369493,TN,INACT,0.05999999865889549	CHEMBL2436052,TP,ACT,1.0	CHEMBL3604278,TP,ACT,0.9900000095367432	CHEMBL246585,TN,INACT,0.0	CHEMBL148019,TP,ACT,1.0	CHEMBL281232,TN,INACT,0.0	CHEMBL2163543,TP,ACT,0.9900000095367432	CHEMBL165012,TN,INACT,0.0	CHEMBL293232,TN,INACT,0.03999999910593033	CHEMBL19808,TN,INACT,0.0	CHEMBL2436051,TP,ACT,1.0	CHEMBL104159,TP,ACT,1.0	CHEMBL2163537,TP,ACT,1.0	CHEMBL78830,TN,INACT,0.0	CHEMBL263031,FN,ACT,0.27000001072883606	CHEMBL542150,TP,ACT,0.9900000095367432	CHEMBL2164298,TP,ACT,0.8899999856948853	CHEMBL603858,TN,INACT,0.0	CHEMBL324119,TP,ACT,1.0	CHEMBL294231,TP,ACT,1.0	CHEMBL461088,TN,INACT,0.0	CHEMBL100729,FN,ACT,0.11999999731779099	CHEMBL1907789,TP,ACT,0.949999988079071	CHEMBL2112592,TN,INACT,0.0	CHEMBL415410,FN,ACT,0.09000000357627869	CHEMBL461502,TN,INACT,0.0	CHEMBL2436055,FN,ACT,0.12999999523162842	CHEMBL42586,TN,INACT,0.0	CHEMBL333621,FN,ACT,0.15000000596046448	CHEMBL42065,TN,INACT,0.0	CHEMBL21328,TN,INACT,0.0	CHEMBL407818,TN,INACT,0.0	CHEMBL424214,TN,INACT,0.03999999910593033	CHEMBL89203,TN,INACT,0.0	CHEMBL553155,TN,INACT,0.0	CHEMBL298203,TN,INACT,0.019999999552965164	CHEMBL2368133,TP,ACT,1.0	CHEMBL241514,TN,INACT,0.0	CHEMBL60407,TP,ACT,1.0	CHEMBL3780248,TN,INACT,0.0	CHEMBL2164296,TP,ACT,0.9700000286102295	CHEMBL119473,TP,ACT,1.0	CHEMBL2207681,TP,ACT,1.0	CHEMBL3604281,TP,ACT,0.9700000286102295	CHEMBL3613175,TP,ACT,1.0	CHEMBL277927,TP,ACT,1.0	CHEMBL2436054,TP,ACT,1.0	CHEMBL2368128,TP,ACT,1.0	CHEMBL42360,TN,INACT,0.0	CHEMBL595022,TN,INACT,0.0	CHEMBL2437067,TP,ACT,1.0	

