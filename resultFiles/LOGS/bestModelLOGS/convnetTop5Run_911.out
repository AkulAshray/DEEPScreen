ImageNetInceptionV2 CHEMBL1977 adam 0.0001 15 0 0 0.8 False True
Number of active compounds :	187
Number of inactive compounds :	187
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL1977_adam_0.0001_15_0_0_0.8_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL1977_adam_0.0001_15_0.8/
---------------------------------
Training samples: 212
Validation samples: 67
--
Training Step: 1  | time: 35.148s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/212
[A[ATraining Step: 2  | total loss: [1m[32m0.70655[0m[0m | time: 43.259s
[2K
| Adam | epoch: 001 | loss: 0.70655 - acc: 0.2812 -- iter: 064/212
[A[ATraining Step: 3  | total loss: [1m[32m0.72191[0m[0m | time: 51.225s
[2K
| Adam | epoch: 001 | loss: 0.72191 - acc: 0.4602 -- iter: 096/212
[A[ATraining Step: 4  | total loss: [1m[32m0.66573[0m[0m | time: 58.971s
[2K
| Adam | epoch: 001 | loss: 0.66573 - acc: 0.5838 -- iter: 128/212
[A[ATraining Step: 5  | total loss: [1m[32m0.64298[0m[0m | time: 66.995s
[2K
| Adam | epoch: 001 | loss: 0.64298 - acc: 0.5907 -- iter: 160/212
[A[ATraining Step: 6  | total loss: [1m[32m0.60644[0m[0m | time: 74.897s
[2K
| Adam | epoch: 001 | loss: 0.60644 - acc: 0.6529 -- iter: 192/212
[A[ATraining Step: 7  | total loss: [1m[32m0.53598[0m[0m | time: 89.470s
[2K
| Adam | epoch: 001 | loss: 0.53598 - acc: 0.7112 | val_loss: 0.69550 - val_acc: 0.4776 -- iter: 212/212
--
Training Step: 8  | total loss: [1m[32m0.43035[0m[0m | time: 5.600s
[2K
| Adam | epoch: 002 | loss: 0.43035 - acc: 0.8455 -- iter: 032/212
[A[ATraining Step: 9  | total loss: [1m[32m0.36021[0m[0m | time: 13.508s
[2K
| Adam | epoch: 002 | loss: 0.36021 - acc: 0.9273 -- iter: 064/212
[A[ATraining Step: 10  | total loss: [1m[32m0.43917[0m[0m | time: 21.456s
[2K
| Adam | epoch: 002 | loss: 0.43917 - acc: 0.8386 -- iter: 096/212
[A[ATraining Step: 11  | total loss: [1m[32m0.45638[0m[0m | time: 29.366s
[2K
| Adam | epoch: 002 | loss: 0.45638 - acc: 0.7967 -- iter: 128/212
[A[ATraining Step: 12  | total loss: [1m[32m0.36856[0m[0m | time: 37.263s
[2K
| Adam | epoch: 002 | loss: 0.36856 - acc: 0.8460 -- iter: 160/212
[A[ATraining Step: 13  | total loss: [1m[32m0.34534[0m[0m | time: 45.224s
[2K
| Adam | epoch: 002 | loss: 0.34534 - acc: 0.8718 -- iter: 192/212
[A[ATraining Step: 14  | total loss: [1m[32m0.30984[0m[0m | time: 56.235s
[2K
| Adam | epoch: 002 | loss: 0.30984 - acc: 0.8859 | val_loss: 0.70337 - val_acc: 0.4776 -- iter: 212/212
--
Training Step: 15  | total loss: [1m[32m0.36178[0m[0m | time: 5.440s
[2K
| Adam | epoch: 003 | loss: 0.36178 - acc: 0.8939 -- iter: 032/212
[A[ATraining Step: 16  | total loss: [1m[32m0.37304[0m[0m | time: 11.043s
[2K
| Adam | epoch: 003 | loss: 0.37304 - acc: 0.8774 -- iter: 064/212
[A[ATraining Step: 17  | total loss: [1m[32m0.30392[0m[0m | time: 18.893s
[2K
| Adam | epoch: 003 | loss: 0.30392 - acc: 0.9035 -- iter: 096/212
[A[ATraining Step: 18  | total loss: [1m[32m0.28931[0m[0m | time: 26.684s
[2K
| Adam | epoch: 003 | loss: 0.28931 - acc: 0.9045 -- iter: 128/212
[A[ATraining Step: 19  | total loss: [1m[32m0.22047[0m[0m | time: 34.604s
[2K
| Adam | epoch: 003 | loss: 0.22047 - acc: 0.9363 -- iter: 160/212
[A[ATraining Step: 20  | total loss: [1m[32m0.19355[0m[0m | time: 42.415s
[2K
| Adam | epoch: 003 | loss: 0.19355 - acc: 0.9467 -- iter: 192/212
[A[ATraining Step: 21  | total loss: [1m[32m0.17172[0m[0m | time: 53.328s
[2K
| Adam | epoch: 003 | loss: 0.17172 - acc: 0.9536 | val_loss: 0.91283 - val_acc: 0.4776 -- iter: 212/212
--
Training Step: 22  | total loss: [1m[32m0.15090[0m[0m | time: 8.019s
[2K
| Adam | epoch: 004 | loss: 0.15090 - acc: 0.9675 -- iter: 032/212
[A[ATraining Step: 23  | total loss: [1m[32m0.14749[0m[0m | time: 13.420s
[2K
| Adam | epoch: 004 | loss: 0.14749 - acc: 0.9679 -- iter: 064/212
[A[ATraining Step: 24  | total loss: [1m[32m0.11612[0m[0m | time: 18.873s
[2K
| Adam | epoch: 004 | loss: 0.11612 - acc: 0.9769 -- iter: 096/212
[A[ATraining Step: 25  | total loss: [1m[32m0.09236[0m[0m | time: 26.670s
[2K
| Adam | epoch: 004 | loss: 0.09236 - acc: 0.9832 -- iter: 128/212
[A[ATraining Step: 26  | total loss: [1m[32m0.08180[0m[0m | time: 34.422s
[2K
| Adam | epoch: 004 | loss: 0.08180 - acc: 0.9794 -- iter: 160/212
[A[ATraining Step: 27  | total loss: [1m[32m0.08349[0m[0m | time: 42.310s
[2K
| Adam | epoch: 004 | loss: 0.08349 - acc: 0.9766 -- iter: 192/212
[A[ATraining Step: 28  | total loss: [1m[32m0.07144[0m[0m | time: 53.148s
[2K
| Adam | epoch: 004 | loss: 0.07144 - acc: 0.9825 | val_loss: 1.14525 - val_acc: 0.4776 -- iter: 212/212
--
Training Step: 29  | total loss: [1m[32m0.06188[0m[0m | time: 7.782s
[2K
| Adam | epoch: 005 | loss: 0.06188 - acc: 0.9867 -- iter: 032/212
[A[ATraining Step: 30  | total loss: [1m[32m0.05870[0m[0m | time: 15.677s
[2K
| Adam | epoch: 005 | loss: 0.05870 - acc: 0.9825 -- iter: 064/212
[A[ATraining Step: 31  | total loss: [1m[32m0.04916[0m[0m | time: 21.080s
[2K
| Adam | epoch: 005 | loss: 0.04916 - acc: 0.9865 -- iter: 096/212
[A[ATraining Step: 32  | total loss: [1m[32m0.04163[0m[0m | time: 26.451s
[2K
| Adam | epoch: 005 | loss: 0.04163 - acc: 0.9896 -- iter: 128/212
[A[ATraining Step: 33  | total loss: [1m[32m0.03485[0m[0m | time: 34.327s
[2K
| Adam | epoch: 005 | loss: 0.03485 - acc: 0.9918 -- iter: 160/212
[A[ATraining Step: 34  | total loss: [1m[32m0.02909[0m[0m | time: 42.127s
[2K
| Adam | epoch: 005 | loss: 0.02909 - acc: 0.9936 -- iter: 192/212
[A[ATraining Step: 35  | total loss: [1m[32m0.02482[0m[0m | time: 53.163s
[2K
| Adam | epoch: 005 | loss: 0.02482 - acc: 0.9949 | val_loss: 0.91462 - val_acc: 0.4776 -- iter: 212/212
--
Training Step: 36  | total loss: [1m[32m0.04485[0m[0m | time: 7.852s
[2K
| Adam | epoch: 006 | loss: 0.04485 - acc: 0.9896 -- iter: 032/212
[A[ATraining Step: 37  | total loss: [1m[32m0.03708[0m[0m | time: 15.589s
[2K
| Adam | epoch: 006 | loss: 0.03708 - acc: 0.9917 -- iter: 064/212
[A[ATraining Step: 38  | total loss: [1m[32m0.04300[0m[0m | time: 23.479s
[2K
| Adam | epoch: 006 | loss: 0.04300 - acc: 0.9872 -- iter: 096/212
[A[ATraining Step: 39  | total loss: [1m[32m0.03657[0m[0m | time: 28.896s
[2K
| Adam | epoch: 006 | loss: 0.03657 - acc: 0.9896 -- iter: 128/212
[A[ATraining Step: 40  | total loss: [1m[32m0.07756[0m[0m | time: 34.366s
[2K
| Adam | epoch: 006 | loss: 0.07756 - acc: 0.9822 -- iter: 160/212
[A[ATraining Step: 41  | total loss: [1m[32m0.06579[0m[0m | time: 42.264s
[2K
| Adam | epoch: 006 | loss: 0.06579 - acc: 0.9855 -- iter: 192/212
[A[ATraining Step: 42  | total loss: [1m[32m0.05615[0m[0m | time: 53.012s
[2K
| Adam | epoch: 006 | loss: 0.05615 - acc: 0.9881 | val_loss: 1.06166 - val_acc: 0.4776 -- iter: 212/212
--
Training Step: 43  | total loss: [1m[32m0.04869[0m[0m | time: 7.852s
[2K
| Adam | epoch: 007 | loss: 0.04869 - acc: 0.9902 -- iter: 032/212
[A[ATraining Step: 44  | total loss: [1m[32m0.04159[0m[0m | time: 15.738s
[2K
| Adam | epoch: 007 | loss: 0.04159 - acc: 0.9919 -- iter: 064/212
[A[ATraining Step: 45  | total loss: [1m[32m0.06024[0m[0m | time: 23.705s
[2K
| Adam | epoch: 007 | loss: 0.06024 - acc: 0.9880 -- iter: 096/212
[A[ATraining Step: 46  | total loss: [1m[32m0.05107[0m[0m | time: 31.410s
[2K
| Adam | epoch: 007 | loss: 0.05107 - acc: 0.9900 -- iter: 128/212
[A[ATraining Step: 47  | total loss: [1m[32m0.07612[0m[0m | time: 36.755s
[2K
| Adam | epoch: 007 | loss: 0.07612 - acc: 0.9865 -- iter: 160/212
[A[ATraining Step: 48  | total loss: [1m[32m0.09951[0m[0m | time: 42.327s
[2K
| Adam | epoch: 007 | loss: 0.09951 - acc: 0.9806 -- iter: 192/212
[A[ATraining Step: 49  | total loss: [1m[32m0.08443[0m[0m | time: 53.265s
[2K
| Adam | epoch: 007 | loss: 0.08443 - acc: 0.9837 | val_loss: 1.77622 - val_acc: 0.4776 -- iter: 212/212
--
Training Step: 50  | total loss: [1m[32m0.07190[0m[0m | time: 7.786s
[2K
| Adam | epoch: 008 | loss: 0.07190 - acc: 0.9862 -- iter: 032/212
[A[ATraining Step: 51  | total loss: [1m[32m0.06236[0m[0m | time: 15.603s
[2K
| Adam | epoch: 008 | loss: 0.06236 - acc: 0.9883 -- iter: 064/212
[A[ATraining Step: 52  | total loss: [1m[32m0.05838[0m[0m | time: 23.441s
[2K
| Adam | epoch: 008 | loss: 0.05838 - acc: 0.9854 -- iter: 096/212
[A[ATraining Step: 53  | total loss: [1m[32m0.05095[0m[0m | time: 31.096s
[2K
| Adam | epoch: 008 | loss: 0.05095 - acc: 0.9875 -- iter: 128/212
[A[ATraining Step: 54  | total loss: [1m[32m0.05741[0m[0m | time: 39.061s
[2K
| Adam | epoch: 008 | loss: 0.05741 - acc: 0.9848 -- iter: 160/212
[A[ATraining Step: 55  | total loss: [1m[32m0.09505[0m[0m | time: 44.367s
[2K
| Adam | epoch: 008 | loss: 0.09505 - acc: 0.9736 -- iter: 192/212
[A[ATraining Step: 56  | total loss: [1m[32m0.08696[0m[0m | time: 52.915s
[2K
| Adam | epoch: 008 | loss: 0.08696 - acc: 0.9773 | val_loss: 1.59304 - val_acc: 0.4776 -- iter: 212/212
--
Training Step: 57  | total loss: [1m[32m0.07622[0m[0m | time: 7.940s
[2K
| Adam | epoch: 009 | loss: 0.07622 - acc: 0.9804 -- iter: 032/212
[A[ATraining Step: 58  | total loss: [1m[32m0.09492[0m[0m | time: 15.657s
[2K
| Adam | epoch: 009 | loss: 0.09492 - acc: 0.9703 -- iter: 064/212
[A[ATraining Step: 59  | total loss: [1m[32m0.08670[0m[0m | time: 23.489s
[2K
| Adam | epoch: 009 | loss: 0.08670 - acc: 0.9701 -- iter: 096/212
[A[ATraining Step: 60  | total loss: [1m[32m0.07665[0m[0m | time: 31.313s
[2K
| Adam | epoch: 009 | loss: 0.07665 - acc: 0.9741 -- iter: 128/212
[A[ATraining Step: 61  | total loss: [1m[32m0.07048[0m[0m | time: 39.070s
[2K
| Adam | epoch: 009 | loss: 0.07048 - acc: 0.9775 -- iter: 160/212
[A[ATraining Step: 62  | total loss: [1m[32m0.06306[0m[0m | time: 46.879s
[2K
| Adam | epoch: 009 | loss: 0.06306 - acc: 0.9804 -- iter: 192/212
[A[ATraining Step: 63  | total loss: [1m[32m0.05631[0m[0m | time: 55.480s
[2K
| Adam | epoch: 009 | loss: 0.05631 - acc: 0.9828 | val_loss: 0.42441 - val_acc: 0.8209 -- iter: 212/212
--
Training Step: 64  | total loss: [1m[32m0.05125[0m[0m | time: 6.612s
[2K
| Adam | epoch: 010 | loss: 0.05125 - acc: 0.9850 -- iter: 032/212
[A[ATraining Step: 65  | total loss: [1m[32m0.04757[0m[0m | time: 16.379s
[2K
| Adam | epoch: 010 | loss: 0.04757 - acc: 0.9868 -- iter: 064/212
[A[ATraining Step: 66  | total loss: [1m[32m0.04335[0m[0m | time: 26.004s
[2K
| Adam | epoch: 010 | loss: 0.04335 - acc: 0.9884 -- iter: 096/212
[A[ATraining Step: 67  | total loss: [1m[32m0.04017[0m[0m | time: 36.154s
[2K
| Adam | epoch: 010 | loss: 0.04017 - acc: 0.9898 -- iter: 128/212
[A[ATraining Step: 68  | total loss: [1m[32m0.03629[0m[0m | time: 45.998s
[2K
| Adam | epoch: 010 | loss: 0.03629 - acc: 0.9910 -- iter: 160/212
[A[ATraining Step: 69  | total loss: [1m[32m0.03306[0m[0m | time: 55.611s
[2K
| Adam | epoch: 010 | loss: 0.03306 - acc: 0.9921 -- iter: 192/212
[A[ATraining Step: 70  | total loss: [1m[32m0.03061[0m[0m | time: 69.204s
[2K
| Adam | epoch: 010 | loss: 0.03061 - acc: 0.9930 | val_loss: 0.69451 - val_acc: 0.7463 -- iter: 212/212
--
Training Step: 71  | total loss: [1m[32m0.03471[0m[0m | time: 6.539s
[2K
| Adam | epoch: 011 | loss: 0.03471 - acc: 0.9902 -- iter: 032/212
[A[ATraining Step: 72  | total loss: [1m[32m0.03135[0m[0m | time: 13.305s
[2K
| Adam | epoch: 011 | loss: 0.03135 - acc: 0.9913 -- iter: 064/212
[A[ATraining Step: 73  | total loss: [1m[32m0.02837[0m[0m | time: 23.139s
[2K
| Adam | epoch: 011 | loss: 0.02837 - acc: 0.9923 -- iter: 096/212
[A[ATraining Step: 74  | total loss: [1m[32m0.02616[0m[0m | time: 32.820s
[2K
| Adam | epoch: 011 | loss: 0.02616 - acc: 0.9931 -- iter: 128/212
[A[ATraining Step: 75  | total loss: [1m[32m0.02468[0m[0m | time: 42.776s
[2K
| Adam | epoch: 011 | loss: 0.02468 - acc: 0.9939 -- iter: 160/212
[A[ATraining Step: 76  | total loss: [1m[32m0.02312[0m[0m | time: 52.778s
[2K
| Adam | epoch: 011 | loss: 0.02312 - acc: 0.9945 -- iter: 192/212
[A[ATraining Step: 77  | total loss: [1m[32m0.02363[0m[0m | time: 66.380s
[2K
| Adam | epoch: 011 | loss: 0.02363 - acc: 0.9951 | val_loss: 0.51248 - val_acc: 0.8358 -- iter: 212/212
--
Training Step: 78  | total loss: [1m[32m0.02164[0m[0m | time: 9.660s
[2K
| Adam | epoch: 012 | loss: 0.02164 - acc: 0.9956 -- iter: 032/212
[A[ATraining Step: 79  | total loss: [1m[32m0.02271[0m[0m | time: 16.363s
[2K
| Adam | epoch: 012 | loss: 0.02271 - acc: 0.9928 -- iter: 064/212
[A[ATraining Step: 80  | total loss: [1m[32m0.05493[0m[0m | time: 23.143s
[2K
| Adam | epoch: 012 | loss: 0.05493 - acc: 0.9885 -- iter: 096/212
[A[ATraining Step: 81  | total loss: [1m[32m0.04971[0m[0m | time: 33.006s
[2K
| Adam | epoch: 012 | loss: 0.04971 - acc: 0.9896 -- iter: 128/212
[A[ATraining Step: 82  | total loss: [1m[32m0.04495[0m[0m | time: 42.958s
[2K
| Adam | epoch: 012 | loss: 0.04495 - acc: 0.9907 -- iter: 160/212
[A[ATraining Step: 83  | total loss: [1m[32m0.04076[0m[0m | time: 51.796s
[2K
| Adam | epoch: 012 | loss: 0.04076 - acc: 0.9916 -- iter: 192/212
[A[ATraining Step: 84  | total loss: [1m[32m0.03846[0m[0m | time: 62.805s
[2K
| Adam | epoch: 012 | loss: 0.03846 - acc: 0.9924 | val_loss: 1.61055 - val_acc: 0.6418 -- iter: 212/212
--
Training Step: 85  | total loss: [1m[32m0.03971[0m[0m | time: 7.744s
[2K
| Adam | epoch: 013 | loss: 0.03971 - acc: 0.9901 -- iter: 032/212
[A[ATraining Step: 86  | total loss: [1m[32m0.08765[0m[0m | time: 15.364s
[2K
| Adam | epoch: 013 | loss: 0.08765 - acc: 0.9879 -- iter: 064/212
[A[ATraining Step: 87  | total loss: [1m[32m0.07930[0m[0m | time: 20.798s
[2K
| Adam | epoch: 013 | loss: 0.07930 - acc: 0.9891 -- iter: 096/212
[A[ATraining Step: 88  | total loss: [1m[32m0.10475[0m[0m | time: 26.154s
[2K
| Adam | epoch: 013 | loss: 0.10475 - acc: 0.9852 -- iter: 128/212
[A[ATraining Step: 89  | total loss: [1m[32m0.09455[0m[0m | time: 33.920s
[2K
| Adam | epoch: 013 | loss: 0.09455 - acc: 0.9867 -- iter: 160/212
[A[ATraining Step: 90  | total loss: [1m[32m0.08690[0m[0m | time: 41.794s
[2K
| Adam | epoch: 013 | loss: 0.08690 - acc: 0.9880 -- iter: 192/212
[A[ATraining Step: 91  | total loss: [1m[32m0.07871[0m[0m | time: 52.558s
[2K
| Adam | epoch: 013 | loss: 0.07871 - acc: 0.9892 | val_loss: 1.64472 - val_acc: 0.6418 -- iter: 212/212
--
Training Step: 92  | total loss: [1m[32m0.07155[0m[0m | time: 7.850s
[2K
| Adam | epoch: 014 | loss: 0.07155 - acc: 0.9903 -- iter: 032/212
[A[ATraining Step: 93  | total loss: [1m[32m0.06503[0m[0m | time: 15.791s
[2K
| Adam | epoch: 014 | loss: 0.06503 - acc: 0.9913 -- iter: 064/212
[A[ATraining Step: 94  | total loss: [1m[32m0.06065[0m[0m | time: 23.568s
[2K
| Adam | epoch: 014 | loss: 0.06065 - acc: 0.9922 -- iter: 096/212
[A[ATraining Step: 95  | total loss: [1m[32m0.05699[0m[0m | time: 29.173s
[2K
| Adam | epoch: 014 | loss: 0.05699 - acc: 0.9929 -- iter: 128/212
[A[ATraining Step: 96  | total loss: [1m[32m0.15359[0m[0m | time: 34.489s
[2K
| Adam | epoch: 014 | loss: 0.15359 - acc: 0.9836 -- iter: 160/212
[A[ATraining Step: 97  | total loss: [1m[32m0.14574[0m[0m | time: 42.262s
[2K
| Adam | epoch: 014 | loss: 0.14574 - acc: 0.9853 -- iter: 192/212
[A[ATraining Step: 98  | total loss: [1m[32m0.14087[0m[0m | time: 53.110s
[2K
| Adam | epoch: 014 | loss: 0.14087 - acc: 0.9836 | val_loss: 1.08632 - val_acc: 0.6866 -- iter: 212/212
--
Training Step: 99  | total loss: [1m[32m0.12845[0m[0m | time: 7.766s
[2K
| Adam | epoch: 015 | loss: 0.12845 - acc: 0.9853 -- iter: 032/212
[A[ATraining Step: 100  | total loss: [1m[32m0.11719[0m[0m | time: 15.476s
[2K
| Adam | epoch: 015 | loss: 0.11719 - acc: 0.9867 -- iter: 064/212
[A[ATraining Step: 101  | total loss: [1m[32m0.10644[0m[0m | time: 23.290s
[2K
| Adam | epoch: 015 | loss: 0.10644 - acc: 0.9881 -- iter: 096/212
[A[ATraining Step: 102  | total loss: [1m[32m0.11123[0m[0m | time: 30.957s
[2K
| Adam | epoch: 015 | loss: 0.11123 - acc: 0.9830 -- iter: 128/212
[A[ATraining Step: 103  | total loss: [1m[32m0.10186[0m[0m | time: 36.406s
[2K
| Adam | epoch: 015 | loss: 0.10186 - acc: 0.9847 -- iter: 160/212
[A[ATraining Step: 104  | total loss: [1m[32m0.09290[0m[0m | time: 41.809s
[2K
| Adam | epoch: 015 | loss: 0.09290 - acc: 0.9862 -- iter: 192/212
[A[ATraining Step: 105  | total loss: [1m[32m0.08486[0m[0m | time: 52.661s
[2K
| Adam | epoch: 015 | loss: 0.08486 - acc: 0.9876 | val_loss: 0.52099 - val_acc: 0.8060 -- iter: 212/212
--
Validation AUC:0.8758928571428571
Validation AUPRC:0.8719583635156651
Test AUC:0.9678571428571429
Test AUPRC:0.9782643826761475
BestTestF1Score	0.93	0.85	0.93	0.92	0.94	33	3	29	2	0.15
BestTestMCCScore	0.89	0.81	0.9	1.0	0.8	28	0	32	7	0.85
BestTestAccuracyScore	0.89	0.81	0.9	1.0	0.8	28	0	32	7	0.85
BestValidationF1Score	0.82	0.59	0.79	0.74	0.91	32	11	21	3	0.15
BestValidationMCC	0.81	0.66	0.82	0.93	0.71	25	2	30	10	0.85
BestValidationAccuracy	0.81	0.66	0.82	0.93	0.71	25	2	30	10	0.85
TestPredictions (Threshold:0.85)
CHEMBL3746222,TP,ACT,1.0	CHEMBL492749,TP,ACT,0.9900000095367432	CHEMBL295416,TN,INACT,0.019999999552965164	CHEMBL2437295,TP,ACT,0.9800000190734863	CHEMBL1164992,TP,ACT,0.9900000095367432	CHEMBL2070838,TN,INACT,0.009999999776482582	CHEMBL1164212,TP,ACT,0.9399999976158142	CHEMBL333890,TN,INACT,0.07999999821186066	CHEMBL3747246,TP,ACT,0.9900000095367432	CHEMBL3108789,FN,ACT,0.5199999809265137	CHEMBL2437045,TP,ACT,0.8600000143051147	CHEMBL2348887,TN,INACT,0.009999999776482582	CHEMBL1257895,TN,INACT,0.009999999776482582	CHEMBL477940,TN,INACT,0.009999999776482582	CHEMBL3747243,TP,ACT,0.9200000166893005	CHEMBL2437291,FN,ACT,0.019999999552965164	CHEMBL3264165,TP,ACT,0.9900000095367432	CHEMBL2058273,TP,ACT,1.0	CHEMBL3263871,TP,ACT,1.0	CHEMBL2172538,FN,ACT,0.5299999713897705	CHEMBL1956370,TN,INACT,0.3199999928474426	CHEMBL236134,TN,INACT,0.019999999552965164	CHEMBL393978,TP,ACT,0.9900000095367432	CHEMBL2070865,TN,INACT,0.12999999523162842	CHEMBL2437041,TP,ACT,0.9300000071525574	CHEMBL494338,TP,ACT,0.9900000095367432	CHEMBL2171447,TP,ACT,1.0	CHEMBL489597,TN,INACT,0.17000000178813934	CHEMBL398106,TN,INACT,0.009999999776482582	CHEMBL491039,TN,INACT,0.10000000149011612	CHEMBL2204685,TN,INACT,0.019999999552965164	CHEMBL235491,TN,INACT,0.009999999776482582	CHEMBL1165082,TP,ACT,0.9900000095367432	CHEMBL192490,TN,INACT,0.019999999552965164	CHEMBL2437300,TN,INACT,0.009999999776482582	CHEMBL2348882,TN,INACT,0.12999999523162842	CHEMBL1173380,TN,INACT,0.009999999776482582	CHEMBL3335625,TN,INACT,0.38999998569488525	CHEMBL2070858,FN,ACT,0.6200000047683716	CHEMBL3746880,TP,ACT,1.0	CHEMBL3746078,TP,ACT,0.9900000095367432	CHEMBL111077,TN,INACT,0.009999999776482582	CHEMBL846,TP,ACT,1.0	CHEMBL1956359,TN,INACT,0.07000000029802322	CHEMBL1834893,TP,ACT,0.8799999952316284	CHEMBL237600,TN,INACT,0.03999999910593033	CHEMBL520305,TN,INACT,0.009999999776482582	CHEMBL2348912,TN,INACT,0.03999999910593033	CHEMBL2437049,TP,ACT,0.9700000286102295	CHEMBL2204691,TN,INACT,0.009999999776482582	CHEMBL3746829,TP,ACT,0.9800000190734863	CHEMBL272304,TP,ACT,0.9900000095367432	CHEMBL1327417,TN,INACT,0.009999999776482582	CHEMBL236543,TN,INACT,0.009999999776482582	CHEMBL107498,TN,INACT,0.0	CHEMBL1834894,TP,ACT,0.8899999856948853	CHEMBL3747701,TP,ACT,0.8600000143051147	CHEMBL394913,TN,INACT,0.0	CHEMBL235113,TN,INACT,0.019999999552965164	CHEMBL2204698,TN,INACT,0.05000000074505806	CHEMBL1164243,TP,ACT,0.9900000095367432	CHEMBL2112315,TP,ACT,0.9300000071525574	CHEMBL2172537,FN,ACT,0.20999999344348907	CHEMBL2437042,TP,ACT,0.9700000286102295	CHEMBL2437302,FN,ACT,0.019999999552965164	CHEMBL3109612,TN,INACT,0.12999999523162842	CHEMBL2172539,FN,ACT,0.1599999964237213	

