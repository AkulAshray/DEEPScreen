ImageNetInceptionV2 CHEMBL4079 adam 0.0005 15 0 0 0.8 False True
Number of active compounds :	174
Number of inactive compounds :	174
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL4079_adam_0.0005_15_0_0_0.8_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL4079_adam_0.0005_15_0.8/
---------------------------------
Training samples: 211
Validation samples: 67
--
Training Step: 1  | time: 121.982s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/211
[A[ATraining Step: 2  | total loss: [1m[32m0.62505[0m[0m | time: 213.933s
[2K
| Adam | epoch: 001 | loss: 0.62505 - acc: 0.4500 -- iter: 064/211
[A[ATraining Step: 3  | total loss: [1m[32m0.67602[0m[0m | time: 291.410s
[2K
| Adam | epoch: 001 | loss: 0.67602 - acc: 0.6443 -- iter: 096/211
[A[ATraining Step: 4  | total loss: [1m[32m1.04605[0m[0m | time: 358.714s
[2K
| Adam | epoch: 001 | loss: 1.04605 - acc: 0.5361 -- iter: 128/211
[A[ATraining Step: 5  | total loss: [1m[32m0.68640[0m[0m | time: 421.401s
[2K
| Adam | epoch: 001 | loss: 0.68640 - acc: 0.7707 -- iter: 160/211
[A[ATraining Step: 6  | total loss: [1m[32m0.65949[0m[0m | time: 485.858s
[2K
| Adam | epoch: 001 | loss: 0.65949 - acc: 0.6570 -- iter: 192/211
[A[ATraining Step: 7  | total loss: [1m[32m0.65371[0m[0m | time: 510.243s
[2K
| Adam | epoch: 001 | loss: 0.65371 - acc: 0.6565 | val_loss: 0.94904 - val_acc: 0.3881 -- iter: 211/211
--
Training Step: 8  | total loss: [1m[32m0.66517[0m[0m | time: 6.984s
[2K
| Adam | epoch: 002 | loss: 0.66517 - acc: 0.7313 -- iter: 032/211
[A[ATraining Step: 9  | total loss: [1m[32m0.45510[0m[0m | time: 76.598s
[2K
| Adam | epoch: 002 | loss: 0.45510 - acc: 0.8457 -- iter: 064/211
[A[ATraining Step: 10  | total loss: [1m[32m0.45115[0m[0m | time: 88.738s
[2K
| Adam | epoch: 002 | loss: 0.45115 - acc: 0.8603 -- iter: 096/211
[A[ATraining Step: 11  | total loss: [1m[32m0.43031[0m[0m | time: 97.808s
[2K
| Adam | epoch: 002 | loss: 0.43031 - acc: 0.8229 -- iter: 128/211
[A[ATraining Step: 12  | total loss: [1m[32m0.43527[0m[0m | time: 106.866s
[2K
| Adam | epoch: 002 | loss: 0.43527 - acc: 0.8182 -- iter: 160/211
[A[ATraining Step: 13  | total loss: [1m[32m0.35590[0m[0m | time: 115.797s
[2K
| Adam | epoch: 002 | loss: 0.35590 - acc: 0.8693 -- iter: 192/211
[A[ATraining Step: 14  | total loss: [1m[32m0.31407[0m[0m | time: 127.977s
[2K
| Adam | epoch: 002 | loss: 0.31407 - acc: 0.8589 | val_loss: 1.07073 - val_acc: 0.3881 -- iter: 211/211
--
Training Step: 15  | total loss: [1m[32m0.26357[0m[0m | time: 5.963s
[2K
| Adam | epoch: 003 | loss: 0.26357 - acc: 0.9019 -- iter: 032/211
[A[ATraining Step: 16  | total loss: [1m[32m0.29430[0m[0m | time: 11.966s
[2K
| Adam | epoch: 003 | loss: 0.29430 - acc: 0.8992 -- iter: 064/211
[A[ATraining Step: 17  | total loss: [1m[32m0.21793[0m[0m | time: 21.029s
[2K
| Adam | epoch: 003 | loss: 0.21793 - acc: 0.9355 -- iter: 096/211
[A[ATraining Step: 18  | total loss: [1m[32m0.24521[0m[0m | time: 30.193s
[2K
| Adam | epoch: 003 | loss: 0.24521 - acc: 0.9037 -- iter: 128/211
[A[ATraining Step: 19  | total loss: [1m[32m0.21525[0m[0m | time: 39.418s
[2K
| Adam | epoch: 003 | loss: 0.21525 - acc: 0.9254 -- iter: 160/211
[A[ATraining Step: 20  | total loss: [1m[32m0.16504[0m[0m | time: 48.607s
[2K
| Adam | epoch: 003 | loss: 0.16504 - acc: 0.9494 -- iter: 192/211
[A[ATraining Step: 21  | total loss: [1m[32m0.13712[0m[0m | time: 60.468s
[2K
| Adam | epoch: 003 | loss: 0.13712 - acc: 0.9554 | val_loss: 0.68460 - val_acc: 0.5970 -- iter: 211/211
--
Training Step: 22  | total loss: [1m[32m0.15380[0m[0m | time: 9.072s
[2K
| Adam | epoch: 004 | loss: 0.15380 - acc: 0.9406 -- iter: 032/211
[A[ATraining Step: 23  | total loss: [1m[32m0.16421[0m[0m | time: 15.017s
[2K
| Adam | epoch: 004 | loss: 0.16421 - acc: 0.9488 -- iter: 064/211
[A[ATraining Step: 24  | total loss: [1m[32m0.15705[0m[0m | time: 20.814s
[2K
| Adam | epoch: 004 | loss: 0.15705 - acc: 0.9484 -- iter: 096/211
[A[ATraining Step: 25  | total loss: [1m[32m0.11777[0m[0m | time: 30.090s
[2K
| Adam | epoch: 004 | loss: 0.11777 - acc: 0.9625 -- iter: 128/211
[A[ATraining Step: 26  | total loss: [1m[32m0.12200[0m[0m | time: 38.645s
[2K
| Adam | epoch: 004 | loss: 0.12200 - acc: 0.9641 -- iter: 160/211
[A[ATraining Step: 27  | total loss: [1m[32m0.13294[0m[0m | time: 47.443s
[2K
| Adam | epoch: 004 | loss: 0.13294 - acc: 0.9573 -- iter: 192/211
[A[ATraining Step: 28  | total loss: [1m[32m0.11596[0m[0m | time: 59.325s
[2K
| Adam | epoch: 004 | loss: 0.11596 - acc: 0.9602 | val_loss: 0.70049 - val_acc: 0.4776 -- iter: 211/211
--
Training Step: 29  | total loss: [1m[32m0.16032[0m[0m | time: 8.873s
[2K
| Adam | epoch: 005 | loss: 0.16032 - acc: 0.9318 -- iter: 032/211
[A[ATraining Step: 30  | total loss: [1m[32m0.14328[0m[0m | time: 17.774s
[2K
| Adam | epoch: 005 | loss: 0.14328 - acc: 0.9406 -- iter: 064/211
[A[ATraining Step: 31  | total loss: [1m[32m0.13373[0m[0m | time: 23.758s
[2K
| Adam | epoch: 005 | loss: 0.13373 - acc: 0.9471 -- iter: 096/211
[A[ATraining Step: 32  | total loss: [1m[32m0.13817[0m[0m | time: 29.630s
[2K
| Adam | epoch: 005 | loss: 0.13817 - acc: 0.9353 -- iter: 128/211
[A[ATraining Step: 33  | total loss: [1m[32m0.12186[0m[0m | time: 38.708s
[2K
| Adam | epoch: 005 | loss: 0.12186 - acc: 0.9495 -- iter: 160/211
[A[ATraining Step: 34  | total loss: [1m[32m0.12657[0m[0m | time: 47.908s
[2K
| Adam | epoch: 005 | loss: 0.12657 - acc: 0.9536 -- iter: 192/211
[A[ATraining Step: 35  | total loss: [1m[32m0.11367[0m[0m | time: 59.924s
[2K
| Adam | epoch: 005 | loss: 0.11367 - acc: 0.9568 | val_loss: 0.70112 - val_acc: 0.6119 -- iter: 211/211
--
Training Step: 36  | total loss: [1m[32m0.10660[0m[0m | time: 9.129s
[2K
| Adam | epoch: 006 | loss: 0.10660 - acc: 0.9592 -- iter: 032/211
[A[ATraining Step: 37  | total loss: [1m[32m0.16262[0m[0m | time: 18.189s
[2K
| Adam | epoch: 006 | loss: 0.16262 - acc: 0.9424 -- iter: 064/211
[A[ATraining Step: 38  | total loss: [1m[32m0.17951[0m[0m | time: 26.909s
[2K
| Adam | epoch: 006 | loss: 0.17951 - acc: 0.9292 -- iter: 096/211
[A[ATraining Step: 39  | total loss: [1m[32m0.17979[0m[0m | time: 33.029s
[2K
| Adam | epoch: 006 | loss: 0.17979 - acc: 0.9248 -- iter: 128/211
[A[ATraining Step: 40  | total loss: [1m[32m0.23398[0m[0m | time: 38.913s
[2K
| Adam | epoch: 006 | loss: 0.23398 - acc: 0.9093 -- iter: 160/211
[A[ATraining Step: 41  | total loss: [1m[32m0.20514[0m[0m | time: 47.635s
[2K
| Adam | epoch: 006 | loss: 0.20514 - acc: 0.9260 -- iter: 192/211
[A[ATraining Step: 42  | total loss: [1m[32m0.17510[0m[0m | time: 59.915s
[2K
| Adam | epoch: 006 | loss: 0.17510 - acc: 0.9337 | val_loss: 2.55061 - val_acc: 0.3881 -- iter: 211/211
--
Training Step: 43  | total loss: [1m[32m0.15232[0m[0m | time: 9.056s
[2K
| Adam | epoch: 007 | loss: 0.15232 - acc: 0.9454 -- iter: 032/211
[A[ATraining Step: 44  | total loss: [1m[32m0.14508[0m[0m | time: 17.672s
[2K
| Adam | epoch: 007 | loss: 0.14508 - acc: 0.9494 -- iter: 064/211
[A[ATraining Step: 45  | total loss: [1m[32m0.13067[0m[0m | time: 26.708s
[2K
| Adam | epoch: 007 | loss: 0.13067 - acc: 0.9580 -- iter: 096/211
[A[ATraining Step: 46  | total loss: [1m[32m0.12658[0m[0m | time: 35.828s
[2K
| Adam | epoch: 007 | loss: 0.12658 - acc: 0.9546 -- iter: 128/211
[A[ATraining Step: 47  | total loss: [1m[32m0.11441[0m[0m | time: 41.807s
[2K
| Adam | epoch: 007 | loss: 0.11441 - acc: 0.9569 -- iter: 160/211
[A[ATraining Step: 48  | total loss: [1m[32m0.14875[0m[0m | time: 47.658s
[2K
| Adam | epoch: 007 | loss: 0.14875 - acc: 0.9554 -- iter: 192/211
[A[ATraining Step: 49  | total loss: [1m[32m0.12671[0m[0m | time: 59.723s
[2K
| Adam | epoch: 007 | loss: 0.12671 - acc: 0.9624 | val_loss: 0.54294 - val_acc: 0.8209 -- iter: 211/211
--
Training Step: 50  | total loss: [1m[32m0.11478[0m[0m | time: 9.961s
[2K
| Adam | epoch: 008 | loss: 0.11478 - acc: 0.9634 -- iter: 032/211
[A[ATraining Step: 51  | total loss: [1m[32m0.10229[0m[0m | time: 19.594s
[2K
| Adam | epoch: 008 | loss: 0.10229 - acc: 0.9690 -- iter: 064/211
[A[ATraining Step: 52  | total loss: [1m[32m0.09120[0m[0m | time: 29.064s
[2K
| Adam | epoch: 008 | loss: 0.09120 - acc: 0.9736 -- iter: 096/211
[A[ATraining Step: 53  | total loss: [1m[32m0.08083[0m[0m | time: 38.690s
[2K
| Adam | epoch: 008 | loss: 0.08083 - acc: 0.9775 -- iter: 128/211
[A[ATraining Step: 54  | total loss: [1m[32m0.07047[0m[0m | time: 48.309s
[2K
| Adam | epoch: 008 | loss: 0.07047 - acc: 0.9808 -- iter: 160/211
[A[ATraining Step: 55  | total loss: [1m[32m0.06205[0m[0m | time: 54.528s
[2K
| Adam | epoch: 008 | loss: 0.06205 - acc: 0.9835 -- iter: 192/211
[A[ATraining Step: 56  | total loss: [1m[32m0.08394[0m[0m | time: 64.137s
[2K
| Adam | epoch: 008 | loss: 0.08394 - acc: 0.9784 | val_loss: 0.60545 - val_acc: 0.8507 -- iter: 211/211
--
Training Step: 57  | total loss: [1m[32m0.07310[0m[0m | time: 10.104s
[2K
| Adam | epoch: 009 | loss: 0.07310 - acc: 0.9814 -- iter: 032/211
[A[ATraining Step: 58  | total loss: [1m[32m0.08079[0m[0m | time: 20.112s
[2K
| Adam | epoch: 009 | loss: 0.08079 - acc: 0.9797 -- iter: 064/211
[A[ATraining Step: 59  | total loss: [1m[32m0.07160[0m[0m | time: 29.775s
[2K
| Adam | epoch: 009 | loss: 0.07160 - acc: 0.9824 -- iter: 096/211
[A[ATraining Step: 60  | total loss: [1m[32m0.06423[0m[0m | time: 39.214s
[2K
| Adam | epoch: 009 | loss: 0.06423 - acc: 0.9848 -- iter: 128/211
[A[ATraining Step: 61  | total loss: [1m[32m0.07392[0m[0m | time: 48.462s
[2K
| Adam | epoch: 009 | loss: 0.07392 - acc: 0.9827 -- iter: 160/211
[A[ATraining Step: 62  | total loss: [1m[32m0.08966[0m[0m | time: 57.906s
[2K
| Adam | epoch: 009 | loss: 0.08966 - acc: 0.9688 -- iter: 192/211
[A[ATraining Step: 63  | total loss: [1m[32m0.08071[0m[0m | time: 67.197s
[2K
| Adam | epoch: 009 | loss: 0.08071 - acc: 0.9728 | val_loss: 1.50640 - val_acc: 0.6418 -- iter: 211/211
--
Training Step: 64  | total loss: [1m[32m0.10071[0m[0m | time: 6.190s
[2K
| Adam | epoch: 010 | loss: 0.10071 - acc: 0.9696 -- iter: 032/211
[A[ATraining Step: 65  | total loss: [1m[32m0.08933[0m[0m | time: 15.614s
[2K
| Adam | epoch: 010 | loss: 0.08933 - acc: 0.9733 -- iter: 064/211
[A[ATraining Step: 66  | total loss: [1m[32m0.08315[0m[0m | time: 25.084s
[2K
| Adam | epoch: 010 | loss: 0.08315 - acc: 0.9728 -- iter: 096/211
[A[ATraining Step: 67  | total loss: [1m[32m0.07787[0m[0m | time: 34.535s
[2K
| Adam | epoch: 010 | loss: 0.07787 - acc: 0.9761 -- iter: 128/211
[A[ATraining Step: 68  | total loss: [1m[32m0.07767[0m[0m | time: 44.264s
[2K
| Adam | epoch: 010 | loss: 0.07767 - acc: 0.9752 -- iter: 160/211
[A[ATraining Step: 69  | total loss: [1m[32m0.08175[0m[0m | time: 53.776s
[2K
| Adam | epoch: 010 | loss: 0.08175 - acc: 0.9708 -- iter: 192/211
[A[ATraining Step: 70  | total loss: [1m[32m0.07822[0m[0m | time: 66.074s
[2K
| Adam | epoch: 010 | loss: 0.07822 - acc: 0.9705 | val_loss: 1.94796 - val_acc: 0.5672 -- iter: 211/211
--
Training Step: 71  | total loss: [1m[32m0.11134[0m[0m | time: 6.224s
[2K
| Adam | epoch: 011 | loss: 0.11134 - acc: 0.9668 -- iter: 032/211
[A[ATraining Step: 72  | total loss: [1m[32m0.11430[0m[0m | time: 12.504s
[2K
| Adam | epoch: 011 | loss: 0.11430 - acc: 0.9646 -- iter: 064/211
[A[ATraining Step: 73  | total loss: [1m[32m0.11291[0m[0m | time: 21.574s
[2K
| Adam | epoch: 011 | loss: 0.11291 - acc: 0.9627 -- iter: 096/211
[A[ATraining Step: 74  | total loss: [1m[32m0.11760[0m[0m | time: 30.998s
[2K
| Adam | epoch: 011 | loss: 0.11760 - acc: 0.9634 -- iter: 128/211
[A[ATraining Step: 75  | total loss: [1m[32m0.10813[0m[0m | time: 40.101s
[2K
| Adam | epoch: 011 | loss: 0.10813 - acc: 0.9673 -- iter: 160/211
[A[ATraining Step: 76  | total loss: [1m[32m0.09958[0m[0m | time: 49.503s
[2K
| Adam | epoch: 011 | loss: 0.09958 - acc: 0.9708 -- iter: 192/211
[A[ATraining Step: 77  | total loss: [1m[32m0.11194[0m[0m | time: 61.751s
[2K
| Adam | epoch: 011 | loss: 0.11194 - acc: 0.9640 | val_loss: 0.61644 - val_acc: 0.8358 -- iter: 211/211
--
Training Step: 78  | total loss: [1m[32m0.10626[0m[0m | time: 9.141s
[2K
| Adam | epoch: 012 | loss: 0.10626 - acc: 0.9678 -- iter: 032/211
[A[ATraining Step: 79  | total loss: [1m[32m0.09756[0m[0m | time: 15.449s
[2K
| Adam | epoch: 012 | loss: 0.09756 - acc: 0.9711 -- iter: 064/211
[A[ATraining Step: 80  | total loss: [1m[32m0.12868[0m[0m | time: 21.612s
[2K
| Adam | epoch: 012 | loss: 0.12868 - acc: 0.9633 -- iter: 096/211
[A[ATraining Step: 81  | total loss: [1m[32m0.11715[0m[0m | time: 31.091s
[2K
| Adam | epoch: 012 | loss: 0.11715 - acc: 0.9670 -- iter: 128/211
[A[ATraining Step: 82  | total loss: [1m[32m0.10643[0m[0m | time: 39.979s
[2K
| Adam | epoch: 012 | loss: 0.10643 - acc: 0.9703 -- iter: 160/211
[A[ATraining Step: 83  | total loss: [1m[32m0.09713[0m[0m | time: 51.609s
[2K
| Adam | epoch: 012 | loss: 0.09713 - acc: 0.9733 -- iter: 192/211
[A[ATraining Step: 84  | total loss: [1m[32m0.09136[0m[0m | time: 65.643s
[2K
| Adam | epoch: 012 | loss: 0.09136 - acc: 0.9759 | val_loss: 0.48608 - val_acc: 0.9254 -- iter: 211/211
--
Training Step: 85  | total loss: [1m[32m0.10961[0m[0m | time: 11.879s
[2K
| Adam | epoch: 013 | loss: 0.10961 - acc: 0.9658 -- iter: 032/211
[A[ATraining Step: 86  | total loss: [1m[32m0.10080[0m[0m | time: 23.170s
[2K
| Adam | epoch: 013 | loss: 0.10080 - acc: 0.9693 -- iter: 064/211
[A[ATraining Step: 87  | total loss: [1m[32m0.09123[0m[0m | time: 30.554s
[2K
| Adam | epoch: 013 | loss: 0.09123 - acc: 0.9723 -- iter: 096/211
[A[ATraining Step: 88  | total loss: [1m[32m0.08397[0m[0m | time: 39.002s
[2K
| Adam | epoch: 013 | loss: 0.08397 - acc: 0.9751 -- iter: 128/211
[A[ATraining Step: 89  | total loss: [1m[32m0.07735[0m[0m | time: 59.030s
[2K
| Adam | epoch: 013 | loss: 0.07735 - acc: 0.9776 -- iter: 160/211
[A[ATraining Step: 90  | total loss: [1m[32m0.07080[0m[0m | time: 79.754s
[2K
| Adam | epoch: 013 | loss: 0.07080 - acc: 0.9798 -- iter: 192/211
[A[ATraining Step: 91  | total loss: [1m[32m0.06597[0m[0m | time: 142.528s
[2K
| Adam | epoch: 013 | loss: 0.06597 - acc: 0.9818 | val_loss: 0.52528 - val_acc: 0.8358 -- iter: 211/211
--
Training Step: 92  | total loss: [1m[32m0.07000[0m[0m | time: 81.212s
[2K
| Adam | epoch: 014 | loss: 0.07000 - acc: 0.9774 -- iter: 032/211
[A[ATraining Step: 93  | total loss: [1m[32m0.06981[0m[0m | time: 118.638s
[2K
| Adam | epoch: 014 | loss: 0.06981 - acc: 0.9765 -- iter: 064/211
[A[ATraining Step: 94  | total loss: [1m[32m0.06398[0m[0m | time: 154.098s
[2K
| Adam | epoch: 014 | loss: 0.06398 - acc: 0.9789 -- iter: 096/211
[A[ATraining Step: 95  | total loss: [1m[32m0.05799[0m[0m | time: 163.209s
[2K
| Adam | epoch: 014 | loss: 0.05799 - acc: 0.9810 -- iter: 128/211
[A[ATraining Step: 96  | total loss: [1m[32m0.11306[0m[0m | time: 172.165s
[2K
| Adam | epoch: 014 | loss: 0.11306 - acc: 0.9724 -- iter: 160/211
[A[ATraining Step: 97  | total loss: [1m[32m0.10243[0m[0m | time: 216.552s
[2K
| Adam | epoch: 014 | loss: 0.10243 - acc: 0.9751 -- iter: 192/211
[A[ATraining Step: 98  | total loss: [1m[32m0.09275[0m[0m | time: 247.698s
[2K
| Adam | epoch: 014 | loss: 0.09275 - acc: 0.9776 | val_loss: 0.63394 - val_acc: 0.8209 -- iter: 211/211
--
Training Step: 99  | total loss: [1m[32m0.08518[0m[0m | time: 88.372s
[2K
| Adam | epoch: 015 | loss: 0.08518 - acc: 0.9799 -- iter: 032/211
[A[ATraining Step: 100  | total loss: [1m[32m0.07946[0m[0m | time: 101.436s
[2K
| Adam | epoch: 015 | loss: 0.07946 - acc: 0.9819 -- iter: 064/211
[A[ATraining Step: 101  | total loss: [1m[32m0.07313[0m[0m | time: 118.477s
[2K
| Adam | epoch: 015 | loss: 0.07313 - acc: 0.9837 -- iter: 096/211
[A[ATraining Step: 102  | total loss: [1m[32m0.06675[0m[0m | time: 148.595s
[2K
| Adam | epoch: 015 | loss: 0.06675 - acc: 0.9853 -- iter: 128/211
[A[ATraining Step: 103  | total loss: [1m[32m0.06055[0m[0m | time: 155.775s
[2K
| Adam | epoch: 015 | loss: 0.06055 - acc: 0.9868 -- iter: 160/211
[A[ATraining Step: 104  | total loss: [1m[32m0.07449[0m[0m | time: 162.888s
[2K
| Adam | epoch: 015 | loss: 0.07449 - acc: 0.9828 -- iter: 192/211
[A[ATraining Step: 105  | total loss: [1m[32m0.06803[0m[0m | time: 177.926s
[2K
| Adam | epoch: 015 | loss: 0.06803 - acc: 0.9846 | val_loss: 0.77419 - val_acc: 0.8060 -- iter: 211/211
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.9033771106941838
Validation AUPRC:0.7695569070709716
Test AUC:0.9926739926739927
Test AUPRC:0.9949245610349591
BestTestF1Score	0.94	0.85	0.93	0.92	0.95	37	3	25	2	0.01
BestTestMCCScore	0.94	0.85	0.93	0.92	0.95	37	3	25	2	0.01
BestTestAccuracyScore	0.94	0.85	0.93	0.92	0.95	37	3	25	2	0.01
BestValidationF1Score	0.86	0.76	0.88	0.8	0.92	24	6	35	2	0.01
BestValidationMCC	0.86	0.76	0.88	0.8	0.92	24	6	35	2	0.01
BestValidationAccuracy	0.86	0.76	0.88	0.8	0.92	24	6	35	2	0.01
TestPredictions (Threshold:0.01)
CHEMBL1438650,TP,ACT,0.07999999821186066	CHEMBL3716137,TP,ACT,0.07999999821186066	CHEMBL525921,TN,INACT,0.0	CHEMBL2392385,TN,INACT,0.009999999776482582	CHEMBL3729970,TP,ACT,0.6200000047683716	CHEMBL1408747,FP,INACT,0.05999999865889549	CHEMBL549792,TN,INACT,0.0	CHEMBL3732018,TP,ACT,0.6700000166893005	CHEMBL3716850,TP,ACT,0.17000000178813934	CHEMBL3729519,TP,ACT,0.5799999833106995	CHEMBL3731114,TP,ACT,0.9599999785423279	CHEMBL1517799,TP,ACT,0.75	CHEMBL557050,TN,INACT,0.0	CHEMBL457180,TN,INACT,0.0	CHEMBL3093246,TP,ACT,0.9800000190734863	CHEMBL1548937,TP,ACT,0.6399999856948853	CHEMBL456113,TN,INACT,0.0	CHEMBL2079522,TP,ACT,0.5799999833106995	CHEMBL3716385,TP,ACT,0.18000000715255737	CHEMBL3732073,TP,ACT,0.6299999952316284	CHEMBL2032286,TP,ACT,0.8299999833106995	CHEMBL3729453,TP,ACT,0.6200000047683716	CHEMBL318188,TN,INACT,0.0	CHEMBL2392355,FP,INACT,0.019999999552965164	CHEMBL498705,TN,INACT,0.0	CHEMBL1922121,TN,INACT,0.0	CHEMBL1730248,FN,ACT,0.009999999776482582	CHEMBL3732155,TP,ACT,0.1599999964237213	CHEMBL3717935,TP,ACT,0.9900000095367432	CHEMBL467373,TP,ACT,0.6000000238418579	CHEMBL1611255,TP,ACT,0.33000001311302185	CHEMBL60254,FN,ACT,0.009999999776482582	CHEMBL1910755,TN,INACT,0.009999999776482582	CHEMBL3093245,TP,ACT,0.9700000286102295	CHEMBL1456007,TP,ACT,0.18000000715255737	CHEMBL498249,TN,INACT,0.0	CHEMBL3715293,TP,ACT,0.05000000074505806	CHEMBL1469482,TP,ACT,0.6100000143051147	CHEMBL456760,TN,INACT,0.0	CHEMBL2392375,TN,INACT,0.0	CHEMBL3719183,TP,ACT,0.3400000035762787	CHEMBL3609567,TN,INACT,0.0	CHEMBL3729052,TP,ACT,0.38999998569488525	CHEMBL1767292,FP,INACT,0.03999999910593033	CHEMBL3715801,TP,ACT,0.6899999976158142	CHEMBL1910758,TN,INACT,0.0	CHEMBL2392246,TN,INACT,0.0	CHEMBL2392233,TN,INACT,0.0	CHEMBL458076,TN,INACT,0.0	CHEMBL557321,TN,INACT,0.0	CHEMBL3733170,TP,ACT,0.07999999821186066	CHEMBL3730458,TP,ACT,0.10000000149011612	CHEMBL3728959,TP,ACT,0.2199999988079071	CHEMBL524820,TN,INACT,0.0	CHEMBL558601,TN,INACT,0.0	CHEMBL2392379,TN,INACT,0.0	CHEMBL3718849,TP,ACT,0.6800000071525574	CHEMBL230761,TN,INACT,0.0	CHEMBL3717041,TP,ACT,0.9399999976158142	CHEMBL3730586,TP,ACT,0.9700000286102295	CHEMBL2392390,TN,INACT,0.0	CHEMBL3093163,TP,ACT,0.8199999928474426	CHEMBL3718495,TP,ACT,0.11999999731779099	CHEMBL101779,TN,INACT,0.0	CHEMBL3728531,TP,ACT,0.9300000071525574	CHEMBL1487253,TP,ACT,0.15000000596046448	CHEMBL3735506,TP,ACT,0.03999999910593033	

