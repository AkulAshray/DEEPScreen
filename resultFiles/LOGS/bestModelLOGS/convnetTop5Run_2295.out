CNNModel CHEMBL4718 adam 0.001 30 256 0 0.6 False True
Number of active compounds :	112
Number of inactive compounds :	112
---------------------------------
Run id: CNNModel_CHEMBL4718_adam_0.001_30_256_0_0.6_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL4718_adam_0.001_30_256_0.6_True/
---------------------------------
Training samples: 115
Validation samples: 37
--
Training Step: 1  | time: 0.729s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/115
[A[ATraining Step: 2  | total loss: [1m[32m0.62378[0m[0m | time: 1.333s
[2K
| Adam | epoch: 001 | loss: 0.62378 - acc: 0.4219 -- iter: 064/115
[A[ATraining Step: 3  | total loss: [1m[32m0.68103[0m[0m | time: 1.973s
[2K
| Adam | epoch: 001 | loss: 0.68103 - acc: 0.4602 -- iter: 096/115
[A[ATraining Step: 4  | total loss: [1m[32m0.69120[0m[0m | time: 3.392s
[2K
| Adam | epoch: 001 | loss: 0.69120 - acc: 0.4666 | val_loss: 0.66959 - val_acc: 0.7568 -- iter: 115/115
--
Training Step: 5  | total loss: [1m[32m0.69198[0m[0m | time: 0.391s
[2K
| Adam | epoch: 002 | loss: 0.69198 - acc: 0.5079 -- iter: 032/115
[A[ATraining Step: 6  | total loss: [1m[32m0.69169[0m[0m | time: 1.017s
[2K
| Adam | epoch: 002 | loss: 0.69169 - acc: 0.5198 -- iter: 064/115
[A[ATraining Step: 7  | total loss: [1m[32m0.68338[0m[0m | time: 1.656s
[2K
| Adam | epoch: 002 | loss: 0.68338 - acc: 0.5829 -- iter: 096/115
[A[ATraining Step: 8  | total loss: [1m[32m0.67737[0m[0m | time: 3.265s
[2K
| Adam | epoch: 002 | loss: 0.67737 - acc: 0.5890 | val_loss: 0.60841 - val_acc: 0.7568 -- iter: 115/115
--
Training Step: 9  | total loss: [1m[32m0.73701[0m[0m | time: 0.403s
[2K
| Adam | epoch: 003 | loss: 0.73701 - acc: 0.5253 -- iter: 032/115
[A[ATraining Step: 10  | total loss: [1m[32m0.67864[0m[0m | time: 0.786s
[2K
| Adam | epoch: 003 | loss: 0.67864 - acc: 0.6311 -- iter: 064/115
[A[ATraining Step: 11  | total loss: [1m[32m0.65489[0m[0m | time: 1.423s
[2K
| Adam | epoch: 003 | loss: 0.65489 - acc: 0.6812 -- iter: 096/115
[A[ATraining Step: 12  | total loss: [1m[32m0.67345[0m[0m | time: 3.046s
[2K
| Adam | epoch: 003 | loss: 0.67345 - acc: 0.6137 | val_loss: 0.62934 - val_acc: 0.7568 -- iter: 115/115
--
Training Step: 13  | total loss: [1m[32m0.67440[0m[0m | time: 0.629s
[2K
| Adam | epoch: 004 | loss: 0.67440 - acc: 0.6052 -- iter: 032/115
[A[ATraining Step: 14  | total loss: [1m[32m0.68137[0m[0m | time: 1.032s
[2K
| Adam | epoch: 004 | loss: 0.68137 - acc: 0.5749 -- iter: 064/115
[A[ATraining Step: 15  | total loss: [1m[32m0.67552[0m[0m | time: 1.427s
[2K
| Adam | epoch: 004 | loss: 0.67552 - acc: 0.5971 -- iter: 096/115
[A[ATraining Step: 16  | total loss: [1m[32m0.67188[0m[0m | time: 3.038s
[2K
| Adam | epoch: 004 | loss: 0.67188 - acc: 0.6100 | val_loss: 0.62433 - val_acc: 0.7568 -- iter: 115/115
--
Training Step: 17  | total loss: [1m[32m0.68053[0m[0m | time: 0.631s
[2K
| Adam | epoch: 005 | loss: 0.68053 - acc: 0.5817 -- iter: 032/115
[A[ATraining Step: 18  | total loss: [1m[32m0.68936[0m[0m | time: 1.251s
[2K
| Adam | epoch: 005 | loss: 0.68936 - acc: 0.5534 -- iter: 064/115
[A[ATraining Step: 19  | total loss: [1m[32m0.69673[0m[0m | time: 1.634s
[2K
| Adam | epoch: 005 | loss: 0.69673 - acc: 0.5252 -- iter: 096/115
[A[ATraining Step: 20  | total loss: [1m[32m0.69284[0m[0m | time: 3.018s
[2K
| Adam | epoch: 005 | loss: 0.69284 - acc: 0.5425 | val_loss: 0.65070 - val_acc: 0.7568 -- iter: 115/115
--
Training Step: 21  | total loss: [1m[32m0.68979[0m[0m | time: 0.611s
[2K
| Adam | epoch: 006 | loss: 0.68979 - acc: 0.5538 -- iter: 032/115
[A[ATraining Step: 22  | total loss: [1m[32m0.68036[0m[0m | time: 1.215s
[2K
| Adam | epoch: 006 | loss: 0.68036 - acc: 0.6033 -- iter: 064/115
[A[ATraining Step: 23  | total loss: [1m[32m0.67678[0m[0m | time: 1.834s
[2K
| Adam | epoch: 006 | loss: 0.67678 - acc: 0.6187 -- iter: 096/115
[A[ATraining Step: 24  | total loss: [1m[32m0.68436[0m[0m | time: 3.208s
[2K
| Adam | epoch: 006 | loss: 0.68436 - acc: 0.5765 | val_loss: 0.64339 - val_acc: 0.7568 -- iter: 115/115
--
Training Step: 25  | total loss: [1m[32m0.68741[0m[0m | time: 0.407s
[2K
| Adam | epoch: 007 | loss: 0.68741 - acc: 0.5628 -- iter: 032/115
[A[ATraining Step: 26  | total loss: [1m[32m0.68950[0m[0m | time: 1.017s
[2K
| Adam | epoch: 007 | loss: 0.68950 - acc: 0.5531 -- iter: 064/115
[A[ATraining Step: 27  | total loss: [1m[32m0.68646[0m[0m | time: 1.632s
[2K
| Adam | epoch: 007 | loss: 0.68646 - acc: 0.5636 -- iter: 096/115
[A[ATraining Step: 28  | total loss: [1m[32m0.68137[0m[0m | time: 3.240s
[2K
| Adam | epoch: 007 | loss: 0.68137 - acc: 0.5868 | val_loss: 0.63881 - val_acc: 0.7568 -- iter: 115/115
--
Training Step: 29  | total loss: [1m[32m0.68652[0m[0m | time: 0.388s
[2K
| Adam | epoch: 008 | loss: 0.68652 - acc: 0.5657 -- iter: 032/115
[A[ATraining Step: 30  | total loss: [1m[32m0.68213[0m[0m | time: 0.777s
[2K
| Adam | epoch: 008 | loss: 0.68213 - acc: 0.5813 -- iter: 064/115
[A[ATraining Step: 31  | total loss: [1m[32m0.67893[0m[0m | time: 1.406s
[2K
| Adam | epoch: 008 | loss: 0.67893 - acc: 0.5929 -- iter: 096/115
[A[ATraining Step: 32  | total loss: [1m[32m0.67804[0m[0m | time: 3.009s
[2K
| Adam | epoch: 008 | loss: 0.67804 - acc: 0.5931 | val_loss: 0.61307 - val_acc: 0.7568 -- iter: 115/115
--
Training Step: 33  | total loss: [1m[32m0.68004[0m[0m | time: 0.636s
[2K
| Adam | epoch: 009 | loss: 0.68004 - acc: 0.5864 -- iter: 032/115
[A[ATraining Step: 34  | total loss: [1m[32m0.67954[0m[0m | time: 1.019s
[2K
| Adam | epoch: 009 | loss: 0.67954 - acc: 0.5879 -- iter: 064/115
[A[ATraining Step: 35  | total loss: [1m[32m0.68476[0m[0m | time: 1.392s
[2K
| Adam | epoch: 009 | loss: 0.68476 - acc: 0.5750 -- iter: 096/115
[A[ATraining Step: 36  | total loss: [1m[32m0.68758[0m[0m | time: 3.003s
[2K
| Adam | epoch: 009 | loss: 0.68758 - acc: 0.5651 | val_loss: 0.62635 - val_acc: 0.7568 -- iter: 115/115
--
Training Step: 37  | total loss: [1m[32m0.68743[0m[0m | time: 0.612s
[2K
| Adam | epoch: 010 | loss: 0.68743 - acc: 0.5646 -- iter: 032/115
[A[ATraining Step: 38  | total loss: [1m[32m0.68709[0m[0m | time: 1.235s
[2K
| Adam | epoch: 010 | loss: 0.68709 - acc: 0.5642 -- iter: 064/115
[A[ATraining Step: 39  | total loss: [1m[32m0.68957[0m[0m | time: 1.604s
[2K
| Adam | epoch: 010 | loss: 0.68957 - acc: 0.5519 -- iter: 096/115
[A[ATraining Step: 40  | total loss: [1m[32m0.68174[0m[0m | time: 2.976s
[2K
| Adam | epoch: 010 | loss: 0.68174 - acc: 0.5866 | val_loss: 0.63724 - val_acc: 0.7568 -- iter: 115/115
--
Training Step: 41  | total loss: [1m[32m0.67515[0m[0m | time: 0.624s
[2K
| Adam | epoch: 011 | loss: 0.67515 - acc: 0.6142 -- iter: 032/115
[A[ATraining Step: 42  | total loss: [1m[32m0.67822[0m[0m | time: 1.232s
[2K
| Adam | epoch: 011 | loss: 0.67822 - acc: 0.5992 -- iter: 064/115
[A[ATraining Step: 43  | total loss: [1m[32m0.67335[0m[0m | time: 1.848s
[2K
| Adam | epoch: 011 | loss: 0.67335 - acc: 0.6148 -- iter: 096/115
[A[ATraining Step: 44  | total loss: [1m[32m0.68092[0m[0m | time: 3.228s
[2K
| Adam | epoch: 011 | loss: 0.68092 - acc: 0.5895 | val_loss: 0.62322 - val_acc: 0.7568 -- iter: 115/115
--
Training Step: 45  | total loss: [1m[32m0.68697[0m[0m | time: 0.401s
[2K
| Adam | epoch: 012 | loss: 0.68697 - acc: 0.5699 -- iter: 032/115
[A[ATraining Step: 46  | total loss: [1m[32m0.69125[0m[0m | time: 0.997s
[2K
| Adam | epoch: 012 | loss: 0.69125 - acc: 0.5538 -- iter: 064/115
[A[ATraining Step: 47  | total loss: [1m[32m0.68823[0m[0m | time: 1.623s
[2K
| Adam | epoch: 012 | loss: 0.68823 - acc: 0.5604 -- iter: 096/115
[A[ATraining Step: 48  | total loss: [1m[32m0.68610[0m[0m | time: 3.234s
[2K
| Adam | epoch: 012 | loss: 0.68610 - acc: 0.5657 | val_loss: 0.64671 - val_acc: 0.7568 -- iter: 115/115
--
Training Step: 49  | total loss: [1m[32m0.68798[0m[0m | time: 0.380s
[2K
| Adam | epoch: 013 | loss: 0.68798 - acc: 0.5554 -- iter: 032/115
[A[ATraining Step: 50  | total loss: [1m[32m0.68232[0m[0m | time: 0.749s
[2K
| Adam | epoch: 013 | loss: 0.68232 - acc: 0.5835 -- iter: 064/115
[A[ATraining Step: 51  | total loss: [1m[32m0.67735[0m[0m | time: 1.391s
[2K
| Adam | epoch: 013 | loss: 0.67735 - acc: 0.6069 -- iter: 096/115
[A[ATraining Step: 52  | total loss: [1m[32m0.68079[0m[0m | time: 3.015s
[2K
| Adam | epoch: 013 | loss: 0.68079 - acc: 0.5909 | val_loss: 0.63134 - val_acc: 0.7568 -- iter: 115/115
--
Training Step: 53  | total loss: [1m[32m0.68114[0m[0m | time: 0.611s
[2K
| Adam | epoch: 014 | loss: 0.68114 - acc: 0.5867 -- iter: 032/115
[A[ATraining Step: 54  | total loss: [1m[32m0.67537[0m[0m | time: 0.988s
[2K
| Adam | epoch: 014 | loss: 0.67537 - acc: 0.6059 -- iter: 064/115
[A[ATraining Step: 55  | total loss: [1m[32m0.68070[0m[0m | time: 1.365s
[2K
| Adam | epoch: 014 | loss: 0.68070 - acc: 0.5870 -- iter: 096/115
[A[ATraining Step: 56  | total loss: [1m[32m0.68530[0m[0m | time: 2.980s
[2K
| Adam | epoch: 014 | loss: 0.68530 - acc: 0.5710 | val_loss: 0.62684 - val_acc: 0.7568 -- iter: 115/115
--
Training Step: 57  | total loss: [1m[32m0.68946[0m[0m | time: 0.613s
[2K
| Adam | epoch: 015 | loss: 0.68946 - acc: 0.5569 -- iter: 032/115
[A[ATraining Step: 58  | total loss: [1m[32m0.68710[0m[0m | time: 1.212s
[2K
| Adam | epoch: 015 | loss: 0.68710 - acc: 0.5619 -- iter: 064/115
[A[ATraining Step: 59  | total loss: [1m[32m0.68537[0m[0m | time: 1.587s
[2K
| Adam | epoch: 015 | loss: 0.68537 - acc: 0.5662 -- iter: 096/115
[A[ATraining Step: 60  | total loss: [1m[32m0.68439[0m[0m | time: 2.983s
[2K
| Adam | epoch: 015 | loss: 0.68439 - acc: 0.5679 | val_loss: 0.63807 - val_acc: 0.7568 -- iter: 115/115
--
Training Step: 61  | total loss: [1m[32m0.68348[0m[0m | time: 0.601s
[2K
| Adam | epoch: 016 | loss: 0.68348 - acc: 0.5693 -- iter: 032/115
[A[ATraining Step: 62  | total loss: [1m[32m0.68523[0m[0m | time: 1.204s
[2K
| Adam | epoch: 016 | loss: 0.68523 - acc: 0.5604 -- iter: 064/115
[A[ATraining Step: 63  | total loss: [1m[32m0.68827[0m[0m | time: 1.812s
[2K
| Adam | epoch: 016 | loss: 0.68827 - acc: 0.5448 -- iter: 096/115
[A[ATraining Step: 64  | total loss: [1m[32m0.68596[0m[0m | time: 3.191s
[2K
| Adam | epoch: 016 | loss: 0.68596 - acc: 0.5548 | val_loss: 0.64744 - val_acc: 0.7568 -- iter: 115/115
--
Training Step: 65  | total loss: [1m[32m0.68512[0m[0m | time: 0.374s
[2K
| Adam | epoch: 017 | loss: 0.68512 - acc: 0.5578 -- iter: 032/115
[A[ATraining Step: 66  | total loss: [1m[32m0.68401[0m[0m | time: 0.974s
[2K
| Adam | epoch: 017 | loss: 0.68401 - acc: 0.5604 -- iter: 064/115
[A[ATraining Step: 67  | total loss: [1m[32m0.68250[0m[0m | time: 1.571s
[2K
| Adam | epoch: 017 | loss: 0.68250 - acc: 0.5681 -- iter: 096/115
[A[ATraining Step: 68  | total loss: [1m[32m0.68256[0m[0m | time: 3.186s
[2K
| Adam | epoch: 017 | loss: 0.68256 - acc: 0.5675 | val_loss: 0.63550 - val_acc: 0.7568 -- iter: 115/115
--
Training Step: 69  | total loss: [1m[32m0.68319[0m[0m | time: 0.383s
[2K
| Adam | epoch: 018 | loss: 0.68319 - acc: 0.5632 -- iter: 032/115
[A[ATraining Step: 70  | total loss: [1m[32m0.68539[0m[0m | time: 0.781s
[2K
| Adam | epoch: 018 | loss: 0.68539 - acc: 0.5529 -- iter: 064/115
[A[ATraining Step: 71  | total loss: [1m[32m0.68699[0m[0m | time: 1.398s
[2K
| Adam | epoch: 018 | loss: 0.68699 - acc: 0.5439 -- iter: 096/115
[A[ATraining Step: 72  | total loss: [1m[32m0.68384[0m[0m | time: 3.005s
[2K
| Adam | epoch: 018 | loss: 0.68384 - acc: 0.5565 | val_loss: 0.62938 - val_acc: 0.7568 -- iter: 115/115
--
Training Step: 73  | total loss: [1m[32m0.68039[0m[0m | time: 0.627s
[2K
| Adam | epoch: 019 | loss: 0.68039 - acc: 0.5711 -- iter: 032/115
[A[ATraining Step: 74  | total loss: [1m[32m0.68488[0m[0m | time: 0.990s
[2K
| Adam | epoch: 019 | loss: 0.68488 - acc: 0.5530 -- iter: 064/115
[A[ATraining Step: 75  | total loss: [1m[32m0.68209[0m[0m | time: 1.355s
[2K
| Adam | epoch: 019 | loss: 0.68209 - acc: 0.5615 -- iter: 096/115
[A[ATraining Step: 76  | total loss: [1m[32m0.67941[0m[0m | time: 2.980s
[2K
| Adam | epoch: 019 | loss: 0.67941 - acc: 0.5690 | val_loss: 0.61091 - val_acc: 0.7568 -- iter: 115/115
--
Training Step: 77  | total loss: [1m[32m0.67891[0m[0m | time: 0.600s
[2K
| Adam | epoch: 020 | loss: 0.67891 - acc: 0.5683 -- iter: 032/115
[A[ATraining Step: 78  | total loss: [1m[32m0.68188[0m[0m | time: 1.202s
[2K
| Adam | epoch: 020 | loss: 0.68188 - acc: 0.5579 -- iter: 064/115
[A[ATraining Step: 79  | total loss: [1m[32m0.67720[0m[0m | time: 1.581s
[2K
| Adam | epoch: 020 | loss: 0.67720 - acc: 0.5681 -- iter: 096/115
[A[ATraining Step: 80  | total loss: [1m[32m0.67372[0m[0m | time: 2.959s
[2K
| Adam | epoch: 020 | loss: 0.67372 - acc: 0.5746 | val_loss: 0.57390 - val_acc: 0.7568 -- iter: 115/115
--
Training Step: 81  | total loss: [1m[32m0.66981[0m[0m | time: 0.607s
[2K
| Adam | epoch: 021 | loss: 0.66981 - acc: 0.5803 -- iter: 032/115
[A[ATraining Step: 82  | total loss: [1m[32m0.67249[0m[0m | time: 3.619s
[2K
| Adam | epoch: 021 | loss: 0.67249 - acc: 0.5754 -- iter: 064/115
[A[ATraining Step: 83  | total loss: [1m[32m0.66801[0m[0m | time: 4.226s
[2K
| Adam | epoch: 021 | loss: 0.66801 - acc: 0.5835 -- iter: 096/115
[A[ATraining Step: 84  | total loss: [1m[32m0.66580[0m[0m | time: 5.625s
[2K
| Adam | epoch: 021 | loss: 0.66580 - acc: 0.5814 | val_loss: 0.58448 - val_acc: 0.7568 -- iter: 115/115
--
Training Step: 85  | total loss: [1m[32m0.66338[0m[0m | time: 0.381s
[2K
| Adam | epoch: 022 | loss: 0.66338 - acc: 0.5812 -- iter: 032/115
[A[ATraining Step: 86  | total loss: [1m[32m0.66057[0m[0m | time: 0.989s
[2K
| Adam | epoch: 022 | loss: 0.66057 - acc: 0.5809 -- iter: 064/115
[A[ATraining Step: 87  | total loss: [1m[32m0.66411[0m[0m | time: 1.596s
[2K
| Adam | epoch: 022 | loss: 0.66411 - acc: 0.5697 -- iter: 096/115
[A[ATraining Step: 88  | total loss: [1m[32m0.66282[0m[0m | time: 3.216s
[2K
| Adam | epoch: 022 | loss: 0.66282 - acc: 0.5628 | val_loss: 0.57098 - val_acc: 0.7568 -- iter: 115/115
--
Training Step: 89  | total loss: [1m[32m0.65929[0m[0m | time: 0.398s
[2K
| Adam | epoch: 023 | loss: 0.65929 - acc: 0.5627 -- iter: 032/115
[A[ATraining Step: 90  | total loss: [1m[32m0.65491[0m[0m | time: 0.770s
[2K
| Adam | epoch: 023 | loss: 0.65491 - acc: 0.5643 -- iter: 064/115
[A[ATraining Step: 91  | total loss: [1m[32m0.65105[0m[0m | time: 1.391s
[2K
| Adam | epoch: 023 | loss: 0.65105 - acc: 0.5658 -- iter: 096/115
[A[ATraining Step: 92  | total loss: [1m[32m0.64462[0m[0m | time: 3.011s
[2K
| Adam | epoch: 023 | loss: 0.64462 - acc: 0.5717 | val_loss: 0.54727 - val_acc: 0.8378 -- iter: 115/115
--
Training Step: 93  | total loss: [1m[32m0.63362[0m[0m | time: 0.618s
[2K
| Adam | epoch: 024 | loss: 0.63362 - acc: 0.5989 -- iter: 032/115
[A[ATraining Step: 94  | total loss: [1m[32m0.63028[0m[0m | time: 0.991s
[2K
| Adam | epoch: 024 | loss: 0.63028 - acc: 0.6109 -- iter: 064/115
[A[ATraining Step: 95  | total loss: [1m[32m0.62525[0m[0m | time: 1.398s
[2K
| Adam | epoch: 024 | loss: 0.62525 - acc: 0.6182 -- iter: 096/115
[A[ATraining Step: 96  | total loss: [1m[32m0.61755[0m[0m | time: 3.010s
[2K
| Adam | epoch: 024 | loss: 0.61755 - acc: 0.6406 | val_loss: 0.50057 - val_acc: 0.8108 -- iter: 115/115
--
Training Step: 97  | total loss: [1m[32m0.60797[0m[0m | time: 0.610s
[2K
| Adam | epoch: 025 | loss: 0.60797 - acc: 0.6578 -- iter: 032/115
[A[ATraining Step: 98  | total loss: [1m[32m0.59920[0m[0m | time: 1.224s
[2K
| Adam | epoch: 025 | loss: 0.59920 - acc: 0.6639 -- iter: 064/115
[A[ATraining Step: 99  | total loss: [1m[32m0.58702[0m[0m | time: 1.599s
[2K
| Adam | epoch: 025 | loss: 0.58702 - acc: 0.6756 -- iter: 096/115
[A[ATraining Step: 100  | total loss: [1m[32m0.60377[0m[0m | time: 2.974s
[2K
| Adam | epoch: 025 | loss: 0.60377 - acc: 0.6607 | val_loss: 0.50325 - val_acc: 0.7838 -- iter: 115/115
--
Training Step: 101  | total loss: [1m[32m0.58758[0m[0m | time: 0.620s
[2K
| Adam | epoch: 026 | loss: 0.58758 - acc: 0.6683 -- iter: 032/115
[A[ATraining Step: 102  | total loss: [1m[32m0.60027[0m[0m | time: 1.231s
[2K
| Adam | epoch: 026 | loss: 0.60027 - acc: 0.6609 -- iter: 064/115
[A[ATraining Step: 103  | total loss: [1m[32m0.59656[0m[0m | time: 1.850s
[2K
| Adam | epoch: 026 | loss: 0.59656 - acc: 0.6667 -- iter: 096/115
[A[ATraining Step: 104  | total loss: [1m[32m0.57870[0m[0m | time: 3.237s
[2K
| Adam | epoch: 026 | loss: 0.57870 - acc: 0.6875 | val_loss: 0.71155 - val_acc: 0.5405 -- iter: 115/115
--
Training Step: 105  | total loss: [1m[32m0.55886[0m[0m | time: 0.374s
[2K
| Adam | epoch: 027 | loss: 0.55886 - acc: 0.7082 -- iter: 032/115
[A[ATraining Step: 106  | total loss: [1m[32m0.53641[0m[0m | time: 0.966s
[2K
| Adam | epoch: 027 | loss: 0.53641 - acc: 0.7321 -- iter: 064/115
[A[ATraining Step: 107  | total loss: [1m[32m0.51644[0m[0m | time: 1.553s
[2K
| Adam | epoch: 027 | loss: 0.51644 - acc: 0.7527 -- iter: 096/115
[A[ATraining Step: 108  | total loss: [1m[32m0.51130[0m[0m | time: 3.172s
[2K
| Adam | epoch: 027 | loss: 0.51130 - acc: 0.7493 | val_loss: 0.60355 - val_acc: 0.7297 -- iter: 115/115
--
Training Step: 109  | total loss: [1m[32m0.49105[0m[0m | time: 0.377s
[2K
| Adam | epoch: 028 | loss: 0.49105 - acc: 0.7618 -- iter: 032/115
[A[ATraining Step: 110  | total loss: [1m[32m0.46407[0m[0m | time: 0.778s
[2K
| Adam | epoch: 028 | loss: 0.46407 - acc: 0.7857 -- iter: 064/115
[A[ATraining Step: 111  | total loss: [1m[32m0.43447[0m[0m | time: 1.380s
[2K
| Adam | epoch: 028 | loss: 0.43447 - acc: 0.8018 -- iter: 096/115
[A[ATraining Step: 112  | total loss: [1m[32m0.41857[0m[0m | time: 2.983s
[2K
| Adam | epoch: 028 | loss: 0.41857 - acc: 0.8123 | val_loss: 0.67066 - val_acc: 0.7568 -- iter: 115/115
--
Training Step: 113  | total loss: [1m[32m0.39685[0m[0m | time: 0.610s
[2K
| Adam | epoch: 029 | loss: 0.39685 - acc: 0.8279 -- iter: 032/115
[A[ATraining Step: 114  | total loss: [1m[32m0.38316[0m[0m | time: 1.006s
[2K
| Adam | epoch: 029 | loss: 0.38316 - acc: 0.8295 -- iter: 064/115
[A[ATraining Step: 115  | total loss: [1m[32m0.38856[0m[0m | time: 1.401s
[2K
| Adam | epoch: 029 | loss: 0.38856 - acc: 0.8202 -- iter: 096/115
[A[ATraining Step: 116  | total loss: [1m[32m0.37238[0m[0m | time: 3.026s
[2K
| Adam | epoch: 029 | loss: 0.37238 - acc: 0.8277 | val_loss: 1.59490 - val_acc: 0.4865 -- iter: 115/115
--
Training Step: 117  | total loss: [1m[32m0.36331[0m[0m | time: 0.615s
[2K
| Adam | epoch: 030 | loss: 0.36331 - acc: 0.8355 -- iter: 032/115
[A[ATraining Step: 118  | total loss: [1m[32m0.37776[0m[0m | time: 1.255s
[2K
| Adam | epoch: 030 | loss: 0.37776 - acc: 0.8301 -- iter: 064/115
[A[ATraining Step: 119  | total loss: [1m[32m0.35311[0m[0m | time: 1.635s
[2K
| Adam | epoch: 030 | loss: 0.35311 - acc: 0.8377 -- iter: 096/115
[A[ATraining Step: 120  | total loss: [1m[32m0.34891[0m[0m | time: 3.032s
[2K
| Adam | epoch: 030 | loss: 0.34891 - acc: 0.8382 | val_loss: 0.61371 - val_acc: 0.7297 -- iter: 115/115
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.7579365079365079
Validation AUPRC:0.898577288787654
Test AUC:0.7204968944099379
Test AUPRC:0.7586248115897261
BestTestF1Score	0.78	0.35	0.7	0.73	0.83	19	7	7	4	0.27
BestTestMCCScore	0.79	0.41	0.73	0.76	0.83	19	6	8	4	0.31
BestTestAccuracyScore	0.79	0.41	0.73	0.76	0.83	19	6	8	4	0.31
BestValidationF1Score	0.9	0.51	0.84	0.84	0.96	27	5	4	1	0.27
BestValidationMCC	0.9	0.53	0.84	0.87	0.93	26	4	5	2	0.31
BestValidationAccuracy	0.9	0.53	0.84	0.87	0.93	26	4	5	2	0.31
TestPredictions (Threshold:0.31)
CHEMBL574738,TP,ACT,0.9900000095367432	CHEMBL1767275,TN,INACT,0.009999999776482582	CHEMBL2207208,FP,INACT,0.8700000047683716	CHEMBL2178352,TP,ACT,0.9599999785423279	CHEMBL3805074,TP,ACT,0.9900000095367432	CHEMBL7917,TP,ACT,0.9800000190734863	CHEMBL489430,FP,INACT,0.3700000047683716	CHEMBL3797559,TP,ACT,0.9599999785423279	CHEMBL2392241,TN,INACT,0.05000000074505806	CHEMBL521734,TN,INACT,0.25	CHEMBL53753,TP,ACT,0.8899999856948853	CHEMBL3797574,TP,ACT,0.6700000166893005	CHEMBL3659489,TP,ACT,0.7200000286102295	CHEMBL1910373,FP,INACT,0.9200000166893005	CHEMBL1241674,FN,ACT,0.1899999976158142	CHEMBL3659491,TP,ACT,0.9800000190734863	CHEMBL2392236,TN,INACT,0.009999999776482582	CHEMBL234944,FP,INACT,0.9900000095367432	CHEMBL1243406,TP,ACT,0.5299999713897705	CHEMBL1241464,FN,ACT,0.25	CHEMBL1240591,FN,ACT,0.23000000417232513	CHEMBL604251,TP,ACT,0.9399999976158142	CHEMBL3659484,TP,ACT,0.9900000095367432	CHEMBL3417208,TP,ACT,0.9100000262260437	CHEMBL3805330,TP,ACT,0.949999988079071	CHEMBL3335362,FP,INACT,0.8199999928474426	CHEMBL1271692,TP,ACT,0.5899999737739563	CHEMBL1173442,TN,INACT,0.17000000178813934	CHEMBL469770,TN,INACT,0.25	CHEMBL599519,FP,INACT,0.9900000095367432	CHEMBL2392390,TN,INACT,0.11999999731779099	CHEMBL1240590,FN,ACT,0.05000000074505806	CHEMBL1240675,TP,ACT,0.9200000166893005	CHEMBL1230609,TP,ACT,0.9900000095367432	CHEMBL1721885,TP,ACT,0.8600000143051147	CHEMBL1288067,TN,INACT,0.27000001072883606	CHEMBL1240786,TP,ACT,1.0	

