CNNModel CHEMBL3663 adam 0.0005 15 32 0 0.6 False True
Number of active compounds :	213
Number of inactive compounds :	213
---------------------------------
Run id: CNNModel_CHEMBL3663_adam_0.0005_15_32_0_0.6_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL3663_adam_0.0005_15_32_0.6_True/
---------------------------------
Training samples: 272
Validation samples: 85
--
Training Step: 1  | time: 0.792s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/272
[A[ATraining Step: 2  | total loss: [1m[32m0.62388[0m[0m | time: 1.403s
[2K
| Adam | epoch: 001 | loss: 0.62388 - acc: 0.4500 -- iter: 064/272
[A[ATraining Step: 3  | total loss: [1m[32m0.68050[0m[0m | time: 1.998s
[2K
| Adam | epoch: 001 | loss: 0.68050 - acc: 0.5420 -- iter: 096/272
[A[ATraining Step: 4  | total loss: [1m[32m0.69100[0m[0m | time: 2.604s
[2K
| Adam | epoch: 001 | loss: 0.69100 - acc: 0.4636 -- iter: 128/272
[A[ATraining Step: 5  | total loss: [1m[32m0.69342[0m[0m | time: 3.426s
[2K
| Adam | epoch: 001 | loss: 0.69342 - acc: 0.4023 -- iter: 160/272
[A[ATraining Step: 6  | total loss: [1m[32m0.69364[0m[0m | time: 4.229s
[2K
| Adam | epoch: 001 | loss: 0.69364 - acc: 0.4048 -- iter: 192/272
[A[ATraining Step: 7  | total loss: [1m[32m0.69380[0m[0m | time: 4.967s
[2K
| Adam | epoch: 001 | loss: 0.69380 - acc: 0.4057 -- iter: 224/272
[A[ATraining Step: 8  | total loss: [1m[32m0.69349[0m[0m | time: 5.711s
[2K
| Adam | epoch: 001 | loss: 0.69349 - acc: 0.4587 -- iter: 256/272
[A[ATraining Step: 9  | total loss: [1m[32m0.69329[0m[0m | time: 7.190s
[2K
| Adam | epoch: 001 | loss: 0.69329 - acc: 0.5137 | val_loss: 0.69315 - val_acc: 0.4706 -- iter: 272/272
--
Training Step: 10  | total loss: [1m[32m0.69306[0m[0m | time: 0.509s
[2K
| Adam | epoch: 002 | loss: 0.69306 - acc: 0.5693 -- iter: 032/272
[A[ATraining Step: 11  | total loss: [1m[32m0.69299[0m[0m | time: 1.137s
[2K
| Adam | epoch: 002 | loss: 0.69299 - acc: 0.5661 -- iter: 064/272
[A[ATraining Step: 12  | total loss: [1m[32m0.69348[0m[0m | time: 1.759s
[2K
| Adam | epoch: 002 | loss: 0.69348 - acc: 0.4801 -- iter: 096/272
[A[ATraining Step: 13  | total loss: [1m[32m0.69298[0m[0m | time: 2.381s
[2K
| Adam | epoch: 002 | loss: 0.69298 - acc: 0.5154 -- iter: 128/272
[A[ATraining Step: 14  | total loss: [1m[32m0.69232[0m[0m | time: 3.002s
[2K
| Adam | epoch: 002 | loss: 0.69232 - acc: 0.5986 -- iter: 160/272
[A[ATraining Step: 15  | total loss: [1m[32m0.69269[0m[0m | time: 3.619s
[2K
| Adam | epoch: 002 | loss: 0.69269 - acc: 0.5600 -- iter: 192/272
[A[ATraining Step: 16  | total loss: [1m[32m0.69301[0m[0m | time: 4.319s
[2K
| Adam | epoch: 002 | loss: 0.69301 - acc: 0.5375 -- iter: 224/272
[A[ATraining Step: 17  | total loss: [1m[32m0.69310[0m[0m | time: 5.166s
[2K
| Adam | epoch: 002 | loss: 0.69310 - acc: 0.5240 -- iter: 256/272
[A[ATraining Step: 18  | total loss: [1m[32m0.69361[0m[0m | time: 6.905s
[2K
| Adam | epoch: 002 | loss: 0.69361 - acc: 0.4941 | val_loss: 0.69392 - val_acc: 0.4706 -- iter: 272/272
--
Training Step: 19  | total loss: [1m[32m0.69239[0m[0m | time: 0.382s
[2K
| Adam | epoch: 003 | loss: 0.69239 - acc: 0.5481 -- iter: 032/272
[A[ATraining Step: 20  | total loss: [1m[32m0.69126[0m[0m | time: 0.780s
[2K
| Adam | epoch: 003 | loss: 0.69126 - acc: 0.5929 -- iter: 064/272
[A[ATraining Step: 21  | total loss: [1m[32m0.69017[0m[0m | time: 1.507s
[2K
| Adam | epoch: 003 | loss: 0.69017 - acc: 0.6223 -- iter: 096/272
[A[ATraining Step: 22  | total loss: [1m[32m0.69190[0m[0m | time: 2.236s
[2K
| Adam | epoch: 003 | loss: 0.69190 - acc: 0.5668 -- iter: 128/272
[A[ATraining Step: 23  | total loss: [1m[32m0.69247[0m[0m | time: 3.019s
[2K
| Adam | epoch: 003 | loss: 0.69247 - acc: 0.5474 -- iter: 160/272
[A[ATraining Step: 24  | total loss: [1m[32m0.69165[0m[0m | time: 3.622s
[2K
| Adam | epoch: 003 | loss: 0.69165 - acc: 0.5517 -- iter: 192/272
[A[ATraining Step: 25  | total loss: [1m[32m0.69406[0m[0m | time: 4.222s
[2K
| Adam | epoch: 003 | loss: 0.69406 - acc: 0.5120 -- iter: 224/272
[A[ATraining Step: 26  | total loss: [1m[32m0.69283[0m[0m | time: 4.837s
[2K
| Adam | epoch: 003 | loss: 0.69283 - acc: 0.5254 -- iter: 256/272
[A[ATraining Step: 27  | total loss: [1m[32m0.69305[0m[0m | time: 6.436s
[2K
| Adam | epoch: 003 | loss: 0.69305 - acc: 0.5189 | val_loss: 0.69499 - val_acc: 0.4706 -- iter: 272/272
--
Training Step: 28  | total loss: [1m[32m0.69411[0m[0m | time: 0.749s
[2K
| Adam | epoch: 004 | loss: 0.69411 - acc: 0.4985 -- iter: 032/272
[A[ATraining Step: 29  | total loss: [1m[32m0.69346[0m[0m | time: 1.117s
[2K
| Adam | epoch: 004 | loss: 0.69346 - acc: 0.5065 -- iter: 064/272
[A[ATraining Step: 30  | total loss: [1m[32m0.69351[0m[0m | time: 1.543s
[2K
| Adam | epoch: 004 | loss: 0.69351 - acc: 0.5049 -- iter: 096/272
[A[ATraining Step: 31  | total loss: [1m[32m0.69335[0m[0m | time: 2.313s
[2K
| Adam | epoch: 004 | loss: 0.69335 - acc: 0.5038 -- iter: 128/272
[A[ATraining Step: 32  | total loss: [1m[32m0.69297[0m[0m | time: 3.068s
[2K
| Adam | epoch: 004 | loss: 0.69297 - acc: 0.5100 -- iter: 160/272
[A[ATraining Step: 33  | total loss: [1m[32m0.69202[0m[0m | time: 3.815s
[2K
| Adam | epoch: 004 | loss: 0.69202 - acc: 0.5284 -- iter: 192/272
[A[ATraining Step: 34  | total loss: [1m[32m0.69187[0m[0m | time: 4.599s
[2K
| Adam | epoch: 004 | loss: 0.69187 - acc: 0.5290 -- iter: 224/272
[A[ATraining Step: 35  | total loss: [1m[32m0.69135[0m[0m | time: 5.196s
[2K
| Adam | epoch: 004 | loss: 0.69135 - acc: 0.5360 -- iter: 256/272
[A[ATraining Step: 36  | total loss: [1m[32m0.69231[0m[0m | time: 6.796s
[2K
| Adam | epoch: 004 | loss: 0.69231 - acc: 0.5159 | val_loss: 0.69296 - val_acc: 0.4706 -- iter: 272/272
--
Training Step: 37  | total loss: [1m[32m0.69193[0m[0m | time: 0.760s
[2K
| Adam | epoch: 005 | loss: 0.69193 - acc: 0.5189 -- iter: 032/272
[A[ATraining Step: 38  | total loss: [1m[32m0.68994[0m[0m | time: 1.524s
[2K
| Adam | epoch: 005 | loss: 0.68994 - acc: 0.5519 -- iter: 064/272
[A[ATraining Step: 39  | total loss: [1m[32m0.69115[0m[0m | time: 1.953s
[2K
| Adam | epoch: 005 | loss: 0.69115 - acc: 0.5300 -- iter: 096/272
[A[ATraining Step: 40  | total loss: [1m[32m0.69278[0m[0m | time: 2.331s
[2K
| Adam | epoch: 005 | loss: 0.69278 - acc: 0.5009 -- iter: 128/272
[A[ATraining Step: 41  | total loss: [1m[32m0.69381[0m[0m | time: 3.074s
[2K
| Adam | epoch: 005 | loss: 0.69381 - acc: 0.4778 -- iter: 160/272
[A[ATraining Step: 42  | total loss: [1m[32m0.69326[0m[0m | time: 3.786s
[2K
| Adam | epoch: 005 | loss: 0.69326 - acc: 0.4818 -- iter: 192/272
[A[ATraining Step: 43  | total loss: [1m[32m0.69313[0m[0m | time: 4.513s
[2K
| Adam | epoch: 005 | loss: 0.69313 - acc: 0.4740 -- iter: 224/272
[A[ATraining Step: 44  | total loss: [1m[32m0.69266[0m[0m | time: 5.271s
[2K
| Adam | epoch: 005 | loss: 0.69266 - acc: 0.5001 -- iter: 256/272
[A[ATraining Step: 45  | total loss: [1m[32m0.69270[0m[0m | time: 7.006s
[2K
| Adam | epoch: 005 | loss: 0.69270 - acc: 0.5107 | val_loss: 0.68574 - val_acc: 0.8353 -- iter: 272/272
--
Training Step: 46  | total loss: [1m[32m0.69214[0m[0m | time: 0.741s
[2K
| Adam | epoch: 006 | loss: 0.69214 - acc: 0.5402 -- iter: 032/272
[A[ATraining Step: 47  | total loss: [1m[32m0.69119[0m[0m | time: 1.542s
[2K
| Adam | epoch: 006 | loss: 0.69119 - acc: 0.5694 -- iter: 064/272
[A[ATraining Step: 48  | total loss: [1m[32m0.68957[0m[0m | time: 2.342s
[2K
| Adam | epoch: 006 | loss: 0.68957 - acc: 0.5834 -- iter: 096/272
[A[ATraining Step: 49  | total loss: [1m[32m0.68813[0m[0m | time: 2.755s
[2K
| Adam | epoch: 006 | loss: 0.68813 - acc: 0.5850 -- iter: 128/272
[A[ATraining Step: 50  | total loss: [1m[32m0.68783[0m[0m | time: 3.164s
[2K
| Adam | epoch: 006 | loss: 0.68783 - acc: 0.5718 -- iter: 160/272
[A[ATraining Step: 51  | total loss: [1m[32m0.68707[0m[0m | time: 3.894s
[2K
| Adam | epoch: 006 | loss: 0.68707 - acc: 0.5609 -- iter: 192/272
[A[ATraining Step: 52  | total loss: [1m[32m0.68459[0m[0m | time: 4.647s
[2K
| Adam | epoch: 006 | loss: 0.68459 - acc: 0.5658 -- iter: 224/272
[A[ATraining Step: 53  | total loss: [1m[32m0.68340[0m[0m | time: 5.398s
[2K
| Adam | epoch: 006 | loss: 0.68340 - acc: 0.5561 -- iter: 256/272
[A[ATraining Step: 54  | total loss: [1m[32m0.68301[0m[0m | time: 7.168s
[2K
| Adam | epoch: 006 | loss: 0.68301 - acc: 0.5616 | val_loss: 0.64652 - val_acc: 0.7765 -- iter: 272/272
--
Training Step: 55  | total loss: [1m[32m0.68242[0m[0m | time: 0.611s
[2K
| Adam | epoch: 007 | loss: 0.68242 - acc: 0.5572 -- iter: 032/272
[A[ATraining Step: 56  | total loss: [1m[32m0.68186[0m[0m | time: 1.277s
[2K
| Adam | epoch: 007 | loss: 0.68186 - acc: 0.5492 -- iter: 064/272
[A[ATraining Step: 57  | total loss: [1m[32m0.67688[0m[0m | time: 1.880s
[2K
| Adam | epoch: 007 | loss: 0.67688 - acc: 0.5813 -- iter: 096/272
[A[ATraining Step: 58  | total loss: [1m[32m0.67545[0m[0m | time: 2.555s
[2K
| Adam | epoch: 007 | loss: 0.67545 - acc: 0.5915 -- iter: 128/272
[A[ATraining Step: 59  | total loss: [1m[32m0.67461[0m[0m | time: 2.963s
[2K
| Adam | epoch: 007 | loss: 0.67461 - acc: 0.5876 -- iter: 160/272
[A[ATraining Step: 60  | total loss: [1m[32m0.66435[0m[0m | time: 3.364s
[2K
| Adam | epoch: 007 | loss: 0.66435 - acc: 0.6257 -- iter: 192/272
[A[ATraining Step: 61  | total loss: [1m[32m0.65055[0m[0m | time: 4.183s
[2K
| Adam | epoch: 007 | loss: 0.65055 - acc: 0.6419 -- iter: 224/272
[A[ATraining Step: 62  | total loss: [1m[32m0.65005[0m[0m | time: 4.938s
[2K
| Adam | epoch: 007 | loss: 0.65005 - acc: 0.6277 -- iter: 256/272
[A[ATraining Step: 63  | total loss: [1m[32m0.64331[0m[0m | time: 6.655s
[2K
| Adam | epoch: 007 | loss: 0.64331 - acc: 0.6392 | val_loss: 0.50363 - val_acc: 0.8235 -- iter: 272/272
--
Training Step: 64  | total loss: [1m[32m0.62754[0m[0m | time: 0.598s
[2K
| Adam | epoch: 008 | loss: 0.62754 - acc: 0.6648 -- iter: 032/272
[A[ATraining Step: 65  | total loss: [1m[32m0.62970[0m[0m | time: 1.195s
[2K
| Adam | epoch: 008 | loss: 0.62970 - acc: 0.6676 -- iter: 064/272
[A[ATraining Step: 66  | total loss: [1m[32m0.62099[0m[0m | time: 1.803s
[2K
| Adam | epoch: 008 | loss: 0.62099 - acc: 0.6776 -- iter: 096/272
[A[ATraining Step: 67  | total loss: [1m[32m0.63304[0m[0m | time: 2.404s
[2K
| Adam | epoch: 008 | loss: 0.63304 - acc: 0.6638 -- iter: 128/272
[A[ATraining Step: 68  | total loss: [1m[32m0.63667[0m[0m | time: 3.055s
[2K
| Adam | epoch: 008 | loss: 0.63667 - acc: 0.6592 -- iter: 160/272
[A[ATraining Step: 69  | total loss: [1m[32m0.63034[0m[0m | time: 3.448s
[2K
| Adam | epoch: 008 | loss: 0.63034 - acc: 0.6625 -- iter: 192/272
[A[ATraining Step: 70  | total loss: [1m[32m0.61039[0m[0m | time: 3.837s
[2K
| Adam | epoch: 008 | loss: 0.61039 - acc: 0.6726 -- iter: 224/272
[A[ATraining Step: 71  | total loss: [1m[32m0.59074[0m[0m | time: 4.573s
[2K
| Adam | epoch: 008 | loss: 0.59074 - acc: 0.6885 -- iter: 256/272
[A[ATraining Step: 72  | total loss: [1m[32m0.58809[0m[0m | time: 6.360s
[2K
| Adam | epoch: 008 | loss: 0.58809 - acc: 0.7025 | val_loss: 0.44293 - val_acc: 0.8235 -- iter: 272/272
--
Training Step: 73  | total loss: [1m[32m0.59348[0m[0m | time: 0.834s
[2K
| Adam | epoch: 009 | loss: 0.59348 - acc: 0.7008 -- iter: 032/272
[A[ATraining Step: 74  | total loss: [1m[32m0.59228[0m[0m | time: 1.441s
[2K
| Adam | epoch: 009 | loss: 0.59228 - acc: 0.6959 -- iter: 064/272
[A[ATraining Step: 75  | total loss: [1m[32m0.59206[0m[0m | time: 2.048s
[2K
| Adam | epoch: 009 | loss: 0.59206 - acc: 0.6950 -- iter: 096/272
[A[ATraining Step: 76  | total loss: [1m[32m0.58446[0m[0m | time: 2.659s
[2K
| Adam | epoch: 009 | loss: 0.58446 - acc: 0.7009 -- iter: 128/272
[A[ATraining Step: 77  | total loss: [1m[32m0.59501[0m[0m | time: 3.259s
[2K
| Adam | epoch: 009 | loss: 0.59501 - acc: 0.6929 -- iter: 160/272
[A[ATraining Step: 78  | total loss: [1m[32m0.58208[0m[0m | time: 3.981s
[2K
| Adam | epoch: 009 | loss: 0.58208 - acc: 0.6988 -- iter: 192/272
[A[ATraining Step: 79  | total loss: [1m[32m0.58715[0m[0m | time: 4.378s
[2K
| Adam | epoch: 009 | loss: 0.58715 - acc: 0.6977 -- iter: 224/272
[A[ATraining Step: 80  | total loss: [1m[32m0.57759[0m[0m | time: 4.753s
[2K
| Adam | epoch: 009 | loss: 0.57759 - acc: 0.7094 -- iter: 256/272
[A[ATraining Step: 81  | total loss: [1m[32m0.56919[0m[0m | time: 6.474s
[2K
| Adam | epoch: 009 | loss: 0.56919 - acc: 0.7198 | val_loss: 0.51089 - val_acc: 0.8000 -- iter: 272/272
--
Training Step: 82  | total loss: [1m[32m0.57122[0m[0m | time: 0.744s
[2K
| Adam | epoch: 010 | loss: 0.57122 - acc: 0.7166 -- iter: 032/272
[A[ATraining Step: 83  | total loss: [1m[32m0.57354[0m[0m | time: 1.552s
[2K
| Adam | epoch: 010 | loss: 0.57354 - acc: 0.7168 -- iter: 064/272
[A[ATraining Step: 84  | total loss: [1m[32m0.57595[0m[0m | time: 2.154s
[2K
| Adam | epoch: 010 | loss: 0.57595 - acc: 0.7108 -- iter: 096/272
[A[ATraining Step: 85  | total loss: [1m[32m0.58283[0m[0m | time: 2.748s
[2K
| Adam | epoch: 010 | loss: 0.58283 - acc: 0.7022 -- iter: 128/272
[A[ATraining Step: 86  | total loss: [1m[32m0.57895[0m[0m | time: 3.368s
[2K
| Adam | epoch: 010 | loss: 0.57895 - acc: 0.7101 -- iter: 160/272
[A[ATraining Step: 87  | total loss: [1m[32m0.58742[0m[0m | time: 3.971s
[2K
| Adam | epoch: 010 | loss: 0.58742 - acc: 0.6985 -- iter: 192/272
[A[ATraining Step: 88  | total loss: [1m[32m0.59680[0m[0m | time: 4.611s
[2K
| Adam | epoch: 010 | loss: 0.59680 - acc: 0.6911 -- iter: 224/272
[A[ATraining Step: 89  | total loss: [1m[32m0.59008[0m[0m | time: 4.920s
[2K
| Adam | epoch: 010 | loss: 0.59008 - acc: 0.6970 -- iter: 256/272
[A[ATraining Step: 90  | total loss: [1m[32m0.59996[0m[0m | time: 6.237s
[2K
| Adam | epoch: 010 | loss: 0.59996 - acc: 0.6898 | val_loss: 0.49974 - val_acc: 0.8353 -- iter: 272/272
--
Training Step: 91  | total loss: [1m[32m0.60787[0m[0m | time: 0.720s
[2K
| Adam | epoch: 011 | loss: 0.60787 - acc: 0.6771 -- iter: 032/272
[A[ATraining Step: 92  | total loss: [1m[32m0.59143[0m[0m | time: 1.460s
[2K
| Adam | epoch: 011 | loss: 0.59143 - acc: 0.7000 -- iter: 064/272
[A[ATraining Step: 93  | total loss: [1m[32m0.58674[0m[0m | time: 2.182s
[2K
| Adam | epoch: 011 | loss: 0.58674 - acc: 0.7081 -- iter: 096/272
[A[ATraining Step: 94  | total loss: [1m[32m0.58200[0m[0m | time: 2.885s
[2K
| Adam | epoch: 011 | loss: 0.58200 - acc: 0.7154 -- iter: 128/272
[A[ATraining Step: 95  | total loss: [1m[32m0.58551[0m[0m | time: 3.651s
[2K
| Adam | epoch: 011 | loss: 0.58551 - acc: 0.7095 -- iter: 160/272
[A[ATraining Step: 96  | total loss: [1m[32m0.58434[0m[0m | time: 4.427s
[2K
| Adam | epoch: 011 | loss: 0.58434 - acc: 0.7136 -- iter: 192/272
[A[ATraining Step: 97  | total loss: [1m[32m0.58004[0m[0m | time: 5.219s
[2K
| Adam | epoch: 011 | loss: 0.58004 - acc: 0.7141 -- iter: 224/272
[A[ATraining Step: 98  | total loss: [1m[32m0.57437[0m[0m | time: 5.957s
[2K
| Adam | epoch: 011 | loss: 0.57437 - acc: 0.7177 -- iter: 256/272
[A[ATraining Step: 99  | total loss: [1m[32m0.57231[0m[0m | time: 7.288s
[2K
| Adam | epoch: 011 | loss: 0.57231 - acc: 0.7147 | val_loss: 0.49764 - val_acc: 0.7882 -- iter: 272/272
--
Training Step: 100  | total loss: [1m[32m0.57797[0m[0m | time: 0.388s
[2K
| Adam | epoch: 012 | loss: 0.57797 - acc: 0.7119 -- iter: 032/272
[A[ATraining Step: 101  | total loss: [1m[32m0.57724[0m[0m | time: 1.125s
[2K
| Adam | epoch: 012 | loss: 0.57724 - acc: 0.7095 -- iter: 064/272
[A[ATraining Step: 102  | total loss: [1m[32m0.56091[0m[0m | time: 1.875s
[2K
| Adam | epoch: 012 | loss: 0.56091 - acc: 0.7229 -- iter: 096/272
[A[ATraining Step: 103  | total loss: [1m[32m0.54165[0m[0m | time: 2.616s
[2K
| Adam | epoch: 012 | loss: 0.54165 - acc: 0.7381 -- iter: 128/272
[A[ATraining Step: 104  | total loss: [1m[32m0.55024[0m[0m | time: 3.302s
[2K
| Adam | epoch: 012 | loss: 0.55024 - acc: 0.7299 -- iter: 160/272
[A[ATraining Step: 105  | total loss: [1m[32m0.57129[0m[0m | time: 4.029s
[2K
| Adam | epoch: 012 | loss: 0.57129 - acc: 0.7101 -- iter: 192/272
[A[ATraining Step: 106  | total loss: [1m[32m0.57968[0m[0m | time: 4.755s
[2K
| Adam | epoch: 012 | loss: 0.57968 - acc: 0.7078 -- iter: 224/272
[A[ATraining Step: 107  | total loss: [1m[32m0.58039[0m[0m | time: 5.561s
[2K
| Adam | epoch: 012 | loss: 0.58039 - acc: 0.7089 -- iter: 256/272
[A[ATraining Step: 108  | total loss: [1m[32m0.57972[0m[0m | time: 7.172s
[2K
| Adam | epoch: 012 | loss: 0.57972 - acc: 0.7130 | val_loss: 0.50420 - val_acc: 0.8118 -- iter: 272/272
--
Training Step: 109  | total loss: [1m[32m0.57273[0m[0m | time: 0.377s
[2K
| Adam | epoch: 013 | loss: 0.57273 - acc: 0.7167 -- iter: 032/272
[A[ATraining Step: 110  | total loss: [1m[32m0.56878[0m[0m | time: 0.862s
[2K
| Adam | epoch: 013 | loss: 0.56878 - acc: 0.7138 -- iter: 064/272
[A[ATraining Step: 111  | total loss: [1m[32m0.56557[0m[0m | time: 1.598s
[2K
| Adam | epoch: 013 | loss: 0.56557 - acc: 0.7112 -- iter: 096/272
[A[ATraining Step: 112  | total loss: [1m[32m0.57389[0m[0m | time: 2.331s
[2K
| Adam | epoch: 013 | loss: 0.57389 - acc: 0.7057 -- iter: 128/272
[A[ATraining Step: 113  | total loss: [1m[32m0.56679[0m[0m | time: 3.064s
[2K
| Adam | epoch: 013 | loss: 0.56679 - acc: 0.7195 -- iter: 160/272
[A[ATraining Step: 114  | total loss: [1m[32m0.56968[0m[0m | time: 3.816s
[2K
| Adam | epoch: 013 | loss: 0.56968 - acc: 0.7132 -- iter: 192/272
[A[ATraining Step: 115  | total loss: [1m[32m0.56413[0m[0m | time: 4.538s
[2K
| Adam | epoch: 013 | loss: 0.56413 - acc: 0.7231 -- iter: 224/272
[A[ATraining Step: 116  | total loss: [1m[32m0.55343[0m[0m | time: 5.251s
[2K
| Adam | epoch: 013 | loss: 0.55343 - acc: 0.7352 -- iter: 256/272
[A[ATraining Step: 117  | total loss: [1m[32m0.54665[0m[0m | time: 7.025s
[2K
| Adam | epoch: 013 | loss: 0.54665 - acc: 0.7460 | val_loss: 0.43298 - val_acc: 0.8588 -- iter: 272/272
--
Training Step: 118  | total loss: [1m[32m0.55101[0m[0m | time: 0.599s
[2K
| Adam | epoch: 014 | loss: 0.55101 - acc: 0.7370 -- iter: 032/272
[A[ATraining Step: 119  | total loss: [1m[32m0.53334[0m[0m | time: 0.918s
[2K
| Adam | epoch: 014 | loss: 0.53334 - acc: 0.7540 -- iter: 064/272
[A[ATraining Step: 120  | total loss: [1m[32m0.54344[0m[0m | time: 1.260s
[2K
| Adam | epoch: 014 | loss: 0.54344 - acc: 0.7473 -- iter: 096/272
[A[ATraining Step: 121  | total loss: [1m[32m0.55047[0m[0m | time: 1.980s
[2K
| Adam | epoch: 014 | loss: 0.55047 - acc: 0.7413 -- iter: 128/272
[A[ATraining Step: 122  | total loss: [1m[32m0.53663[0m[0m | time: 2.760s
[2K
| Adam | epoch: 014 | loss: 0.53663 - acc: 0.7547 -- iter: 160/272
[A[ATraining Step: 123  | total loss: [1m[32m0.53711[0m[0m | time: 3.521s
[2K
| Adam | epoch: 014 | loss: 0.53711 - acc: 0.7480 -- iter: 192/272
[A[ATraining Step: 124  | total loss: [1m[32m0.53397[0m[0m | time: 4.276s
[2K
| Adam | epoch: 014 | loss: 0.53397 - acc: 0.7513 -- iter: 224/272
[A[ATraining Step: 125  | total loss: [1m[32m0.53379[0m[0m | time: 5.020s
[2K
| Adam | epoch: 014 | loss: 0.53379 - acc: 0.7543 -- iter: 256/272
[A[ATraining Step: 126  | total loss: [1m[32m0.54922[0m[0m | time: 6.748s
[2K
| Adam | epoch: 014 | loss: 0.54922 - acc: 0.7476 | val_loss: 0.44718 - val_acc: 0.8471 -- iter: 272/272
--
Training Step: 127  | total loss: [1m[32m0.52713[0m[0m | time: 0.795s
[2K
| Adam | epoch: 015 | loss: 0.52713 - acc: 0.7635 -- iter: 032/272
[A[ATraining Step: 128  | total loss: [1m[32m0.51929[0m[0m | time: 1.400s
[2K
| Adam | epoch: 015 | loss: 0.51929 - acc: 0.7684 -- iter: 064/272
[A[ATraining Step: 129  | total loss: [1m[32m0.51307[0m[0m | time: 1.727s
[2K
| Adam | epoch: 015 | loss: 0.51307 - acc: 0.7728 -- iter: 096/272
[A[ATraining Step: 130  | total loss: [1m[32m0.50764[0m[0m | time: 2.038s
[2K
| Adam | epoch: 015 | loss: 0.50764 - acc: 0.7705 -- iter: 128/272
[A[ATraining Step: 131  | total loss: [1m[32m0.49812[0m[0m | time: 2.631s
[2K
| Adam | epoch: 015 | loss: 0.49812 - acc: 0.7747 -- iter: 160/272
[A[ATraining Step: 132  | total loss: [1m[32m0.49727[0m[0m | time: 3.226s
[2K
| Adam | epoch: 015 | loss: 0.49727 - acc: 0.7754 -- iter: 192/272
[A[ATraining Step: 133  | total loss: [1m[32m0.47999[0m[0m | time: 3.980s
[2K
| Adam | epoch: 015 | loss: 0.47999 - acc: 0.7885 -- iter: 224/272
[A[ATraining Step: 134  | total loss: [1m[32m0.48821[0m[0m | time: 4.690s
[2K
| Adam | epoch: 015 | loss: 0.48821 - acc: 0.7815 -- iter: 256/272
[A[ATraining Step: 135  | total loss: [1m[32m0.49219[0m[0m | time: 6.415s
[2K
| Adam | epoch: 015 | loss: 0.49219 - acc: 0.7815 | val_loss: 0.42480 - val_acc: 0.8588 -- iter: 272/272
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.8988888888888888
Validation AUPRC:0.8816111614114225
Test AUC:0.8555555555555556
Test AUPRC:0.8592230738090175
BestTestF1Score	0.84	0.67	0.84	0.84	0.84	38	7	33	7	0.5
BestTestMCCScore	0.84	0.67	0.84	0.84	0.84	38	7	33	7	0.5
BestTestAccuracyScore	0.84	0.67	0.84	0.84	0.84	38	7	33	7	0.5
BestValidationF1Score	0.86	0.72	0.86	0.88	0.84	38	5	35	7	0.5
BestValidationMCC	0.86	0.72	0.86	0.88	0.84	38	5	35	7	0.5
BestValidationAccuracy	0.86	0.72	0.86	0.88	0.84	38	5	35	7	0.5
TestPredictions (Threshold:0.5)
CHEMBL521201,FP,INACT,0.8600000143051147	CHEMBL137653,TN,INACT,0.2199999988079071	CHEMBL390420,TP,ACT,0.8399999737739563	CHEMBL2011302,TN,INACT,0.49000000953674316	CHEMBL222550,TP,ACT,0.699999988079071	CHEMBL425761,FN,ACT,0.20999999344348907	CHEMBL142648,TN,INACT,0.17000000178813934	CHEMBL316834,TN,INACT,0.07999999821186066	CHEMBL355701,TP,ACT,0.8500000238418579	CHEMBL168417,TP,ACT,0.6800000071525574	CHEMBL172088,FP,INACT,0.8799999952316284	CHEMBL412230,TP,ACT,0.75	CHEMBL264809,TP,ACT,0.8799999952316284	CHEMBL78061,TP,ACT,0.7699999809265137	CHEMBL26128,TN,INACT,0.2800000011920929	CHEMBL175456,TP,ACT,0.8299999833106995	CHEMBL438610,TN,INACT,0.4399999976158142	CHEMBL401038,TN,INACT,0.10000000149011612	CHEMBL71034,TP,ACT,0.8999999761581421	CHEMBL70852,FN,ACT,0.15000000596046448	CHEMBL474863,TN,INACT,0.07999999821186066	CHEMBL2372812,FN,ACT,0.14000000059604645	CHEMBL277236,TN,INACT,0.12999999523162842	CHEMBL602471,TN,INACT,0.10000000149011612	CHEMBL1933747,FP,INACT,0.5199999809265137	CHEMBL1081198,TN,INACT,0.1599999964237213	CHEMBL408163,TP,ACT,0.8100000023841858	CHEMBL539433,TN,INACT,0.18000000715255737	CHEMBL539207,TP,ACT,0.8899999856948853	CHEMBL355759,TP,ACT,0.8700000047683716	CHEMBL565084,TN,INACT,0.14000000059604645	CHEMBL78242,TP,ACT,0.7699999809265137	CHEMBL56081,TN,INACT,0.1599999964237213	CHEMBL3144402,FN,ACT,0.14000000059604645	CHEMBL193015,TP,ACT,0.6700000166893005	CHEMBL73829,TN,INACT,0.41999998688697815	CHEMBL332862,TP,ACT,0.8999999761581421	CHEMBL281577,FN,ACT,0.1599999964237213	CHEMBL72437,TP,ACT,0.8999999761581421	CHEMBL117488,TN,INACT,0.10999999940395355	CHEMBL1642269,TN,INACT,0.11999999731779099	CHEMBL216415,TP,ACT,0.8799999952316284	CHEMBL410772,TP,ACT,0.8299999833106995	CHEMBL327030,TP,ACT,0.6899999976158142	CHEMBL359934,TP,ACT,0.8100000023841858	CHEMBL428647,FP,INACT,0.7900000214576721	CHEMBL1627247,TP,ACT,0.8799999952316284	CHEMBL1627260,TN,INACT,0.46000000834465027	CHEMBL406825,TP,ACT,0.8899999856948853	CHEMBL1642271,TN,INACT,0.15000000596046448	CHEMBL457077,FP,INACT,0.5299999713897705	CHEMBL367440,TP,ACT,0.7799999713897705	CHEMBL1241391,TN,INACT,0.3799999952316284	CHEMBL216646,TN,INACT,0.10000000149011612	CHEMBL3143004,TP,ACT,0.8799999952316284	CHEMBL1738705,FP,INACT,0.7699999809265137	CHEMBL276046,TP,ACT,0.8999999761581421	CHEMBL435910,TP,ACT,0.8100000023841858	CHEMBL80333,TN,INACT,0.33000001311302185	CHEMBL282575,TN,INACT,0.09000000357627869	CHEMBL19378,TP,ACT,0.75	CHEMBL61048,TN,INACT,0.20000000298023224	CHEMBL555424,TP,ACT,0.8399999737739563	CHEMBL312875,TP,ACT,0.8100000023841858	CHEMBL3142928,FN,ACT,0.15000000596046448	CHEMBL202731,FP,INACT,0.8399999737739563	CHEMBL2371612,TP,ACT,0.8199999928474426	CHEMBL462230,TP,ACT,0.6000000238418579	CHEMBL508925,TP,ACT,0.6100000143051147	CHEMBL19214,FN,ACT,0.1599999964237213	CHEMBL66278,TN,INACT,0.12999999523162842	CHEMBL373585,TP,ACT,0.7900000214576721	CHEMBL424823,TP,ACT,0.8199999928474426	CHEMBL1627243,TN,INACT,0.1899999976158142	CHEMBL73003,TP,ACT,0.8700000047683716	CHEMBL1790815,TN,INACT,0.14000000059604645	CHEMBL129842,TN,INACT,0.10999999940395355	CHEMBL515258,TN,INACT,0.10999999940395355	CHEMBL3335245,TN,INACT,0.10999999940395355	CHEMBL374029,TP,ACT,0.7200000286102295	CHEMBL56671,TN,INACT,0.18000000715255737	CHEMBL422705,TP,ACT,0.8600000143051147	CHEMBL436789,TP,ACT,0.8199999928474426	CHEMBL522387,TN,INACT,0.33000001311302185	CHEMBL132006,TN,INACT,0.11999999731779099	

