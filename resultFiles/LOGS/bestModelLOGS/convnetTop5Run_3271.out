CNNModel CHEMBL1957 RMSprop 0.001 30 128 0 0.6 False True
Number of active compounds :	1567
Number of inactive compounds :	1567
---------------------------------
Run id: CNNModel_CHEMBL1957_RMSprop_0.001_30_128_0_0.6_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL1957_RMSprop_0.001_30_128_0.6_True/
---------------------------------
Training samples: 2003
Validation samples: 626
--
Training Step: 1  | time: 1.882s
[2K
| RMSProp | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 0032/2003
[A[ATraining Step: 2  | total loss: [1m[32m0.62308[0m[0m | time: 2.941s
[2K
| RMSProp | epoch: 001 | loss: 0.62308 - acc: 0.5625 -- iter: 0064/2003
[A[ATraining Step: 3  | total loss: [1m[32m0.67987[0m[0m | time: 4.170s
[2K
| RMSProp | epoch: 001 | loss: 0.67987 - acc: 0.5625 -- iter: 0096/2003
[A[ATraining Step: 4  | total loss: [1m[32m0.69065[0m[0m | time: 5.397s
[2K
| RMSProp | epoch: 001 | loss: 0.69065 - acc: 0.3984 -- iter: 0128/2003
[A[ATraining Step: 5  | total loss: [1m[32m0.69230[0m[0m | time: 6.660s
[2K
| RMSProp | epoch: 001 | loss: 0.69230 - acc: 0.5120 -- iter: 0160/2003
[A[ATraining Step: 6  | total loss: [1m[32m0.69261[0m[0m | time: 7.837s
[2K
| RMSProp | epoch: 001 | loss: 0.69261 - acc: 0.5445 -- iter: 0192/2003
[A[ATraining Step: 7  | total loss: [1m[32m0.69248[0m[0m | time: 8.910s
[2K
| RMSProp | epoch: 001 | loss: 0.69248 - acc: 0.5928 -- iter: 0224/2003
[A[ATraining Step: 8  | total loss: [1m[32m0.69326[0m[0m | time: 10.085s
[2K
| RMSProp | epoch: 001 | loss: 0.69326 - acc: 0.4879 -- iter: 0256/2003
[A[ATraining Step: 9  | total loss: [1m[32m0.69339[0m[0m | time: 11.316s
[2K
| RMSProp | epoch: 001 | loss: 0.69339 - acc: 0.4777 -- iter: 0288/2003
[A[ATraining Step: 10  | total loss: [1m[32m0.69327[0m[0m | time: 12.497s
[2K
| RMSProp | epoch: 001 | loss: 0.69327 - acc: 0.4732 -- iter: 0320/2003
[A[ATraining Step: 11  | total loss: [1m[32m0.69340[0m[0m | time: 13.731s
[2K
| RMSProp | epoch: 001 | loss: 0.69340 - acc: 0.4415 -- iter: 0352/2003
[A[ATraining Step: 12  | total loss: [1m[32m0.69346[0m[0m | time: 14.845s
[2K
| RMSProp | epoch: 001 | loss: 0.69346 - acc: 0.4397 -- iter: 0384/2003
[A[ATraining Step: 13  | total loss: [1m[32m0.69344[0m[0m | time: 15.792s
[2K
| RMSProp | epoch: 001 | loss: 0.69344 - acc: 0.4120 -- iter: 0416/2003
[A[ATraining Step: 14  | total loss: [1m[32m0.69322[0m[0m | time: 16.671s
[2K
| RMSProp | epoch: 001 | loss: 0.69322 - acc: 0.4352 -- iter: 0448/2003
[A[ATraining Step: 15  | total loss: [1m[32m0.69303[0m[0m | time: 17.574s
[2K
| RMSProp | epoch: 001 | loss: 0.69303 - acc: 0.4728 -- iter: 0480/2003
[A[ATraining Step: 16  | total loss: [1m[32m0.69301[0m[0m | time: 18.430s
[2K
| RMSProp | epoch: 001 | loss: 0.69301 - acc: 0.4830 -- iter: 0512/2003
[A[ATraining Step: 17  | total loss: [1m[32m0.69296[0m[0m | time: 19.317s
[2K
| RMSProp | epoch: 001 | loss: 0.69296 - acc: 0.5116 -- iter: 0544/2003
[A[ATraining Step: 18  | total loss: [1m[32m0.69285[0m[0m | time: 20.302s
[2K
| RMSProp | epoch: 001 | loss: 0.69285 - acc: 0.5400 -- iter: 0576/2003
[A[ATraining Step: 19  | total loss: [1m[32m0.69260[0m[0m | time: 21.277s
[2K
| RMSProp | epoch: 001 | loss: 0.69260 - acc: 0.5996 -- iter: 0608/2003
[A[ATraining Step: 20  | total loss: [1m[32m0.69274[0m[0m | time: 22.008s
[2K
| RMSProp | epoch: 001 | loss: 0.69274 - acc: 0.5776 -- iter: 0640/2003
[A[ATraining Step: 21  | total loss: [1m[32m0.69288[0m[0m | time: 22.869s
[2K
| RMSProp | epoch: 001 | loss: 0.69288 - acc: 0.5438 -- iter: 0672/2003
[A[ATraining Step: 22  | total loss: [1m[32m0.69301[0m[0m | time: 23.697s
[2K
| RMSProp | epoch: 001 | loss: 0.69301 - acc: 0.5213 -- iter: 0704/2003
[A[ATraining Step: 23  | total loss: [1m[32m0.69297[0m[0m | time: 24.563s
[2K
| RMSProp | epoch: 001 | loss: 0.69297 - acc: 0.5242 -- iter: 0736/2003
[A[ATraining Step: 24  | total loss: [1m[32m0.69327[0m[0m | time: 25.422s
[2K
| RMSProp | epoch: 001 | loss: 0.69327 - acc: 0.4647 -- iter: 0768/2003
[A[ATraining Step: 25  | total loss: [1m[32m0.69328[0m[0m | time: 26.304s
[2K
| RMSProp | epoch: 001 | loss: 0.69328 - acc: 0.4743 -- iter: 0800/2003
[A[ATraining Step: 26  | total loss: [1m[32m0.69328[0m[0m | time: 27.202s
[2K
| RMSProp | epoch: 001 | loss: 0.69328 - acc: 0.4728 -- iter: 0832/2003
[A[ATraining Step: 27  | total loss: [1m[32m0.69332[0m[0m | time: 28.062s
[2K
| RMSProp | epoch: 001 | loss: 0.69332 - acc: 0.4557 -- iter: 0864/2003
[A[ATraining Step: 28  | total loss: [1m[32m0.69330[0m[0m | time: 28.947s
[2K
| RMSProp | epoch: 001 | loss: 0.69330 - acc: 0.4590 -- iter: 0896/2003
[A[ATraining Step: 29  | total loss: [1m[32m0.69318[0m[0m | time: 29.727s
[2K
| RMSProp | epoch: 001 | loss: 0.69318 - acc: 0.5070 -- iter: 0928/2003
[A[ATraining Step: 30  | total loss: [1m[32m0.69330[0m[0m | time: 30.726s
[2K
| RMSProp | epoch: 001 | loss: 0.69330 - acc: 0.4831 -- iter: 0960/2003
[A[ATraining Step: 31  | total loss: [1m[32m0.69325[0m[0m | time: 31.751s
[2K
| RMSProp | epoch: 001 | loss: 0.69325 - acc: 0.4870 -- iter: 0992/2003
[A[ATraining Step: 32  | total loss: [1m[32m0.69322[0m[0m | time: 32.471s
[2K
| RMSProp | epoch: 001 | loss: 0.69322 - acc: 0.4759 -- iter: 1024/2003
[A[ATraining Step: 33  | total loss: [1m[32m0.69326[0m[0m | time: 33.300s
[2K
| RMSProp | epoch: 001 | loss: 0.69326 - acc: 0.4674 -- iter: 1056/2003
[A[ATraining Step: 34  | total loss: [1m[32m0.69331[0m[0m | time: 34.167s
[2K
| RMSProp | epoch: 001 | loss: 0.69331 - acc: 0.4409 -- iter: 1088/2003
[A[ATraining Step: 35  | total loss: [1m[32m0.69333[0m[0m | time: 35.060s
[2K
| RMSProp | epoch: 001 | loss: 0.69333 - acc: 0.4468 -- iter: 1120/2003
[A[ATraining Step: 36  | total loss: [1m[32m0.69336[0m[0m | time: 35.982s
[2K
| RMSProp | epoch: 001 | loss: 0.69336 - acc: 0.4193 -- iter: 1152/2003
[A[ATraining Step: 37  | total loss: [1m[32m0.69335[0m[0m | time: 36.937s
[2K
| RMSProp | epoch: 001 | loss: 0.69335 - acc: 0.4479 -- iter: 1184/2003
[A[ATraining Step: 38  | total loss: [1m[32m0.69332[0m[0m | time: 37.960s
[2K
| RMSProp | epoch: 001 | loss: 0.69332 - acc: 0.4704 -- iter: 1216/2003
[A[ATraining Step: 39  | total loss: [1m[32m0.69331[0m[0m | time: 38.859s
[2K
| RMSProp | epoch: 001 | loss: 0.69331 - acc: 0.4700 -- iter: 1248/2003
[A[ATraining Step: 40  | total loss: [1m[32m0.69331[0m[0m | time: 39.712s
[2K
| RMSProp | epoch: 001 | loss: 0.69331 - acc: 0.4581 -- iter: 1280/2003
[A[ATraining Step: 41  | total loss: [1m[32m0.69326[0m[0m | time: 40.739s
[2K
| RMSProp | epoch: 001 | loss: 0.69326 - acc: 0.4658 -- iter: 1312/2003
[A[ATraining Step: 42  | total loss: [1m[32m0.69318[0m[0m | time: 41.744s
[2K
| RMSProp | epoch: 001 | loss: 0.69318 - acc: 0.4944 -- iter: 1344/2003
[A[ATraining Step: 43  | total loss: [1m[32m0.69316[0m[0m | time: 42.544s
[2K
| RMSProp | epoch: 001 | loss: 0.69316 - acc: 0.4954 -- iter: 1376/2003
[A[ATraining Step: 44  | total loss: [1m[32m0.69323[0m[0m | time: 43.362s
[2K
| RMSProp | epoch: 001 | loss: 0.69323 - acc: 0.4854 -- iter: 1408/2003
[A[ATraining Step: 45  | total loss: [1m[32m0.69324[0m[0m | time: 44.205s
[2K
| RMSProp | epoch: 001 | loss: 0.69324 - acc: 0.4773 -- iter: 1440/2003
[A[ATraining Step: 46  | total loss: [1m[32m0.69314[0m[0m | time: 45.053s
[2K
| RMSProp | epoch: 001 | loss: 0.69314 - acc: 0.5019 -- iter: 1472/2003
[A[ATraining Step: 47  | total loss: [1m[32m0.69320[0m[0m | time: 45.926s
[2K
| RMSProp | epoch: 001 | loss: 0.69320 - acc: 0.4862 -- iter: 1504/2003
[A[ATraining Step: 48  | total loss: [1m[32m0.69318[0m[0m | time: 46.839s
[2K
| RMSProp | epoch: 001 | loss: 0.69318 - acc: 0.4935 -- iter: 1536/2003
[A[ATraining Step: 49  | total loss: [1m[32m0.69314[0m[0m | time: 47.752s
[2K
| RMSProp | epoch: 001 | loss: 0.69314 - acc: 0.4994 -- iter: 1568/2003
[A[ATraining Step: 50  | total loss: [1m[32m0.69313[0m[0m | time: 48.628s
[2K
| RMSProp | epoch: 001 | loss: 0.69313 - acc: 0.5044 -- iter: 1600/2003
[A[ATraining Step: 51  | total loss: [1m[32m0.69307[0m[0m | time: 49.883s
[2K
| RMSProp | epoch: 001 | loss: 0.69307 - acc: 0.5132 -- iter: 1632/2003
[A[ATraining Step: 52  | total loss: [1m[32m0.69307[0m[0m | time: 51.024s
[2K
| RMSProp | epoch: 001 | loss: 0.69307 - acc: 0.5113 -- iter: 1664/2003
[A[ATraining Step: 53  | total loss: [1m[32m0.69316[0m[0m | time: 52.552s
[2K
| RMSProp | epoch: 001 | loss: 0.69316 - acc: 0.5050 -- iter: 1696/2003
[A[ATraining Step: 54  | total loss: [1m[32m0.69318[0m[0m | time: 54.513s
[2K
| RMSProp | epoch: 001 | loss: 0.69318 - acc: 0.4997 -- iter: 1728/2003
[A[ATraining Step: 55  | total loss: [1m[32m0.69320[0m[0m | time: 56.523s
[2K
| RMSProp | epoch: 001 | loss: 0.69320 - acc: 0.4908 -- iter: 1760/2003
[A[ATraining Step: 56  | total loss: [1m[32m0.69318[0m[0m | time: 57.543s
[2K
| RMSProp | epoch: 001 | loss: 0.69318 - acc: 0.4965 -- iter: 1792/2003
[A[ATraining Step: 57  | total loss: [1m[32m0.69317[0m[0m | time: 58.550s
[2K
| RMSProp | epoch: 001 | loss: 0.69317 - acc: 0.5013 -- iter: 1824/2003
[A[ATraining Step: 58  | total loss: [1m[32m0.69318[0m[0m | time: 59.730s
[2K
| RMSProp | epoch: 001 | loss: 0.69318 - acc: 0.5011 -- iter: 1856/2003
[A[ATraining Step: 59  | total loss: [1m[32m0.69318[0m[0m | time: 60.854s
[2K
| RMSProp | epoch: 001 | loss: 0.69318 - acc: 0.5010 -- iter: 1888/2003
[A[ATraining Step: 60  | total loss: [1m[32m0.69319[0m[0m | time: 62.022s
[2K
| RMSProp | epoch: 001 | loss: 0.69319 - acc: 0.4926 -- iter: 1920/2003
[A[ATraining Step: 61  | total loss: [1m[32m0.69321[0m[0m | time: 63.270s
[2K
| RMSProp | epoch: 001 | loss: 0.69321 - acc: 0.4895 -- iter: 1952/2003
[A[ATraining Step: 62  | total loss: [1m[32m0.69322[0m[0m | time: 64.514s
[2K
| RMSProp | epoch: 001 | loss: 0.69322 - acc: 0.4948 -- iter: 1984/2003
[A[ATraining Step: 63  | total loss: [1m[32m0.69321[0m[0m | time: 69.438s
[2K
| RMSProp | epoch: 001 | loss: 0.69321 - acc: 0.4995 | val_loss: 0.69316 - val_acc: 0.4840 -- iter: 2003/2003
--
Training Step: 64  | total loss: [1m[32m0.69325[0m[0m | time: 0.692s
[2K
| RMSProp | epoch: 002 | loss: 0.69325 - acc: 0.4897 -- iter: 0032/2003
[A[ATraining Step: 65  | total loss: [1m[32m0.69326[0m[0m | time: 1.900s
[2K
| RMSProp | epoch: 002 | loss: 0.69326 - acc: 0.4812 -- iter: 0064/2003
[A[ATraining Step: 66  | total loss: [1m[32m0.69324[0m[0m | time: 3.166s
[2K
| RMSProp | epoch: 002 | loss: 0.69324 - acc: 0.4797 -- iter: 0096/2003
[A[ATraining Step: 67  | total loss: [1m[32m0.69323[0m[0m | time: 4.356s
[2K
| RMSProp | epoch: 002 | loss: 0.69323 - acc: 0.4784 -- iter: 0128/2003
[A[ATraining Step: 68  | total loss: [1m[32m0.69324[0m[0m | time: 5.332s
[2K
| RMSProp | epoch: 002 | loss: 0.69324 - acc: 0.4846 -- iter: 0160/2003
[A[ATraining Step: 69  | total loss: [1m[32m0.69331[0m[0m | time: 6.443s
[2K
| RMSProp | epoch: 002 | loss: 0.69331 - acc: 0.4718 -- iter: 0192/2003
[A[ATraining Step: 70  | total loss: [1m[32m0.69331[0m[0m | time: 7.663s
[2K
| RMSProp | epoch: 002 | loss: 0.69331 - acc: 0.4643 -- iter: 0224/2003
[A[ATraining Step: 71  | total loss: [1m[32m0.69329[0m[0m | time: 8.934s
[2K
| RMSProp | epoch: 002 | loss: 0.69329 - acc: 0.4754 -- iter: 0256/2003
[A[ATraining Step: 72  | total loss: [1m[32m0.69327[0m[0m | time: 10.067s
[2K
| RMSProp | epoch: 002 | loss: 0.69327 - acc: 0.4641 -- iter: 0288/2003
[A[ATraining Step: 73  | total loss: [1m[32m0.69325[0m[0m | time: 11.239s
[2K
| RMSProp | epoch: 002 | loss: 0.69325 - acc: 0.4681 -- iter: 0320/2003
[A[ATraining Step: 74  | total loss: [1m[32m0.69327[0m[0m | time: 12.454s
[2K
| RMSProp | epoch: 002 | loss: 0.69327 - acc: 0.4579 -- iter: 0352/2003
[A[ATraining Step: 75  | total loss: [1m[32m0.69324[0m[0m | time: 13.776s
[2K
| RMSProp | epoch: 002 | loss: 0.69324 - acc: 0.4726 -- iter: 0384/2003
[A[ATraining Step: 76  | total loss: [1m[32m0.69322[0m[0m | time: 15.126s
[2K
| RMSProp | epoch: 002 | loss: 0.69322 - acc: 0.4756 -- iter: 0416/2003
[A[ATraining Step: 77  | total loss: [1m[32m0.69317[0m[0m | time: 16.118s
[2K
| RMSProp | epoch: 002 | loss: 0.69317 - acc: 0.5013 -- iter: 0448/2003
[A[ATraining Step: 78  | total loss: [1m[32m0.69311[0m[0m | time: 17.250s
[2K
| RMSProp | epoch: 002 | loss: 0.69311 - acc: 0.5110 -- iter: 0480/2003
[A[ATraining Step: 79  | total loss: [1m[32m0.69289[0m[0m | time: 18.480s
[2K
| RMSProp | epoch: 002 | loss: 0.69289 - acc: 0.5293 -- iter: 0512/2003
[A[ATraining Step: 80  | total loss: [1m[32m0.69289[0m[0m | time: 19.718s
[2K
| RMSProp | epoch: 002 | loss: 0.69289 - acc: 0.5295 -- iter: 0544/2003
[A[ATraining Step: 81  | total loss: [1m[32m0.69299[0m[0m | time: 20.919s
[2K
| RMSProp | epoch: 002 | loss: 0.69299 - acc: 0.5233 -- iter: 0576/2003
[A[ATraining Step: 82  | total loss: [1m[32m0.69332[0m[0m | time: 22.117s
[2K
| RMSProp | epoch: 002 | loss: 0.69332 - acc: 0.5054 -- iter: 0608/2003
[A[ATraining Step: 83  | total loss: [1m[32m0.69327[0m[0m | time: 23.311s
[2K
| RMSProp | epoch: 002 | loss: 0.69327 - acc: 0.5079 -- iter: 0640/2003
[A[ATraining Step: 84  | total loss: [1m[32m0.69333[0m[0m | time: 24.492s
[2K
| RMSProp | epoch: 002 | loss: 0.69333 - acc: 0.5009 -- iter: 0672/2003
[A[ATraining Step: 85  | total loss: [1m[32m0.69334[0m[0m | time: 25.690s
[2K
| RMSProp | epoch: 002 | loss: 0.69334 - acc: 0.4946 -- iter: 0704/2003
[A[ATraining Step: 86  | total loss: [1m[32m0.69340[0m[0m | time: 26.919s
[2K
| RMSProp | epoch: 002 | loss: 0.69340 - acc: 0.4795 -- iter: 0736/2003
[A[ATraining Step: 87  | total loss: [1m[32m0.69341[0m[0m | time: 28.239s
[2K
| RMSProp | epoch: 002 | loss: 0.69341 - acc: 0.4722 -- iter: 0768/2003
[A[ATraining Step: 88  | total loss: [1m[32m0.69337[0m[0m | time: 29.573s
[2K
| RMSProp | epoch: 002 | loss: 0.69337 - acc: 0.4812 -- iter: 0800/2003
[A[ATraining Step: 89  | total loss: [1m[32m0.69335[0m[0m | time: 30.710s
[2K
| RMSProp | epoch: 002 | loss: 0.69335 - acc: 0.4831 -- iter: 0832/2003
[A[ATraining Step: 90  | total loss: [1m[32m0.69341[0m[0m | time: 31.845s
[2K
| RMSProp | epoch: 002 | loss: 0.69341 - acc: 0.4785 -- iter: 0864/2003
[A[ATraining Step: 91  | total loss: [1m[32m0.69340[0m[0m | time: 32.984s
[2K
| RMSProp | epoch: 002 | loss: 0.69340 - acc: 0.4744 -- iter: 0896/2003
[A[ATraining Step: 92  | total loss: [1m[32m0.69334[0m[0m | time: 34.093s
[2K
| RMSProp | epoch: 002 | loss: 0.69334 - acc: 0.4895 -- iter: 0928/2003
[A[ATraining Step: 93  | total loss: [1m[32m0.69334[0m[0m | time: 35.322s
[2K
| RMSProp | epoch: 002 | loss: 0.69334 - acc: 0.4905 -- iter: 0960/2003
[A[ATraining Step: 94  | total loss: [1m[32m0.69340[0m[0m | time: 36.605s
[2K
| RMSProp | epoch: 002 | loss: 0.69340 - acc: 0.4821 -- iter: 0992/2003
[A[ATraining Step: 95  | total loss: [1m[32m0.69341[0m[0m | time: 37.849s
[2K
| RMSProp | epoch: 002 | loss: 0.69341 - acc: 0.4745 -- iter: 1024/2003
[A[ATraining Step: 96  | total loss: [1m[32m0.69333[0m[0m | time: 38.979s
[2K
| RMSProp | epoch: 002 | loss: 0.69333 - acc: 0.4896 -- iter: 1056/2003
[A[ATraining Step: 97  | total loss: [1m[32m0.69338[0m[0m | time: 40.142s
[2K
| RMSProp | epoch: 002 | loss: 0.69338 - acc: 0.4875 -- iter: 1088/2003
[A[ATraining Step: 98  | total loss: [1m[32m0.69339[0m[0m | time: 41.398s
[2K
| RMSProp | epoch: 002 | loss: 0.69339 - acc: 0.4887 -- iter: 1120/2003
[A[ATraining Step: 99  | total loss: [1m[32m0.69319[0m[0m | time: 42.649s
[2K
| RMSProp | epoch: 002 | loss: 0.69319 - acc: 0.5055 -- iter: 1152/2003
[A[ATraining Step: 100  | total loss: [1m[32m0.69351[0m[0m | time: 43.843s
[2K
| RMSProp | epoch: 002 | loss: 0.69351 - acc: 0.4924 -- iter: 1184/2003
[A[ATraining Step: 101  | total loss: [1m[32m0.69362[0m[0m | time: 45.307s
[2K
| RMSProp | epoch: 002 | loss: 0.69362 - acc: 0.4838 -- iter: 1216/2003
[A[ATraining Step: 102  | total loss: [1m[32m0.69351[0m[0m | time: 46.471s
[2K
| RMSProp | epoch: 002 | loss: 0.69351 - acc: 0.4979 -- iter: 1248/2003
[A[ATraining Step: 103  | total loss: [1m[32m0.69347[0m[0m | time: 47.615s
[2K
| RMSProp | epoch: 002 | loss: 0.69347 - acc: 0.4981 -- iter: 1280/2003
[A[ATraining Step: 104  | total loss: [1m[32m0.69339[0m[0m | time: 48.780s
[2K
| RMSProp | epoch: 002 | loss: 0.69339 - acc: 0.5015 -- iter: 1312/2003
[A[ATraining Step: 105  | total loss: [1m[32m0.69340[0m[0m | time: 49.895s
[2K
| RMSProp | epoch: 002 | loss: 0.69340 - acc: 0.4982 -- iter: 1344/2003
[A[ATraining Step: 106  | total loss: [1m[32m0.69338[0m[0m | time: 51.210s
[2K
| RMSProp | epoch: 002 | loss: 0.69338 - acc: 0.4984 -- iter: 1376/2003
[A[ATraining Step: 107  | total loss: [1m[32m0.69315[0m[0m | time: 52.519s
[2K
| RMSProp | epoch: 002 | loss: 0.69315 - acc: 0.5110 -- iter: 1408/2003
[A[ATraining Step: 108  | total loss: [1m[32m0.69291[0m[0m | time: 53.757s
[2K
| RMSProp | epoch: 002 | loss: 0.69291 - acc: 0.5193 -- iter: 1440/2003
[A[ATraining Step: 109  | total loss: [1m[32m0.69259[0m[0m | time: 54.889s
[2K
| RMSProp | epoch: 002 | loss: 0.69259 - acc: 0.5267 -- iter: 1472/2003
[A[ATraining Step: 110  | total loss: [1m[32m0.69239[0m[0m | time: 55.964s
[2K
| RMSProp | epoch: 002 | loss: 0.69239 - acc: 0.5303 -- iter: 1504/2003
[A[ATraining Step: 111  | total loss: [1m[32m0.69304[0m[0m | time: 57.281s
[2K
| RMSProp | epoch: 002 | loss: 0.69304 - acc: 0.5179 -- iter: 1536/2003
[A[ATraining Step: 112  | total loss: [1m[32m0.69281[0m[0m | time: 58.599s
[2K
| RMSProp | epoch: 002 | loss: 0.69281 - acc: 0.5224 -- iter: 1568/2003
[A[ATraining Step: 113  | total loss: [1m[32m0.69259[0m[0m | time: 60.321s
[2K
| RMSProp | epoch: 002 | loss: 0.69259 - acc: 0.5264 -- iter: 1600/2003
[A[ATraining Step: 114  | total loss: [1m[32m0.69215[0m[0m | time: 62.457s
[2K
| RMSProp | epoch: 002 | loss: 0.69215 - acc: 0.5331 -- iter: 1632/2003
[A[ATraining Step: 115  | total loss: [1m[32m0.69206[0m[0m | time: 63.697s
[2K
| RMSProp | epoch: 002 | loss: 0.69206 - acc: 0.5329 -- iter: 1664/2003
[A[ATraining Step: 116  | total loss: [1m[32m0.69198[0m[0m | time: 64.878s
[2K
| RMSProp | epoch: 002 | loss: 0.69198 - acc: 0.5328 -- iter: 1696/2003
[A[ATraining Step: 117  | total loss: [1m[32m0.69302[0m[0m | time: 66.077s
[2K
| RMSProp | epoch: 002 | loss: 0.69302 - acc: 0.5170 -- iter: 1728/2003
[A[ATraining Step: 118  | total loss: [1m[32m0.69233[0m[0m | time: 67.220s
[2K
| RMSProp | epoch: 002 | loss: 0.69233 - acc: 0.5309 -- iter: 1760/2003
[A[ATraining Step: 119  | total loss: [1m[32m0.69269[0m[0m | time: 68.493s
[2K
| RMSProp | epoch: 002 | loss: 0.69269 - acc: 0.5247 -- iter: 1792/2003
[A[ATraining Step: 120  | total loss: [1m[32m0.69264[0m[0m | time: 69.744s
[2K
| RMSProp | epoch: 002 | loss: 0.69264 - acc: 0.5254 -- iter: 1824/2003
[A[ATraining Step: 121  | total loss: [1m[32m0.69332[0m[0m | time: 71.023s
[2K
| RMSProp | epoch: 002 | loss: 0.69332 - acc: 0.5134 -- iter: 1856/2003
[A[ATraining Step: 122  | total loss: [1m[32m0.69333[0m[0m | time: 72.083s
[2K
| RMSProp | epoch: 002 | loss: 0.69333 - acc: 0.5121 -- iter: 1888/2003
[A[ATraining Step: 123  | total loss: [1m[32m0.69308[0m[0m | time: 72.976s
[2K
| RMSProp | epoch: 002 | loss: 0.69308 - acc: 0.5171 -- iter: 1920/2003
[A[ATraining Step: 124  | total loss: [1m[32m0.69391[0m[0m | time: 73.962s
[2K
| RMSProp | epoch: 002 | loss: 0.69391 - acc: 0.4998 -- iter: 1952/2003
[A[ATraining Step: 125  | total loss: [1m[32m0.69378[0m[0m | time: 75.011s
[2K
| RMSProp | epoch: 002 | loss: 0.69378 - acc: 0.5029 -- iter: 1984/2003
[A[ATraining Step: 126  | total loss: [1m[32m0.69366[0m[0m | time: 79.837s
[2K
| RMSProp | epoch: 002 | loss: 0.69366 - acc: 0.5058 | val_loss: 0.69330 - val_acc: 0.4904 -- iter: 2003/2003
--
Training Step: 127  | total loss: [1m[32m0.69388[0m[0m | time: 0.723s
[2K
| RMSProp | epoch: 003 | loss: 0.69388 - acc: 0.4958 -- iter: 0032/2003
[A[ATraining Step: 128  | total loss: [1m[32m0.69400[0m[0m | time: 1.381s
[2K
| RMSProp | epoch: 003 | loss: 0.69400 - acc: 0.4831 -- iter: 0064/2003
[A[ATraining Step: 129  | total loss: [1m[32m0.69387[0m[0m | time: 2.501s
[2K
| RMSProp | epoch: 003 | loss: 0.69387 - acc: 0.4979 -- iter: 0096/2003
[A[ATraining Step: 130  | total loss: [1m[32m0.69345[0m[0m | time: 3.717s
[2K
| RMSProp | epoch: 003 | loss: 0.69345 - acc: 0.5138 -- iter: 0128/2003
[A[ATraining Step: 131  | total loss: [1m[32m0.69330[0m[0m | time: 4.899s
[2K
| RMSProp | epoch: 003 | loss: 0.69330 - acc: 0.5155 -- iter: 0160/2003
[A[ATraining Step: 132  | total loss: [1m[32m0.69289[0m[0m | time: 7.604s
[2K
| RMSProp | epoch: 003 | loss: 0.69289 - acc: 0.5202 -- iter: 0192/2003
[A[ATraining Step: 133  | total loss: [1m[32m0.69230[0m[0m | time: 15.189s
[2K
| RMSProp | epoch: 003 | loss: 0.69230 - acc: 0.5244 -- iter: 0224/2003
[A[ATraining Step: 134  | total loss: [1m[32m0.69169[0m[0m | time: 25.790s
[2K
| RMSProp | epoch: 003 | loss: 0.69169 - acc: 0.5282 -- iter: 0256/2003
[A[ATraining Step: 135  | total loss: [1m[32m0.69300[0m[0m | time: 26.860s
[2K
| RMSProp | epoch: 003 | loss: 0.69300 - acc: 0.5223 -- iter: 0288/2003
[A[ATraining Step: 136  | total loss: [1m[32m0.69367[0m[0m | time: 28.073s
[2K
| RMSProp | epoch: 003 | loss: 0.69367 - acc: 0.5107 -- iter: 0320/2003
[A[ATraining Step: 137  | total loss: [1m[32m0.69380[0m[0m | time: 29.222s
[2K
| RMSProp | epoch: 003 | loss: 0.69380 - acc: 0.5034 -- iter: 0352/2003
[A[ATraining Step: 138  | total loss: [1m[32m0.69373[0m[0m | time: 30.452s
[2K
| RMSProp | epoch: 003 | loss: 0.69373 - acc: 0.5030 -- iter: 0384/2003
[A[ATraining Step: 139  | total loss: [1m[32m0.69366[0m[0m | time: 31.724s
[2K
| RMSProp | epoch: 003 | loss: 0.69366 - acc: 0.5090 -- iter: 0416/2003
[A[ATraining Step: 140  | total loss: [1m[32m0.69356[0m[0m | time: 33.099s
[2K
| RMSProp | epoch: 003 | loss: 0.69356 - acc: 0.5112 -- iter: 0448/2003
[A[ATraining Step: 141  | total loss: [1m[32m0.69375[0m[0m | time: 34.228s
[2K
| RMSProp | epoch: 003 | loss: 0.69375 - acc: 0.4976 -- iter: 0480/2003
[A[ATraining Step: 142  | total loss: [1m[32m0.69375[0m[0m | time: 35.406s
[2K
| RMSProp | epoch: 003 | loss: 0.69375 - acc: 0.4916 -- iter: 0512/2003
[A[ATraining Step: 143  | total loss: [1m[32m0.69369[0m[0m | time: 36.764s
[2K
| RMSProp | epoch: 003 | loss: 0.69369 - acc: 0.4987 -- iter: 0544/2003
[A[ATraining Step: 144  | total loss: [1m[32m0.69364[0m[0m | time: 38.120s
[2K
| RMSProp | epoch: 003 | loss: 0.69364 - acc: 0.5019 -- iter: 0576/2003
[A[ATraining Step: 145  | total loss: [1m[32m0.69351[0m[0m | time: 39.262s
[2K
| RMSProp | epoch: 003 | loss: 0.69351 - acc: 0.5080 -- iter: 0608/2003
[A[ATraining Step: 146  | total loss: [1m[32m0.69334[0m[0m | time: 40.310s
[2K
| RMSProp | epoch: 003 | loss: 0.69334 - acc: 0.5134 -- iter: 0640/2003
[A[ATraining Step: 147  | total loss: [1m[32m0.69369[0m[0m | time: 41.650s
[2K
| RMSProp | epoch: 003 | loss: 0.69369 - acc: 0.5027 -- iter: 0672/2003
[A[ATraining Step: 148  | total loss: [1m[32m0.69374[0m[0m | time: 42.702s
[2K
| RMSProp | epoch: 003 | loss: 0.69374 - acc: 0.4931 -- iter: 0704/2003
[A[ATraining Step: 149  | total loss: [1m[32m0.69368[0m[0m | time: 43.818s
[2K
| RMSProp | epoch: 003 | loss: 0.69368 - acc: 0.4906 -- iter: 0736/2003
[A[ATraining Step: 150  | total loss: [1m[32m0.69360[0m[0m | time: 45.076s
[2K
| RMSProp | epoch: 003 | loss: 0.69360 - acc: 0.4978 -- iter: 0768/2003
[A[ATraining Step: 151  | total loss: [1m[32m0.69339[0m[0m | time: 46.279s
[2K
| RMSProp | epoch: 003 | loss: 0.69339 - acc: 0.5074 -- iter: 0800/2003
[A[ATraining Step: 152  | total loss: [1m[32m0.69359[0m[0m | time: 47.604s
[2K
| RMSProp | epoch: 003 | loss: 0.69359 - acc: 0.5004 -- iter: 0832/2003
[A[ATraining Step: 153  | total loss: [1m[32m0.69355[0m[0m | time: 48.724s
[2K
| RMSProp | epoch: 003 | loss: 0.69355 - acc: 0.5004 -- iter: 0864/2003
[A[ATraining Step: 154  | total loss: [1m[32m0.69352[0m[0m | time: 49.945s
[2K
| RMSProp | epoch: 003 | loss: 0.69352 - acc: 0.5003 -- iter: 0896/2003
[A[ATraining Step: 155  | total loss: [1m[32m0.69344[0m[0m | time: 51.143s
[2K
| RMSProp | epoch: 003 | loss: 0.69344 - acc: 0.5034 -- iter: 0928/2003
[A[ATraining Step: 156  | total loss: [1m[32m0.69346[0m[0m | time: 52.412s
[2K
| RMSProp | epoch: 003 | loss: 0.69346 - acc: 0.5000 -- iter: 0960/2003
[A[ATraining Step: 157  | total loss: [1m[32m0.69337[0m[0m | time: 53.567s
[2K
| RMSProp | epoch: 003 | loss: 0.69337 - acc: 0.5031 -- iter: 0992/2003
[A[ATraining Step: 158  | total loss: [1m[32m0.69344[0m[0m | time: 54.692s
[2K
| RMSProp | epoch: 003 | loss: 0.69344 - acc: 0.4997 -- iter: 1024/2003
[A[ATraining Step: 159  | total loss: [1m[32m0.69349[0m[0m | time: 55.958s
[2K
| RMSProp | epoch: 003 | loss: 0.69349 - acc: 0.4934 -- iter: 1056/2003
[A[ATraining Step: 160  | total loss: [1m[32m0.69332[0m[0m | time: 57.197s
[2K
| RMSProp | epoch: 003 | loss: 0.69332 - acc: 0.5066 -- iter: 1088/2003
[A[ATraining Step: 161  | total loss: [1m[32m0.69339[0m[0m | time: 58.336s
[2K
| RMSProp | epoch: 003 | loss: 0.69339 - acc: 0.5059 -- iter: 1120/2003
[A[ATraining Step: 162  | total loss: [1m[32m0.69364[0m[0m | time: 59.411s
[2K
| RMSProp | epoch: 003 | loss: 0.69364 - acc: 0.4991 -- iter: 1152/2003
[A[ATraining Step: 163  | total loss: [1m[32m0.69359[0m[0m | time: 60.603s
[2K
| RMSProp | epoch: 003 | loss: 0.69359 - acc: 0.4992 -- iter: 1184/2003
[A[ATraining Step: 164  | total loss: [1m[32m0.69356[0m[0m | time: 61.870s
[2K
| RMSProp | epoch: 003 | loss: 0.69356 - acc: 0.4961 -- iter: 1216/2003
[A[ATraining Step: 165  | total loss: [1m[32m0.69354[0m[0m | time: 62.879s
[2K
| RMSProp | epoch: 003 | loss: 0.69354 - acc: 0.4903 -- iter: 1248/2003
[A[ATraining Step: 166  | total loss: [1m[32m0.69344[0m[0m | time: 64.322s
[2K
| RMSProp | epoch: 003 | loss: 0.69344 - acc: 0.4975 -- iter: 1280/2003
[A[ATraining Step: 167  | total loss: [1m[32m0.69354[0m[0m | time: 65.561s
[2K
| RMSProp | epoch: 003 | loss: 0.69354 - acc: 0.4946 -- iter: 1312/2003
[A[ATraining Step: 168  | total loss: [1m[32m0.69359[0m[0m | time: 66.907s
[2K
| RMSProp | epoch: 003 | loss: 0.69359 - acc: 0.4889 -- iter: 1344/2003
[A[ATraining Step: 169  | total loss: [1m[32m0.69355[0m[0m | time: 68.006s
[2K
| RMSProp | epoch: 003 | loss: 0.69355 - acc: 0.4931 -- iter: 1376/2003
[A[ATraining Step: 170  | total loss: [1m[32m0.69343[0m[0m | time: 69.119s
[2K
| RMSProp | epoch: 003 | loss: 0.69343 - acc: 0.4970 -- iter: 1408/2003
[A[ATraining Step: 171  | total loss: [1m[32m0.69359[0m[0m | time: 70.311s
[2K
| RMSProp | epoch: 003 | loss: 0.69359 - acc: 0.4910 -- iter: 1440/2003
[A[ATraining Step: 172  | total loss: [1m[32m0.69366[0m[0m | time: 71.516s
[2K
| RMSProp | epoch: 003 | loss: 0.69366 - acc: 0.4825 -- iter: 1472/2003
[A[ATraining Step: 173  | total loss: [1m[32m0.69359[0m[0m | time: 72.773s
[2K
| RMSProp | epoch: 003 | loss: 0.69359 - acc: 0.4874 -- iter: 1504/2003
[A[ATraining Step: 174  | total loss: [1m[32m0.69350[0m[0m | time: 74.085s
[2K
| RMSProp | epoch: 003 | loss: 0.69350 - acc: 0.4918 -- iter: 1536/2003
[A[ATraining Step: 175  | total loss: [1m[32m0.69326[0m[0m | time: 75.367s
[2K
| RMSProp | epoch: 003 | loss: 0.69326 - acc: 0.5020 -- iter: 1568/2003
[A[ATraining Step: 176  | total loss: [1m[32m0.69323[0m[0m | time: 76.424s
[2K
| RMSProp | epoch: 003 | loss: 0.69323 - acc: 0.5018 -- iter: 1600/2003
[A[ATraining Step: 177  | total loss: [1m[32m0.69303[0m[0m | time: 77.569s
[2K
| RMSProp | epoch: 003 | loss: 0.69303 - acc: 0.5079 -- iter: 1632/2003
[A[ATraining Step: 178  | total loss: [1m[32m0.69284[0m[0m | time: 78.859s
[2K
| RMSProp | epoch: 003 | loss: 0.69284 - acc: 0.5102 -- iter: 1664/2003
[A[ATraining Step: 179  | total loss: [1m[32m0.69239[0m[0m | time: 80.282s
[2K
| RMSProp | epoch: 003 | loss: 0.69239 - acc: 0.5154 -- iter: 1696/2003
[A[ATraining Step: 180  | total loss: [1m[32m0.69337[0m[0m | time: 81.475s
[2K
| RMSProp | epoch: 003 | loss: 0.69337 - acc: 0.5139 -- iter: 1728/2003
[A[ATraining Step: 181  | total loss: [1m[32m0.69325[0m[0m | time: 82.588s
[2K
| RMSProp | epoch: 003 | loss: 0.69325 - acc: 0.5156 -- iter: 1760/2003
[A[ATraining Step: 182  | total loss: [1m[32m0.69312[0m[0m | time: 83.807s
[2K
| RMSProp | epoch: 003 | loss: 0.69312 - acc: 0.5172 -- iter: 1792/2003
[A[ATraining Step: 183  | total loss: [1m[32m0.69351[0m[0m | time: 84.977s
[2K
| RMSProp | epoch: 003 | loss: 0.69351 - acc: 0.5092 -- iter: 1824/2003
[A[ATraining Step: 184  | total loss: [1m[32m0.69369[0m[0m | time: 86.274s
[2K
| RMSProp | epoch: 003 | loss: 0.69369 - acc: 0.5020 -- iter: 1856/2003
[A[ATraining Step: 185  | total loss: [1m[32m0.69364[0m[0m | time: 87.382s
[2K
| RMSProp | epoch: 003 | loss: 0.69364 - acc: 0.5018 -- iter: 1888/2003
[A[ATraining Step: 186  | total loss: [1m[32m0.69343[0m[0m | time: 88.647s
[2K
| RMSProp | epoch: 003 | loss: 0.69343 - acc: 0.5110 -- iter: 1920/2003
[A[ATraining Step: 187  | total loss: [1m[32m0.69351[0m[0m | time: 90.062s
[2K
| RMSProp | epoch: 003 | loss: 0.69351 - acc: 0.5068 -- iter: 1952/2003
[A[ATraining Step: 188  | total loss: [1m[32m0.69338[0m[0m | time: 91.216s
[2K
| RMSProp | epoch: 003 | loss: 0.69338 - acc: 0.5092 -- iter: 1984/2003
[A[ATraining Step: 189  | total loss: [1m[32m0.69351[0m[0m | time: 99.950s
[2K
| RMSProp | epoch: 003 | loss: 0.69351 - acc: 0.5052 | val_loss: 0.69364 - val_acc: 0.4904 -- iter: 2003/2003
--
Training Step: 190  | total loss: [1m[32m0.69340[0m[0m | time: 1.139s
[2K
| RMSProp | epoch: 004 | loss: 0.69340 - acc: 0.5078 -- iter: 0032/2003
[A[ATraining Step: 191  | total loss: [1m[32m0.69352[0m[0m | time: 1.859s
[2K
| RMSProp | epoch: 004 | loss: 0.69352 - acc: 0.5039 -- iter: 0064/2003
[A[ATraining Step: 192  | total loss: [1m[32m0.69341[0m[0m | time: 2.571s
[2K
| RMSProp | epoch: 004 | loss: 0.69341 - acc: 0.5061 -- iter: 0096/2003
[A[ATraining Step: 193  | total loss: [1m[32m0.69327[0m[0m | time: 3.873s
[2K
| RMSProp | epoch: 004 | loss: 0.69327 - acc: 0.5082 -- iter: 0128/2003
[A[ATraining Step: 194  | total loss: [1m[32m0.69290[0m[0m | time: 5.048s
[2K
| RMSProp | epoch: 004 | loss: 0.69290 - acc: 0.5167 -- iter: 0160/2003
[A[ATraining Step: 195  | total loss: [1m[32m0.69438[0m[0m | time: 6.214s
[2K
| RMSProp | epoch: 004 | loss: 0.69438 - acc: 0.4994 -- iter: 0192/2003
[A[ATraining Step: 196  | total loss: [1m[32m0.69431[0m[0m | time: 7.538s
[2K
| RMSProp | epoch: 004 | loss: 0.69431 - acc: 0.4964 -- iter: 0224/2003
[A[ATraining Step: 197  | total loss: [1m[32m0.69410[0m[0m | time: 8.983s
[2K
| RMSProp | epoch: 004 | loss: 0.69410 - acc: 0.5030 -- iter: 0256/2003
[A[ATraining Step: 198  | total loss: [1m[32m0.69408[0m[0m | time: 10.172s
[2K
| RMSProp | epoch: 004 | loss: 0.69408 - acc: 0.4995 -- iter: 0288/2003
[A[ATraining Step: 199  | total loss: [1m[32m0.69410[0m[0m | time: 11.315s
[2K
| RMSProp | epoch: 004 | loss: 0.69410 - acc: 0.4933 -- iter: 0320/2003
[A[ATraining Step: 200  | total loss: [1m[32m0.69394[0m[0m | time: 21.325s
[2K
| RMSProp | epoch: 004 | loss: 0.69394 - acc: 0.5034 | val_loss: 0.69430 - val_acc: 0.4904 -- iter: 0352/2003
--
Training Step: 201  | total loss: [1m[32m0.69346[0m[0m | time: 22.463s
[2K
| RMSProp | epoch: 004 | loss: 0.69346 - acc: 0.5187 -- iter: 0384/2003
[A[ATraining Step: 202  | total loss: [1m[32m0.69347[0m[0m | time: 23.627s
[2K
| RMSProp | epoch: 004 | loss: 0.69347 - acc: 0.5168 -- iter: 0416/2003
[A[ATraining Step: 203  | total loss: [1m[32m0.69357[0m[0m | time: 24.968s
[2K
| RMSProp | epoch: 004 | loss: 0.69357 - acc: 0.5120 -- iter: 0448/2003
[A[ATraining Step: 204  | total loss: [1m[32m0.69352[0m[0m | time: 26.281s
[2K
| RMSProp | epoch: 004 | loss: 0.69352 - acc: 0.5139 -- iter: 0480/2003
[A[ATraining Step: 205  | total loss: [1m[32m0.69310[0m[0m | time: 27.496s
[2K
| RMSProp | epoch: 004 | loss: 0.69310 - acc: 0.5219 -- iter: 0512/2003
[A[ATraining Step: 206  | total loss: [1m[32m0.69375[0m[0m | time: 28.735s
[2K
| RMSProp | epoch: 004 | loss: 0.69375 - acc: 0.5103 -- iter: 0544/2003
[A[ATraining Step: 207  | total loss: [1m[32m0.69395[0m[0m | time: 30.005s
[2K
| RMSProp | epoch: 004 | loss: 0.69395 - acc: 0.5031 -- iter: 0576/2003
[A[ATraining Step: 208  | total loss: [1m[32m0.69370[0m[0m | time: 31.363s
[2K
| RMSProp | epoch: 004 | loss: 0.69370 - acc: 0.5090 -- iter: 0608/2003
[A[ATraining Step: 209  | total loss: [1m[32m0.69388[0m[0m | time: 32.444s
[2K
| RMSProp | epoch: 004 | loss: 0.69388 - acc: 0.5018 -- iter: 0640/2003
[A[ATraining Step: 210  | total loss: [1m[32m0.69379[0m[0m | time: 33.702s
[2K
| RMSProp | epoch: 004 | loss: 0.69379 - acc: 0.5048 -- iter: 0672/2003
[A[ATraining Step: 211  | total loss: [1m[32m0.69372[0m[0m | time: 34.887s
[2K
| RMSProp | epoch: 004 | loss: 0.69372 - acc: 0.5043 -- iter: 0704/2003
[A[ATraining Step: 212  | total loss: [1m[32m0.69391[0m[0m | time: 35.998s
[2K
| RMSProp | epoch: 004 | loss: 0.69391 - acc: 0.4945 -- iter: 0736/2003
[A[ATraining Step: 213  | total loss: [1m[32m0.69395[0m[0m | time: 37.282s
[2K
| RMSProp | epoch: 004 | loss: 0.69395 - acc: 0.4826 -- iter: 0768/2003
[A[ATraining Step: 214  | total loss: [1m[32m0.69350[0m[0m | time: 38.567s
[2K
| RMSProp | epoch: 004 | loss: 0.69350 - acc: 0.4999 -- iter: 0800/2003
[A[ATraining Step: 215  | total loss: [1m[32m0.69187[0m[0m | time: 40.016s
[2K
| RMSProp | epoch: 004 | loss: 0.69187 - acc: 0.5093 -- iter: 0832/2003
[A[ATraining Step: 216  | total loss: [1m[32m0.70404[0m[0m | time: 41.049s
[2K
| RMSProp | epoch: 004 | loss: 0.70404 - acc: 0.5053 -- iter: 0864/2003
[A[ATraining Step: 217  | total loss: [1m[32m0.70266[0m[0m | time: 41.919s
[2K
| RMSProp | epoch: 004 | loss: 0.70266 - acc: 0.5204 -- iter: 0896/2003
[A[ATraining Step: 218  | total loss: [1m[32m0.70207[0m[0m | time: 42.737s
[2K
| RMSProp | epoch: 004 | loss: 0.70207 - acc: 0.5121 -- iter: 0928/2003
[A[ATraining Step: 219  | total loss: [1m[32m0.70067[0m[0m | time: 43.712s
[2K
| RMSProp | epoch: 004 | loss: 0.70067 - acc: 0.5265 -- iter: 0960/2003
[A[ATraining Step: 220  | total loss: [1m[32m0.70056[0m[0m | time: 44.788s
[2K
| RMSProp | epoch: 004 | loss: 0.70056 - acc: 0.5145 -- iter: 0992/2003
[A[ATraining Step: 221  | total loss: [1m[32m0.70007[0m[0m | time: 45.633s
[2K
| RMSProp | epoch: 004 | loss: 0.70007 - acc: 0.5068 -- iter: 1024/2003
[A[ATraining Step: 222  | total loss: [1m[32m0.69961[0m[0m | time: 46.470s
[2K
| RMSProp | epoch: 004 | loss: 0.69961 - acc: 0.4998 -- iter: 1056/2003
[A[ATraining Step: 223  | total loss: [1m[32m0.69907[0m[0m | time: 47.437s
[2K
| RMSProp | epoch: 004 | loss: 0.69907 - acc: 0.4936 -- iter: 1088/2003
[A[ATraining Step: 224  | total loss: [1m[32m0.69845[0m[0m | time: 48.313s
[2K
| RMSProp | epoch: 004 | loss: 0.69845 - acc: 0.5005 -- iter: 1120/2003
[A[ATraining Step: 225  | total loss: [1m[32m0.69801[0m[0m | time: 49.238s
[2K
| RMSProp | epoch: 004 | loss: 0.69801 - acc: 0.4942 -- iter: 1152/2003
[A[ATraining Step: 226  | total loss: [1m[32m0.69740[0m[0m | time: 50.150s
[2K
| RMSProp | epoch: 004 | loss: 0.69740 - acc: 0.5135 -- iter: 1184/2003
[A[ATraining Step: 227  | total loss: [1m[32m0.69711[0m[0m | time: 51.060s
[2K
| RMSProp | epoch: 004 | loss: 0.69711 - acc: 0.5090 -- iter: 1216/2003
[A[ATraining Step: 228  | total loss: [1m[32m0.69697[0m[0m | time: 51.947s
[2K
| RMSProp | epoch: 004 | loss: 0.69697 - acc: 0.4988 -- iter: 1248/2003
[A[ATraining Step: 229  | total loss: [1m[32m0.69648[0m[0m | time: 52.879s
[2K
| RMSProp | epoch: 004 | loss: 0.69648 - acc: 0.5083 -- iter: 1280/2003
[A[ATraining Step: 230  | total loss: [1m[32m0.69596[0m[0m | time: 53.696s
[2K
| RMSProp | epoch: 004 | loss: 0.69596 - acc: 0.5137 -- iter: 1312/2003
[A[ATraining Step: 231  | total loss: [1m[32m0.69585[0m[0m | time: 54.835s
[2K
| RMSProp | epoch: 004 | loss: 0.69585 - acc: 0.5092 -- iter: 1344/2003
[A[ATraining Step: 232  | total loss: [1m[32m0.69551[0m[0m | time: 55.887s
[2K
| RMSProp | epoch: 004 | loss: 0.69551 - acc: 0.5114 -- iter: 1376/2003
[A[ATraining Step: 233  | total loss: [1m[32m0.69501[0m[0m | time: 56.756s
[2K
| RMSProp | epoch: 004 | loss: 0.69501 - acc: 0.5196 -- iter: 1408/2003
[A[ATraining Step: 234  | total loss: [1m[32m0.69486[0m[0m | time: 57.624s
[2K
| RMSProp | epoch: 004 | loss: 0.69486 - acc: 0.5177 -- iter: 1440/2003
[A[ATraining Step: 235  | total loss: [1m[32m0.69408[0m[0m | time: 58.569s
[2K
| RMSProp | epoch: 004 | loss: 0.69408 - acc: 0.5315 -- iter: 1472/2003
[A[ATraining Step: 236  | total loss: [1m[32m0.69491[0m[0m | time: 59.543s
[2K
| RMSProp | epoch: 004 | loss: 0.69491 - acc: 0.5159 -- iter: 1504/2003
[A[ATraining Step: 237  | total loss: [1m[32m0.69476[0m[0m | time: 60.453s
[2K
| RMSProp | epoch: 004 | loss: 0.69476 - acc: 0.5143 -- iter: 1536/2003
[A[ATraining Step: 238  | total loss: [1m[32m0.69459[0m[0m | time: 61.463s
[2K
| RMSProp | epoch: 004 | loss: 0.69459 - acc: 0.5129 -- iter: 1568/2003
[A[ATraining Step: 239  | total loss: [1m[32m0.69433[0m[0m | time: 62.345s
[2K
| RMSProp | epoch: 004 | loss: 0.69433 - acc: 0.5147 -- iter: 1600/2003
[A[ATraining Step: 240  | total loss: [1m[32m0.69448[0m[0m | time: 63.264s
[2K
| RMSProp | epoch: 004 | loss: 0.69448 - acc: 0.5070 -- iter: 1632/2003
[A[ATraining Step: 241  | total loss: [1m[32m0.69427[0m[0m | time: 64.097s
[2K
| RMSProp | epoch: 004 | loss: 0.69427 - acc: 0.5094 -- iter: 1664/2003
[A[ATraining Step: 242  | total loss: [1m[32m0.69461[0m[0m | time: 65.218s
[2K
| RMSProp | epoch: 004 | loss: 0.69461 - acc: 0.4928 -- iter: 1696/2003
[A[ATraining Step: 243  | total loss: [1m[32m0.69445[0m[0m | time: 66.544s
[2K
| RMSProp | epoch: 004 | loss: 0.69445 - acc: 0.4967 -- iter: 1728/2003
[A[ATraining Step: 244  | total loss: [1m[32m0.69424[0m[0m | time: 67.914s
[2K
| RMSProp | epoch: 004 | loss: 0.69424 - acc: 0.5095 -- iter: 1760/2003
[A[ATraining Step: 245  | total loss: [1m[32m0.69399[0m[0m | time: 76.688s
[2K
| RMSProp | epoch: 004 | loss: 0.69399 - acc: 0.5148 -- iter: 1792/2003
[A[ATraining Step: 246  | total loss: [1m[32m0.69410[0m[0m | time: 79.329s
[2K
| RMSProp | epoch: 004 | loss: 0.69410 - acc: 0.5102 -- iter: 1824/2003
[A[ATraining Step: 247  | total loss: [1m[32m0.69402[0m[0m | time: 82.706s
[2K
| RMSProp | epoch: 004 | loss: 0.69402 - acc: 0.5092 -- iter: 1856/2003
[A[ATraining Step: 248  | total loss: [1m[32m0.69415[0m[0m | time: 83.760s
[2K
| RMSProp | epoch: 004 | loss: 0.69415 - acc: 0.4989 -- iter: 1888/2003
[A[ATraining Step: 249  | total loss: [1m[32m0.69405[0m[0m | time: 84.976s
[2K
| RMSProp | epoch: 004 | loss: 0.69405 - acc: 0.4990 -- iter: 1920/2003
[A[ATraining Step: 250  | total loss: [1m[32m0.69398[0m[0m | time: 86.200s
[2K
| RMSProp | epoch: 004 | loss: 0.69398 - acc: 0.4960 -- iter: 1952/2003
[A[ATraining Step: 251  | total loss: [1m[32m0.69391[0m[0m | time: 87.339s
[2K
| RMSProp | epoch: 004 | loss: 0.69391 - acc: 0.4995 -- iter: 1984/2003
[A[ATraining Step: 252  | total loss: [1m[32m0.69386[0m[0m | time: 93.550s
[2K
| RMSProp | epoch: 004 | loss: 0.69386 - acc: 0.4964 | val_loss: 0.69311 - val_acc: 0.5639 -- iter: 2003/2003
--
Training Step: 253  | total loss: [1m[32m0.69386[0m[0m | time: 1.349s
[2K
| RMSProp | epoch: 005 | loss: 0.69386 - acc: 0.4905 -- iter: 0032/2003
[A[ATraining Step: 254  | total loss: [1m[32m0.69378[0m[0m | time: 2.855s
[2K
| RMSProp | epoch: 005 | loss: 0.69378 - acc: 0.4946 -- iter: 0064/2003
[A[ATraining Step: 255  | total loss: [1m[32m0.69371[0m[0m | time: 7.708s
[2K
| RMSProp | epoch: 005 | loss: 0.69371 - acc: 0.4983 -- iter: 0096/2003
[A[ATraining Step: 256  | total loss: [1m[32m0.69361[0m[0m | time: 11.885s
[2K
| RMSProp | epoch: 005 | loss: 0.69361 - acc: 0.5011 -- iter: 0128/2003
[A[ATraining Step: 257  | total loss: [1m[32m0.69348[0m[0m | time: 17.334s
[2K
| RMSProp | epoch: 005 | loss: 0.69348 - acc: 0.5036 -- iter: 0160/2003
[A[ATraining Step: 258  | total loss: [1m[32m0.69354[0m[0m | time: 18.496s
[2K
| RMSProp | epoch: 005 | loss: 0.69354 - acc: 0.5001 -- iter: 0192/2003
[A[ATraining Step: 259  | total loss: [1m[32m0.69341[0m[0m | time: 19.761s
[2K
| RMSProp | epoch: 005 | loss: 0.69341 - acc: 0.5064 -- iter: 0224/2003
[A[ATraining Step: 260  | total loss: [1m[32m0.69380[0m[0m | time: 21.008s
[2K
| RMSProp | epoch: 005 | loss: 0.69380 - acc: 0.4901 -- iter: 0256/2003
[A[ATraining Step: 261  | total loss: [1m[32m0.69372[0m[0m | time: 22.213s
[2K
| RMSProp | epoch: 005 | loss: 0.69372 - acc: 0.5036 -- iter: 0288/2003
[A[ATraining Step: 262  | total loss: [1m[32m0.69361[0m[0m | time: 23.608s
[2K
| RMSProp | epoch: 005 | loss: 0.69361 - acc: 0.5095 -- iter: 0320/2003
[A[ATraining Step: 263  | total loss: [1m[32m0.69337[0m[0m | time: 24.895s
[2K
| RMSProp | epoch: 005 | loss: 0.69337 - acc: 0.5210 -- iter: 0352/2003
[A[ATraining Step: 264  | total loss: [1m[32m0.69280[0m[0m | time: 26.093s
[2K
| RMSProp | epoch: 005 | loss: 0.69280 - acc: 0.5346 -- iter: 0384/2003
[A[ATraining Step: 265  | total loss: [1m[32m0.69355[0m[0m | time: 27.415s
[2K
| RMSProp | epoch: 005 | loss: 0.69355 - acc: 0.5248 -- iter: 0416/2003
[A[ATraining Step: 266  | total loss: [1m[32m0.69343[0m[0m | time: 28.717s
[2K
| RMSProp | epoch: 005 | loss: 0.69343 - acc: 0.5255 -- iter: 0448/2003
[A[ATraining Step: 267  | total loss: [1m[32m0.69342[0m[0m | time: 30.127s
[2K
| RMSProp | epoch: 005 | loss: 0.69342 - acc: 0.5229 -- iter: 0480/2003
[A[ATraining Step: 268  | total loss: [1m[32m0.69359[0m[0m | time: 38.058s
[2K
| RMSProp | epoch: 005 | loss: 0.69359 - acc: 0.5144 -- iter: 0512/2003
[A[ATraining Step: 269  | total loss: [1m[32m0.69362[0m[0m | time: 41.160s
[2K
| RMSProp | epoch: 005 | loss: 0.69362 - acc: 0.5098 -- iter: 0544/2003
[A[ATraining Step: 270  | total loss: [1m[32m0.69352[0m[0m | time: 42.340s
[2K
| RMSProp | epoch: 005 | loss: 0.69352 - acc: 0.5120 -- iter: 0576/2003
[A[ATraining Step: 271  | total loss: [1m[32m0.69365[0m[0m | time: 43.343s
[2K
| RMSProp | epoch: 005 | loss: 0.69365 - acc: 0.5014 -- iter: 0608/2003
[A[ATraining Step: 272  | total loss: [1m[32m0.69360[0m[0m | time: 44.561s
[2K
| RMSProp | epoch: 005 | loss: 0.69360 - acc: 0.5044 -- iter: 0640/2003
[A[ATraining Step: 273  | total loss: [1m[32m0.69354[0m[0m | time: 45.747s
[2K
| RMSProp | epoch: 005 | loss: 0.69354 - acc: 0.5039 -- iter: 0672/2003
[A[ATraining Step: 274  | total loss: [1m[32m0.69348[0m[0m | time: 47.163s
[2K
| RMSProp | epoch: 005 | loss: 0.69348 - acc: 0.5036 -- iter: 0704/2003
[A[ATraining Step: 275  | total loss: [1m[32m0.69326[0m[0m | time: 48.417s
[2K
| RMSProp | epoch: 005 | loss: 0.69326 - acc: 0.5126 -- iter: 0736/2003
[A[ATraining Step: 276  | total loss: [1m[32m0.69406[0m[0m | time: 49.635s
[2K
| RMSProp | epoch: 005 | loss: 0.69406 - acc: 0.5082 -- iter: 0768/2003
[A[ATraining Step: 277  | total loss: [1m[32m0.69375[0m[0m | time: 50.941s
[2K
| RMSProp | epoch: 005 | loss: 0.69375 - acc: 0.5199 -- iter: 0800/2003
[A[ATraining Step: 278  | total loss: [1m[32m0.69309[0m[0m | time: 52.290s
[2K
| RMSProp | epoch: 005 | loss: 0.69309 - acc: 0.5304 -- iter: 0832/2003
[A[ATraining Step: 279  | total loss: [1m[32m0.69255[0m[0m | time: 53.641s
[2K
| RMSProp | epoch: 005 | loss: 0.69255 - acc: 0.5336 -- iter: 0864/2003
[A[ATraining Step: 280  | total loss: [1m[32m0.69367[0m[0m | time: 54.566s
[2K
| RMSProp | epoch: 005 | loss: 0.69367 - acc: 0.5271 -- iter: 0896/2003
[A[ATraining Step: 281  | total loss: [1m[32m0.69331[0m[0m | time: 55.711s
[2K
| RMSProp | epoch: 005 | loss: 0.69331 - acc: 0.5306 -- iter: 0928/2003
[A[ATraining Step: 282  | total loss: [1m[32m0.69403[0m[0m | time: 56.969s
[2K
| RMSProp | epoch: 005 | loss: 0.69403 - acc: 0.5182 -- iter: 0960/2003
[A[ATraining Step: 283  | total loss: [1m[32m0.69363[0m[0m | time: 58.328s
[2K
| RMSProp | epoch: 005 | loss: 0.69363 - acc: 0.5258 -- iter: 0992/2003
[A[ATraining Step: 284  | total loss: [1m[32m0.69323[0m[0m | time: 59.409s
[2K
| RMSProp | epoch: 005 | loss: 0.69323 - acc: 0.5294 -- iter: 1024/2003
[A[ATraining Step: 285  | total loss: [1m[32m0.69302[0m[0m | time: 60.700s
[2K
| RMSProp | epoch: 005 | loss: 0.69302 - acc: 0.5296 -- iter: 1056/2003
[A[ATraining Step: 286  | total loss: [1m[32m0.69284[0m[0m | time: 61.968s
[2K
| RMSProp | epoch: 005 | loss: 0.69284 - acc: 0.5298 -- iter: 1088/2003
[A[ATraining Step: 287  | total loss: [1m[32m0.69336[0m[0m | time: 63.179s
[2K
| RMSProp | epoch: 005 | loss: 0.69336 - acc: 0.5237 -- iter: 1120/2003
[A[ATraining Step: 288  | total loss: [1m[32m0.69318[0m[0m | time: 64.369s
[2K
| RMSProp | epoch: 005 | loss: 0.69318 - acc: 0.5244 -- iter: 1152/2003
[A[ATraining Step: 289  | total loss: [1m[32m0.69322[0m[0m | time: 65.645s
[2K
| RMSProp | epoch: 005 | loss: 0.69322 - acc: 0.5220 -- iter: 1184/2003
[A[ATraining Step: 290  | total loss: [1m[32m0.69337[0m[0m | time: 66.983s
[2K
| RMSProp | epoch: 005 | loss: 0.69337 - acc: 0.5167 -- iter: 1216/2003
[A[ATraining Step: 291  | total loss: [1m[32m0.69347[0m[0m | time: 68.191s
[2K
| RMSProp | epoch: 005 | loss: 0.69347 - acc: 0.5119 -- iter: 1248/2003
[A[ATraining Step: 292  | total loss: [1m[32m0.69322[0m[0m | time: 69.494s
[2K
| RMSProp | epoch: 005 | loss: 0.69322 - acc: 0.5169 -- iter: 1280/2003
[A[ATraining Step: 293  | total loss: [1m[32m0.69290[0m[0m | time: 70.705s
[2K
| RMSProp | epoch: 005 | loss: 0.69290 - acc: 0.5215 -- iter: 1312/2003
[A[ATraining Step: 294  | total loss: [1m[32m0.69344[0m[0m | time: 71.891s
[2K
| RMSProp | epoch: 005 | loss: 0.69344 - acc: 0.5131 -- iter: 1344/2003
[A[ATraining Step: 295  | total loss: [1m[32m0.69326[0m[0m | time: 73.238s
[2K
| RMSProp | epoch: 005 | loss: 0.69326 - acc: 0.5149 -- iter: 1376/2003
[A[ATraining Step: 296  | total loss: [1m[32m0.69324[0m[0m | time: 74.622s
[2K
| RMSProp | epoch: 005 | loss: 0.69324 - acc: 0.5134 -- iter: 1408/2003
[A[ATraining Step: 297  | total loss: [1m[32m0.69359[0m[0m | time: 76.107s
[2K
| RMSProp | epoch: 005 | loss: 0.69359 - acc: 0.5027 -- iter: 1440/2003
[A[ATraining Step: 298  | total loss: [1m[32m0.69358[0m[0m | time: 77.160s
[2K
| RMSProp | epoch: 005 | loss: 0.69358 - acc: 0.5024 -- iter: 1472/2003
[A[ATraining Step: 299  | total loss: [1m[32m0.69371[0m[0m | time: 78.375s
[2K
| RMSProp | epoch: 005 | loss: 0.69371 - acc: 0.4928 -- iter: 1504/2003
[A[ATraining Step: 300  | total loss: [1m[32m0.69359[0m[0m | time: 79.700s
[2K
| RMSProp | epoch: 005 | loss: 0.69359 - acc: 0.5060 -- iter: 1536/2003
[A[ATraining Step: 301  | total loss: [1m[32m0.69469[0m[0m | time: 81.129s
[2K
| RMSProp | epoch: 005 | loss: 0.69469 - acc: 0.4867 -- iter: 1568/2003
[A[ATraining Step: 302  | total loss: [1m[32m0.69460[0m[0m | time: 82.738s
[2K
| RMSProp | epoch: 005 | loss: 0.69460 - acc: 0.4818 -- iter: 1600/2003
[A[ATraining Step: 303  | total loss: [1m[32m0.69446[0m[0m | time: 83.893s
[2K
| RMSProp | epoch: 005 | loss: 0.69446 - acc: 0.4867 -- iter: 1632/2003
[A[ATraining Step: 304  | total loss: [1m[32m0.69429[0m[0m | time: 85.230s
[2K
| RMSProp | epoch: 005 | loss: 0.69429 - acc: 0.5037 -- iter: 1664/2003
[A[ATraining Step: 305  | total loss: [1m[32m0.69423[0m[0m | time: 86.525s
[2K
| RMSProp | epoch: 005 | loss: 0.69423 - acc: 0.5033 -- iter: 1696/2003
[A[ATraining Step: 306  | total loss: [1m[32m0.69443[0m[0m | time: 87.764s
[2K
| RMSProp | epoch: 005 | loss: 0.69443 - acc: 0.4873 -- iter: 1728/2003
[A[ATraining Step: 307  | total loss: [1m[32m0.69429[0m[0m | time: 89.044s
[2K
| RMSProp | epoch: 005 | loss: 0.69429 - acc: 0.4917 -- iter: 1760/2003
[A[ATraining Step: 308  | total loss: [1m[32m0.69400[0m[0m | time: 90.444s
[2K
| RMSProp | epoch: 005 | loss: 0.69400 - acc: 0.5082 -- iter: 1792/2003
[A[ATraining Step: 309  | total loss: [1m[32m0.69401[0m[0m | time: 91.788s
[2K
| RMSProp | epoch: 005 | loss: 0.69401 - acc: 0.5042 -- iter: 1824/2003
[A[ATraining Step: 310  | total loss: [1m[32m0.69368[0m[0m | time: 92.878s
[2K
| RMSProp | epoch: 005 | loss: 0.69368 - acc: 0.5163 -- iter: 1856/2003
[A[ATraining Step: 311  | total loss: [1m[32m0.69381[0m[0m | time: 94.102s
[2K
| RMSProp | epoch: 005 | loss: 0.69381 - acc: 0.5116 -- iter: 1888/2003
[A[ATraining Step: 312  | total loss: [1m[32m0.69339[0m[0m | time: 95.432s
[2K
| RMSProp | epoch: 005 | loss: 0.69339 - acc: 0.5229 -- iter: 1920/2003
[A[ATraining Step: 313  | total loss: [1m[32m0.69402[0m[0m | time: 96.595s
[2K
| RMSProp | epoch: 005 | loss: 0.69402 - acc: 0.5081 -- iter: 1952/2003
[A[ATraining Step: 314  | total loss: [1m[32m0.69375[0m[0m | time: 97.740s
[2K
| RMSProp | epoch: 005 | loss: 0.69375 - acc: 0.5167 -- iter: 1984/2003
[A[ATraining Step: 315  | total loss: [1m[32m0.69337[0m[0m | time: 103.518s
[2K
| RMSProp | epoch: 005 | loss: 0.69337 - acc: 0.5244 | val_loss: 0.69301 - val_acc: 0.5096 -- iter: 2003/2003
--
Training Step: 316  | total loss: [1m[32m0.69307[0m[0m | time: 1.382s
[2K
| RMSProp | epoch: 006 | loss: 0.69307 - acc: 0.5282 -- iter: 0032/2003
[A[ATraining Step: 317  | total loss: [1m[32m0.69295[0m[0m | time: 2.648s
[2K
| RMSProp | epoch: 006 | loss: 0.69295 - acc: 0.5285 -- iter: 0064/2003
[A[ATraining Step: 318  | total loss: [1m[32m0.69322[0m[0m | time: 3.898s
[2K
| RMSProp | epoch: 006 | loss: 0.69322 - acc: 0.5225 -- iter: 0096/2003
[A[ATraining Step: 319  | total loss: [1m[32m0.69337[0m[0m | time: 4.674s
[2K
| RMSProp | epoch: 006 | loss: 0.69337 - acc: 0.5171 -- iter: 0128/2003
[A[ATraining Step: 320  | total loss: [1m[32m0.69418[0m[0m | time: 5.357s
[2K
| RMSProp | epoch: 006 | loss: 0.69418 - acc: 0.4917 -- iter: 0160/2003
[A[ATraining Step: 321  | total loss: [1m[32m0.69360[0m[0m | time: 6.236s
[2K
| RMSProp | epoch: 006 | loss: 0.69360 - acc: 0.5163 -- iter: 0192/2003
[A[ATraining Step: 322  | total loss: [1m[32m0.69648[0m[0m | time: 7.374s
[2K
| RMSProp | epoch: 006 | loss: 0.69648 - acc: 0.5115 -- iter: 0224/2003
[A[ATraining Step: 323  | total loss: [1m[32m0.69607[0m[0m | time: 8.509s
[2K
| RMSProp | epoch: 006 | loss: 0.69607 - acc: 0.5166 -- iter: 0256/2003
[A[ATraining Step: 324  | total loss: [1m[32m0.69567[0m[0m | time: 9.649s
[2K
| RMSProp | epoch: 006 | loss: 0.69567 - acc: 0.5181 -- iter: 0288/2003
[A[ATraining Step: 325  | total loss: [1m[32m0.69507[0m[0m | time: 10.605s
[2K
| RMSProp | epoch: 006 | loss: 0.69507 - acc: 0.5256 -- iter: 0320/2003
[A[ATraining Step: 326  | total loss: [1m[32m0.69446[0m[0m | time: 11.720s
[2K
| RMSProp | epoch: 006 | loss: 0.69446 - acc: 0.5293 -- iter: 0352/2003
[A[ATraining Step: 327  | total loss: [1m[32m0.69401[0m[0m | time: 13.050s
[2K
| RMSProp | epoch: 006 | loss: 0.69401 - acc: 0.5295 -- iter: 0384/2003
[A[ATraining Step: 328  | total loss: [1m[32m0.69387[0m[0m | time: 14.335s
[2K
| RMSProp | epoch: 006 | loss: 0.69387 - acc: 0.5297 -- iter: 0416/2003
[A[ATraining Step: 329  | total loss: [1m[32m0.69402[0m[0m | time: 15.639s
[2K
| RMSProp | epoch: 006 | loss: 0.69402 - acc: 0.5267 -- iter: 0448/2003
[A[ATraining Step: 330  | total loss: [1m[32m0.69314[0m[0m | time: 16.997s
[2K
| RMSProp | epoch: 006 | loss: 0.69314 - acc: 0.5334 -- iter: 0480/2003
[A[ATraining Step: 331  | total loss: [1m[32m0.69492[0m[0m | time: 18.279s
[2K
| RMSProp | epoch: 006 | loss: 0.69492 - acc: 0.5207 -- iter: 0512/2003
[A[ATraining Step: 332  | total loss: [1m[32m0.69468[0m[0m | time: 19.403s
[2K
| RMSProp | epoch: 006 | loss: 0.69468 - acc: 0.5218 -- iter: 0544/2003
[A[ATraining Step: 333  | total loss: [1m[32m0.69485[0m[0m | time: 20.458s
[2K
| RMSProp | epoch: 006 | loss: 0.69485 - acc: 0.5102 -- iter: 0576/2003
[A[ATraining Step: 334  | total loss: [1m[32m0.69459[0m[0m | time: 21.663s
[2K
| RMSProp | epoch: 006 | loss: 0.69459 - acc: 0.5123 -- iter: 0608/2003
[A[ATraining Step: 335  | total loss: [1m[32m0.69446[0m[0m | time: 22.908s
[2K
| RMSProp | epoch: 006 | loss: 0.69446 - acc: 0.5111 -- iter: 0640/2003
[A[ATraining Step: 336  | total loss: [1m[32m0.69409[0m[0m | time: 24.177s
[2K
| RMSProp | epoch: 006 | loss: 0.69409 - acc: 0.5225 -- iter: 0672/2003
[A[ATraining Step: 337  | total loss: [1m[32m0.69352[0m[0m | time: 25.520s
[2K
| RMSProp | epoch: 006 | loss: 0.69352 - acc: 0.5296 -- iter: 0704/2003
[A[ATraining Step: 338  | total loss: [1m[32m0.69305[0m[0m | time: 26.806s
[2K
| RMSProp | epoch: 006 | loss: 0.69305 - acc: 0.5329 -- iter: 0736/2003
[A[ATraining Step: 339  | total loss: [1m[32m0.69470[0m[0m | time: 27.982s
[2K
| RMSProp | epoch: 006 | loss: 0.69470 - acc: 0.5171 -- iter: 0768/2003
[A[ATraining Step: 340  | total loss: [1m[32m0.69452[0m[0m | time: 29.391s
[2K
| RMSProp | epoch: 006 | loss: 0.69452 - acc: 0.5154 -- iter: 0800/2003
[A[ATraining Step: 341  | total loss: [1m[32m0.69423[0m[0m | time: 30.698s
[2K
| RMSProp | epoch: 006 | loss: 0.69423 - acc: 0.5201 -- iter: 0832/2003
[A[ATraining Step: 342  | total loss: [1m[32m0.69415[0m[0m | time: 31.967s
[2K
| RMSProp | epoch: 006 | loss: 0.69415 - acc: 0.5181 -- iter: 0864/2003
[A[ATraining Step: 343  | total loss: [1m[32m0.69434[0m[0m | time: 33.205s
[2K
| RMSProp | epoch: 006 | loss: 0.69434 - acc: 0.5069 -- iter: 0896/2003
[A[ATraining Step: 344  | total loss: [1m[32m0.69422[0m[0m | time: 34.470s
[2K
| RMSProp | epoch: 006 | loss: 0.69422 - acc: 0.5062 -- iter: 0928/2003
[A[ATraining Step: 345  | total loss: [1m[32m0.69394[0m[0m | time: 35.798s
[2K
| RMSProp | epoch: 006 | loss: 0.69394 - acc: 0.5150 -- iter: 0960/2003
[A[ATraining Step: 346  | total loss: [1m[32m0.69361[0m[0m | time: 36.992s
[2K
| RMSProp | epoch: 006 | loss: 0.69361 - acc: 0.5197 -- iter: 0992/2003
[A[ATraining Step: 347  | total loss: [1m[32m0.69350[0m[0m | time: 38.239s
[2K
| RMSProp | epoch: 006 | loss: 0.69350 - acc: 0.5209 -- iter: 1024/2003
[A[ATraining Step: 348  | total loss: [1m[32m0.69386[0m[0m | time: 39.610s
[2K
| RMSProp | epoch: 006 | loss: 0.69386 - acc: 0.5125 -- iter: 1056/2003
[A[ATraining Step: 349  | total loss: [1m[32m0.69387[0m[0m | time: 40.900s
[2K
| RMSProp | epoch: 006 | loss: 0.69387 - acc: 0.5082 -- iter: 1088/2003
[A[ATraining Step: 350  | total loss: [1m[32m0.69360[0m[0m | time: 42.151s
[2K
| RMSProp | epoch: 006 | loss: 0.69360 - acc: 0.5167 -- iter: 1120/2003
[A[ATraining Step: 351  | total loss: [1m[32m0.69357[0m[0m | time: 43.530s
[2K
| RMSProp | epoch: 006 | loss: 0.69357 - acc: 0.5150 -- iter: 1152/2003
[A[ATraining Step: 352  | total loss: [1m[32m0.69380[0m[0m | time: 44.808s
[2K
| RMSProp | epoch: 006 | loss: 0.69380 - acc: 0.5073 -- iter: 1184/2003
[A[ATraining Step: 353  | total loss: [1m[32m0.69389[0m[0m | time: 46.163s
[2K
| RMSProp | epoch: 006 | loss: 0.69389 - acc: 0.5003 -- iter: 1216/2003
[A[ATraining Step: 354  | total loss: [1m[32m0.69383[0m[0m | time: 47.357s
[2K
| RMSProp | epoch: 006 | loss: 0.69383 - acc: 0.4972 -- iter: 1248/2003
[A[ATraining Step: 355  | total loss: [1m[32m0.69369[0m[0m | time: 48.507s
[2K
| RMSProp | epoch: 006 | loss: 0.69369 - acc: 0.5037 -- iter: 1280/2003
[A[ATraining Step: 356  | total loss: [1m[32m0.69329[0m[0m | time: 49.814s
[2K
| RMSProp | epoch: 006 | loss: 0.69329 - acc: 0.5189 -- iter: 1312/2003
[A[ATraining Step: 357  | total loss: [1m[32m0.69327[0m[0m | time: 51.040s
[2K
| RMSProp | epoch: 006 | loss: 0.69327 - acc: 0.5171 -- iter: 1344/2003
[A[ATraining Step: 358  | total loss: [1m[32m0.69311[0m[0m | time: 52.168s
[2K
| RMSProp | epoch: 006 | loss: 0.69311 - acc: 0.5185 -- iter: 1376/2003
[A[ATraining Step: 359  | total loss: [1m[32m0.69333[0m[0m | time: 53.406s
[2K
| RMSProp | epoch: 006 | loss: 0.69333 - acc: 0.5135 -- iter: 1408/2003
[A[ATraining Step: 360  | total loss: [1m[32m0.69366[0m[0m | time: 54.793s
[2K
| RMSProp | epoch: 006 | loss: 0.69366 - acc: 0.4996 -- iter: 1440/2003
[A[ATraining Step: 361  | total loss: [1m[32m0.69364[0m[0m | time: 55.974s
[2K
| RMSProp | epoch: 006 | loss: 0.69364 - acc: 0.4997 -- iter: 1472/2003
[A[ATraining Step: 362  | total loss: [1m[32m0.69350[0m[0m | time: 57.268s
[2K
| RMSProp | epoch: 006 | loss: 0.69350 - acc: 0.4997 -- iter: 1504/2003
[A[ATraining Step: 363  | total loss: [1m[32m0.69362[0m[0m | time: 58.610s
[2K
| RMSProp | epoch: 006 | loss: 0.69362 - acc: 0.4966 -- iter: 1536/2003
[A[ATraining Step: 364  | total loss: [1m[32m0.69360[0m[0m | time: 59.931s
[2K
| RMSProp | epoch: 006 | loss: 0.69360 - acc: 0.4907 -- iter: 1568/2003
[A[ATraining Step: 365  | total loss: [1m[32m0.69336[0m[0m | time: 61.014s
[2K
| RMSProp | epoch: 006 | loss: 0.69336 - acc: 0.5041 -- iter: 1600/2003
[A[ATraining Step: 366  | total loss: [1m[32m0.69521[0m[0m | time: 62.201s
[2K
| RMSProp | epoch: 006 | loss: 0.69521 - acc: 0.4850 -- iter: 1632/2003
[A[ATraining Step: 367  | total loss: [1m[32m0.69498[0m[0m | time: 63.521s
[2K
| RMSProp | epoch: 006 | loss: 0.69498 - acc: 0.4959 -- iter: 1664/2003
[A[ATraining Step: 368  | total loss: [1m[32m0.69497[0m[0m | time: 64.693s
[2K
| RMSProp | epoch: 006 | loss: 0.69497 - acc: 0.4900 -- iter: 1696/2003
[A[ATraining Step: 369  | total loss: [1m[32m0.69480[0m[0m | time: 66.106s
[2K
| RMSProp | epoch: 006 | loss: 0.69480 - acc: 0.4879 -- iter: 1728/2003
[A[ATraining Step: 370  | total loss: [1m[32m0.69458[0m[0m | time: 67.514s
[2K
| RMSProp | epoch: 006 | loss: 0.69458 - acc: 0.4985 -- iter: 1760/2003
[A[ATraining Step: 371  | total loss: [1m[32m0.69443[0m[0m | time: 68.867s
[2K
| RMSProp | epoch: 006 | loss: 0.69443 - acc: 0.4986 -- iter: 1792/2003
[A[ATraining Step: 372  | total loss: [1m[32m0.69421[0m[0m | time: 69.801s
[2K
| RMSProp | epoch: 006 | loss: 0.69421 - acc: 0.5019 -- iter: 1824/2003
[A[ATraining Step: 373  | total loss: [1m[32m0.69412[0m[0m | time: 71.151s
[2K
| RMSProp | epoch: 006 | loss: 0.69412 - acc: 0.4986 -- iter: 1856/2003
[A[ATraining Step: 374  | total loss: [1m[32m0.69412[0m[0m | time: 72.497s
[2K
| RMSProp | epoch: 006 | loss: 0.69412 - acc: 0.4831 -- iter: 1888/2003
[A[ATraining Step: 375  | total loss: [1m[32m0.69393[0m[0m | time: 74.165s
[2K
| RMSProp | epoch: 006 | loss: 0.69393 - acc: 0.4879 -- iter: 1920/2003
[A[ATraining Step: 376  | total loss: [1m[32m0.69318[0m[0m | time: 75.772s
[2K
| RMSProp | epoch: 006 | loss: 0.69318 - acc: 0.5047 -- iter: 1952/2003
[A[ATraining Step: 377  | total loss: [1m[32m0.69389[0m[0m | time: 76.946s
[2K
| RMSProp | epoch: 006 | loss: 0.69389 - acc: 0.5011 -- iter: 1984/2003
[A[ATraining Step: 378  | total loss: [1m[32m0.69378[0m[0m | time: 82.667s
[2K
| RMSProp | epoch: 006 | loss: 0.69378 - acc: 0.5010 | val_loss: 0.69399 - val_acc: 0.4904 -- iter: 2003/2003
--
Training Step: 379  | total loss: [1m[32m0.69338[0m[0m | time: 1.472s
[2K
| RMSProp | epoch: 007 | loss: 0.69338 - acc: 0.5103 -- iter: 0032/2003
[A[ATraining Step: 380  | total loss: [1m[32m0.69366[0m[0m | time: 2.772s
[2K
| RMSProp | epoch: 007 | loss: 0.69366 - acc: 0.5061 -- iter: 0064/2003
[A[ATraining Step: 381  | total loss: [1m[32m0.69336[0m[0m | time: 4.010s
[2K
| RMSProp | epoch: 007 | loss: 0.69336 - acc: 0.5087 -- iter: 0096/2003
[A[ATraining Step: 382  | total loss: [1m[32m0.69311[0m[0m | time: 5.406s
[2K
| RMSProp | epoch: 007 | loss: 0.69311 - acc: 0.5109 -- iter: 0128/2003
[A[ATraining Step: 383  | total loss: [1m[32m0.69406[0m[0m | time: 6.499s
[2K
| RMSProp | epoch: 007 | loss: 0.69406 - acc: 0.4973 -- iter: 0160/2003
[A[ATraining Step: 384  | total loss: [1m[32m0.69371[0m[0m | time: 7.372s
[2K
| RMSProp | epoch: 007 | loss: 0.69371 - acc: 0.5107 -- iter: 0192/2003
[A[ATraining Step: 385  | total loss: [1m[32m0.69306[0m[0m | time: 8.703s
[2K
| RMSProp | epoch: 007 | loss: 0.69306 - acc: 0.5228 -- iter: 0224/2003
[A[ATraining Step: 386  | total loss: [1m[32m0.69308[0m[0m | time: 9.773s
[2K
| RMSProp | epoch: 007 | loss: 0.69308 - acc: 0.5205 -- iter: 0256/2003
[A[ATraining Step: 387  | total loss: [1m[32m0.69354[0m[0m | time: 11.091s
[2K
| RMSProp | epoch: 007 | loss: 0.69354 - acc: 0.5122 -- iter: 0288/2003
[A[ATraining Step: 388  | total loss: [1m[32m0.69313[0m[0m | time: 12.324s
[2K
| RMSProp | epoch: 007 | loss: 0.69313 - acc: 0.5204 -- iter: 0320/2003
[A[ATraining Step: 389  | total loss: [1m[32m0.69306[0m[0m | time: 13.609s
[2K
| RMSProp | epoch: 007 | loss: 0.69306 - acc: 0.5184 -- iter: 0352/2003
[A[ATraining Step: 390  | total loss: [1m[32m0.69267[0m[0m | time: 14.952s
[2K
| RMSProp | epoch: 007 | loss: 0.69267 - acc: 0.5228 -- iter: 0384/2003
[A[ATraining Step: 391  | total loss: [1m[32m0.69342[0m[0m | time: 16.247s
[2K
| RMSProp | epoch: 007 | loss: 0.69342 - acc: 0.5111 -- iter: 0416/2003
[A[ATraining Step: 392  | total loss: [1m[32m0.69377[0m[0m | time: 17.455s
[2K
| RMSProp | epoch: 007 | loss: 0.69377 - acc: 0.4975 -- iter: 0448/2003
[A[ATraining Step: 393  | total loss: [1m[32m0.69356[0m[0m | time: 18.741s
[2K
| RMSProp | epoch: 007 | loss: 0.69356 - acc: 0.5103 -- iter: 0480/2003
[A[ATraining Step: 394  | total loss: [1m[32m0.69350[0m[0m | time: 20.060s
[2K
| RMSProp | epoch: 007 | loss: 0.69350 - acc: 0.5061 -- iter: 0512/2003
[A[ATraining Step: 395  | total loss: [1m[32m0.69331[0m[0m | time: 21.195s
[2K
| RMSProp | epoch: 007 | loss: 0.69331 - acc: 0.5305 -- iter: 0544/2003
[A[ATraining Step: 396  | total loss: [1m[32m0.69293[0m[0m | time: 22.431s
[2K
| RMSProp | epoch: 007 | loss: 0.69293 - acc: 0.5337 -- iter: 0576/2003
[A[ATraining Step: 397  | total loss: [1m[32m0.69226[0m[0m | time: 23.747s
[2K
| RMSProp | epoch: 007 | loss: 0.69226 - acc: 0.5397 -- iter: 0608/2003
[A[ATraining Step: 398  | total loss: [1m[32m0.69135[0m[0m | time: 25.122s
[2K
| RMSProp | epoch: 007 | loss: 0.69135 - acc: 0.5451 -- iter: 0640/2003
[A[ATraining Step: 399  | total loss: [1m[32m0.69580[0m[0m | time: 26.588s
[2K
| RMSProp | epoch: 007 | loss: 0.69580 - acc: 0.5281 -- iter: 0672/2003
[A[ATraining Step: 400  | total loss: [1m[32m0.69504[0m[0m | time: 32.101s
[2K
| RMSProp | epoch: 007 | loss: 0.69504 - acc: 0.5284 | val_loss: 0.69259 - val_acc: 0.4904 -- iter: 0704/2003
--
Training Step: 401  | total loss: [1m[32m0.69377[0m[0m | time: 33.238s
[2K
| RMSProp | epoch: 007 | loss: 0.69377 - acc: 0.5474 -- iter: 0736/2003
[A[ATraining Step: 402  | total loss: [1m[32m0.69281[0m[0m | time: 34.205s
[2K
| RMSProp | epoch: 007 | loss: 0.69281 - acc: 0.5489 -- iter: 0768/2003
[A[ATraining Step: 403  | total loss: [1m[32m0.69098[0m[0m | time: 34.882s
[2K
| RMSProp | epoch: 007 | loss: 0.69098 - acc: 0.5566 -- iter: 0800/2003
[A[ATraining Step: 404  | total loss: [1m[32m0.69189[0m[0m | time: 35.730s
[2K
| RMSProp | epoch: 007 | loss: 0.69189 - acc: 0.5478 -- iter: 0832/2003
[A[ATraining Step: 405  | total loss: [1m[32m0.69098[0m[0m | time: 36.581s
[2K
| RMSProp | epoch: 007 | loss: 0.69098 - acc: 0.5492 -- iter: 0864/2003
[A[ATraining Step: 406  | total loss: [1m[32m0.68737[0m[0m | time: 37.508s
[2K
| RMSProp | epoch: 007 | loss: 0.68737 - acc: 0.5631 -- iter: 0896/2003
[A[ATraining Step: 407  | total loss: [1m[32m0.69518[0m[0m | time: 38.419s
[2K
| RMSProp | epoch: 007 | loss: 0.69518 - acc: 0.5411 -- iter: 0928/2003
[A[ATraining Step: 408  | total loss: [1m[32m0.69439[0m[0m | time: 39.341s
[2K
| RMSProp | epoch: 007 | loss: 0.69439 - acc: 0.5402 -- iter: 0960/2003
[A[ATraining Step: 409  | total loss: [1m[32m0.69391[0m[0m | time: 40.302s
[2K
| RMSProp | epoch: 007 | loss: 0.69391 - acc: 0.5330 -- iter: 0992/2003
[A[ATraining Step: 410  | total loss: [1m[32m0.69289[0m[0m | time: 41.307s
[2K
| RMSProp | epoch: 007 | loss: 0.69289 - acc: 0.5328 -- iter: 1024/2003
[A[ATraining Step: 411  | total loss: [1m[32m0.69162[0m[0m | time: 42.240s
[2K
| RMSProp | epoch: 007 | loss: 0.69162 - acc: 0.5577 -- iter: 1056/2003
[A[ATraining Step: 412  | total loss: [1m[32m0.68927[0m[0m | time: 43.305s
[2K
| RMSProp | epoch: 007 | loss: 0.68927 - acc: 0.5769 -- iter: 1088/2003
[A[ATraining Step: 413  | total loss: [1m[32m0.69167[0m[0m | time: 44.323s
[2K
| RMSProp | epoch: 007 | loss: 0.69167 - acc: 0.5567 -- iter: 1120/2003
[A[ATraining Step: 414  | total loss: [1m[32m0.69106[0m[0m | time: 45.243s
[2K
| RMSProp | epoch: 007 | loss: 0.69106 - acc: 0.5479 -- iter: 1152/2003
[A[ATraining Step: 415  | total loss: [1m[32m0.69047[0m[0m | time: 46.079s
[2K
| RMSProp | epoch: 007 | loss: 0.69047 - acc: 0.5494 -- iter: 1184/2003
[A[ATraining Step: 416  | total loss: [1m[32m0.69077[0m[0m | time: 46.952s
[2K
| RMSProp | epoch: 007 | loss: 0.69077 - acc: 0.5476 -- iter: 1216/2003
[A[ATraining Step: 417  | total loss: [1m[32m0.68905[0m[0m | time: 47.876s
[2K
| RMSProp | epoch: 007 | loss: 0.68905 - acc: 0.5616 -- iter: 1248/2003
[A[ATraining Step: 418  | total loss: [1m[32m0.68876[0m[0m | time: 48.840s
[2K
| RMSProp | epoch: 007 | loss: 0.68876 - acc: 0.5585 -- iter: 1280/2003
[A[ATraining Step: 419  | total loss: [1m[32m0.68729[0m[0m | time: 49.779s
[2K
| RMSProp | epoch: 007 | loss: 0.68729 - acc: 0.5589 -- iter: 1312/2003
[A[ATraining Step: 420  | total loss: [1m[32m0.68995[0m[0m | time: 50.707s
[2K
| RMSProp | epoch: 007 | loss: 0.68995 - acc: 0.5437 -- iter: 1344/2003
[A[ATraining Step: 421  | total loss: [1m[32m0.68919[0m[0m | time: 51.627s
[2K
| RMSProp | epoch: 007 | loss: 0.68919 - acc: 0.5487 -- iter: 1376/2003
[A[ATraining Step: 422  | total loss: [1m[32m0.68824[0m[0m | time: 52.613s
[2K
| RMSProp | epoch: 007 | loss: 0.68824 - acc: 0.5532 -- iter: 1408/2003
[A[ATraining Step: 423  | total loss: [1m[32m0.68850[0m[0m | time: 53.605s
[2K
| RMSProp | epoch: 007 | loss: 0.68850 - acc: 0.5479 -- iter: 1440/2003
[A[ATraining Step: 424  | total loss: [1m[32m0.68770[0m[0m | time: 54.813s
[2K
| RMSProp | epoch: 007 | loss: 0.68770 - acc: 0.5524 -- iter: 1472/2003
[A[ATraining Step: 425  | total loss: [1m[32m0.68518[0m[0m | time: 56.068s
[2K
| RMSProp | epoch: 007 | loss: 0.68518 - acc: 0.5535 -- iter: 1504/2003
[A[ATraining Step: 426  | total loss: [1m[32m0.68634[0m[0m | time: 57.386s
[2K
| RMSProp | epoch: 007 | loss: 0.68634 - acc: 0.5419 -- iter: 1536/2003
[A[ATraining Step: 427  | total loss: [1m[32m0.68415[0m[0m | time: 58.510s
[2K
| RMSProp | epoch: 007 | loss: 0.68415 - acc: 0.5533 -- iter: 1568/2003
[A[ATraining Step: 428  | total loss: [1m[32m0.68056[0m[0m | time: 59.903s
[2K
| RMSProp | epoch: 007 | loss: 0.68056 - acc: 0.5573 -- iter: 1600/2003
[A[ATraining Step: 429  | total loss: [1m[32m0.67739[0m[0m | time: 61.304s
[2K
| RMSProp | epoch: 007 | loss: 0.67739 - acc: 0.5610 -- iter: 1632/2003
[A[ATraining Step: 430  | total loss: [1m[32m0.67258[0m[0m | time: 62.571s
[2K
| RMSProp | epoch: 007 | loss: 0.67258 - acc: 0.5674 -- iter: 1664/2003
[A[ATraining Step: 431  | total loss: [1m[32m0.65960[0m[0m | time: 63.784s
[2K
| RMSProp | epoch: 007 | loss: 0.65960 - acc: 0.5919 -- iter: 1696/2003
[A[ATraining Step: 432  | total loss: [1m[32m0.65584[0m[0m | time: 65.049s
[2K
| RMSProp | epoch: 007 | loss: 0.65584 - acc: 0.6046 -- iter: 1728/2003
[A[ATraining Step: 433  | total loss: [1m[32m0.65650[0m[0m | time: 66.291s
[2K
| RMSProp | epoch: 007 | loss: 0.65650 - acc: 0.6066 -- iter: 1760/2003
[A[ATraining Step: 434  | total loss: [1m[32m0.66204[0m[0m | time: 67.502s
[2K
| RMSProp | epoch: 007 | loss: 0.66204 - acc: 0.5991 -- iter: 1792/2003
[A[ATraining Step: 435  | total loss: [1m[32m0.66994[0m[0m | time: 68.843s
[2K
| RMSProp | epoch: 007 | loss: 0.66994 - acc: 0.5923 -- iter: 1824/2003
[A[ATraining Step: 436  | total loss: [1m[32m0.66812[0m[0m | time: 70.185s
[2K
| RMSProp | epoch: 007 | loss: 0.66812 - acc: 0.5956 -- iter: 1856/2003
[A[ATraining Step: 437  | total loss: [1m[32m0.66621[0m[0m | time: 71.421s
[2K
| RMSProp | epoch: 007 | loss: 0.66621 - acc: 0.6016 -- iter: 1888/2003
[A[ATraining Step: 438  | total loss: [1m[32m0.66400[0m[0m | time: 72.784s
[2K
| RMSProp | epoch: 007 | loss: 0.66400 - acc: 0.6040 -- iter: 1920/2003
[A[ATraining Step: 439  | total loss: [1m[32m0.66277[0m[0m | time: 74.099s
[2K
| RMSProp | epoch: 007 | loss: 0.66277 - acc: 0.5967 -- iter: 1952/2003
[A[ATraining Step: 440  | total loss: [1m[32m0.65625[0m[0m | time: 75.093s
[2K
| RMSProp | epoch: 007 | loss: 0.65625 - acc: 0.6120 -- iter: 1984/2003
[A[ATraining Step: 441  | total loss: [1m[32m0.65058[0m[0m | time: 82.909s
[2K
| RMSProp | epoch: 007 | loss: 0.65058 - acc: 0.6227 | val_loss: 0.71383 - val_acc: 0.5671 -- iter: 2003/2003
--
Training Step: 442  | total loss: [1m[32m0.64745[0m[0m | time: 1.327s
[2K
| RMSProp | epoch: 008 | loss: 0.64745 - acc: 0.6261 -- iter: 0032/2003
[A[ATraining Step: 443  | total loss: [1m[32m0.64620[0m[0m | time: 2.662s
[2K
| RMSProp | epoch: 008 | loss: 0.64620 - acc: 0.6260 -- iter: 0064/2003
[A[ATraining Step: 444  | total loss: [1m[32m0.64908[0m[0m | time: 3.900s
[2K
| RMSProp | epoch: 008 | loss: 0.64908 - acc: 0.6259 -- iter: 0096/2003
[A[ATraining Step: 445  | total loss: [1m[32m0.64102[0m[0m | time: 5.088s
[2K
| RMSProp | epoch: 008 | loss: 0.64102 - acc: 0.6383 -- iter: 0128/2003
[A[ATraining Step: 446  | total loss: [1m[32m0.63576[0m[0m | time: 6.438s
[2K
| RMSProp | epoch: 008 | loss: 0.63576 - acc: 0.6432 -- iter: 0160/2003
[A[ATraining Step: 447  | total loss: [1m[32m0.62535[0m[0m | time: 7.226s
[2K
| RMSProp | epoch: 008 | loss: 0.62535 - acc: 0.6508 -- iter: 0192/2003
[A[ATraining Step: 448  | total loss: [1m[32m0.62278[0m[0m | time: 8.094s
[2K
| RMSProp | epoch: 008 | loss: 0.62278 - acc: 0.6541 -- iter: 0224/2003
[A[ATraining Step: 449  | total loss: [1m[32m0.61559[0m[0m | time: 16.255s
[2K
| RMSProp | epoch: 008 | loss: 0.61559 - acc: 0.6571 -- iter: 0256/2003
[A[ATraining Step: 450  | total loss: [1m[32m0.63069[0m[0m | time: 18.637s
[2K
| RMSProp | epoch: 008 | loss: 0.63069 - acc: 0.6320 -- iter: 0288/2003
[A[ATraining Step: 451  | total loss: [1m[32m0.63797[0m[0m | time: 19.735s
[2K
| RMSProp | epoch: 008 | loss: 0.63797 - acc: 0.6251 -- iter: 0320/2003
[A[ATraining Step: 452  | total loss: [1m[32m0.63717[0m[0m | time: 21.016s
[2K
| RMSProp | epoch: 008 | loss: 0.63717 - acc: 0.6344 -- iter: 0352/2003
[A[ATraining Step: 453  | total loss: [1m[32m0.63317[0m[0m | time: 22.335s
[2K
| RMSProp | epoch: 008 | loss: 0.63317 - acc: 0.6429 -- iter: 0384/2003
[A[ATraining Step: 454  | total loss: [1m[32m0.62553[0m[0m | time: 23.618s
[2K
| RMSProp | epoch: 008 | loss: 0.62553 - acc: 0.6598 -- iter: 0416/2003
[A[ATraining Step: 455  | total loss: [1m[32m0.62752[0m[0m | time: 24.919s
[2K
| RMSProp | epoch: 008 | loss: 0.62752 - acc: 0.6532 -- iter: 0448/2003
[A[ATraining Step: 456  | total loss: [1m[32m0.62194[0m[0m | time: 26.173s
[2K
| RMSProp | epoch: 008 | loss: 0.62194 - acc: 0.6598 -- iter: 0480/2003
[A[ATraining Step: 457  | total loss: [1m[32m0.61503[0m[0m | time: 27.313s
[2K
| RMSProp | epoch: 008 | loss: 0.61503 - acc: 0.6625 -- iter: 0512/2003
[A[ATraining Step: 458  | total loss: [1m[32m0.61851[0m[0m | time: 28.766s
[2K
| RMSProp | epoch: 008 | loss: 0.61851 - acc: 0.6619 -- iter: 0544/2003
[A[ATraining Step: 459  | total loss: [1m[32m0.60907[0m[0m | time: 29.985s
[2K
| RMSProp | epoch: 008 | loss: 0.60907 - acc: 0.6770 -- iter: 0576/2003
[A[ATraining Step: 460  | total loss: [1m[32m0.60917[0m[0m | time: 31.407s
[2K
| RMSProp | epoch: 008 | loss: 0.60917 - acc: 0.6812 -- iter: 0608/2003
[A[ATraining Step: 461  | total loss: [1m[32m0.61059[0m[0m | time: 32.642s
[2K
| RMSProp | epoch: 008 | loss: 0.61059 - acc: 0.6818 -- iter: 0640/2003
[A[ATraining Step: 462  | total loss: [1m[32m0.60605[0m[0m | time: 33.823s
[2K
| RMSProp | epoch: 008 | loss: 0.60605 - acc: 0.6855 -- iter: 0672/2003
[A[ATraining Step: 463  | total loss: [1m[32m0.59881[0m[0m | time: 34.852s
[2K
| RMSProp | epoch: 008 | loss: 0.59881 - acc: 0.6951 -- iter: 0704/2003
[A[ATraining Step: 464  | total loss: [1m[32m0.59625[0m[0m | time: 35.948s
[2K
| RMSProp | epoch: 008 | loss: 0.59625 - acc: 0.7006 -- iter: 0736/2003
[A[ATraining Step: 465  | total loss: [1m[32m0.60789[0m[0m | time: 37.146s
[2K
| RMSProp | epoch: 008 | loss: 0.60789 - acc: 0.6930 -- iter: 0768/2003
[A[ATraining Step: 466  | total loss: [1m[32m0.62494[0m[0m | time: 38.398s
[2K
| RMSProp | epoch: 008 | loss: 0.62494 - acc: 0.6768 -- iter: 0800/2003
[A[ATraining Step: 467  | total loss: [1m[32m0.62243[0m[0m | time: 39.608s
[2K
| RMSProp | epoch: 008 | loss: 0.62243 - acc: 0.6779 -- iter: 0832/2003
[A[ATraining Step: 468  | total loss: [1m[32m0.62354[0m[0m | time: 40.810s
[2K
| RMSProp | epoch: 008 | loss: 0.62354 - acc: 0.6726 -- iter: 0864/2003
[A[ATraining Step: 469  | total loss: [1m[32m0.62276[0m[0m | time: 41.949s
[2K
| RMSProp | epoch: 008 | loss: 0.62276 - acc: 0.6710 -- iter: 0896/2003
[A[ATraining Step: 470  | total loss: [1m[32m0.61833[0m[0m | time: 43.227s
[2K
| RMSProp | epoch: 008 | loss: 0.61833 - acc: 0.6820 -- iter: 0928/2003
[A[ATraining Step: 471  | total loss: [1m[32m0.62198[0m[0m | time: 44.559s
[2K
| RMSProp | epoch: 008 | loss: 0.62198 - acc: 0.6669 -- iter: 0960/2003
[A[ATraining Step: 472  | total loss: [1m[32m0.61817[0m[0m | time: 45.899s
[2K
| RMSProp | epoch: 008 | loss: 0.61817 - acc: 0.6752 -- iter: 0992/2003
[A[ATraining Step: 473  | total loss: [1m[32m0.60364[0m[0m | time: 46.942s
[2K
| RMSProp | epoch: 008 | loss: 0.60364 - acc: 0.6921 -- iter: 1024/2003
[A[ATraining Step: 474  | total loss: [1m[32m0.62457[0m[0m | time: 48.263s
[2K
| RMSProp | epoch: 008 | loss: 0.62457 - acc: 0.6885 -- iter: 1056/2003
[A[ATraining Step: 475  | total loss: [1m[32m0.62472[0m[0m | time: 49.529s
[2K
| RMSProp | epoch: 008 | loss: 0.62472 - acc: 0.6790 -- iter: 1088/2003
[A[ATraining Step: 476  | total loss: [1m[32m0.62998[0m[0m | time: 50.723s
[2K
| RMSProp | epoch: 008 | loss: 0.62998 - acc: 0.6705 -- iter: 1120/2003
[A[ATraining Step: 477  | total loss: [1m[32m0.62648[0m[0m | time: 51.903s
[2K
| RMSProp | epoch: 008 | loss: 0.62648 - acc: 0.6784 -- iter: 1152/2003
[A[ATraining Step: 478  | total loss: [1m[32m0.62511[0m[0m | time: 53.205s
[2K
| RMSProp | epoch: 008 | loss: 0.62511 - acc: 0.6762 -- iter: 1184/2003
[A[ATraining Step: 479  | total loss: [1m[32m0.62736[0m[0m | time: 54.514s
[2K
| RMSProp | epoch: 008 | loss: 0.62736 - acc: 0.6649 -- iter: 1216/2003
[A[ATraining Step: 480  | total loss: [1m[32m0.61906[0m[0m | time: 55.748s
[2K
| RMSProp | epoch: 008 | loss: 0.61906 - acc: 0.6796 -- iter: 1248/2003
[A[ATraining Step: 481  | total loss: [1m[32m0.61806[0m[0m | time: 56.888s
[2K
| RMSProp | epoch: 008 | loss: 0.61806 - acc: 0.6773 -- iter: 1280/2003
[A[ATraining Step: 482  | total loss: [1m[32m0.61705[0m[0m | time: 58.067s
[2K
| RMSProp | epoch: 008 | loss: 0.61705 - acc: 0.6814 -- iter: 1312/2003
[A[ATraining Step: 483  | total loss: [1m[32m0.60492[0m[0m | time: 59.448s
[2K
| RMSProp | epoch: 008 | loss: 0.60492 - acc: 0.6945 -- iter: 1344/2003
[A[ATraining Step: 484  | total loss: [1m[32m0.60900[0m[0m | time: 60.566s
[2K
| RMSProp | epoch: 008 | loss: 0.60900 - acc: 0.6938 -- iter: 1376/2003
[A[ATraining Step: 485  | total loss: [1m[32m0.63094[0m[0m | time: 61.896s
[2K
| RMSProp | epoch: 008 | loss: 0.63094 - acc: 0.6713 -- iter: 1408/2003
[A[ATraining Step: 486  | total loss: [1m[32m0.62595[0m[0m | time: 63.079s
[2K
| RMSProp | epoch: 008 | loss: 0.62595 - acc: 0.6761 -- iter: 1440/2003
[A[ATraining Step: 487  | total loss: [1m[32m0.62947[0m[0m | time: 64.209s
[2K
| RMSProp | epoch: 008 | loss: 0.62947 - acc: 0.6616 -- iter: 1472/2003
[A[ATraining Step: 488  | total loss: [1m[32m0.62809[0m[0m | time: 65.436s
[2K
| RMSProp | epoch: 008 | loss: 0.62809 - acc: 0.6548 -- iter: 1504/2003
[A[ATraining Step: 489  | total loss: [1m[32m0.62558[0m[0m | time: 66.770s
[2K
| RMSProp | epoch: 008 | loss: 0.62558 - acc: 0.6643 -- iter: 1536/2003
[A[ATraining Step: 490  | total loss: [1m[32m0.61954[0m[0m | time: 68.051s
[2K
| RMSProp | epoch: 008 | loss: 0.61954 - acc: 0.6666 -- iter: 1568/2003
[A[ATraining Step: 491  | total loss: [1m[32m0.61556[0m[0m | time: 69.091s
[2K
| RMSProp | epoch: 008 | loss: 0.61556 - acc: 0.6719 -- iter: 1600/2003
[A[ATraining Step: 492  | total loss: [1m[32m0.61751[0m[0m | time: 70.449s
[2K
| RMSProp | epoch: 008 | loss: 0.61751 - acc: 0.6734 -- iter: 1632/2003
[A[ATraining Step: 493  | total loss: [1m[32m0.60914[0m[0m | time: 71.768s
[2K
| RMSProp | epoch: 008 | loss: 0.60914 - acc: 0.6779 -- iter: 1664/2003
[A[ATraining Step: 494  | total loss: [1m[32m0.59998[0m[0m | time: 73.055s
[2K
| RMSProp | epoch: 008 | loss: 0.59998 - acc: 0.6883 -- iter: 1696/2003
[A[ATraining Step: 495  | total loss: [1m[32m0.60491[0m[0m | time: 74.176s
[2K
| RMSProp | epoch: 008 | loss: 0.60491 - acc: 0.6913 -- iter: 1728/2003
[A[ATraining Step: 496  | total loss: [1m[32m0.59526[0m[0m | time: 75.385s
[2K
| RMSProp | epoch: 008 | loss: 0.59526 - acc: 0.6909 -- iter: 1760/2003
[A[ATraining Step: 497  | total loss: [1m[32m0.58652[0m[0m | time: 76.667s
[2K
| RMSProp | epoch: 008 | loss: 0.58652 - acc: 0.7062 -- iter: 1792/2003
[A[ATraining Step: 498  | total loss: [1m[32m0.58816[0m[0m | time: 77.987s
[2K
| RMSProp | epoch: 008 | loss: 0.58816 - acc: 0.7044 -- iter: 1824/2003
[A[ATraining Step: 499  | total loss: [1m[32m0.59394[0m[0m | time: 79.257s
[2K
| RMSProp | epoch: 008 | loss: 0.59394 - acc: 0.7027 -- iter: 1856/2003
[A[ATraining Step: 500  | total loss: [1m[32m0.58931[0m[0m | time: 80.553s
[2K
| RMSProp | epoch: 008 | loss: 0.58931 - acc: 0.7074 -- iter: 1888/2003
[A[ATraining Step: 501  | total loss: [1m[32m0.58394[0m[0m | time: 81.916s
[2K
| RMSProp | epoch: 008 | loss: 0.58394 - acc: 0.7117 -- iter: 1920/2003
[A[ATraining Step: 502  | total loss: [1m[32m0.58196[0m[0m | time: 83.268s
[2K
| RMSProp | epoch: 008 | loss: 0.58196 - acc: 0.7155 -- iter: 1952/2003
[A[ATraining Step: 503  | total loss: [1m[32m0.58764[0m[0m | time: 84.639s
[2K
| RMSProp | epoch: 008 | loss: 0.58764 - acc: 0.7064 -- iter: 1984/2003
[A[ATraining Step: 504  | total loss: [1m[32m0.60821[0m[0m | time: 90.397s
[2K
| RMSProp | epoch: 008 | loss: 0.60821 - acc: 0.6889 | val_loss: 0.55748 - val_acc: 0.7220 -- iter: 2003/2003
--
Training Step: 505  | total loss: [1m[32m0.59573[0m[0m | time: 1.323s
[2K
| RMSProp | epoch: 009 | loss: 0.59573 - acc: 0.6950 -- iter: 0032/2003
[A[ATraining Step: 506  | total loss: [1m[32m0.58867[0m[0m | time: 2.567s
[2K
| RMSProp | epoch: 009 | loss: 0.58867 - acc: 0.7005 -- iter: 0064/2003
[A[ATraining Step: 507  | total loss: [1m[32m0.58941[0m[0m | time: 4.020s
[2K
| RMSProp | epoch: 009 | loss: 0.58941 - acc: 0.7055 -- iter: 0096/2003
[A[ATraining Step: 508  | total loss: [1m[32m0.58826[0m[0m | time: 5.364s
[2K
| RMSProp | epoch: 009 | loss: 0.58826 - acc: 0.7131 -- iter: 0128/2003
[A[ATraining Step: 509  | total loss: [1m[32m0.59465[0m[0m | time: 6.467s
[2K
| RMSProp | epoch: 009 | loss: 0.59465 - acc: 0.7042 -- iter: 0160/2003
[A[ATraining Step: 510  | total loss: [1m[32m0.60277[0m[0m | time: 7.694s
[2K
| RMSProp | epoch: 009 | loss: 0.60277 - acc: 0.6932 -- iter: 0192/2003
[A[ATraining Step: 511  | total loss: [1m[32m0.58649[0m[0m | time: 8.481s
[2K
| RMSProp | epoch: 009 | loss: 0.58649 - acc: 0.7176 -- iter: 0224/2003
[A[ATraining Step: 512  | total loss: [1m[32m0.58202[0m[0m | time: 9.192s
[2K
| RMSProp | epoch: 009 | loss: 0.58202 - acc: 0.7195 -- iter: 0256/2003
[A[ATraining Step: 513  | total loss: [1m[32m0.56733[0m[0m | time: 10.266s
[2K
| RMSProp | epoch: 009 | loss: 0.56733 - acc: 0.7213 -- iter: 0288/2003
[A[ATraining Step: 514  | total loss: [1m[32m0.57994[0m[0m | time: 11.257s
[2K
| RMSProp | epoch: 009 | loss: 0.57994 - acc: 0.7085 -- iter: 0320/2003
[A[ATraining Step: 515  | total loss: [1m[32m0.57547[0m[0m | time: 12.560s
[2K
| RMSProp | epoch: 009 | loss: 0.57547 - acc: 0.7158 -- iter: 0352/2003
[A[ATraining Step: 516  | total loss: [1m[32m0.57205[0m[0m | time: 13.715s
[2K
| RMSProp | epoch: 009 | loss: 0.57205 - acc: 0.7223 -- iter: 0384/2003
[A[ATraining Step: 517  | total loss: [1m[32m0.57608[0m[0m | time: 14.766s
[2K
| RMSProp | epoch: 009 | loss: 0.57608 - acc: 0.7189 -- iter: 0416/2003
[A[ATraining Step: 518  | total loss: [1m[32m0.58035[0m[0m | time: 15.845s
[2K
| RMSProp | epoch: 009 | loss: 0.58035 - acc: 0.7063 -- iter: 0448/2003
[A[ATraining Step: 519  | total loss: [1m[32m0.58556[0m[0m | time: 17.036s
[2K
| RMSProp | epoch: 009 | loss: 0.58556 - acc: 0.6982 -- iter: 0480/2003
[A[ATraining Step: 520  | total loss: [1m[32m0.58792[0m[0m | time: 18.234s
[2K
| RMSProp | epoch: 009 | loss: 0.58792 - acc: 0.6971 -- iter: 0512/2003
[A[ATraining Step: 521  | total loss: [1m[32m0.57768[0m[0m | time: 19.583s
[2K
| RMSProp | epoch: 009 | loss: 0.57768 - acc: 0.7118 -- iter: 0544/2003
[A[ATraining Step: 522  | total loss: [1m[32m0.56892[0m[0m | time: 20.772s
[2K
| RMSProp | epoch: 009 | loss: 0.56892 - acc: 0.7219 -- iter: 0576/2003
[A[ATraining Step: 523  | total loss: [1m[32m0.57258[0m[0m | time: 21.728s
[2K
| RMSProp | epoch: 009 | loss: 0.57258 - acc: 0.7153 -- iter: 0608/2003
[A[ATraining Step: 524  | total loss: [1m[32m0.56182[0m[0m | time: 22.920s
[2K
| RMSProp | epoch: 009 | loss: 0.56182 - acc: 0.7250 -- iter: 0640/2003
[A[ATraining Step: 525  | total loss: [1m[32m0.54602[0m[0m | time: 24.204s
[2K
| RMSProp | epoch: 009 | loss: 0.54602 - acc: 0.7400 -- iter: 0672/2003
[A[ATraining Step: 526  | total loss: [1m[32m0.54609[0m[0m | time: 25.391s
[2K
| RMSProp | epoch: 009 | loss: 0.54609 - acc: 0.7379 -- iter: 0704/2003
[A[ATraining Step: 527  | total loss: [1m[32m0.55141[0m[0m | time: 26.443s
[2K
| RMSProp | epoch: 009 | loss: 0.55141 - acc: 0.7360 -- iter: 0736/2003
[A[ATraining Step: 528  | total loss: [1m[32m0.55935[0m[0m | time: 27.820s
[2K
| RMSProp | epoch: 009 | loss: 0.55935 - acc: 0.7280 -- iter: 0768/2003
[A[ATraining Step: 529  | total loss: [1m[32m0.55212[0m[0m | time: 29.151s
[2K
| RMSProp | epoch: 009 | loss: 0.55212 - acc: 0.7365 -- iter: 0800/2003
[A[ATraining Step: 530  | total loss: [1m[32m0.55328[0m[0m | time: 30.369s
[2K
| RMSProp | epoch: 009 | loss: 0.55328 - acc: 0.7347 -- iter: 0832/2003
[A[ATraining Step: 531  | total loss: [1m[32m0.56132[0m[0m | time: 31.511s
[2K
| RMSProp | epoch: 009 | loss: 0.56132 - acc: 0.7268 -- iter: 0864/2003
[A[ATraining Step: 532  | total loss: [1m[32m0.55045[0m[0m | time: 32.885s
[2K
| RMSProp | epoch: 009 | loss: 0.55045 - acc: 0.7354 -- iter: 0896/2003
[A[ATraining Step: 533  | total loss: [1m[32m0.54076[0m[0m | time: 34.256s
[2K
| RMSProp | epoch: 009 | loss: 0.54076 - acc: 0.7431 -- iter: 0928/2003
[A[ATraining Step: 534  | total loss: [1m[32m0.53606[0m[0m | time: 35.423s
[2K
| RMSProp | epoch: 009 | loss: 0.53606 - acc: 0.7438 -- iter: 0960/2003
[A[ATraining Step: 535  | total loss: [1m[32m0.56111[0m[0m | time: 36.599s
[2K
| RMSProp | epoch: 009 | loss: 0.56111 - acc: 0.7288 -- iter: 0992/2003
[A[ATraining Step: 536  | total loss: [1m[32m0.56975[0m[0m | time: 37.864s
[2K
| RMSProp | epoch: 009 | loss: 0.56975 - acc: 0.7184 -- iter: 1024/2003
[A[ATraining Step: 537  | total loss: [1m[32m0.56568[0m[0m | time: 39.271s
[2K
| RMSProp | epoch: 009 | loss: 0.56568 - acc: 0.7247 -- iter: 1056/2003
[A[ATraining Step: 538  | total loss: [1m[32m0.56080[0m[0m | time: 40.574s
[2K
| RMSProp | epoch: 009 | loss: 0.56080 - acc: 0.7272 -- iter: 1088/2003
[A[ATraining Step: 539  | total loss: [1m[32m0.55694[0m[0m | time: 41.915s
[2K
| RMSProp | epoch: 009 | loss: 0.55694 - acc: 0.7358 -- iter: 1120/2003
[A[ATraining Step: 540  | total loss: [1m[32m0.56420[0m[0m | time: 43.010s
[2K
| RMSProp | epoch: 009 | loss: 0.56420 - acc: 0.7247 -- iter: 1152/2003
[A[ATraining Step: 541  | total loss: [1m[32m0.57303[0m[0m | time: 44.238s
[2K
| RMSProp | epoch: 009 | loss: 0.57303 - acc: 0.7147 -- iter: 1184/2003
[A[ATraining Step: 542  | total loss: [1m[32m0.57987[0m[0m | time: 45.445s
[2K
| RMSProp | epoch: 009 | loss: 0.57987 - acc: 0.7089 -- iter: 1216/2003
[A[ATraining Step: 543  | total loss: [1m[32m0.57646[0m[0m | time: 46.653s
[2K
| RMSProp | epoch: 009 | loss: 0.57646 - acc: 0.7130 -- iter: 1248/2003
[A[ATraining Step: 544  | total loss: [1m[32m0.57701[0m[0m | time: 47.981s
[2K
| RMSProp | epoch: 009 | loss: 0.57701 - acc: 0.7073 -- iter: 1280/2003
[A[ATraining Step: 545  | total loss: [1m[32m0.57385[0m[0m | time: 49.301s
[2K
| RMSProp | epoch: 009 | loss: 0.57385 - acc: 0.7116 -- iter: 1312/2003
[A[ATraining Step: 546  | total loss: [1m[32m0.56825[0m[0m | time: 50.328s
[2K
| RMSProp | epoch: 009 | loss: 0.56825 - acc: 0.7154 -- iter: 1344/2003
[A[ATraining Step: 547  | total loss: [1m[32m0.56260[0m[0m | time: 51.532s
[2K
| RMSProp | epoch: 009 | loss: 0.56260 - acc: 0.7158 -- iter: 1376/2003
[A[ATraining Step: 548  | total loss: [1m[32m0.56070[0m[0m | time: 52.849s
[2K
| RMSProp | epoch: 009 | loss: 0.56070 - acc: 0.7129 -- iter: 1408/2003
[A[ATraining Step: 549  | total loss: [1m[32m0.56354[0m[0m | time: 54.094s
[2K
| RMSProp | epoch: 009 | loss: 0.56354 - acc: 0.7041 -- iter: 1440/2003
[A[ATraining Step: 550  | total loss: [1m[32m0.55981[0m[0m | time: 55.244s
[2K
| RMSProp | epoch: 009 | loss: 0.55981 - acc: 0.7087 -- iter: 1472/2003
[A[ATraining Step: 551  | total loss: [1m[32m0.55439[0m[0m | time: 56.695s
[2K
| RMSProp | epoch: 009 | loss: 0.55439 - acc: 0.7160 -- iter: 1504/2003
[A[ATraining Step: 552  | total loss: [1m[32m0.55711[0m[0m | time: 58.106s
[2K
| RMSProp | epoch: 009 | loss: 0.55711 - acc: 0.7163 -- iter: 1536/2003
[A[ATraining Step: 553  | total loss: [1m[32m0.54518[0m[0m | time: 59.393s
[2K
| RMSProp | epoch: 009 | loss: 0.54518 - acc: 0.7290 -- iter: 1568/2003
[A[ATraining Step: 554  | total loss: [1m[32m0.53657[0m[0m | time: 60.562s
[2K
| RMSProp | epoch: 009 | loss: 0.53657 - acc: 0.7374 -- iter: 1600/2003
[A[ATraining Step: 555  | total loss: [1m[32m0.52743[0m[0m | time: 61.727s
[2K
| RMSProp | epoch: 009 | loss: 0.52743 - acc: 0.7417 -- iter: 1632/2003
[A[ATraining Step: 556  | total loss: [1m[32m0.53380[0m[0m | time: 63.286s
[2K
| RMSProp | epoch: 009 | loss: 0.53380 - acc: 0.7301 -- iter: 1664/2003
[A[ATraining Step: 557  | total loss: [1m[32m0.53367[0m[0m | time: 64.811s
[2K
| RMSProp | epoch: 009 | loss: 0.53367 - acc: 0.7289 -- iter: 1696/2003
[A[ATraining Step: 558  | total loss: [1m[32m0.51902[0m[0m | time: 66.124s
[2K
| RMSProp | epoch: 009 | loss: 0.51902 - acc: 0.7404 -- iter: 1728/2003
[A[ATraining Step: 559  | total loss: [1m[32m0.51049[0m[0m | time: 67.444s
[2K
| RMSProp | epoch: 009 | loss: 0.51049 - acc: 0.7539 -- iter: 1760/2003
[A[ATraining Step: 560  | total loss: [1m[32m0.50413[0m[0m | time: 68.591s
[2K
| RMSProp | epoch: 009 | loss: 0.50413 - acc: 0.7566 -- iter: 1792/2003
[A[ATraining Step: 561  | total loss: [1m[32m0.49628[0m[0m | time: 69.864s
[2K
| RMSProp | epoch: 009 | loss: 0.49628 - acc: 0.7653 -- iter: 1824/2003
[A[ATraining Step: 562  | total loss: [1m[32m0.48709[0m[0m | time: 71.263s
[2K
| RMSProp | epoch: 009 | loss: 0.48709 - acc: 0.7732 -- iter: 1856/2003
[A[ATraining Step: 563  | total loss: [1m[32m0.50757[0m[0m | time: 72.517s
[2K
| RMSProp | epoch: 009 | loss: 0.50757 - acc: 0.7677 -- iter: 1888/2003
[A[ATraining Step: 564  | total loss: [1m[32m0.51116[0m[0m | time: 73.842s
[2K
| RMSProp | epoch: 009 | loss: 0.51116 - acc: 0.7628 -- iter: 1920/2003
[A[ATraining Step: 565  | total loss: [1m[32m0.50236[0m[0m | time: 75.110s
[2K
| RMSProp | epoch: 009 | loss: 0.50236 - acc: 0.7647 -- iter: 1952/2003
[A[ATraining Step: 566  | total loss: [1m[32m0.48106[0m[0m | time: 76.371s
[2K
| RMSProp | epoch: 009 | loss: 0.48106 - acc: 0.7788 -- iter: 1984/2003
[A[ATraining Step: 567  | total loss: [1m[32m0.47441[0m[0m | time: 82.296s
[2K
| RMSProp | epoch: 009 | loss: 0.47441 - acc: 0.7759 | val_loss: 0.53126 - val_acc: 0.7460 -- iter: 2003/2003
--
Training Step: 568  | total loss: [1m[32m0.48385[0m[0m | time: 1.183s
[2K
| RMSProp | epoch: 010 | loss: 0.48385 - acc: 0.7640 -- iter: 0032/2003
[A[ATraining Step: 569  | total loss: [1m[32m0.47928[0m[0m | time: 2.514s
[2K
| RMSProp | epoch: 010 | loss: 0.47928 - acc: 0.7657 -- iter: 0064/2003
[A[ATraining Step: 570  | total loss: [1m[32m0.49154[0m[0m | time: 3.909s
[2K
| RMSProp | epoch: 010 | loss: 0.49154 - acc: 0.7610 -- iter: 0096/2003
[A[ATraining Step: 571  | total loss: [1m[32m0.49418[0m[0m | time: 5.002s
[2K
| RMSProp | epoch: 010 | loss: 0.49418 - acc: 0.7505 -- iter: 0128/2003
[A[ATraining Step: 572  | total loss: [1m[32m0.49179[0m[0m | time: 6.084s
[2K
| RMSProp | epoch: 010 | loss: 0.49179 - acc: 0.7567 -- iter: 0160/2003
[A[ATraining Step: 573  | total loss: [1m[32m0.47531[0m[0m | time: 7.453s
[2K
| RMSProp | epoch: 010 | loss: 0.47531 - acc: 0.7717 -- iter: 0192/2003
[A[ATraining Step: 574  | total loss: [1m[32m0.47329[0m[0m | time: 8.800s
[2K
| RMSProp | epoch: 010 | loss: 0.47329 - acc: 0.7695 -- iter: 0224/2003
[A[ATraining Step: 575  | total loss: [1m[32m0.48139[0m[0m | time: 9.608s
[2K
| RMSProp | epoch: 010 | loss: 0.48139 - acc: 0.7613 -- iter: 0256/2003
[A[ATraining Step: 576  | total loss: [1m[32m0.48133[0m[0m | time: 10.312s
[2K
| RMSProp | epoch: 010 | loss: 0.48133 - acc: 0.7589 -- iter: 0288/2003
[A[ATraining Step: 577  | total loss: [1m[32m0.46092[0m[0m | time: 11.587s
[2K
| RMSProp | epoch: 010 | loss: 0.46092 - acc: 0.7777 -- iter: 0320/2003
[A[ATraining Step: 578  | total loss: [1m[32m0.46929[0m[0m | time: 12.825s
[2K
| RMSProp | epoch: 010 | loss: 0.46929 - acc: 0.7749 -- iter: 0352/2003
[A[ATraining Step: 579  | total loss: [1m[32m0.49784[0m[0m | time: 14.131s
[2K
| RMSProp | epoch: 010 | loss: 0.49784 - acc: 0.7662 -- iter: 0384/2003
[A[ATraining Step: 580  | total loss: [1m[32m0.52023[0m[0m | time: 15.392s
[2K
| RMSProp | epoch: 010 | loss: 0.52023 - acc: 0.7552 -- iter: 0416/2003
[A[ATraining Step: 581  | total loss: [1m[32m0.51810[0m[0m | time: 16.704s
[2K
| RMSProp | epoch: 010 | loss: 0.51810 - acc: 0.7578 -- iter: 0448/2003
[A[ATraining Step: 582  | total loss: [1m[32m0.51620[0m[0m | time: 17.937s
[2K
| RMSProp | epoch: 010 | loss: 0.51620 - acc: 0.7570 -- iter: 0480/2003
[A[ATraining Step: 583  | total loss: [1m[32m0.50540[0m[0m | time: 18.989s
[2K
| RMSProp | epoch: 010 | loss: 0.50540 - acc: 0.7657 -- iter: 0512/2003
[A[ATraining Step: 584  | total loss: [1m[32m0.50092[0m[0m | time: 19.949s
[2K
| RMSProp | epoch: 010 | loss: 0.50092 - acc: 0.7704 -- iter: 0544/2003
[A[ATraining Step: 585  | total loss: [1m[32m0.49289[0m[0m | time: 20.804s
[2K
| RMSProp | epoch: 010 | loss: 0.49289 - acc: 0.7715 -- iter: 0576/2003
[A[ATraining Step: 586  | total loss: [1m[32m0.49901[0m[0m | time: 21.952s
[2K
| RMSProp | epoch: 010 | loss: 0.49901 - acc: 0.7599 -- iter: 0608/2003
[A[ATraining Step: 587  | total loss: [1m[32m0.48873[0m[0m | time: 23.033s
[2K
| RMSProp | epoch: 010 | loss: 0.48873 - acc: 0.7746 -- iter: 0640/2003
[A[ATraining Step: 588  | total loss: [1m[32m0.48574[0m[0m | time: 23.854s
[2K
| RMSProp | epoch: 010 | loss: 0.48574 - acc: 0.7690 -- iter: 0672/2003
[A[ATraining Step: 589  | total loss: [1m[32m0.48184[0m[0m | time: 24.829s
[2K
| RMSProp | epoch: 010 | loss: 0.48184 - acc: 0.7671 -- iter: 0704/2003
[A[ATraining Step: 590  | total loss: [1m[32m0.47261[0m[0m | time: 25.830s
[2K
| RMSProp | epoch: 010 | loss: 0.47261 - acc: 0.7748 -- iter: 0736/2003
[A[ATraining Step: 591  | total loss: [1m[32m0.45592[0m[0m | time: 26.831s
[2K
| RMSProp | epoch: 010 | loss: 0.45592 - acc: 0.7910 -- iter: 0768/2003
[A[ATraining Step: 592  | total loss: [1m[32m0.45863[0m[0m | time: 27.769s
[2K
| RMSProp | epoch: 010 | loss: 0.45863 - acc: 0.7901 -- iter: 0800/2003
[A[ATraining Step: 593  | total loss: [1m[32m0.46577[0m[0m | time: 28.739s
[2K
| RMSProp | epoch: 010 | loss: 0.46577 - acc: 0.7860 -- iter: 0832/2003
[A[ATraining Step: 594  | total loss: [1m[32m0.47251[0m[0m | time: 29.710s
[2K
| RMSProp | epoch: 010 | loss: 0.47251 - acc: 0.7793 -- iter: 0864/2003
[A[ATraining Step: 595  | total loss: [1m[32m0.47590[0m[0m | time: 30.525s
[2K
| RMSProp | epoch: 010 | loss: 0.47590 - acc: 0.7795 -- iter: 0896/2003
[A[ATraining Step: 596  | total loss: [1m[32m0.46249[0m[0m | time: 31.365s
[2K
| RMSProp | epoch: 010 | loss: 0.46249 - acc: 0.7891 -- iter: 0928/2003
[A[ATraining Step: 597  | total loss: [1m[32m0.45383[0m[0m | time: 32.490s
[2K
| RMSProp | epoch: 010 | loss: 0.45383 - acc: 0.7914 -- iter: 0960/2003
[A[ATraining Step: 598  | total loss: [1m[32m0.48486[0m[0m | time: 33.550s
[2K
| RMSProp | epoch: 010 | loss: 0.48486 - acc: 0.7779 -- iter: 0992/2003
[A[ATraining Step: 599  | total loss: [1m[32m0.48373[0m[0m | time: 34.309s
[2K
| RMSProp | epoch: 010 | loss: 0.48373 - acc: 0.7751 -- iter: 1024/2003
[A[ATraining Step: 600  | total loss: [1m[32m0.48838[0m[0m | time: 38.445s
[2K
| RMSProp | epoch: 010 | loss: 0.48838 - acc: 0.7663 | val_loss: 0.53495 - val_acc: 0.7252 -- iter: 1056/2003
--
Training Step: 601  | total loss: [1m[32m0.48750[0m[0m | time: 39.469s
[2K
| RMSProp | epoch: 010 | loss: 0.48750 - acc: 0.7710 -- iter: 1088/2003
[A[ATraining Step: 602  | total loss: [1m[32m0.50283[0m[0m | time: 40.456s
[2K
| RMSProp | epoch: 010 | loss: 0.50283 - acc: 0.7564 -- iter: 1120/2003
[A[ATraining Step: 603  | total loss: [1m[32m0.51186[0m[0m | time: 41.429s
[2K
| RMSProp | epoch: 010 | loss: 0.51186 - acc: 0.7495 -- iter: 1152/2003
[A[ATraining Step: 604  | total loss: [1m[32m0.50002[0m[0m | time: 42.278s
[2K
| RMSProp | epoch: 010 | loss: 0.50002 - acc: 0.7589 -- iter: 1184/2003
[A[ATraining Step: 605  | total loss: [1m[32m0.50100[0m[0m | time: 43.320s
[2K
| RMSProp | epoch: 010 | loss: 0.50100 - acc: 0.7549 -- iter: 1216/2003
[A[ATraining Step: 606  | total loss: [1m[32m0.49217[0m[0m | time: 44.370s
[2K
| RMSProp | epoch: 010 | loss: 0.49217 - acc: 0.7606 -- iter: 1248/2003
[A[ATraining Step: 607  | total loss: [1m[32m0.48780[0m[0m | time: 45.165s
[2K
| RMSProp | epoch: 010 | loss: 0.48780 - acc: 0.7627 -- iter: 1280/2003
[A[ATraining Step: 608  | total loss: [1m[32m0.47321[0m[0m | time: 46.011s
[2K
| RMSProp | epoch: 010 | loss: 0.47321 - acc: 0.7739 -- iter: 1312/2003
[A[ATraining Step: 609  | total loss: [1m[32m0.46171[0m[0m | time: 46.964s
[2K
| RMSProp | epoch: 010 | loss: 0.46171 - acc: 0.7778 -- iter: 1344/2003
[A[ATraining Step: 610  | total loss: [1m[32m0.45004[0m[0m | time: 47.952s
[2K
| RMSProp | epoch: 010 | loss: 0.45004 - acc: 0.7844 -- iter: 1376/2003
[A[ATraining Step: 611  | total loss: [1m[32m0.44214[0m[0m | time: 48.797s
[2K
| RMSProp | epoch: 010 | loss: 0.44214 - acc: 0.7872 -- iter: 1408/2003
[A[ATraining Step: 612  | total loss: [1m[32m0.46513[0m[0m | time: 49.725s
[2K
| RMSProp | epoch: 010 | loss: 0.46513 - acc: 0.7741 -- iter: 1440/2003
[A[ATraining Step: 613  | total loss: [1m[32m0.46444[0m[0m | time: 50.675s
[2K
| RMSProp | epoch: 010 | loss: 0.46444 - acc: 0.7717 -- iter: 1472/2003
[A[ATraining Step: 614  | total loss: [1m[32m0.44377[0m[0m | time: 51.610s
[2K
| RMSProp | epoch: 010 | loss: 0.44377 - acc: 0.7883 -- iter: 1504/2003
[A[ATraining Step: 615  | total loss: [1m[32m0.42389[0m[0m | time: 52.745s
[2K
| RMSProp | epoch: 010 | loss: 0.42389 - acc: 0.8032 -- iter: 1536/2003
[A[ATraining Step: 616  | total loss: [1m[32m0.43690[0m[0m | time: 53.968s
[2K
| RMSProp | epoch: 010 | loss: 0.43690 - acc: 0.7979 -- iter: 1568/2003
[A[ATraining Step: 617  | total loss: [1m[32m0.42772[0m[0m | time: 55.371s
[2K
| RMSProp | epoch: 010 | loss: 0.42772 - acc: 0.8025 -- iter: 1600/2003
[A[ATraining Step: 618  | total loss: [1m[32m0.43079[0m[0m | time: 56.747s
[2K
| RMSProp | epoch: 010 | loss: 0.43079 - acc: 0.7941 -- iter: 1632/2003
[A[ATraining Step: 619  | total loss: [1m[32m0.42628[0m[0m | time: 57.882s
[2K
| RMSProp | epoch: 010 | loss: 0.42628 - acc: 0.8022 -- iter: 1664/2003
[A[ATraining Step: 620  | total loss: [1m[32m0.43673[0m[0m | time: 59.169s
[2K
| RMSProp | epoch: 010 | loss: 0.43673 - acc: 0.7938 -- iter: 1696/2003
[A[ATraining Step: 621  | total loss: [1m[32m0.42581[0m[0m | time: 60.505s
[2K
| RMSProp | epoch: 010 | loss: 0.42581 - acc: 0.8020 -- iter: 1728/2003
[A[ATraining Step: 622  | total loss: [1m[32m0.42585[0m[0m | time: 61.898s
[2K
| RMSProp | epoch: 010 | loss: 0.42585 - acc: 0.7999 -- iter: 1760/2003
[A[ATraining Step: 623  | total loss: [1m[32m0.41858[0m[0m | time: 63.144s
[2K
| RMSProp | epoch: 010 | loss: 0.41858 - acc: 0.8074 -- iter: 1792/2003
[A[ATraining Step: 624  | total loss: [1m[32m0.40663[0m[0m | time: 64.616s
[2K
| RMSProp | epoch: 010 | loss: 0.40663 - acc: 0.8110 -- iter: 1824/2003
[A[ATraining Step: 625  | total loss: [1m[32m0.40377[0m[0m | time: 65.866s
[2K
| RMSProp | epoch: 010 | loss: 0.40377 - acc: 0.8112 -- iter: 1856/2003
[A[ATraining Step: 626  | total loss: [1m[32m0.42928[0m[0m | time: 66.987s
[2K
| RMSProp | epoch: 010 | loss: 0.42928 - acc: 0.8019 -- iter: 1888/2003
[A[ATraining Step: 627  | total loss: [1m[32m0.45705[0m[0m | time: 68.337s
[2K
| RMSProp | epoch: 010 | loss: 0.45705 - acc: 0.7842 -- iter: 1920/2003
[A[ATraining Step: 628  | total loss: [1m[32m0.46272[0m[0m | time: 69.785s
[2K
| RMSProp | epoch: 010 | loss: 0.46272 - acc: 0.7808 -- iter: 1952/2003
[A[ATraining Step: 629  | total loss: [1m[32m0.45048[0m[0m | time: 71.119s
[2K
| RMSProp | epoch: 010 | loss: 0.45048 - acc: 0.7902 -- iter: 1984/2003
[A[ATraining Step: 630  | total loss: [1m[32m0.44152[0m[0m | time: 76.756s
[2K
| RMSProp | epoch: 010 | loss: 0.44152 - acc: 0.7987 | val_loss: 0.45618 - val_acc: 0.7907 -- iter: 2003/2003
--
Training Step: 631  | total loss: [1m[32m0.45954[0m[0m | time: 1.213s
[2K
| RMSProp | epoch: 011 | loss: 0.45954 - acc: 0.8001 -- iter: 0032/2003
[A[ATraining Step: 632  | total loss: [1m[32m0.45495[0m[0m | time: 2.689s
[2K
| RMSProp | epoch: 011 | loss: 0.45495 - acc: 0.8013 -- iter: 0064/2003
[A[ATraining Step: 633  | total loss: [1m[32m0.44590[0m[0m | time: 4.160s
[2K
| RMSProp | epoch: 011 | loss: 0.44590 - acc: 0.8118 -- iter: 0096/2003
[A[ATraining Step: 634  | total loss: [1m[32m0.44324[0m[0m | time: 5.349s
[2K
| RMSProp | epoch: 011 | loss: 0.44324 - acc: 0.8119 -- iter: 0128/2003
[A[ATraining Step: 635  | total loss: [1m[32m0.44064[0m[0m | time: 6.874s
[2K
| RMSProp | epoch: 011 | loss: 0.44064 - acc: 0.8151 -- iter: 0160/2003
[A[ATraining Step: 636  | total loss: [1m[32m0.44688[0m[0m | time: 8.249s
[2K
| RMSProp | epoch: 011 | loss: 0.44688 - acc: 0.8054 -- iter: 0192/2003
[A[ATraining Step: 637  | total loss: [1m[32m0.44839[0m[0m | time: 9.658s
[2K
| RMSProp | epoch: 011 | loss: 0.44839 - acc: 0.7999 -- iter: 0224/2003
[A[ATraining Step: 638  | total loss: [1m[32m0.43871[0m[0m | time: 10.897s
[2K
| RMSProp | epoch: 011 | loss: 0.43871 - acc: 0.8074 -- iter: 0256/2003
[A[ATraining Step: 639  | total loss: [1m[32m0.42505[0m[0m | time: 11.705s
[2K
| RMSProp | epoch: 011 | loss: 0.42505 - acc: 0.8110 -- iter: 0288/2003
[A[ATraining Step: 640  | total loss: [1m[32m0.44240[0m[0m | time: 12.492s
[2K
| RMSProp | epoch: 011 | loss: 0.44240 - acc: 0.8036 -- iter: 0320/2003
[A[ATraining Step: 641  | total loss: [1m[32m0.43158[0m[0m | time: 13.820s
[2K
| RMSProp | epoch: 011 | loss: 0.43158 - acc: 0.8075 -- iter: 0352/2003
[A[ATraining Step: 642  | total loss: [1m[32m0.41581[0m[0m | time: 15.256s
[2K
| RMSProp | epoch: 011 | loss: 0.41581 - acc: 0.8174 -- iter: 0384/2003
[A[ATraining Step: 643  | total loss: [1m[32m0.41552[0m[0m | time: 16.580s
[2K
| RMSProp | epoch: 011 | loss: 0.41552 - acc: 0.8169 -- iter: 0416/2003
[A[ATraining Step: 644  | total loss: [1m[32m0.41922[0m[0m | time: 17.962s
[2K
| RMSProp | epoch: 011 | loss: 0.41922 - acc: 0.8133 -- iter: 0448/2003
[A[ATraining Step: 645  | total loss: [1m[32m0.41556[0m[0m | time: 19.365s
[2K
| RMSProp | epoch: 011 | loss: 0.41556 - acc: 0.8132 -- iter: 0480/2003
[A[ATraining Step: 646  | total loss: [1m[32m0.41000[0m[0m | time: 20.435s
[2K
| RMSProp | epoch: 011 | loss: 0.41000 - acc: 0.8163 -- iter: 0512/2003
[A[ATraining Step: 647  | total loss: [1m[32m0.41619[0m[0m | time: 21.835s
[2K
| RMSProp | epoch: 011 | loss: 0.41619 - acc: 0.8128 -- iter: 0544/2003
[A[ATraining Step: 648  | total loss: [1m[32m0.41805[0m[0m | time: 23.108s
[2K
| RMSProp | epoch: 011 | loss: 0.41805 - acc: 0.8127 -- iter: 0576/2003
[A[ATraining Step: 649  | total loss: [1m[32m0.40605[0m[0m | time: 24.522s
[2K
| RMSProp | epoch: 011 | loss: 0.40605 - acc: 0.8190 -- iter: 0608/2003
[A[ATraining Step: 650  | total loss: [1m[32m0.40782[0m[0m | time: 25.719s
[2K
| RMSProp | epoch: 011 | loss: 0.40782 - acc: 0.8152 -- iter: 0640/2003
[A[ATraining Step: 651  | total loss: [1m[32m0.40535[0m[0m | time: 27.011s
[2K
| RMSProp | epoch: 011 | loss: 0.40535 - acc: 0.8149 -- iter: 0672/2003
[A[ATraining Step: 652  | total loss: [1m[32m0.40047[0m[0m | time: 28.315s
[2K
| RMSProp | epoch: 011 | loss: 0.40047 - acc: 0.8209 -- iter: 0704/2003
[A[ATraining Step: 653  | total loss: [1m[32m0.47466[0m[0m | time: 29.671s
[2K
| RMSProp | epoch: 011 | loss: 0.47466 - acc: 0.7951 -- iter: 0736/2003
[A[ATraining Step: 654  | total loss: [1m[32m0.46063[0m[0m | time: 30.983s
[2K
| RMSProp | epoch: 011 | loss: 0.46063 - acc: 0.8031 -- iter: 0768/2003
[A[ATraining Step: 655  | total loss: [1m[32m0.46455[0m[0m | time: 32.365s
[2K
| RMSProp | epoch: 011 | loss: 0.46455 - acc: 0.7915 -- iter: 0800/2003
[A[ATraining Step: 656  | total loss: [1m[32m0.45310[0m[0m | time: 33.774s
[2K
| RMSProp | epoch: 011 | loss: 0.45310 - acc: 0.7967 -- iter: 0832/2003
[A[ATraining Step: 657  | total loss: [1m[32m0.44523[0m[0m | time: 35.047s
[2K
| RMSProp | epoch: 011 | loss: 0.44523 - acc: 0.7983 -- iter: 0864/2003
[A[ATraining Step: 658  | total loss: [1m[32m0.44086[0m[0m | time: 36.429s
[2K
| RMSProp | epoch: 011 | loss: 0.44086 - acc: 0.8091 -- iter: 0896/2003
[A[ATraining Step: 659  | total loss: [1m[32m0.42815[0m[0m | time: 37.770s
[2K
| RMSProp | epoch: 011 | loss: 0.42815 - acc: 0.8126 -- iter: 0928/2003
[A[ATraining Step: 660  | total loss: [1m[32m0.41459[0m[0m | time: 39.219s
[2K
| RMSProp | epoch: 011 | loss: 0.41459 - acc: 0.8219 -- iter: 0960/2003
[A[ATraining Step: 661  | total loss: [1m[32m0.41449[0m[0m | time: 40.632s
[2K
| RMSProp | epoch: 011 | loss: 0.41449 - acc: 0.8210 -- iter: 0992/2003
[A[ATraining Step: 662  | total loss: [1m[32m0.41993[0m[0m | time: 41.800s
[2K
| RMSProp | epoch: 011 | loss: 0.41993 - acc: 0.8202 -- iter: 1024/2003
[A[ATraining Step: 663  | total loss: [1m[32m0.41508[0m[0m | time: 43.000s
[2K
| RMSProp | epoch: 011 | loss: 0.41508 - acc: 0.8225 -- iter: 1056/2003
[A[ATraining Step: 664  | total loss: [1m[32m0.42248[0m[0m | time: 44.249s
[2K
| RMSProp | epoch: 011 | loss: 0.42248 - acc: 0.8215 -- iter: 1088/2003
[A[ATraining Step: 665  | total loss: [1m[32m0.42945[0m[0m | time: 45.689s
[2K
| RMSProp | epoch: 011 | loss: 0.42945 - acc: 0.8144 -- iter: 1120/2003
[A[ATraining Step: 666  | total loss: [1m[32m0.42970[0m[0m | time: 46.981s
[2K
| RMSProp | epoch: 011 | loss: 0.42970 - acc: 0.8048 -- iter: 1152/2003
[A[ATraining Step: 667  | total loss: [1m[32m0.43829[0m[0m | time: 48.396s
[2K
| RMSProp | epoch: 011 | loss: 0.43829 - acc: 0.8056 -- iter: 1184/2003
[A[ATraining Step: 668  | total loss: [1m[32m0.45083[0m[0m | time: 49.877s
[2K
| RMSProp | epoch: 011 | loss: 0.45083 - acc: 0.8000 -- iter: 1216/2003
[A[ATraining Step: 669  | total loss: [1m[32m0.43637[0m[0m | time: 51.043s
[2K
| RMSProp | epoch: 011 | loss: 0.43637 - acc: 0.8075 -- iter: 1248/2003
[A[ATraining Step: 670  | total loss: [1m[32m0.42818[0m[0m | time: 52.311s
[2K
| RMSProp | epoch: 011 | loss: 0.42818 - acc: 0.8143 -- iter: 1280/2003
[A[ATraining Step: 671  | total loss: [1m[32m0.43040[0m[0m | time: 53.660s
[2K
| RMSProp | epoch: 011 | loss: 0.43040 - acc: 0.8016 -- iter: 1312/2003
[A[ATraining Step: 672  | total loss: [1m[32m0.44500[0m[0m | time: 55.200s
[2K
| RMSProp | epoch: 011 | loss: 0.44500 - acc: 0.7933 -- iter: 1344/2003
[A[ATraining Step: 673  | total loss: [1m[32m0.44869[0m[0m | time: 56.430s
[2K
| RMSProp | epoch: 011 | loss: 0.44869 - acc: 0.7890 -- iter: 1376/2003
[A[ATraining Step: 674  | total loss: [1m[32m0.43576[0m[0m | time: 57.605s
[2K
| RMSProp | epoch: 011 | loss: 0.43576 - acc: 0.8007 -- iter: 1408/2003
[A[ATraining Step: 675  | total loss: [1m[32m0.44084[0m[0m | time: 58.982s
[2K
| RMSProp | epoch: 011 | loss: 0.44084 - acc: 0.7863 -- iter: 1440/2003
[A[ATraining Step: 676  | total loss: [1m[32m0.43135[0m[0m | time: 60.250s
[2K
| RMSProp | epoch: 011 | loss: 0.43135 - acc: 0.7951 -- iter: 1472/2003
[A[ATraining Step: 677  | total loss: [1m[32m0.40832[0m[0m | time: 61.486s
[2K
| RMSProp | epoch: 011 | loss: 0.40832 - acc: 0.8094 -- iter: 1504/2003
[A[ATraining Step: 678  | total loss: [1m[32m0.40232[0m[0m | time: 62.949s
[2K
| RMSProp | epoch: 011 | loss: 0.40232 - acc: 0.8097 -- iter: 1536/2003
[A[ATraining Step: 679  | total loss: [1m[32m0.38966[0m[0m | time: 64.278s
[2K
| RMSProp | epoch: 011 | loss: 0.38966 - acc: 0.8131 -- iter: 1568/2003
[A[ATraining Step: 680  | total loss: [1m[32m0.36987[0m[0m | time: 65.676s
[2K
| RMSProp | epoch: 011 | loss: 0.36987 - acc: 0.8224 -- iter: 1600/2003
[A[ATraining Step: 681  | total loss: [1m[32m0.36029[0m[0m | time: 66.817s
[2K
| RMSProp | epoch: 011 | loss: 0.36029 - acc: 0.8277 -- iter: 1632/2003
[A[ATraining Step: 682  | total loss: [1m[32m0.37026[0m[0m | time: 68.244s
[2K
| RMSProp | epoch: 011 | loss: 0.37026 - acc: 0.8199 -- iter: 1664/2003
[A[ATraining Step: 683  | total loss: [1m[32m0.37900[0m[0m | time: 69.683s
[2K
| RMSProp | epoch: 011 | loss: 0.37900 - acc: 0.8160 -- iter: 1696/2003
[A[ATraining Step: 684  | total loss: [1m[32m0.37723[0m[0m | time: 71.192s
[2K
| RMSProp | epoch: 011 | loss: 0.37723 - acc: 0.8157 -- iter: 1728/2003
[A[ATraining Step: 685  | total loss: [1m[32m0.37415[0m[0m | time: 72.280s
[2K
| RMSProp | epoch: 011 | loss: 0.37415 - acc: 0.8185 -- iter: 1760/2003
[A[ATraining Step: 686  | total loss: [1m[32m0.36760[0m[0m | time: 73.495s
[2K
| RMSProp | epoch: 011 | loss: 0.36760 - acc: 0.8241 -- iter: 1792/2003
[A[ATraining Step: 687  | total loss: [1m[32m0.38105[0m[0m | time: 74.839s
[2K
| RMSProp | epoch: 011 | loss: 0.38105 - acc: 0.8230 -- iter: 1824/2003
[A[ATraining Step: 688  | total loss: [1m[32m0.41015[0m[0m | time: 76.113s
[2K
| RMSProp | epoch: 011 | loss: 0.41015 - acc: 0.8094 -- iter: 1856/2003
[A[ATraining Step: 689  | total loss: [1m[32m0.40929[0m[0m | time: 77.490s
[2K
| RMSProp | epoch: 011 | loss: 0.40929 - acc: 0.8097 -- iter: 1888/2003
[A[ATraining Step: 690  | total loss: [1m[32m0.41210[0m[0m | time: 78.818s
[2K
| RMSProp | epoch: 011 | loss: 0.41210 - acc: 0.8100 -- iter: 1920/2003
[A[ATraining Step: 691  | total loss: [1m[32m0.40021[0m[0m | time: 80.284s
[2K
| RMSProp | epoch: 011 | loss: 0.40021 - acc: 0.8165 -- iter: 1952/2003
[A[ATraining Step: 692  | total loss: [1m[32m0.40675[0m[0m | time: 81.543s
[2K
| RMSProp | epoch: 011 | loss: 0.40675 - acc: 0.8192 -- iter: 1984/2003
[A[ATraining Step: 693  | total loss: [1m[32m0.40436[0m[0m | time: 87.700s
[2K
| RMSProp | epoch: 011 | loss: 0.40436 - acc: 0.8123 | val_loss: 0.44040 - val_acc: 0.7971 -- iter: 2003/2003
--
Training Step: 694  | total loss: [1m[32m0.40507[0m[0m | time: 1.459s
[2K
| RMSProp | epoch: 012 | loss: 0.40507 - acc: 0.8061 -- iter: 0032/2003
[A[ATraining Step: 695  | total loss: [1m[32m0.40024[0m[0m | time: 2.820s
[2K
| RMSProp | epoch: 012 | loss: 0.40024 - acc: 0.8098 -- iter: 0064/2003
[A[ATraining Step: 696  | total loss: [1m[32m0.39155[0m[0m | time: 4.114s
[2K
| RMSProp | epoch: 012 | loss: 0.39155 - acc: 0.8164 -- iter: 0096/2003
[A[ATraining Step: 697  | total loss: [1m[32m0.39277[0m[0m | time: 5.507s
[2K
| RMSProp | epoch: 012 | loss: 0.39277 - acc: 0.8129 -- iter: 0128/2003
[A[ATraining Step: 698  | total loss: [1m[32m0.38952[0m[0m | time: 6.969s
[2K
| RMSProp | epoch: 012 | loss: 0.38952 - acc: 0.8191 -- iter: 0160/2003
[A[ATraining Step: 699  | total loss: [1m[32m0.38556[0m[0m | time: 8.279s
[2K
| RMSProp | epoch: 012 | loss: 0.38556 - acc: 0.8184 -- iter: 0192/2003
[A[ATraining Step: 700  | total loss: [1m[32m0.38485[0m[0m | time: 9.609s
[2K
| RMSProp | epoch: 012 | loss: 0.38485 - acc: 0.8147 -- iter: 0224/2003
[A[ATraining Step: 701  | total loss: [1m[32m0.39820[0m[0m | time: 10.878s
[2K
| RMSProp | epoch: 012 | loss: 0.39820 - acc: 0.8113 -- iter: 0256/2003
[A[ATraining Step: 702  | total loss: [1m[32m0.38141[0m[0m | time: 12.281s
[2K
| RMSProp | epoch: 012 | loss: 0.38141 - acc: 0.8240 -- iter: 0288/2003
[A[ATraining Step: 703  | total loss: [1m[32m0.37879[0m[0m | time: 13.036s
[2K
| RMSProp | epoch: 012 | loss: 0.37879 - acc: 0.8291 -- iter: 0320/2003
[A[ATraining Step: 704  | total loss: [1m[32m0.35231[0m[0m | time: 13.554s
[2K
| RMSProp | epoch: 012 | loss: 0.35231 - acc: 0.8462 -- iter: 0352/2003
[A[ATraining Step: 705  | total loss: [1m[32m0.32169[0m[0m | time: 14.516s
[2K
| RMSProp | epoch: 012 | loss: 0.32169 - acc: 0.8615 -- iter: 0384/2003
[A[ATraining Step: 706  | total loss: [1m[32m0.32878[0m[0m | time: 15.560s
[2K
| RMSProp | epoch: 012 | loss: 0.32878 - acc: 0.8629 -- iter: 0416/2003
[A[ATraining Step: 707  | total loss: [1m[32m0.34384[0m[0m | time: 16.520s
[2K
| RMSProp | epoch: 012 | loss: 0.34384 - acc: 0.8516 -- iter: 0448/2003
[A[ATraining Step: 708  | total loss: [1m[32m0.38001[0m[0m | time: 17.514s
[2K
| RMSProp | epoch: 012 | loss: 0.38001 - acc: 0.8289 -- iter: 0480/2003
[A[ATraining Step: 709  | total loss: [1m[32m0.39175[0m[0m | time: 18.582s
[2K
| RMSProp | epoch: 012 | loss: 0.39175 - acc: 0.8179 -- iter: 0512/2003
[A[ATraining Step: 710  | total loss: [1m[32m0.40011[0m[0m | time: 19.587s
[2K
| RMSProp | epoch: 012 | loss: 0.40011 - acc: 0.8080 -- iter: 0544/2003
[A[ATraining Step: 711  | total loss: [1m[32m0.39473[0m[0m | time: 20.456s
[2K
| RMSProp | epoch: 012 | loss: 0.39473 - acc: 0.8116 -- iter: 0576/2003
[A[ATraining Step: 712  | total loss: [1m[32m0.39533[0m[0m | time: 21.281s
[2K
| RMSProp | epoch: 012 | loss: 0.39533 - acc: 0.8148 -- iter: 0608/2003
[A[ATraining Step: 713  | total loss: [1m[32m0.39355[0m[0m | time: 22.066s
[2K
| RMSProp | epoch: 012 | loss: 0.39355 - acc: 0.8177 -- iter: 0640/2003
[A[ATraining Step: 714  | total loss: [1m[32m0.37854[0m[0m | time: 22.979s
[2K
| RMSProp | epoch: 012 | loss: 0.37854 - acc: 0.8297 -- iter: 0672/2003
[A[ATraining Step: 715  | total loss: [1m[32m0.38075[0m[0m | time: 23.911s
[2K
| RMSProp | epoch: 012 | loss: 0.38075 - acc: 0.8311 -- iter: 0704/2003
[A[ATraining Step: 716  | total loss: [1m[32m0.37601[0m[0m | time: 24.746s
[2K
| RMSProp | epoch: 012 | loss: 0.37601 - acc: 0.8323 -- iter: 0736/2003
[A[ATraining Step: 717  | total loss: [1m[32m0.37033[0m[0m | time: 25.625s
[2K
| RMSProp | epoch: 012 | loss: 0.37033 - acc: 0.8397 -- iter: 0768/2003
[A[ATraining Step: 718  | total loss: [1m[32m0.35935[0m[0m | time: 26.539s
[2K
| RMSProp | epoch: 012 | loss: 0.35935 - acc: 0.8495 -- iter: 0800/2003
[A[ATraining Step: 719  | total loss: [1m[32m0.35749[0m[0m | time: 27.607s
[2K
| RMSProp | epoch: 012 | loss: 0.35749 - acc: 0.8552 -- iter: 0832/2003
[A[ATraining Step: 720  | total loss: [1m[32m0.36911[0m[0m | time: 28.739s
[2K
| RMSProp | epoch: 012 | loss: 0.36911 - acc: 0.8509 -- iter: 0864/2003
[A[ATraining Step: 721  | total loss: [1m[32m0.39743[0m[0m | time: 29.842s
[2K
| RMSProp | epoch: 012 | loss: 0.39743 - acc: 0.8408 -- iter: 0896/2003
[A[ATraining Step: 722  | total loss: [1m[32m0.38620[0m[0m | time: 30.725s
[2K
| RMSProp | epoch: 012 | loss: 0.38620 - acc: 0.8505 -- iter: 0928/2003
[A[ATraining Step: 723  | total loss: [1m[32m0.37333[0m[0m | time: 31.650s
[2K
| RMSProp | epoch: 012 | loss: 0.37333 - acc: 0.8592 -- iter: 0960/2003
[A[ATraining Step: 724  | total loss: [1m[32m0.36886[0m[0m | time: 32.617s
[2K
| RMSProp | epoch: 012 | loss: 0.36886 - acc: 0.8639 -- iter: 0992/2003
[A[ATraining Step: 725  | total loss: [1m[32m0.36100[0m[0m | time: 33.521s
[2K
| RMSProp | epoch: 012 | loss: 0.36100 - acc: 0.8650 -- iter: 1024/2003
[A[ATraining Step: 726  | total loss: [1m[32m0.34397[0m[0m | time: 34.543s
[2K
| RMSProp | epoch: 012 | loss: 0.34397 - acc: 0.8691 -- iter: 1056/2003
[A[ATraining Step: 727  | total loss: [1m[32m0.32820[0m[0m | time: 35.622s
[2K
| RMSProp | epoch: 012 | loss: 0.32820 - acc: 0.8760 -- iter: 1088/2003
[A[ATraining Step: 728  | total loss: [1m[32m0.32003[0m[0m | time: 36.562s
[2K
| RMSProp | epoch: 012 | loss: 0.32003 - acc: 0.8821 -- iter: 1120/2003
[A[ATraining Step: 729  | total loss: [1m[32m0.32227[0m[0m | time: 37.649s
[2K
| RMSProp | epoch: 012 | loss: 0.32227 - acc: 0.8814 -- iter: 1152/2003
[A[ATraining Step: 730  | total loss: [1m[32m0.32850[0m[0m | time: 38.854s
[2K
| RMSProp | epoch: 012 | loss: 0.32850 - acc: 0.8776 -- iter: 1184/2003
[A[ATraining Step: 731  | total loss: [1m[32m0.31290[0m[0m | time: 39.983s
[2K
| RMSProp | epoch: 012 | loss: 0.31290 - acc: 0.8836 -- iter: 1216/2003
[A[ATraining Step: 732  | total loss: [1m[32m0.30320[0m[0m | time: 41.171s
[2K
| RMSProp | epoch: 012 | loss: 0.30320 - acc: 0.8859 -- iter: 1248/2003
[A[ATraining Step: 733  | total loss: [1m[32m0.30216[0m[0m | time: 42.416s
[2K
| RMSProp | epoch: 012 | loss: 0.30216 - acc: 0.8879 -- iter: 1280/2003
[A[ATraining Step: 734  | total loss: [1m[32m0.28652[0m[0m | time: 43.716s
[2K
| RMSProp | epoch: 012 | loss: 0.28652 - acc: 0.8960 -- iter: 1312/2003
[A[ATraining Step: 735  | total loss: [1m[32m0.26630[0m[0m | time: 44.752s
[2K
| RMSProp | epoch: 012 | loss: 0.26630 - acc: 0.9033 -- iter: 1344/2003
[A[ATraining Step: 736  | total loss: [1m[32m0.26558[0m[0m | time: 45.992s
[2K
| RMSProp | epoch: 012 | loss: 0.26558 - acc: 0.9036 -- iter: 1376/2003
[A[ATraining Step: 737  | total loss: [1m[32m0.29029[0m[0m | time: 47.170s
[2K
| RMSProp | epoch: 012 | loss: 0.29029 - acc: 0.9007 -- iter: 1408/2003
[A[ATraining Step: 738  | total loss: [1m[32m0.28793[0m[0m | time: 48.474s
[2K
| RMSProp | epoch: 012 | loss: 0.28793 - acc: 0.8950 -- iter: 1440/2003
[A[ATraining Step: 739  | total loss: [1m[32m0.28614[0m[0m | time: 49.740s
[2K
| RMSProp | epoch: 012 | loss: 0.28614 - acc: 0.8961 -- iter: 1472/2003
[A[ATraining Step: 740  | total loss: [1m[32m0.28660[0m[0m | time: 50.933s
[2K
| RMSProp | epoch: 012 | loss: 0.28660 - acc: 0.8940 -- iter: 1504/2003
[A[ATraining Step: 741  | total loss: [1m[32m0.28225[0m[0m | time: 52.226s
[2K
| RMSProp | epoch: 012 | loss: 0.28225 - acc: 0.8953 -- iter: 1536/2003
[A[ATraining Step: 742  | total loss: [1m[32m0.28178[0m[0m | time: 53.651s
[2K
| RMSProp | epoch: 012 | loss: 0.28178 - acc: 0.8964 -- iter: 1568/2003
[A[ATraining Step: 743  | total loss: [1m[32m0.27938[0m[0m | time: 54.688s
[2K
| RMSProp | epoch: 012 | loss: 0.27938 - acc: 0.8973 -- iter: 1600/2003
[A[ATraining Step: 744  | total loss: [1m[32m0.28408[0m[0m | time: 56.070s
[2K
| RMSProp | epoch: 012 | loss: 0.28408 - acc: 0.8951 -- iter: 1632/2003
[A[ATraining Step: 745  | total loss: [1m[32m0.32542[0m[0m | time: 57.367s
[2K
| RMSProp | epoch: 012 | loss: 0.32542 - acc: 0.8743 -- iter: 1664/2003
[A[ATraining Step: 746  | total loss: [1m[32m0.32343[0m[0m | time: 58.632s
[2K
| RMSProp | epoch: 012 | loss: 0.32343 - acc: 0.8775 -- iter: 1696/2003
[A[ATraining Step: 747  | total loss: [1m[32m0.31429[0m[0m | time: 59.988s
[2K
| RMSProp | epoch: 012 | loss: 0.31429 - acc: 0.8835 -- iter: 1728/2003
[A[ATraining Step: 748  | total loss: [1m[32m0.32665[0m[0m | time: 61.328s
[2K
| RMSProp | epoch: 012 | loss: 0.32665 - acc: 0.8827 -- iter: 1760/2003
[A[ATraining Step: 749  | total loss: [1m[32m0.30872[0m[0m | time: 62.551s
[2K
| RMSProp | epoch: 012 | loss: 0.30872 - acc: 0.8944 -- iter: 1792/2003
[A[ATraining Step: 750  | total loss: [1m[32m0.30494[0m[0m | time: 63.829s
[2K
| RMSProp | epoch: 012 | loss: 0.30494 - acc: 0.8925 -- iter: 1824/2003
[A[ATraining Step: 751  | total loss: [1m[32m0.31722[0m[0m | time: 65.209s
[2K
| RMSProp | epoch: 012 | loss: 0.31722 - acc: 0.8751 -- iter: 1856/2003
[A[ATraining Step: 752  | total loss: [1m[32m0.33339[0m[0m | time: 66.449s
[2K
| RMSProp | epoch: 012 | loss: 0.33339 - acc: 0.8657 -- iter: 1888/2003
[A[ATraining Step: 753  | total loss: [1m[32m0.32756[0m[0m | time: 67.689s
[2K
| RMSProp | epoch: 012 | loss: 0.32756 - acc: 0.8698 -- iter: 1920/2003
[A[ATraining Step: 754  | total loss: [1m[32m0.30806[0m[0m | time: 68.798s
[2K
| RMSProp | epoch: 012 | loss: 0.30806 - acc: 0.8828 -- iter: 1952/2003
[A[ATraining Step: 755  | total loss: [1m[32m0.30493[0m[0m | time: 70.023s
[2K
| RMSProp | epoch: 012 | loss: 0.30493 - acc: 0.8820 -- iter: 1984/2003
[A[ATraining Step: 756  | total loss: [1m[32m0.29092[0m[0m | time: 76.414s
[2K
| RMSProp | epoch: 012 | loss: 0.29092 - acc: 0.8876 | val_loss: 0.37095 - val_acc: 0.8435 -- iter: 2003/2003
--
Training Step: 757  | total loss: [1m[32m0.29324[0m[0m | time: 1.088s
[2K
| RMSProp | epoch: 013 | loss: 0.29324 - acc: 0.8863 -- iter: 0032/2003
[A[ATraining Step: 758  | total loss: [1m[32m0.29059[0m[0m | time: 2.279s
[2K
| RMSProp | epoch: 013 | loss: 0.29059 - acc: 0.8883 -- iter: 0064/2003
[A[ATraining Step: 759  | total loss: [1m[32m0.28175[0m[0m | time: 3.542s
[2K
| RMSProp | epoch: 013 | loss: 0.28175 - acc: 0.8901 -- iter: 0096/2003
[A[ATraining Step: 760  | total loss: [1m[32m0.27160[0m[0m | time: 4.855s
[2K
| RMSProp | epoch: 013 | loss: 0.27160 - acc: 0.8917 -- iter: 0128/2003
[A[ATraining Step: 761  | total loss: [1m[32m0.26294[0m[0m | time: 6.136s
[2K
| RMSProp | epoch: 013 | loss: 0.26294 - acc: 0.8963 -- iter: 0160/2003
[A[ATraining Step: 762  | total loss: [1m[32m0.25311[0m[0m | time: 7.468s
[2K
| RMSProp | epoch: 013 | loss: 0.25311 - acc: 0.9004 -- iter: 0192/2003
[A[ATraining Step: 763  | total loss: [1m[32m0.25515[0m[0m | time: 8.842s
[2K
| RMSProp | epoch: 013 | loss: 0.25515 - acc: 0.8947 -- iter: 0224/2003
[A[ATraining Step: 764  | total loss: [1m[32m0.26691[0m[0m | time: 10.129s
[2K
| RMSProp | epoch: 013 | loss: 0.26691 - acc: 0.8928 -- iter: 0256/2003
[A[ATraining Step: 765  | total loss: [1m[32m0.27686[0m[0m | time: 11.546s
[2K
| RMSProp | epoch: 013 | loss: 0.27686 - acc: 0.8816 -- iter: 0288/2003
[A[ATraining Step: 766  | total loss: [1m[32m0.29775[0m[0m | time: 12.863s
[2K
| RMSProp | epoch: 013 | loss: 0.29775 - acc: 0.8685 -- iter: 0320/2003
[A[ATraining Step: 767  | total loss: [1m[32m0.31762[0m[0m | time: 13.518s
[2K
| RMSProp | epoch: 013 | loss: 0.31762 - acc: 0.8566 -- iter: 0352/2003
[A[ATraining Step: 768  | total loss: [1m[32m0.30162[0m[0m | time: 14.260s
[2K
| RMSProp | epoch: 013 | loss: 0.30162 - acc: 0.8604 -- iter: 0384/2003
[A[ATraining Step: 769  | total loss: [1m[32m0.27915[0m[0m | time: 15.284s
[2K
| RMSProp | epoch: 013 | loss: 0.27915 - acc: 0.8744 -- iter: 0416/2003
[A[ATraining Step: 770  | total loss: [1m[32m0.28907[0m[0m | time: 16.016s
[2K
| RMSProp | epoch: 013 | loss: 0.28907 - acc: 0.8713 -- iter: 0448/2003
[A[ATraining Step: 771  | total loss: [1m[32m0.30313[0m[0m | time: 16.909s
[2K
| RMSProp | epoch: 013 | loss: 0.30313 - acc: 0.8686 -- iter: 0480/2003
[A[ATraining Step: 772  | total loss: [1m[32m0.29412[0m[0m | time: 17.773s
[2K
| RMSProp | epoch: 013 | loss: 0.29412 - acc: 0.8755 -- iter: 0512/2003
[A[ATraining Step: 773  | total loss: [1m[32m0.28971[0m[0m | time: 18.696s
[2K
| RMSProp | epoch: 013 | loss: 0.28971 - acc: 0.8754 -- iter: 0544/2003
[A[ATraining Step: 774  | total loss: [1m[32m0.28651[0m[0m | time: 19.633s
[2K
| RMSProp | epoch: 013 | loss: 0.28651 - acc: 0.8816 -- iter: 0576/2003
[A[ATraining Step: 775  | total loss: [1m[32m0.29132[0m[0m | time: 20.576s
[2K
| RMSProp | epoch: 013 | loss: 0.29132 - acc: 0.8778 -- iter: 0608/2003
[A[ATraining Step: 776  | total loss: [1m[32m0.30171[0m[0m | time: 21.523s
[2K
| RMSProp | epoch: 013 | loss: 0.30171 - acc: 0.8713 -- iter: 0640/2003
[A[ATraining Step: 777  | total loss: [1m[32m0.29769[0m[0m | time: 22.480s
[2K
| RMSProp | epoch: 013 | loss: 0.29769 - acc: 0.8748 -- iter: 0672/2003
[A[ATraining Step: 778  | total loss: [1m[32m0.29207[0m[0m | time: 23.544s
[2K
| RMSProp | epoch: 013 | loss: 0.29207 - acc: 0.8779 -- iter: 0704/2003
[A[ATraining Step: 779  | total loss: [1m[32m0.28765[0m[0m | time: 24.640s
[2K
| RMSProp | epoch: 013 | loss: 0.28765 - acc: 0.8776 -- iter: 0736/2003
[A[ATraining Step: 780  | total loss: [1m[32m0.28433[0m[0m | time: 25.630s
[2K
| RMSProp | epoch: 013 | loss: 0.28433 - acc: 0.8805 -- iter: 0768/2003
[A[ATraining Step: 781  | total loss: [1m[32m0.26696[0m[0m | time: 26.450s
[2K
| RMSProp | epoch: 013 | loss: 0.26696 - acc: 0.8862 -- iter: 0800/2003
[A[ATraining Step: 782  | total loss: [1m[32m0.27629[0m[0m | time: 27.278s
[2K
| RMSProp | epoch: 013 | loss: 0.27629 - acc: 0.8820 -- iter: 0832/2003
[A[ATraining Step: 783  | total loss: [1m[32m0.29843[0m[0m | time: 28.288s
[2K
| RMSProp | epoch: 013 | loss: 0.29843 - acc: 0.8750 -- iter: 0864/2003
[A[ATraining Step: 784  | total loss: [1m[32m0.30641[0m[0m | time: 29.276s
[2K
| RMSProp | epoch: 013 | loss: 0.30641 - acc: 0.8750 -- iter: 0896/2003
[A[ATraining Step: 785  | total loss: [1m[32m0.29742[0m[0m | time: 30.169s
[2K
| RMSProp | epoch: 013 | loss: 0.29742 - acc: 0.8781 -- iter: 0928/2003
[A[ATraining Step: 786  | total loss: [1m[32m0.28887[0m[0m | time: 31.133s
[2K
| RMSProp | epoch: 013 | loss: 0.28887 - acc: 0.8841 -- iter: 0960/2003
[A[ATraining Step: 787  | total loss: [1m[32m0.30881[0m[0m | time: 32.052s
[2K
| RMSProp | epoch: 013 | loss: 0.30881 - acc: 0.8738 -- iter: 0992/2003
[A[ATraining Step: 788  | total loss: [1m[32m0.32601[0m[0m | time: 32.947s
[2K
| RMSProp | epoch: 013 | loss: 0.32601 - acc: 0.8708 -- iter: 1024/2003
[A[ATraining Step: 789  | total loss: [1m[32m0.32037[0m[0m | time: 33.822s
[2K
| RMSProp | epoch: 013 | loss: 0.32037 - acc: 0.8681 -- iter: 1056/2003
[A[ATraining Step: 790  | total loss: [1m[32m0.30982[0m[0m | time: 34.884s
[2K
| RMSProp | epoch: 013 | loss: 0.30982 - acc: 0.8719 -- iter: 1088/2003
[A[ATraining Step: 791  | total loss: [1m[32m0.30935[0m[0m | time: 35.999s
[2K
| RMSProp | epoch: 013 | loss: 0.30935 - acc: 0.8753 -- iter: 1120/2003
[A[ATraining Step: 792  | total loss: [1m[32m0.30285[0m[0m | time: 36.747s
[2K
| RMSProp | epoch: 013 | loss: 0.30285 - acc: 0.8753 -- iter: 1152/2003
[A[ATraining Step: 793  | total loss: [1m[32m0.30670[0m[0m | time: 37.608s
[2K
| RMSProp | epoch: 013 | loss: 0.30670 - acc: 0.8753 -- iter: 1184/2003
[A[ATraining Step: 794  | total loss: [1m[32m0.30533[0m[0m | time: 38.514s
[2K
| RMSProp | epoch: 013 | loss: 0.30533 - acc: 0.8752 -- iter: 1216/2003
[A[ATraining Step: 795  | total loss: [1m[32m0.28931[0m[0m | time: 39.417s
[2K
| RMSProp | epoch: 013 | loss: 0.28931 - acc: 0.8815 -- iter: 1248/2003
[A[ATraining Step: 796  | total loss: [1m[32m0.29354[0m[0m | time: 40.377s
[2K
| RMSProp | epoch: 013 | loss: 0.29354 - acc: 0.8777 -- iter: 1280/2003
[A[ATraining Step: 797  | total loss: [1m[32m0.30060[0m[0m | time: 41.392s
[2K
| RMSProp | epoch: 013 | loss: 0.30060 - acc: 0.8712 -- iter: 1312/2003
[A[ATraining Step: 798  | total loss: [1m[32m0.30306[0m[0m | time: 42.291s
[2K
| RMSProp | epoch: 013 | loss: 0.30306 - acc: 0.8653 -- iter: 1344/2003
[A[ATraining Step: 799  | total loss: [1m[32m0.31096[0m[0m | time: 43.155s
[2K
| RMSProp | epoch: 013 | loss: 0.31096 - acc: 0.8632 -- iter: 1376/2003
[A[ATraining Step: 800  | total loss: [1m[32m0.31604[0m[0m | time: 49.055s
[2K
| RMSProp | epoch: 013 | loss: 0.31604 - acc: 0.8550 | val_loss: 0.39908 - val_acc: 0.8275 -- iter: 1408/2003
--
Training Step: 801  | total loss: [1m[32m0.34299[0m[0m | time: 50.839s
[2K
| RMSProp | epoch: 013 | loss: 0.34299 - acc: 0.8445 -- iter: 1440/2003
[A[ATraining Step: 802  | total loss: [1m[32m0.33640[0m[0m | time: 51.977s
[2K
| RMSProp | epoch: 013 | loss: 0.33640 - acc: 0.8444 -- iter: 1472/2003
[A[ATraining Step: 803  | total loss: [1m[32m0.31732[0m[0m | time: 53.252s
[2K
| RMSProp | epoch: 013 | loss: 0.31732 - acc: 0.8568 -- iter: 1504/2003
[A[ATraining Step: 804  | total loss: [1m[32m0.29925[0m[0m | time: 54.442s
[2K
| RMSProp | epoch: 013 | loss: 0.29925 - acc: 0.8649 -- iter: 1536/2003
[A[ATraining Step: 805  | total loss: [1m[32m0.28095[0m[0m | time: 55.667s
[2K
| RMSProp | epoch: 013 | loss: 0.28095 - acc: 0.8753 -- iter: 1568/2003
[A[ATraining Step: 806  | total loss: [1m[32m0.27002[0m[0m | time: 57.008s
[2K
| RMSProp | epoch: 013 | loss: 0.27002 - acc: 0.8815 -- iter: 1600/2003
[A[ATraining Step: 807  | total loss: [1m[32m0.27364[0m[0m | time: 58.262s
[2K
| RMSProp | epoch: 013 | loss: 0.27364 - acc: 0.8777 -- iter: 1632/2003
[A[ATraining Step: 808  | total loss: [1m[32m0.27244[0m[0m | time: 59.469s
[2K
| RMSProp | epoch: 013 | loss: 0.27244 - acc: 0.8806 -- iter: 1664/2003
[A[ATraining Step: 809  | total loss: [1m[32m0.27536[0m[0m | time: 60.699s
[2K
| RMSProp | epoch: 013 | loss: 0.27536 - acc: 0.8800 -- iter: 1696/2003
[A[ATraining Step: 810  | total loss: [1m[32m0.28320[0m[0m | time: 61.872s
[2K
| RMSProp | epoch: 013 | loss: 0.28320 - acc: 0.8764 -- iter: 1728/2003
[A[ATraining Step: 811  | total loss: [1m[32m0.27602[0m[0m | time: 63.317s
[2K
| RMSProp | epoch: 013 | loss: 0.27602 - acc: 0.8825 -- iter: 1760/2003
[A[ATraining Step: 812  | total loss: [1m[32m0.26910[0m[0m | time: 64.663s
[2K
| RMSProp | epoch: 013 | loss: 0.26910 - acc: 0.8849 -- iter: 1792/2003
[A[ATraining Step: 813  | total loss: [1m[32m0.29492[0m[0m | time: 65.924s
[2K
| RMSProp | epoch: 013 | loss: 0.29492 - acc: 0.8745 -- iter: 1824/2003
[A[ATraining Step: 814  | total loss: [1m[32m0.29310[0m[0m | time: 67.036s
[2K
| RMSProp | epoch: 013 | loss: 0.29310 - acc: 0.8714 -- iter: 1856/2003
[A[ATraining Step: 815  | total loss: [1m[32m0.30704[0m[0m | time: 68.111s
[2K
| RMSProp | epoch: 013 | loss: 0.30704 - acc: 0.8655 -- iter: 1888/2003
[A[ATraining Step: 816  | total loss: [1m[32m0.29333[0m[0m | time: 69.377s
[2K
| RMSProp | epoch: 013 | loss: 0.29333 - acc: 0.8727 -- iter: 1920/2003
[A[ATraining Step: 817  | total loss: [1m[32m0.28751[0m[0m | time: 70.597s
[2K
| RMSProp | epoch: 013 | loss: 0.28751 - acc: 0.8792 -- iter: 1952/2003
[A[ATraining Step: 818  | total loss: [1m[32m0.27000[0m[0m | time: 71.864s
[2K
| RMSProp | epoch: 013 | loss: 0.27000 - acc: 0.8913 -- iter: 1984/2003
[A[ATraining Step: 819  | total loss: [1m[32m0.25616[0m[0m | time: 77.923s
[2K
| RMSProp | epoch: 013 | loss: 0.25616 - acc: 0.8990 | val_loss: 0.35571 - val_acc: 0.8562 -- iter: 2003/2003
--
Training Step: 820  | total loss: [1m[32m0.25628[0m[0m | time: 1.129s
[2K
| RMSProp | epoch: 014 | loss: 0.25628 - acc: 0.8998 -- iter: 0032/2003
[A[ATraining Step: 821  | total loss: [1m[32m0.24278[0m[0m | time: 2.154s
[2K
| RMSProp | epoch: 014 | loss: 0.24278 - acc: 0.9067 -- iter: 0064/2003
[A[ATraining Step: 822  | total loss: [1m[32m0.22805[0m[0m | time: 3.255s
[2K
| RMSProp | epoch: 014 | loss: 0.22805 - acc: 0.9160 -- iter: 0096/2003
[A[ATraining Step: 823  | total loss: [1m[32m0.23699[0m[0m | time: 4.435s
[2K
| RMSProp | epoch: 014 | loss: 0.23699 - acc: 0.9150 -- iter: 0128/2003
[A[ATraining Step: 824  | total loss: [1m[32m0.23279[0m[0m | time: 5.590s
[2K
| RMSProp | epoch: 014 | loss: 0.23279 - acc: 0.9141 -- iter: 0160/2003
[A[ATraining Step: 825  | total loss: [1m[32m0.24049[0m[0m | time: 6.960s
[2K
| RMSProp | epoch: 014 | loss: 0.24049 - acc: 0.9102 -- iter: 0192/2003
[A[ATraining Step: 826  | total loss: [1m[32m0.23284[0m[0m | time: 8.237s
[2K
| RMSProp | epoch: 014 | loss: 0.23284 - acc: 0.9130 -- iter: 0224/2003
[A[ATraining Step: 827  | total loss: [1m[32m0.22170[0m[0m | time: 9.409s
[2K
| RMSProp | epoch: 014 | loss: 0.22170 - acc: 0.9185 -- iter: 0256/2003
[A[ATraining Step: 828  | total loss: [1m[32m0.21138[0m[0m | time: 10.620s
[2K
| RMSProp | epoch: 014 | loss: 0.21138 - acc: 0.9204 -- iter: 0288/2003
[A[ATraining Step: 829  | total loss: [1m[32m0.22495[0m[0m | time: 11.859s
[2K
| RMSProp | epoch: 014 | loss: 0.22495 - acc: 0.9096 -- iter: 0320/2003
[A[ATraining Step: 830  | total loss: [1m[32m0.21560[0m[0m | time: 12.958s
[2K
| RMSProp | epoch: 014 | loss: 0.21560 - acc: 0.9155 -- iter: 0352/2003
[A[ATraining Step: 831  | total loss: [1m[32m0.21650[0m[0m | time: 13.575s
[2K
| RMSProp | epoch: 014 | loss: 0.21650 - acc: 0.9177 -- iter: 0384/2003
[A[ATraining Step: 832  | total loss: [1m[32m0.20537[0m[0m | time: 14.277s
[2K
| RMSProp | epoch: 014 | loss: 0.20537 - acc: 0.9207 -- iter: 0416/2003
[A[ATraining Step: 833  | total loss: [1m[32m0.19513[0m[0m | time: 15.771s
[2K
| RMSProp | epoch: 014 | loss: 0.19513 - acc: 0.9234 -- iter: 0448/2003
[A[ATraining Step: 834  | total loss: [1m[32m0.24177[0m[0m | time: 17.051s
[2K
| RMSProp | epoch: 014 | loss: 0.24177 - acc: 0.9029 -- iter: 0480/2003
[A[ATraining Step: 835  | total loss: [1m[32m0.25238[0m[0m | time: 18.375s
[2K
| RMSProp | epoch: 014 | loss: 0.25238 - acc: 0.9001 -- iter: 0512/2003
[A[ATraining Step: 836  | total loss: [1m[32m0.27860[0m[0m | time: 19.411s
[2K
| RMSProp | epoch: 014 | loss: 0.27860 - acc: 0.8851 -- iter: 0544/2003
[A[ATraining Step: 837  | total loss: [1m[32m0.28470[0m[0m | time: 20.590s
[2K
| RMSProp | epoch: 014 | loss: 0.28470 - acc: 0.8810 -- iter: 0576/2003
[A[ATraining Step: 838  | total loss: [1m[32m0.27840[0m[0m | time: 21.776s
[2K
| RMSProp | epoch: 014 | loss: 0.27840 - acc: 0.8835 -- iter: 0608/2003
[A[ATraining Step: 839  | total loss: [1m[32m0.27106[0m[0m | time: 22.941s
[2K
| RMSProp | epoch: 014 | loss: 0.27106 - acc: 0.8889 -- iter: 0640/2003
[A[ATraining Step: 840  | total loss: [1m[32m0.27203[0m[0m | time: 24.280s
[2K
| RMSProp | epoch: 014 | loss: 0.27203 - acc: 0.8938 -- iter: 0672/2003
[A[ATraining Step: 841  | total loss: [1m[32m0.28488[0m[0m | time: 25.659s
[2K
| RMSProp | epoch: 014 | loss: 0.28488 - acc: 0.8825 -- iter: 0704/2003
[A[ATraining Step: 842  | total loss: [1m[32m0.30686[0m[0m | time: 26.924s
[2K
| RMSProp | epoch: 014 | loss: 0.30686 - acc: 0.8724 -- iter: 0736/2003
[A[ATraining Step: 843  | total loss: [1m[32m0.29522[0m[0m | time: 28.084s
[2K
| RMSProp | epoch: 014 | loss: 0.29522 - acc: 0.8789 -- iter: 0768/2003
[A[ATraining Step: 844  | total loss: [1m[32m0.28721[0m[0m | time: 29.329s
[2K
| RMSProp | epoch: 014 | loss: 0.28721 - acc: 0.8816 -- iter: 0800/2003
[A[ATraining Step: 845  | total loss: [1m[32m0.27792[0m[0m | time: 30.480s
[2K
| RMSProp | epoch: 014 | loss: 0.27792 - acc: 0.8841 -- iter: 0832/2003
[A[ATraining Step: 846  | total loss: [1m[32m0.27859[0m[0m | time: 31.629s
[2K
| RMSProp | epoch: 014 | loss: 0.27859 - acc: 0.8832 -- iter: 0864/2003
[A[ATraining Step: 847  | total loss: [1m[32m0.26504[0m[0m | time: 33.035s
[2K
| RMSProp | epoch: 014 | loss: 0.26504 - acc: 0.8917 -- iter: 0896/2003
[A[ATraining Step: 848  | total loss: [1m[32m0.25373[0m[0m | time: 34.407s
[2K
| RMSProp | epoch: 014 | loss: 0.25373 - acc: 0.8932 -- iter: 0928/2003
[A[ATraining Step: 849  | total loss: [1m[32m0.26127[0m[0m | time: 35.612s
[2K
| RMSProp | epoch: 014 | loss: 0.26127 - acc: 0.8851 -- iter: 0960/2003
[A[ATraining Step: 850  | total loss: [1m[32m0.26007[0m[0m | time: 36.870s
[2K
| RMSProp | epoch: 014 | loss: 0.26007 - acc: 0.8841 -- iter: 0992/2003
[A[ATraining Step: 851  | total loss: [1m[32m0.27216[0m[0m | time: 38.156s
[2K
| RMSProp | epoch: 014 | loss: 0.27216 - acc: 0.8801 -- iter: 1024/2003
[A[ATraining Step: 852  | total loss: [1m[32m0.25970[0m[0m | time: 39.519s
[2K
| RMSProp | epoch: 014 | loss: 0.25970 - acc: 0.8858 -- iter: 1056/2003
[A[ATraining Step: 853  | total loss: [1m[32m0.25729[0m[0m | time: 40.813s
[2K
| RMSProp | epoch: 014 | loss: 0.25729 - acc: 0.8879 -- iter: 1088/2003
[A[ATraining Step: 854  | total loss: [1m[32m0.27156[0m[0m | time: 41.840s
[2K
| RMSProp | epoch: 014 | loss: 0.27156 - acc: 0.8834 -- iter: 1120/2003
[A[ATraining Step: 855  | total loss: [1m[32m0.26220[0m[0m | time: 43.007s
[2K
| RMSProp | epoch: 014 | loss: 0.26220 - acc: 0.8920 -- iter: 1152/2003
[A[ATraining Step: 856  | total loss: [1m[32m0.24934[0m[0m | time: 44.339s
[2K
| RMSProp | epoch: 014 | loss: 0.24934 - acc: 0.8965 -- iter: 1184/2003
[A[ATraining Step: 857  | total loss: [1m[32m0.25898[0m[0m | time: 45.670s
[2K
| RMSProp | epoch: 014 | loss: 0.25898 - acc: 0.8881 -- iter: 1216/2003
[A[ATraining Step: 858  | total loss: [1m[32m0.25477[0m[0m | time: 46.874s
[2K
| RMSProp | epoch: 014 | loss: 0.25477 - acc: 0.8899 -- iter: 1248/2003
[A[ATraining Step: 859  | total loss: [1m[32m0.24479[0m[0m | time: 48.140s
[2K
| RMSProp | epoch: 014 | loss: 0.24479 - acc: 0.8978 -- iter: 1280/2003
[A[ATraining Step: 860  | total loss: [1m[32m0.23293[0m[0m | time: 49.518s
[2K
| RMSProp | epoch: 014 | loss: 0.23293 - acc: 0.9018 -- iter: 1312/2003
[A[ATraining Step: 861  | total loss: [1m[32m0.21552[0m[0m | time: 50.886s
[2K
| RMSProp | epoch: 014 | loss: 0.21552 - acc: 0.9116 -- iter: 1344/2003
[A[ATraining Step: 862  | total loss: [1m[32m0.20005[0m[0m | time: 51.971s
[2K
| RMSProp | epoch: 014 | loss: 0.20005 - acc: 0.9173 -- iter: 1376/2003
[A[ATraining Step: 863  | total loss: [1m[32m0.19524[0m[0m | time: 53.241s
[2K
| RMSProp | epoch: 014 | loss: 0.19524 - acc: 0.9193 -- iter: 1408/2003
[A[ATraining Step: 864  | total loss: [1m[32m0.21060[0m[0m | time: 54.547s
[2K
| RMSProp | epoch: 014 | loss: 0.21060 - acc: 0.9149 -- iter: 1440/2003
[A[ATraining Step: 865  | total loss: [1m[32m0.20460[0m[0m | time: 55.635s
[2K
| RMSProp | epoch: 014 | loss: 0.20460 - acc: 0.9172 -- iter: 1472/2003
[A[ATraining Step: 866  | total loss: [1m[32m0.19755[0m[0m | time: 56.805s
[2K
| RMSProp | epoch: 014 | loss: 0.19755 - acc: 0.9192 -- iter: 1504/2003
[A[ATraining Step: 867  | total loss: [1m[32m0.19010[0m[0m | time: 57.990s
[2K
| RMSProp | epoch: 014 | loss: 0.19010 - acc: 0.9242 -- iter: 1536/2003
[A[ATraining Step: 868  | total loss: [1m[32m0.17787[0m[0m | time: 59.306s
[2K
| RMSProp | epoch: 014 | loss: 0.17787 - acc: 0.9286 -- iter: 1568/2003
[A[ATraining Step: 869  | total loss: [1m[32m0.18680[0m[0m | time: 60.541s
[2K
| RMSProp | epoch: 014 | loss: 0.18680 - acc: 0.9264 -- iter: 1600/2003
[A[ATraining Step: 870  | total loss: [1m[32m0.18005[0m[0m | time: 61.849s
[2K
| RMSProp | epoch: 014 | loss: 0.18005 - acc: 0.9275 -- iter: 1632/2003
[A[ATraining Step: 871  | total loss: [1m[32m0.17735[0m[0m | time: 63.157s
[2K
| RMSProp | epoch: 014 | loss: 0.17735 - acc: 0.9254 -- iter: 1664/2003
[A[ATraining Step: 872  | total loss: [1m[32m0.16924[0m[0m | time: 64.422s
[2K
| RMSProp | epoch: 014 | loss: 0.16924 - acc: 0.9297 -- iter: 1696/2003
[A[ATraining Step: 873  | total loss: [1m[32m0.15583[0m[0m | time: 65.706s
[2K
| RMSProp | epoch: 014 | loss: 0.15583 - acc: 0.9367 -- iter: 1728/2003
[A[ATraining Step: 874  | total loss: [1m[32m0.16065[0m[0m | time: 67.015s
[2K
| RMSProp | epoch: 014 | loss: 0.16065 - acc: 0.9368 -- iter: 1760/2003
[A[ATraining Step: 875  | total loss: [1m[32m0.15814[0m[0m | time: 68.298s
[2K
| RMSProp | epoch: 014 | loss: 0.15814 - acc: 0.9400 -- iter: 1792/2003
[A[ATraining Step: 876  | total loss: [1m[32m0.14589[0m[0m | time: 69.572s
[2K
| RMSProp | epoch: 014 | loss: 0.14589 - acc: 0.9460 -- iter: 1824/2003
[A[ATraining Step: 877  | total loss: [1m[32m0.14293[0m[0m | time: 70.606s
[2K
| RMSProp | epoch: 014 | loss: 0.14293 - acc: 0.9483 -- iter: 1856/2003
[A[ATraining Step: 878  | total loss: [1m[32m0.17470[0m[0m | time: 71.789s
[2K
| RMSProp | epoch: 014 | loss: 0.17470 - acc: 0.9378 -- iter: 1888/2003
[A[ATraining Step: 879  | total loss: [1m[32m0.16577[0m[0m | time: 72.959s
[2K
| RMSProp | epoch: 014 | loss: 0.16577 - acc: 0.9440 -- iter: 1920/2003
[A[ATraining Step: 880  | total loss: [1m[32m0.15933[0m[0m | time: 74.347s
[2K
| RMSProp | epoch: 014 | loss: 0.15933 - acc: 0.9465 -- iter: 1952/2003
[A[ATraining Step: 881  | total loss: [1m[32m0.17115[0m[0m | time: 75.535s
[2K
| RMSProp | epoch: 014 | loss: 0.17115 - acc: 0.9362 -- iter: 1984/2003
[A[ATraining Step: 882  | total loss: [1m[32m0.17944[0m[0m | time: 81.974s
[2K
| RMSProp | epoch: 014 | loss: 0.17944 - acc: 0.9332 | val_loss: 0.50050 - val_acc: 0.7827 -- iter: 2003/2003
--
Training Step: 883  | total loss: [1m[32m0.18989[0m[0m | time: 1.286s
[2K
| RMSProp | epoch: 015 | loss: 0.18989 - acc: 0.9243 -- iter: 0032/2003
[A[ATraining Step: 884  | total loss: [1m[32m0.21299[0m[0m | time: 2.497s
[2K
| RMSProp | epoch: 015 | loss: 0.21299 - acc: 0.9131 -- iter: 0064/2003
[A[ATraining Step: 885  | total loss: [1m[32m0.21288[0m[0m | time: 3.419s
[2K
| RMSProp | epoch: 015 | loss: 0.21288 - acc: 0.9124 -- iter: 0096/2003
[A[ATraining Step: 886  | total loss: [1m[32m0.21027[0m[0m | time: 4.520s
[2K
| RMSProp | epoch: 015 | loss: 0.21027 - acc: 0.9181 -- iter: 0128/2003
[A[ATraining Step: 887  | total loss: [1m[32m0.21101[0m[0m | time: 5.752s
[2K
| RMSProp | epoch: 015 | loss: 0.21101 - acc: 0.9200 -- iter: 0160/2003
[A[ATraining Step: 888  | total loss: [1m[32m0.21423[0m[0m | time: 7.016s
[2K
| RMSProp | epoch: 015 | loss: 0.21423 - acc: 0.9155 -- iter: 0192/2003
[A[ATraining Step: 889  | total loss: [1m[32m0.22075[0m[0m | time: 8.367s
[2K
| RMSProp | epoch: 015 | loss: 0.22075 - acc: 0.9115 -- iter: 0224/2003
[A[ATraining Step: 890  | total loss: [1m[32m0.21486[0m[0m | time: 9.785s
[2K
| RMSProp | epoch: 015 | loss: 0.21486 - acc: 0.9172 -- iter: 0256/2003
[A[ATraining Step: 891  | total loss: [1m[32m0.23372[0m[0m | time: 11.115s
[2K
| RMSProp | epoch: 015 | loss: 0.23372 - acc: 0.9067 -- iter: 0288/2003
[A[ATraining Step: 892  | total loss: [1m[32m0.22351[0m[0m | time: 12.413s
[2K
| RMSProp | epoch: 015 | loss: 0.22351 - acc: 0.9098 -- iter: 0320/2003
[A[ATraining Step: 893  | total loss: [1m[32m0.22775[0m[0m | time: 13.759s
[2K
| RMSProp | epoch: 015 | loss: 0.22775 - acc: 0.9063 -- iter: 0352/2003
[A[ATraining Step: 894  | total loss: [1m[32m0.21751[0m[0m | time: 14.919s
[2K
| RMSProp | epoch: 015 | loss: 0.21751 - acc: 0.9094 -- iter: 0384/2003
[A[ATraining Step: 895  | total loss: [1m[32m0.21326[0m[0m | time: 15.768s
[2K
| RMSProp | epoch: 015 | loss: 0.21326 - acc: 0.9122 -- iter: 0416/2003
[A[ATraining Step: 896  | total loss: [1m[32m0.19947[0m[0m | time: 16.596s
[2K
| RMSProp | epoch: 015 | loss: 0.19947 - acc: 0.9210 -- iter: 0448/2003
[A[ATraining Step: 897  | total loss: [1m[32m0.18191[0m[0m | time: 17.869s
[2K
| RMSProp | epoch: 015 | loss: 0.18191 - acc: 0.9289 -- iter: 0480/2003
[A[ATraining Step: 898  | total loss: [1m[32m0.17857[0m[0m | time: 18.938s
[2K
| RMSProp | epoch: 015 | loss: 0.17857 - acc: 0.9298 -- iter: 0512/2003
[A[ATraining Step: 899  | total loss: [1m[32m0.21737[0m[0m | time: 20.134s
[2K
| RMSProp | epoch: 015 | loss: 0.21737 - acc: 0.9149 -- iter: 0544/2003
[A[ATraining Step: 900  | total loss: [1m[32m0.21522[0m[0m | time: 21.333s
[2K
| RMSProp | epoch: 015 | loss: 0.21522 - acc: 0.9109 -- iter: 0576/2003
[A[ATraining Step: 901  | total loss: [1m[32m0.22646[0m[0m | time: 22.601s
[2K
| RMSProp | epoch: 015 | loss: 0.22646 - acc: 0.9105 -- iter: 0608/2003
[A[ATraining Step: 902  | total loss: [1m[32m0.24524[0m[0m | time: 23.942s
[2K
| RMSProp | epoch: 015 | loss: 0.24524 - acc: 0.9069 -- iter: 0640/2003
[A[ATraining Step: 903  | total loss: [1m[32m0.23921[0m[0m | time: 25.330s
[2K
| RMSProp | epoch: 015 | loss: 0.23921 - acc: 0.9131 -- iter: 0672/2003
[A[ATraining Step: 904  | total loss: [1m[32m0.23063[0m[0m | time: 26.539s
[2K
| RMSProp | epoch: 015 | loss: 0.23063 - acc: 0.9155 -- iter: 0704/2003
[A[ATraining Step: 905  | total loss: [1m[32m0.22427[0m[0m | time: 27.757s
[2K
| RMSProp | epoch: 015 | loss: 0.22427 - acc: 0.9115 -- iter: 0736/2003
[A[ATraining Step: 906  | total loss: [1m[32m0.21417[0m[0m | time: 28.802s
[2K
| RMSProp | epoch: 015 | loss: 0.21417 - acc: 0.9141 -- iter: 0768/2003
[A[ATraining Step: 907  | total loss: [1m[32m0.19826[0m[0m | time: 29.881s
[2K
| RMSProp | epoch: 015 | loss: 0.19826 - acc: 0.9196 -- iter: 0800/2003
[A[ATraining Step: 908  | total loss: [1m[32m0.20246[0m[0m | time: 31.092s
[2K
| RMSProp | epoch: 015 | loss: 0.20246 - acc: 0.9182 -- iter: 0832/2003
[A[ATraining Step: 909  | total loss: [1m[32m0.21637[0m[0m | time: 32.398s
[2K
| RMSProp | epoch: 015 | loss: 0.21637 - acc: 0.9108 -- iter: 0864/2003
[A[ATraining Step: 910  | total loss: [1m[32m0.21707[0m[0m | time: 33.439s
[2K
| RMSProp | epoch: 015 | loss: 0.21707 - acc: 0.9072 -- iter: 0896/2003
[A[ATraining Step: 911  | total loss: [1m[32m0.22735[0m[0m | time: 34.379s
[2K
| RMSProp | epoch: 015 | loss: 0.22735 - acc: 0.9009 -- iter: 0928/2003
[A[ATraining Step: 912  | total loss: [1m[32m0.21662[0m[0m | time: 35.553s
[2K
| RMSProp | epoch: 015 | loss: 0.21662 - acc: 0.9076 -- iter: 0960/2003
[A[ATraining Step: 913  | total loss: [1m[32m0.20388[0m[0m | time: 36.654s
[2K
| RMSProp | epoch: 015 | loss: 0.20388 - acc: 0.9138 -- iter: 0992/2003
[A[ATraining Step: 914  | total loss: [1m[32m0.19679[0m[0m | time: 37.867s
[2K
| RMSProp | epoch: 015 | loss: 0.19679 - acc: 0.9193 -- iter: 1024/2003
[A[ATraining Step: 915  | total loss: [1m[32m0.19036[0m[0m | time: 39.139s
[2K
| RMSProp | epoch: 015 | loss: 0.19036 - acc: 0.9180 -- iter: 1056/2003
[A[ATraining Step: 916  | total loss: [1m[32m0.19254[0m[0m | time: 40.524s
[2K
| RMSProp | epoch: 015 | loss: 0.19254 - acc: 0.9168 -- iter: 1088/2003
[A[ATraining Step: 917  | total loss: [1m[32m0.20194[0m[0m | time: 41.689s
[2K
| RMSProp | epoch: 015 | loss: 0.20194 - acc: 0.9157 -- iter: 1120/2003
[A[ATraining Step: 918  | total loss: [1m[32m0.20227[0m[0m | time: 42.889s
[2K
| RMSProp | epoch: 015 | loss: 0.20227 - acc: 0.9179 -- iter: 1152/2003
[A[ATraining Step: 919  | total loss: [1m[32m0.19381[0m[0m | time: 44.128s
[2K
| RMSProp | epoch: 015 | loss: 0.19381 - acc: 0.9230 -- iter: 1184/2003
[A[ATraining Step: 920  | total loss: [1m[32m0.19599[0m[0m | time: 45.402s
[2K
| RMSProp | epoch: 015 | loss: 0.19599 - acc: 0.9213 -- iter: 1216/2003
[A[ATraining Step: 921  | total loss: [1m[32m0.20390[0m[0m | time: 46.752s
[2K
| RMSProp | epoch: 015 | loss: 0.20390 - acc: 0.9198 -- iter: 1248/2003
[A[ATraining Step: 922  | total loss: [1m[32m0.19520[0m[0m | time: 48.094s
[2K
| RMSProp | epoch: 015 | loss: 0.19520 - acc: 0.9247 -- iter: 1280/2003
[A[ATraining Step: 923  | total loss: [1m[32m0.19069[0m[0m | time: 49.395s
[2K
| RMSProp | epoch: 015 | loss: 0.19069 - acc: 0.9229 -- iter: 1312/2003
[A[ATraining Step: 924  | total loss: [1m[32m0.18992[0m[0m | time: 50.430s
[2K
| RMSProp | epoch: 015 | loss: 0.18992 - acc: 0.9243 -- iter: 1344/2003
[A[ATraining Step: 925  | total loss: [1m[32m0.20161[0m[0m | time: 51.685s
[2K
| RMSProp | epoch: 015 | loss: 0.20161 - acc: 0.9194 -- iter: 1376/2003
[A[ATraining Step: 926  | total loss: [1m[32m0.22870[0m[0m | time: 53.016s
[2K
| RMSProp | epoch: 015 | loss: 0.22870 - acc: 0.8993 -- iter: 1408/2003
[A[ATraining Step: 927  | total loss: [1m[32m0.21503[0m[0m | time: 54.281s
[2K
| RMSProp | epoch: 015 | loss: 0.21503 - acc: 0.9094 -- iter: 1440/2003
[A[ATraining Step: 928  | total loss: [1m[32m0.21529[0m[0m | time: 55.558s
[2K
| RMSProp | epoch: 015 | loss: 0.21529 - acc: 0.9122 -- iter: 1472/2003
[A[ATraining Step: 929  | total loss: [1m[32m0.20734[0m[0m | time: 56.717s
[2K
| RMSProp | epoch: 015 | loss: 0.20734 - acc: 0.9147 -- iter: 1504/2003
[A[ATraining Step: 930  | total loss: [1m[32m0.19471[0m[0m | time: 57.968s
[2K
| RMSProp | epoch: 015 | loss: 0.19471 - acc: 0.9170 -- iter: 1536/2003
[A[ATraining Step: 931  | total loss: [1m[32m0.18757[0m[0m | time: 59.374s
[2K
| RMSProp | epoch: 015 | loss: 0.18757 - acc: 0.9191 -- iter: 1568/2003
[A[ATraining Step: 932  | total loss: [1m[32m0.18106[0m[0m | time: 60.555s
[2K
| RMSProp | epoch: 015 | loss: 0.18106 - acc: 0.9240 -- iter: 1600/2003
[A[ATraining Step: 933  | total loss: [1m[32m0.17089[0m[0m | time: 61.839s
[2K
| RMSProp | epoch: 015 | loss: 0.17089 - acc: 0.9285 -- iter: 1632/2003
[A[ATraining Step: 934  | total loss: [1m[32m0.16357[0m[0m | time: 63.283s
[2K
| RMSProp | epoch: 015 | loss: 0.16357 - acc: 0.9325 -- iter: 1664/2003
[A[ATraining Step: 935  | total loss: [1m[32m0.15381[0m[0m | time: 64.655s
[2K
| RMSProp | epoch: 015 | loss: 0.15381 - acc: 0.9361 -- iter: 1696/2003
[A[ATraining Step: 936  | total loss: [1m[32m0.15099[0m[0m | time: 65.872s
[2K
| RMSProp | epoch: 015 | loss: 0.15099 - acc: 0.9394 -- iter: 1728/2003
[A[ATraining Step: 937  | total loss: [1m[32m0.16660[0m[0m | time: 67.051s
[2K
| RMSProp | epoch: 015 | loss: 0.16660 - acc: 0.9330 -- iter: 1760/2003
[A[ATraining Step: 938  | total loss: [1m[32m0.15328[0m[0m | time: 68.432s
[2K
| RMSProp | epoch: 015 | loss: 0.15328 - acc: 0.9397 -- iter: 1792/2003
[A[ATraining Step: 939  | total loss: [1m[32m0.15171[0m[0m | time: 69.604s
[2K
| RMSProp | epoch: 015 | loss: 0.15171 - acc: 0.9395 -- iter: 1824/2003
[A[ATraining Step: 940  | total loss: [1m[32m0.15062[0m[0m | time: 70.757s
[2K
| RMSProp | epoch: 015 | loss: 0.15062 - acc: 0.9393 -- iter: 1856/2003
[A[ATraining Step: 941  | total loss: [1m[32m0.15118[0m[0m | time: 72.017s
[2K
| RMSProp | epoch: 015 | loss: 0.15118 - acc: 0.9422 -- iter: 1888/2003
[A[ATraining Step: 942  | total loss: [1m[32m0.14121[0m[0m | time: 73.371s
[2K
| RMSProp | epoch: 015 | loss: 0.14121 - acc: 0.9480 -- iter: 1920/2003
[A[ATraining Step: 943  | total loss: [1m[32m0.13200[0m[0m | time: 74.479s
[2K
| RMSProp | epoch: 015 | loss: 0.13200 - acc: 0.9532 -- iter: 1952/2003
[A[ATraining Step: 944  | total loss: [1m[32m0.12240[0m[0m | time: 75.737s
[2K
| RMSProp | epoch: 015 | loss: 0.12240 - acc: 0.9547 -- iter: 1984/2003
[A[ATraining Step: 945  | total loss: [1m[32m0.11150[0m[0m | time: 81.635s
[2K
| RMSProp | epoch: 015 | loss: 0.11150 - acc: 0.9593 | val_loss: 0.84570 - val_acc: 0.7668 -- iter: 2003/2003
--
Training Step: 946  | total loss: [1m[32m0.10674[0m[0m | time: 1.182s
[2K
| RMSProp | epoch: 016 | loss: 0.10674 - acc: 0.9602 -- iter: 0032/2003
[A[ATraining Step: 947  | total loss: [1m[32m0.13610[0m[0m | time: 2.444s
[2K
| RMSProp | epoch: 016 | loss: 0.13610 - acc: 0.9517 -- iter: 0064/2003
[A[ATraining Step: 948  | total loss: [1m[32m0.13424[0m[0m | time: 3.753s
[2K
| RMSProp | epoch: 016 | loss: 0.13424 - acc: 0.9503 -- iter: 0096/2003
[A[ATraining Step: 949  | total loss: [1m[32m0.12617[0m[0m | time: 4.995s
[2K
| RMSProp | epoch: 016 | loss: 0.12617 - acc: 0.9521 -- iter: 0128/2003
[A[ATraining Step: 950  | total loss: [1m[32m0.12538[0m[0m | time: 6.293s
[2K
| RMSProp | epoch: 016 | loss: 0.12538 - acc: 0.9538 -- iter: 0160/2003
[A[ATraining Step: 951  | total loss: [1m[32m0.11896[0m[0m | time: 7.655s
[2K
| RMSProp | epoch: 016 | loss: 0.11896 - acc: 0.9584 -- iter: 0192/2003
[A[ATraining Step: 952  | total loss: [1m[32m0.12151[0m[0m | time: 8.958s
[2K
| RMSProp | epoch: 016 | loss: 0.12151 - acc: 0.9563 -- iter: 0224/2003
[A[ATraining Step: 953  | total loss: [1m[32m0.13388[0m[0m | time: 10.210s
[2K
| RMSProp | epoch: 016 | loss: 0.13388 - acc: 0.9544 -- iter: 0256/2003
[A[ATraining Step: 954  | total loss: [1m[32m0.14712[0m[0m | time: 11.226s
[2K
| RMSProp | epoch: 016 | loss: 0.14712 - acc: 0.9496 -- iter: 0288/2003
[A[ATraining Step: 955  | total loss: [1m[32m0.16259[0m[0m | time: 12.090s
[2K
| RMSProp | epoch: 016 | loss: 0.16259 - acc: 0.9422 -- iter: 0320/2003
[A[ATraining Step: 956  | total loss: [1m[32m0.15690[0m[0m | time: 13.137s
[2K
| RMSProp | epoch: 016 | loss: 0.15690 - acc: 0.9417 -- iter: 0352/2003
[A[ATraining Step: 957  | total loss: [1m[32m0.14953[0m[0m | time: 14.109s
[2K
| RMSProp | epoch: 016 | loss: 0.14953 - acc: 0.9444 -- iter: 0384/2003
[A[ATraining Step: 958  | total loss: [1m[32m0.16802[0m[0m | time: 14.967s
[2K
| RMSProp | epoch: 016 | loss: 0.16802 - acc: 0.9406 -- iter: 0416/2003
[A[ATraining Step: 959  | total loss: [1m[32m0.16973[0m[0m | time: 15.489s
[2K
| RMSProp | epoch: 016 | loss: 0.16973 - acc: 0.9371 -- iter: 0448/2003
[A[ATraining Step: 960  | total loss: [1m[32m0.15919[0m[0m | time: 16.017s
[2K
| RMSProp | epoch: 016 | loss: 0.15919 - acc: 0.9434 -- iter: 0480/2003
[A[ATraining Step: 961  | total loss: [1m[32m0.14513[0m[0m | time: 16.906s
[2K
| RMSProp | epoch: 016 | loss: 0.14513 - acc: 0.9491 -- iter: 0512/2003
[A[ATraining Step: 962  | total loss: [1m[32m0.13642[0m[0m | time: 17.760s
[2K
| RMSProp | epoch: 016 | loss: 0.13642 - acc: 0.9511 -- iter: 0544/2003
[A[ATraining Step: 963  | total loss: [1m[32m0.12382[0m[0m | time: 18.652s
[2K
| RMSProp | epoch: 016 | loss: 0.12382 - acc: 0.9560 -- iter: 0576/2003
[A[ATraining Step: 964  | total loss: [1m[32m0.11306[0m[0m | time: 19.606s
[2K
| RMSProp | epoch: 016 | loss: 0.11306 - acc: 0.9604 -- iter: 0608/2003
[A[ATraining Step: 965  | total loss: [1m[32m0.10268[0m[0m | time: 20.595s
[2K
| RMSProp | epoch: 016 | loss: 0.10268 - acc: 0.9643 -- iter: 0640/2003
[A[ATraining Step: 966  | total loss: [1m[32m0.09413[0m[0m | time: 21.500s
[2K
| RMSProp | epoch: 016 | loss: 0.09413 - acc: 0.9679 -- iter: 0672/2003
[A[ATraining Step: 967  | total loss: [1m[32m0.09152[0m[0m | time: 22.414s
[2K
| RMSProp | epoch: 016 | loss: 0.09152 - acc: 0.9680 -- iter: 0704/2003
[A[ATraining Step: 968  | total loss: [1m[32m0.10859[0m[0m | time: 23.432s
[2K
| RMSProp | epoch: 016 | loss: 0.10859 - acc: 0.9618 -- iter: 0736/2003
[A[ATraining Step: 969  | total loss: [1m[32m0.13115[0m[0m | time: 24.442s
[2K
| RMSProp | epoch: 016 | loss: 0.13115 - acc: 0.9531 -- iter: 0768/2003
[A[ATraining Step: 970  | total loss: [1m[32m0.13732[0m[0m | time: 25.407s
[2K
| RMSProp | epoch: 016 | loss: 0.13732 - acc: 0.9516 -- iter: 0800/2003
[A[ATraining Step: 971  | total loss: [1m[32m0.13193[0m[0m | time: 26.150s
[2K
| RMSProp | epoch: 016 | loss: 0.13193 - acc: 0.9533 -- iter: 0832/2003
[A[ATraining Step: 972  | total loss: [1m[32m0.12301[0m[0m | time: 26.989s
[2K
| RMSProp | epoch: 016 | loss: 0.12301 - acc: 0.9580 -- iter: 0864/2003
[A[ATraining Step: 973  | total loss: [1m[32m0.11665[0m[0m | time: 27.846s
[2K
| RMSProp | epoch: 016 | loss: 0.11665 - acc: 0.9559 -- iter: 0896/2003
[A[ATraining Step: 974  | total loss: [1m[32m0.11384[0m[0m | time: 28.843s
[2K
| RMSProp | epoch: 016 | loss: 0.11384 - acc: 0.9572 -- iter: 0928/2003
[A[ATraining Step: 975  | total loss: [1m[32m0.12746[0m[0m | time: 29.749s
[2K
| RMSProp | epoch: 016 | loss: 0.12746 - acc: 0.9490 -- iter: 0960/2003
[A[ATraining Step: 976  | total loss: [1m[32m0.13702[0m[0m | time: 30.833s
[2K
| RMSProp | epoch: 016 | loss: 0.13702 - acc: 0.9447 -- iter: 0992/2003
[A[ATraining Step: 977  | total loss: [1m[32m0.14034[0m[0m | time: 31.767s
[2K
| RMSProp | epoch: 016 | loss: 0.14034 - acc: 0.9471 -- iter: 1024/2003
[A[ATraining Step: 978  | total loss: [1m[32m0.16161[0m[0m | time: 32.631s
[2K
| RMSProp | epoch: 016 | loss: 0.16161 - acc: 0.9399 -- iter: 1056/2003
[A[ATraining Step: 979  | total loss: [1m[32m0.17119[0m[0m | time: 33.620s
[2K
| RMSProp | epoch: 016 | loss: 0.17119 - acc: 0.9334 -- iter: 1088/2003
[A[ATraining Step: 980  | total loss: [1m[32m0.16354[0m[0m | time: 34.608s
[2K
| RMSProp | epoch: 016 | loss: 0.16354 - acc: 0.9369 -- iter: 1120/2003
[A[ATraining Step: 981  | total loss: [1m[32m0.15403[0m[0m | time: 35.626s
[2K
| RMSProp | epoch: 016 | loss: 0.15403 - acc: 0.9401 -- iter: 1152/2003
[A[ATraining Step: 982  | total loss: [1m[32m0.16072[0m[0m | time: 36.344s
[2K
| RMSProp | epoch: 016 | loss: 0.16072 - acc: 0.9367 -- iter: 1184/2003
[A[ATraining Step: 983  | total loss: [1m[32m0.15185[0m[0m | time: 37.166s
[2K
| RMSProp | epoch: 016 | loss: 0.15185 - acc: 0.9431 -- iter: 1216/2003
[A[ATraining Step: 984  | total loss: [1m[32m0.15324[0m[0m | time: 38.129s
[2K
| RMSProp | epoch: 016 | loss: 0.15324 - acc: 0.9425 -- iter: 1248/2003
[A[ATraining Step: 985  | total loss: [1m[32m0.15888[0m[0m | time: 39.110s
[2K
| RMSProp | epoch: 016 | loss: 0.15888 - acc: 0.9389 -- iter: 1280/2003
[A[ATraining Step: 986  | total loss: [1m[32m0.16601[0m[0m | time: 40.030s
[2K
| RMSProp | epoch: 016 | loss: 0.16601 - acc: 0.9325 -- iter: 1312/2003
[A[ATraining Step: 987  | total loss: [1m[32m0.18715[0m[0m | time: 41.031s
[2K
| RMSProp | epoch: 016 | loss: 0.18715 - acc: 0.9267 -- iter: 1344/2003
[A[ATraining Step: 988  | total loss: [1m[32m0.17487[0m[0m | time: 42.210s
[2K
| RMSProp | epoch: 016 | loss: 0.17487 - acc: 0.9341 -- iter: 1376/2003
[A[ATraining Step: 989  | total loss: [1m[32m0.17809[0m[0m | time: 43.547s
[2K
| RMSProp | epoch: 016 | loss: 0.17809 - acc: 0.9313 -- iter: 1408/2003
[A[ATraining Step: 990  | total loss: [1m[32m0.16931[0m[0m | time: 44.891s
[2K
| RMSProp | epoch: 016 | loss: 0.16931 - acc: 0.9350 -- iter: 1440/2003
[A[ATraining Step: 991  | total loss: [1m[32m0.17837[0m[0m | time: 46.148s
[2K
| RMSProp | epoch: 016 | loss: 0.17837 - acc: 0.9322 -- iter: 1472/2003
[A[ATraining Step: 992  | total loss: [1m[32m0.16969[0m[0m | time: 47.358s
[2K
| RMSProp | epoch: 016 | loss: 0.16969 - acc: 0.9389 -- iter: 1504/2003
[A[ATraining Step: 993  | total loss: [1m[32m0.15720[0m[0m | time: 48.702s
[2K
| RMSProp | epoch: 016 | loss: 0.15720 - acc: 0.9450 -- iter: 1536/2003
[A[ATraining Step: 994  | total loss: [1m[32m0.15530[0m[0m | time: 49.856s
[2K
| RMSProp | epoch: 016 | loss: 0.15530 - acc: 0.9443 -- iter: 1568/2003
[A[ATraining Step: 995  | total loss: [1m[32m0.15014[0m[0m | time: 51.121s
[2K
| RMSProp | epoch: 016 | loss: 0.15014 - acc: 0.9467 -- iter: 1600/2003
[A[ATraining Step: 996  | total loss: [1m[32m0.13618[0m[0m | time: 52.283s
[2K
| RMSProp | epoch: 016 | loss: 0.13618 - acc: 0.9521 -- iter: 1632/2003
[A[ATraining Step: 997  | total loss: [1m[32m0.12571[0m[0m | time: 53.606s
[2K
| RMSProp | epoch: 016 | loss: 0.12571 - acc: 0.9569 -- iter: 1664/2003
[A[ATraining Step: 998  | total loss: [1m[32m0.11462[0m[0m | time: 54.988s
[2K
| RMSProp | epoch: 016 | loss: 0.11462 - acc: 0.9612 -- iter: 1696/2003
[A[ATraining Step: 999  | total loss: [1m[32m0.10560[0m[0m | time: 56.041s
[2K
| RMSProp | epoch: 016 | loss: 0.10560 - acc: 0.9651 -- iter: 1728/2003
[A[ATraining Step: 1000  | total loss: [1m[32m0.10864[0m[0m | time: 62.086s
[2K
| RMSProp | epoch: 016 | loss: 0.10864 - acc: 0.9592 | val_loss: 0.55409 - val_acc: 0.8658 -- iter: 1760/2003
--
Training Step: 1001  | total loss: [1m[32m0.10225[0m[0m | time: 63.338s
[2K
| RMSProp | epoch: 016 | loss: 0.10225 - acc: 0.9601 -- iter: 1792/2003
[A[ATraining Step: 1002  | total loss: [1m[32m0.09449[0m[0m | time: 64.494s
[2K
| RMSProp | epoch: 016 | loss: 0.09449 - acc: 0.9641 -- iter: 1824/2003
[A[ATraining Step: 1003  | total loss: [1m[32m0.08748[0m[0m | time: 65.644s
[2K
| RMSProp | epoch: 016 | loss: 0.08748 - acc: 0.9677 -- iter: 1856/2003
[A[ATraining Step: 1004  | total loss: [1m[32m0.10221[0m[0m | time: 66.937s
[2K
| RMSProp | epoch: 016 | loss: 0.10221 - acc: 0.9616 -- iter: 1888/2003
[A[ATraining Step: 1005  | total loss: [1m[32m0.12033[0m[0m | time: 68.342s
[2K
| RMSProp | epoch: 016 | loss: 0.12033 - acc: 0.9560 -- iter: 1920/2003
[A[ATraining Step: 1006  | total loss: [1m[32m0.11565[0m[0m | time: 69.539s
[2K
| RMSProp | epoch: 016 | loss: 0.11565 - acc: 0.9573 -- iter: 1952/2003
[A[ATraining Step: 1007  | total loss: [1m[32m0.14670[0m[0m | time: 70.805s
[2K
| RMSProp | epoch: 016 | loss: 0.14670 - acc: 0.9459 -- iter: 1984/2003
[A[ATraining Step: 1008  | total loss: [1m[32m0.14243[0m[0m | time: 76.690s
[2K
| RMSProp | epoch: 016 | loss: 0.14243 - acc: 0.9420 | val_loss: 0.45769 - val_acc: 0.8403 -- iter: 2003/2003
--
Training Step: 1009  | total loss: [1m[32m0.13836[0m[0m | time: 1.432s
[2K
| RMSProp | epoch: 017 | loss: 0.13836 - acc: 0.9447 -- iter: 0032/2003
[A[ATraining Step: 1010  | total loss: [1m[32m0.12804[0m[0m | time: 2.643s
[2K
| RMSProp | epoch: 017 | loss: 0.12804 - acc: 0.9502 -- iter: 0064/2003
[A[ATraining Step: 1011  | total loss: [1m[32m0.11806[0m[0m | time: 3.881s
[2K
| RMSProp | epoch: 017 | loss: 0.11806 - acc: 0.9552 -- iter: 0096/2003
[A[ATraining Step: 1012  | total loss: [1m[32m0.10912[0m[0m | time: 4.974s
[2K
| RMSProp | epoch: 017 | loss: 0.10912 - acc: 0.9597 -- iter: 0128/2003
[A[ATraining Step: 1013  | total loss: [1m[32m0.10590[0m[0m | time: 6.116s
[2K
| RMSProp | epoch: 017 | loss: 0.10590 - acc: 0.9606 -- iter: 0160/2003
[A[ATraining Step: 1014  | total loss: [1m[32m0.12673[0m[0m | time: 7.261s
[2K
| RMSProp | epoch: 017 | loss: 0.12673 - acc: 0.9520 -- iter: 0192/2003
[A[ATraining Step: 1015  | total loss: [1m[32m0.12030[0m[0m | time: 8.667s
[2K
| RMSProp | epoch: 017 | loss: 0.12030 - acc: 0.9537 -- iter: 0224/2003
[A[ATraining Step: 1016  | total loss: [1m[32m0.11438[0m[0m | time: 10.051s
[2K
| RMSProp | epoch: 017 | loss: 0.11438 - acc: 0.9552 -- iter: 0256/2003
[A[ATraining Step: 1017  | total loss: [1m[32m0.11117[0m[0m | time: 11.328s
[2K
| RMSProp | epoch: 017 | loss: 0.11117 - acc: 0.9565 -- iter: 0288/2003
[A[ATraining Step: 1018  | total loss: [1m[32m0.11308[0m[0m | time: 12.460s
[2K
| RMSProp | epoch: 017 | loss: 0.11308 - acc: 0.9578 -- iter: 0320/2003
[A[ATraining Step: 1019  | total loss: [1m[32m0.11081[0m[0m | time: 13.620s
[2K
| RMSProp | epoch: 017 | loss: 0.11081 - acc: 0.9589 -- iter: 0352/2003
[A[ATraining Step: 1020  | total loss: [1m[32m0.12053[0m[0m | time: 14.824s
[2K
| RMSProp | epoch: 017 | loss: 0.12053 - acc: 0.9567 -- iter: 0384/2003
[A[ATraining Step: 1021  | total loss: [1m[32m0.15388[0m[0m | time: 15.947s
[2K
| RMSProp | epoch: 017 | loss: 0.15388 - acc: 0.9454 -- iter: 0416/2003
[A[ATraining Step: 1022  | total loss: [1m[32m0.15215[0m[0m | time: 17.283s
[2K
| RMSProp | epoch: 017 | loss: 0.15215 - acc: 0.9446 -- iter: 0448/2003
[A[ATraining Step: 1023  | total loss: [1m[32m0.14415[0m[0m | time: 18.182s
[2K
| RMSProp | epoch: 017 | loss: 0.14415 - acc: 0.9470 -- iter: 0480/2003
[A[ATraining Step: 1024  | total loss: [1m[32m0.15344[0m[0m | time: 18.988s
[2K
| RMSProp | epoch: 017 | loss: 0.15344 - acc: 0.9418 -- iter: 0512/2003
[A[ATraining Step: 1025  | total loss: [1m[32m0.13968[0m[0m | time: 20.294s
[2K
| RMSProp | epoch: 017 | loss: 0.13968 - acc: 0.9476 -- iter: 0544/2003
[A[ATraining Step: 1026  | total loss: [1m[32m0.15009[0m[0m | time: 21.262s
[2K
| RMSProp | epoch: 017 | loss: 0.15009 - acc: 0.9466 -- iter: 0576/2003
[A[ATraining Step: 1027  | total loss: [1m[32m0.14377[0m[0m | time: 22.540s
[2K
| RMSProp | epoch: 017 | loss: 0.14377 - acc: 0.9520 -- iter: 0608/2003
[A[ATraining Step: 1028  | total loss: [1m[32m0.13936[0m[0m | time: 23.907s
[2K
| RMSProp | epoch: 017 | loss: 0.13936 - acc: 0.9505 -- iter: 0640/2003
[A[ATraining Step: 1029  | total loss: [1m[32m0.12770[0m[0m | time: 25.267s
[2K
| RMSProp | epoch: 017 | loss: 0.12770 - acc: 0.9555 -- iter: 0672/2003
[A[ATraining Step: 1030  | total loss: [1m[32m0.12362[0m[0m | time: 26.325s
[2K
| RMSProp | epoch: 017 | loss: 0.12362 - acc: 0.9537 -- iter: 0704/2003
[A[ATraining Step: 1031  | total loss: [1m[32m0.13060[0m[0m | time: 27.634s
[2K
| RMSProp | epoch: 017 | loss: 0.13060 - acc: 0.9552 -- iter: 0736/2003
[A[ATraining Step: 1032  | total loss: [1m[32m0.12594[0m[0m | time: 28.934s
[2K
| RMSProp | epoch: 017 | loss: 0.12594 - acc: 0.9565 -- iter: 0768/2003
[A[ATraining Step: 1033  | total loss: [1m[32m0.14119[0m[0m | time: 30.232s
[2K
| RMSProp | epoch: 017 | loss: 0.14119 - acc: 0.9484 -- iter: 0800/2003
[A[ATraining Step: 1034  | total loss: [1m[32m0.13905[0m[0m | time: 31.462s
[2K
| RMSProp | epoch: 017 | loss: 0.13905 - acc: 0.9504 -- iter: 0832/2003
[A[ATraining Step: 1035  | total loss: [1m[32m0.13659[0m[0m | time: 32.749s
[2K
| RMSProp | epoch: 017 | loss: 0.13659 - acc: 0.9522 -- iter: 0864/2003
[A[ATraining Step: 1036  | total loss: [1m[32m0.15250[0m[0m | time: 34.102s
[2K
| RMSProp | epoch: 017 | loss: 0.15250 - acc: 0.9414 -- iter: 0896/2003
[A[ATraining Step: 1037  | total loss: [1m[32m0.15301[0m[0m | time: 35.331s
[2K
| RMSProp | epoch: 017 | loss: 0.15301 - acc: 0.9379 -- iter: 0928/2003
[A[ATraining Step: 1038  | total loss: [1m[32m0.14240[0m[0m | time: 36.488s
[2K
| RMSProp | epoch: 017 | loss: 0.14240 - acc: 0.9441 -- iter: 0960/2003
[A[ATraining Step: 1039  | total loss: [1m[32m0.13660[0m[0m | time: 37.698s
[2K
| RMSProp | epoch: 017 | loss: 0.13660 - acc: 0.9466 -- iter: 0992/2003
[A[ATraining Step: 1040  | total loss: [1m[32m0.13158[0m[0m | time: 38.992s
[2K
| RMSProp | epoch: 017 | loss: 0.13158 - acc: 0.9457 -- iter: 1024/2003
[A[ATraining Step: 1041  | total loss: [1m[32m0.13090[0m[0m | time: 40.212s
[2K
| RMSProp | epoch: 017 | loss: 0.13090 - acc: 0.9480 -- iter: 1056/2003
[A[ATraining Step: 1042  | total loss: [1m[32m0.13983[0m[0m | time: 41.488s
[2K
| RMSProp | epoch: 017 | loss: 0.13983 - acc: 0.9407 -- iter: 1088/2003
[A[ATraining Step: 1043  | total loss: [1m[32m0.12970[0m[0m | time: 42.779s
[2K
| RMSProp | epoch: 017 | loss: 0.12970 - acc: 0.9466 -- iter: 1120/2003
[A[ATraining Step: 1044  | total loss: [1m[32m0.13577[0m[0m | time: 44.098s
[2K
| RMSProp | epoch: 017 | loss: 0.13577 - acc: 0.9457 -- iter: 1152/2003
[A[ATraining Step: 1045  | total loss: [1m[32m0.12992[0m[0m | time: 45.344s
[2K
| RMSProp | epoch: 017 | loss: 0.12992 - acc: 0.9480 -- iter: 1184/2003
[A[ATraining Step: 1046  | total loss: [1m[32m0.13326[0m[0m | time: 46.780s
[2K
| RMSProp | epoch: 017 | loss: 0.13326 - acc: 0.9469 -- iter: 1216/2003
[A[ATraining Step: 1047  | total loss: [1m[32m0.13380[0m[0m | time: 47.944s
[2K
| RMSProp | epoch: 017 | loss: 0.13380 - acc: 0.9491 -- iter: 1248/2003
[A[ATraining Step: 1048  | total loss: [1m[32m0.12317[0m[0m | time: 48.810s
[2K
| RMSProp | epoch: 017 | loss: 0.12317 - acc: 0.9542 -- iter: 1280/2003
[A[ATraining Step: 1049  | total loss: [1m[32m0.11291[0m[0m | time: 49.867s
[2K
| RMSProp | epoch: 017 | loss: 0.11291 - acc: 0.9588 -- iter: 1312/2003
[A[ATraining Step: 1050  | total loss: [1m[32m0.10264[0m[0m | time: 50.950s
[2K
| RMSProp | epoch: 017 | loss: 0.10264 - acc: 0.9629 -- iter: 1344/2003
[A[ATraining Step: 1051  | total loss: [1m[32m0.10040[0m[0m | time: 51.698s
[2K
| RMSProp | epoch: 017 | loss: 0.10040 - acc: 0.9635 -- iter: 1376/2003
[A[ATraining Step: 1052  | total loss: [1m[32m0.12000[0m[0m | time: 52.576s
[2K
| RMSProp | epoch: 017 | loss: 0.12000 - acc: 0.9609 -- iter: 1408/2003
[A[ATraining Step: 1053  | total loss: [1m[32m0.12341[0m[0m | time: 53.500s
[2K
| RMSProp | epoch: 017 | loss: 0.12341 - acc: 0.9586 -- iter: 1440/2003
[A[ATraining Step: 1054  | total loss: [1m[32m0.11995[0m[0m | time: 54.406s
[2K
| RMSProp | epoch: 017 | loss: 0.11995 - acc: 0.9565 -- iter: 1472/2003
[A[ATraining Step: 1055  | total loss: [1m[32m0.12501[0m[0m | time: 55.302s
[2K
| RMSProp | epoch: 017 | loss: 0.12501 - acc: 0.9577 -- iter: 1504/2003
[A[ATraining Step: 1056  | total loss: [1m[32m0.42662[0m[0m | time: 56.306s
[2K
| RMSProp | epoch: 017 | loss: 0.42662 - acc: 0.9088 -- iter: 1536/2003
[A[ATraining Step: 1057  | total loss: [1m[32m0.40003[0m[0m | time: 57.234s
[2K
| RMSProp | epoch: 017 | loss: 0.40003 - acc: 0.9179 -- iter: 1568/2003
[A[ATraining Step: 1058  | total loss: [1m[32m0.37639[0m[0m | time: 58.201s
[2K
| RMSProp | epoch: 017 | loss: 0.37639 - acc: 0.9230 -- iter: 1600/2003
[A[ATraining Step: 1059  | total loss: [1m[32m0.34318[0m[0m | time: 59.335s
[2K
| RMSProp | epoch: 017 | loss: 0.34318 - acc: 0.9307 -- iter: 1632/2003
[A[ATraining Step: 1060  | total loss: [1m[32m0.31279[0m[0m | time: 60.607s
[2K
| RMSProp | epoch: 017 | loss: 0.31279 - acc: 0.9376 -- iter: 1664/2003
[A[ATraining Step: 1061  | total loss: [1m[32m0.28285[0m[0m | time: 62.046s
[2K
| RMSProp | epoch: 017 | loss: 0.28285 - acc: 0.9439 -- iter: 1696/2003
[A[ATraining Step: 1062  | total loss: [1m[32m0.26234[0m[0m | time: 63.281s
[2K
| RMSProp | epoch: 017 | loss: 0.26234 - acc: 0.9464 -- iter: 1728/2003
[A[ATraining Step: 1063  | total loss: [1m[32m0.24062[0m[0m | time: 66.801s
[2K
| RMSProp | epoch: 017 | loss: 0.24062 - acc: 0.9517 -- iter: 1760/2003
[A[ATraining Step: 1064  | total loss: [1m[32m0.21705[0m[0m | time: 67.726s
[2K
| RMSProp | epoch: 017 | loss: 0.21705 - acc: 0.9565 -- iter: 1792/2003
[A[ATraining Step: 1065  | total loss: [1m[32m0.20235[0m[0m | time: 68.884s
[2K
| RMSProp | epoch: 017 | loss: 0.20235 - acc: 0.9578 -- iter: 1824/2003
[A[ATraining Step: 1066  | total loss: [1m[32m0.19114[0m[0m | time: 69.993s
[2K
| RMSProp | epoch: 017 | loss: 0.19114 - acc: 0.9557 -- iter: 1856/2003
[A[ATraining Step: 1067  | total loss: [1m[32m0.18019[0m[0m | time: 71.307s
[2K
| RMSProp | epoch: 017 | loss: 0.18019 - acc: 0.9570 -- iter: 1888/2003
[A[ATraining Step: 1068  | total loss: [1m[32m0.17594[0m[0m | time: 72.540s
[2K
| RMSProp | epoch: 017 | loss: 0.17594 - acc: 0.9582 -- iter: 1920/2003
[A[ATraining Step: 1069  | total loss: [1m[32m0.17539[0m[0m | time: 73.727s
[2K
| RMSProp | epoch: 017 | loss: 0.17539 - acc: 0.9530 -- iter: 1952/2003
[A[ATraining Step: 1070  | total loss: [1m[32m0.15966[0m[0m | time: 75.154s
[2K
| RMSProp | epoch: 017 | loss: 0.15966 - acc: 0.9577 -- iter: 1984/2003
[A[ATraining Step: 1071  | total loss: [1m[32m0.14672[0m[0m | time: 81.346s
[2K
| RMSProp | epoch: 017 | loss: 0.14672 - acc: 0.9619 | val_loss: 0.47357 - val_acc: 0.8626 -- iter: 2003/2003
--
Training Step: 1072  | total loss: [1m[32m0.13244[0m[0m | time: 1.198s
[2K
| RMSProp | epoch: 018 | loss: 0.13244 - acc: 0.9657 -- iter: 0032/2003
[A[ATraining Step: 1073  | total loss: [1m[32m0.12190[0m[0m | time: 2.228s
[2K
| RMSProp | epoch: 018 | loss: 0.12190 - acc: 0.9692 -- iter: 0064/2003
[A[ATraining Step: 1074  | total loss: [1m[32m0.11011[0m[0m | time: 3.502s
[2K
| RMSProp | epoch: 018 | loss: 0.11011 - acc: 0.9723 -- iter: 0096/2003
[A[ATraining Step: 1075  | total loss: [1m[32m0.10146[0m[0m | time: 4.773s
[2K
| RMSProp | epoch: 018 | loss: 0.10146 - acc: 0.9750 -- iter: 0128/2003
[A[ATraining Step: 1076  | total loss: [1m[32m0.09297[0m[0m | time: 6.121s
[2K
| RMSProp | epoch: 018 | loss: 0.09297 - acc: 0.9775 -- iter: 0160/2003
[A[ATraining Step: 1077  | total loss: [1m[32m0.08535[0m[0m | time: 7.435s
[2K
| RMSProp | epoch: 018 | loss: 0.08535 - acc: 0.9798 -- iter: 0192/2003
[A[ATraining Step: 1078  | total loss: [1m[32m0.08095[0m[0m | time: 8.755s
[2K
| RMSProp | epoch: 018 | loss: 0.08095 - acc: 0.9818 -- iter: 0224/2003
[A[ATraining Step: 1079  | total loss: [1m[32m0.13441[0m[0m | time: 9.954s
[2K
| RMSProp | epoch: 018 | loss: 0.13441 - acc: 0.9680 -- iter: 0256/2003
[A[ATraining Step: 1080  | total loss: [1m[32m0.12364[0m[0m | time: 11.331s
[2K
| RMSProp | epoch: 018 | loss: 0.12364 - acc: 0.9712 -- iter: 0288/2003
[A[ATraining Step: 1081  | total loss: [1m[32m0.11641[0m[0m | time: 12.750s
[2K
| RMSProp | epoch: 018 | loss: 0.11641 - acc: 0.9709 -- iter: 0320/2003
[A[ATraining Step: 1082  | total loss: [1m[32m0.10771[0m[0m | time: 13.953s
[2K
| RMSProp | epoch: 018 | loss: 0.10771 - acc: 0.9707 -- iter: 0352/2003
[A[ATraining Step: 1083  | total loss: [1m[32m0.09949[0m[0m | time: 15.215s
[2K
| RMSProp | epoch: 018 | loss: 0.09949 - acc: 0.9737 -- iter: 0384/2003
[A[ATraining Step: 1084  | total loss: [1m[32m0.10494[0m[0m | time: 16.522s
[2K
| RMSProp | epoch: 018 | loss: 0.10494 - acc: 0.9700 -- iter: 0416/2003
[A[ATraining Step: 1085  | total loss: [1m[32m0.11926[0m[0m | time: 17.674s
[2K
| RMSProp | epoch: 018 | loss: 0.11926 - acc: 0.9668 -- iter: 0448/2003
[A[ATraining Step: 1086  | total loss: [1m[32m0.14351[0m[0m | time: 18.780s
[2K
| RMSProp | epoch: 018 | loss: 0.14351 - acc: 0.9576 -- iter: 0480/2003
[A[ATraining Step: 1087  | total loss: [1m[32m0.14690[0m[0m | time: 19.501s
[2K
| RMSProp | epoch: 018 | loss: 0.14690 - acc: 0.9587 -- iter: 0512/2003
[A[ATraining Step: 1088  | total loss: [1m[32m0.13881[0m[0m | time: 20.332s
[2K
| RMSProp | epoch: 018 | loss: 0.13881 - acc: 0.9628 -- iter: 0544/2003
[A[ATraining Step: 1089  | total loss: [1m[32m0.12579[0m[0m | time: 21.653s
[2K
| RMSProp | epoch: 018 | loss: 0.12579 - acc: 0.9666 -- iter: 0576/2003
[A[ATraining Step: 1090  | total loss: [1m[32m0.11454[0m[0m | time: 22.948s
[2K
| RMSProp | epoch: 018 | loss: 0.11454 - acc: 0.9699 -- iter: 0608/2003
[A[ATraining Step: 1091  | total loss: [1m[32m0.11971[0m[0m | time: 24.127s
[2K
| RMSProp | epoch: 018 | loss: 0.11971 - acc: 0.9667 -- iter: 0640/2003
[A[ATraining Step: 1092  | total loss: [1m[32m0.11560[0m[0m | time: 25.459s
[2K
| RMSProp | epoch: 018 | loss: 0.11560 - acc: 0.9638 -- iter: 0672/2003
[A[ATraining Step: 1093  | total loss: [1m[32m0.10622[0m[0m | time: 26.794s
[2K
| RMSProp | epoch: 018 | loss: 0.10622 - acc: 0.9674 -- iter: 0704/2003
[A[ATraining Step: 1094  | total loss: [1m[32m0.10944[0m[0m | time: 28.027s
[2K
| RMSProp | epoch: 018 | loss: 0.10944 - acc: 0.9644 -- iter: 0736/2003
[A[ATraining Step: 1095  | total loss: [1m[32m0.10868[0m[0m | time: 29.113s
[2K
| RMSProp | epoch: 018 | loss: 0.10868 - acc: 0.9648 -- iter: 0768/2003
[A[ATraining Step: 1096  | total loss: [1m[32m0.09837[0m[0m | time: 30.300s
[2K
| RMSProp | epoch: 018 | loss: 0.09837 - acc: 0.9683 -- iter: 0800/2003
[A[ATraining Step: 1097  | total loss: [1m[32m0.11024[0m[0m | time: 31.611s
[2K
| RMSProp | epoch: 018 | loss: 0.11024 - acc: 0.9653 -- iter: 0832/2003
[A[ATraining Step: 1098  | total loss: [1m[32m0.13462[0m[0m | time: 32.913s
[2K
| RMSProp | epoch: 018 | loss: 0.13462 - acc: 0.9531 -- iter: 0864/2003
[A[ATraining Step: 1099  | total loss: [1m[32m0.12907[0m[0m | time: 34.059s
[2K
| RMSProp | epoch: 018 | loss: 0.12907 - acc: 0.9547 -- iter: 0896/2003
[A[ATraining Step: 1100  | total loss: [1m[32m0.13275[0m[0m | time: 35.234s
[2K
| RMSProp | epoch: 018 | loss: 0.13275 - acc: 0.9530 -- iter: 0928/2003
[A[ATraining Step: 1101  | total loss: [1m[32m0.12777[0m[0m | time: 36.475s
[2K
| RMSProp | epoch: 018 | loss: 0.12777 - acc: 0.9545 -- iter: 0960/2003
[A[ATraining Step: 1102  | total loss: [1m[32m0.11879[0m[0m | time: 37.678s
[2K
| RMSProp | epoch: 018 | loss: 0.11879 - acc: 0.9591 -- iter: 0992/2003
[A[ATraining Step: 1103  | total loss: [1m[32m0.10888[0m[0m | time: 38.875s
[2K
| RMSProp | epoch: 018 | loss: 0.10888 - acc: 0.9632 -- iter: 1024/2003
[A[ATraining Step: 1104  | total loss: [1m[32m0.11120[0m[0m | time: 40.221s
[2K
| RMSProp | epoch: 018 | loss: 0.11120 - acc: 0.9637 -- iter: 1056/2003
[A[ATraining Step: 1105  | total loss: [1m[32m0.11480[0m[0m | time: 41.471s
[2K
| RMSProp | epoch: 018 | loss: 0.11480 - acc: 0.9611 -- iter: 1088/2003
[A[ATraining Step: 1106  | total loss: [1m[32m0.11516[0m[0m | time: 42.742s
[2K
| RMSProp | epoch: 018 | loss: 0.11516 - acc: 0.9587 -- iter: 1120/2003
[A[ATraining Step: 1107  | total loss: [1m[32m0.13506[0m[0m | time: 43.941s
[2K
| RMSProp | epoch: 018 | loss: 0.13506 - acc: 0.9535 -- iter: 1152/2003
[A[ATraining Step: 1108  | total loss: [1m[32m0.12669[0m[0m | time: 45.200s
[2K
| RMSProp | epoch: 018 | loss: 0.12669 - acc: 0.9550 -- iter: 1184/2003
[A[ATraining Step: 1109  | total loss: [1m[32m0.11719[0m[0m | time: 46.593s
[2K
| RMSProp | epoch: 018 | loss: 0.11719 - acc: 0.9595 -- iter: 1216/2003
[A[ATraining Step: 1110  | total loss: [1m[32m0.11531[0m[0m | time: 47.873s
[2K
| RMSProp | epoch: 018 | loss: 0.11531 - acc: 0.9604 -- iter: 1248/2003
[A[ATraining Step: 1111  | total loss: [1m[32m0.11499[0m[0m | time: 49.225s
[2K
| RMSProp | epoch: 018 | loss: 0.11499 - acc: 0.9550 -- iter: 1280/2003
[A[ATraining Step: 1112  | total loss: [1m[32m0.11851[0m[0m | time: 50.473s
[2K
| RMSProp | epoch: 018 | loss: 0.11851 - acc: 0.9533 -- iter: 1312/2003
[A[ATraining Step: 1113  | total loss: [1m[32m0.11388[0m[0m | time: 51.658s
[2K
| RMSProp | epoch: 018 | loss: 0.11388 - acc: 0.9548 -- iter: 1344/2003
[A[ATraining Step: 1114  | total loss: [1m[32m0.10473[0m[0m | time: 52.873s
[2K
| RMSProp | epoch: 018 | loss: 0.10473 - acc: 0.9593 -- iter: 1376/2003
[A[ATraining Step: 1115  | total loss: [1m[32m0.09912[0m[0m | time: 54.309s
[2K
| RMSProp | epoch: 018 | loss: 0.09912 - acc: 0.9634 -- iter: 1408/2003
[A[ATraining Step: 1116  | total loss: [1m[32m0.09058[0m[0m | time: 55.584s
[2K
| RMSProp | epoch: 018 | loss: 0.09058 - acc: 0.9671 -- iter: 1440/2003
[A[ATraining Step: 1117  | total loss: [1m[32m0.11354[0m[0m | time: 56.724s
[2K
| RMSProp | epoch: 018 | loss: 0.11354 - acc: 0.9641 -- iter: 1472/2003
[A[ATraining Step: 1118  | total loss: [1m[32m0.13104[0m[0m | time: 57.949s
[2K
| RMSProp | epoch: 018 | loss: 0.13104 - acc: 0.9583 -- iter: 1504/2003
[A[ATraining Step: 1119  | total loss: [1m[32m0.12861[0m[0m | time: 59.224s
[2K
| RMSProp | epoch: 018 | loss: 0.12861 - acc: 0.9562 -- iter: 1536/2003
[A[ATraining Step: 1120  | total loss: [1m[32m0.16306[0m[0m | time: 60.492s
[2K
| RMSProp | epoch: 018 | loss: 0.16306 - acc: 0.9544 -- iter: 1568/2003
[A[ATraining Step: 1121  | total loss: [1m[32m0.15318[0m[0m | time: 61.711s
[2K
| RMSProp | epoch: 018 | loss: 0.15318 - acc: 0.9589 -- iter: 1600/2003
[A[ATraining Step: 1122  | total loss: [1m[32m0.14365[0m[0m | time: 62.973s
[2K
| RMSProp | epoch: 018 | loss: 0.14365 - acc: 0.9630 -- iter: 1632/2003
[A[ATraining Step: 1123  | total loss: [1m[32m0.13085[0m[0m | time: 64.496s
[2K
| RMSProp | epoch: 018 | loss: 0.13085 - acc: 0.9667 -- iter: 1664/2003
[A[ATraining Step: 1124  | total loss: [1m[32m0.11895[0m[0m | time: 65.784s
[2K
| RMSProp | epoch: 018 | loss: 0.11895 - acc: 0.9701 -- iter: 1696/2003
[A[ATraining Step: 1125  | total loss: [1m[32m0.10796[0m[0m | time: 66.991s
[2K
| RMSProp | epoch: 018 | loss: 0.10796 - acc: 0.9731 -- iter: 1728/2003
[A[ATraining Step: 1126  | total loss: [1m[32m0.10555[0m[0m | time: 68.083s
[2K
| RMSProp | epoch: 018 | loss: 0.10555 - acc: 0.9695 -- iter: 1760/2003
[A[ATraining Step: 1127  | total loss: [1m[32m0.12552[0m[0m | time: 69.203s
[2K
| RMSProp | epoch: 018 | loss: 0.12552 - acc: 0.9600 -- iter: 1792/2003
[A[ATraining Step: 1128  | total loss: [1m[32m0.11725[0m[0m | time: 70.209s
[2K
| RMSProp | epoch: 018 | loss: 0.11725 - acc: 0.9640 -- iter: 1824/2003
[A[ATraining Step: 1129  | total loss: [1m[32m0.10637[0m[0m | time: 71.485s
[2K
| RMSProp | epoch: 018 | loss: 0.10637 - acc: 0.9676 -- iter: 1856/2003
[A[ATraining Step: 1130  | total loss: [1m[32m0.09817[0m[0m | time: 72.668s
[2K
| RMSProp | epoch: 018 | loss: 0.09817 - acc: 0.9709 -- iter: 1888/2003
[A[ATraining Step: 1131  | total loss: [1m[32m0.09090[0m[0m | time: 73.847s
[2K
| RMSProp | epoch: 018 | loss: 0.09090 - acc: 0.9738 -- iter: 1920/2003
[A[ATraining Step: 1132  | total loss: [1m[32m0.08328[0m[0m | time: 74.922s
[2K
| RMSProp | epoch: 018 | loss: 0.08328 - acc: 0.9764 -- iter: 1952/2003
[A[ATraining Step: 1133  | total loss: [1m[32m0.07588[0m[0m | time: 76.023s
[2K
| RMSProp | epoch: 018 | loss: 0.07588 - acc: 0.9788 -- iter: 1984/2003
[A[ATraining Step: 1134  | total loss: [1m[32m0.06850[0m[0m | time: 81.968s
[2K
| RMSProp | epoch: 018 | loss: 0.06850 - acc: 0.9809 | val_loss: 0.55594 - val_acc: 0.8834 -- iter: 2003/2003
--
Training Step: 1135  | total loss: [1m[32m0.06204[0m[0m | time: 1.191s
[2K
| RMSProp | epoch: 019 | loss: 0.06204 - acc: 0.9828 -- iter: 0032/2003
[A[ATraining Step: 1136  | total loss: [1m[32m0.05598[0m[0m | time: 2.442s
[2K
| RMSProp | epoch: 019 | loss: 0.05598 - acc: 0.9845 -- iter: 0064/2003
[A[ATraining Step: 1137  | total loss: [1m[32m0.07028[0m[0m | time: 3.643s
[2K
| RMSProp | epoch: 019 | loss: 0.07028 - acc: 0.9829 -- iter: 0096/2003
[A[ATraining Step: 1138  | total loss: [1m[32m0.12999[0m[0m | time: 4.758s
[2K
| RMSProp | epoch: 019 | loss: 0.12999 - acc: 0.9690 -- iter: 0128/2003
[A[ATraining Step: 1139  | total loss: [1m[32m0.13269[0m[0m | time: 5.926s
[2K
| RMSProp | epoch: 019 | loss: 0.13269 - acc: 0.9659 -- iter: 0160/2003
[A[ATraining Step: 1140  | total loss: [1m[32m0.12080[0m[0m | time: 7.265s
[2K
| RMSProp | epoch: 019 | loss: 0.12080 - acc: 0.9693 -- iter: 0192/2003
[A[ATraining Step: 1141  | total loss: [1m[32m0.11098[0m[0m | time: 8.380s
[2K
| RMSProp | epoch: 019 | loss: 0.11098 - acc: 0.9724 -- iter: 0224/2003
[A[ATraining Step: 1142  | total loss: [1m[32m0.10115[0m[0m | time: 9.629s
[2K
| RMSProp | epoch: 019 | loss: 0.10115 - acc: 0.9751 -- iter: 0256/2003
[A[ATraining Step: 1143  | total loss: [1m[32m0.09201[0m[0m | time: 11.073s
[2K
| RMSProp | epoch: 019 | loss: 0.09201 - acc: 0.9776 -- iter: 0288/2003
[A[ATraining Step: 1144  | total loss: [1m[32m0.08358[0m[0m | time: 12.372s
[2K
| RMSProp | epoch: 019 | loss: 0.08358 - acc: 0.9798 -- iter: 0320/2003
[A[ATraining Step: 1145  | total loss: [1m[32m0.07550[0m[0m | time: 13.596s
[2K
| RMSProp | epoch: 019 | loss: 0.07550 - acc: 0.9819 -- iter: 0352/2003
[A[ATraining Step: 1146  | total loss: [1m[32m0.07790[0m[0m | time: 14.879s
[2K
| RMSProp | epoch: 019 | loss: 0.07790 - acc: 0.9774 -- iter: 0384/2003
[A[ATraining Step: 1147  | total loss: [1m[32m0.11558[0m[0m | time: 16.178s
[2K
| RMSProp | epoch: 019 | loss: 0.11558 - acc: 0.9641 -- iter: 0416/2003
[A[ATraining Step: 1148  | total loss: [1m[32m0.11561[0m[0m | time: 17.320s
[2K
| RMSProp | epoch: 019 | loss: 0.11561 - acc: 0.9645 -- iter: 0448/2003
[A[ATraining Step: 1149  | total loss: [1m[32m0.10835[0m[0m | time: 18.540s
[2K
| RMSProp | epoch: 019 | loss: 0.10835 - acc: 0.9650 -- iter: 0480/2003
[A[ATraining Step: 1150  | total loss: [1m[32m0.10122[0m[0m | time: 19.769s
[2K
| RMSProp | epoch: 019 | loss: 0.10122 - acc: 0.9653 -- iter: 0512/2003
[A[ATraining Step: 1151  | total loss: [1m[32m0.09804[0m[0m | time: 20.637s
[2K
| RMSProp | epoch: 019 | loss: 0.09804 - acc: 0.9625 -- iter: 0544/2003
[A[ATraining Step: 1152  | total loss: [1m[32m0.12337[0m[0m | time: 21.533s
[2K
| RMSProp | epoch: 019 | loss: 0.12337 - acc: 0.9610 -- iter: 0576/2003
[A[ATraining Step: 1153  | total loss: [1m[32m0.11312[0m[0m | time: 22.682s
[2K
| RMSProp | epoch: 019 | loss: 0.11312 - acc: 0.9649 -- iter: 0608/2003
[A[ATraining Step: 1154  | total loss: [1m[32m0.10734[0m[0m | time: 23.964s
[2K
| RMSProp | epoch: 019 | loss: 0.10734 - acc: 0.9653 -- iter: 0640/2003
[A[ATraining Step: 1155  | total loss: [1m[32m0.10567[0m[0m | time: 25.175s
[2K
| RMSProp | epoch: 019 | loss: 0.10567 - acc: 0.9657 -- iter: 0672/2003
[A[ATraining Step: 1156  | total loss: [1m[32m0.10544[0m[0m | time: 26.589s
[2K
| RMSProp | epoch: 019 | loss: 0.10544 - acc: 0.9628 -- iter: 0704/2003
[A[ATraining Step: 1157  | total loss: [1m[32m0.10343[0m[0m | time: 27.834s
[2K
| RMSProp | epoch: 019 | loss: 0.10343 - acc: 0.9603 -- iter: 0736/2003
[A[ATraining Step: 1158  | total loss: [1m[32m0.11843[0m[0m | time: 29.170s
[2K
| RMSProp | epoch: 019 | loss: 0.11843 - acc: 0.9486 -- iter: 0768/2003
[A[ATraining Step: 1159  | total loss: [1m[32m0.12031[0m[0m | time: 30.648s
[2K
| RMSProp | epoch: 019 | loss: 0.12031 - acc: 0.9475 -- iter: 0800/2003
[A[ATraining Step: 1160  | total loss: [1m[32m0.13623[0m[0m | time: 32.054s
[2K
| RMSProp | epoch: 019 | loss: 0.13623 - acc: 0.9434 -- iter: 0832/2003
[A[ATraining Step: 1161  | total loss: [1m[32m0.12589[0m[0m | time: 33.403s
[2K
| RMSProp | epoch: 019 | loss: 0.12589 - acc: 0.9491 -- iter: 0864/2003
[A[ATraining Step: 1162  | total loss: [1m[32m0.11640[0m[0m | time: 34.459s
[2K
| RMSProp | epoch: 019 | loss: 0.11640 - acc: 0.9542 -- iter: 0896/2003
[A[ATraining Step: 1163  | total loss: [1m[32m0.11940[0m[0m | time: 35.349s
[2K
| RMSProp | epoch: 019 | loss: 0.11940 - acc: 0.9556 -- iter: 0928/2003
[A[ATraining Step: 1164  | total loss: [1m[32m0.11230[0m[0m | time: 36.295s
[2K
| RMSProp | epoch: 019 | loss: 0.11230 - acc: 0.9601 -- iter: 0960/2003
[A[ATraining Step: 1165  | total loss: [1m[32m0.10286[0m[0m | time: 37.204s
[2K
| RMSProp | epoch: 019 | loss: 0.10286 - acc: 0.9641 -- iter: 0992/2003
[A[ATraining Step: 1166  | total loss: [1m[32m0.09416[0m[0m | time: 38.155s
[2K
| RMSProp | epoch: 019 | loss: 0.09416 - acc: 0.9676 -- iter: 1024/2003
[A[ATraining Step: 1167  | total loss: [1m[32m0.08811[0m[0m | time: 39.116s
[2K
| RMSProp | epoch: 019 | loss: 0.08811 - acc: 0.9678 -- iter: 1056/2003
[A[ATraining Step: 1168  | total loss: [1m[32m0.08772[0m[0m | time: 40.035s
[2K
| RMSProp | epoch: 019 | loss: 0.08772 - acc: 0.9647 -- iter: 1088/2003
[A[ATraining Step: 1169  | total loss: [1m[32m0.09035[0m[0m | time: 41.051s
[2K
| RMSProp | epoch: 019 | loss: 0.09035 - acc: 0.9620 -- iter: 1120/2003
[A[ATraining Step: 1170  | total loss: [1m[32m0.08555[0m[0m | time: 41.957s
[2K
| RMSProp | epoch: 019 | loss: 0.08555 - acc: 0.9658 -- iter: 1152/2003
[A[ATraining Step: 1171  | total loss: [1m[32m0.08381[0m[0m | time: 43.056s
[2K
| RMSProp | epoch: 019 | loss: 0.08381 - acc: 0.9630 -- iter: 1184/2003
[A[ATraining Step: 1172  | total loss: [1m[32m0.08575[0m[0m | time: 44.100s
[2K
| RMSProp | epoch: 019 | loss: 0.08575 - acc: 0.9604 -- iter: 1216/2003
[A[ATraining Step: 1173  | total loss: [1m[32m0.07769[0m[0m | time: 44.933s
[2K
| RMSProp | epoch: 019 | loss: 0.07769 - acc: 0.9644 -- iter: 1248/2003
[A[ATraining Step: 1174  | total loss: [1m[32m0.07071[0m[0m | time: 45.834s
[2K
| RMSProp | epoch: 019 | loss: 0.07071 - acc: 0.9679 -- iter: 1280/2003
[A[ATraining Step: 1175  | total loss: [1m[32m0.07458[0m[0m | time: 46.731s
[2K
| RMSProp | epoch: 019 | loss: 0.07458 - acc: 0.9680 -- iter: 1312/2003
[A[ATraining Step: 1176  | total loss: [1m[32m0.07503[0m[0m | time: 47.595s
[2K
| RMSProp | epoch: 019 | loss: 0.07503 - acc: 0.9681 -- iter: 1344/2003
[A[ATraining Step: 1177  | total loss: [1m[32m0.07729[0m[0m | time: 48.543s
[2K
| RMSProp | epoch: 019 | loss: 0.07729 - acc: 0.9682 -- iter: 1376/2003
[A[ATraining Step: 1178  | total loss: [1m[32m0.08939[0m[0m | time: 49.497s
[2K
| RMSProp | epoch: 019 | loss: 0.08939 - acc: 0.9620 -- iter: 1408/2003
[A[ATraining Step: 1179  | total loss: [1m[32m0.08323[0m[0m | time: 50.431s
[2K
| RMSProp | epoch: 019 | loss: 0.08323 - acc: 0.9658 -- iter: 1440/2003
[A[ATraining Step: 1180  | total loss: [1m[32m0.07638[0m[0m | time: 51.415s
[2K
| RMSProp | epoch: 019 | loss: 0.07638 - acc: 0.9692 -- iter: 1472/2003
[A[ATraining Step: 1181  | total loss: [1m[32m0.06994[0m[0m | time: 52.367s
[2K
| RMSProp | epoch: 019 | loss: 0.06994 - acc: 0.9723 -- iter: 1504/2003
[A[ATraining Step: 1182  | total loss: [1m[32m0.06471[0m[0m | time: 53.508s
[2K
| RMSProp | epoch: 019 | loss: 0.06471 - acc: 0.9751 -- iter: 1536/2003
[A[ATraining Step: 1183  | total loss: [1m[32m0.05879[0m[0m | time: 54.544s
[2K
| RMSProp | epoch: 019 | loss: 0.05879 - acc: 0.9775 -- iter: 1568/2003
[A[ATraining Step: 1184  | total loss: [1m[32m0.10211[0m[0m | time: 55.289s
[2K
| RMSProp | epoch: 019 | loss: 0.10211 - acc: 0.9735 -- iter: 1600/2003
[A[ATraining Step: 1185  | total loss: [1m[32m0.09487[0m[0m | time: 56.420s
[2K
| RMSProp | epoch: 019 | loss: 0.09487 - acc: 0.9762 -- iter: 1632/2003
[A[ATraining Step: 1186  | total loss: [1m[32m0.08865[0m[0m | time: 57.687s
[2K
| RMSProp | epoch: 019 | loss: 0.08865 - acc: 0.9754 -- iter: 1664/2003
[A[ATraining Step: 1187  | total loss: [1m[32m0.08451[0m[0m | time: 58.989s
[2K
| RMSProp | epoch: 019 | loss: 0.08451 - acc: 0.9779 -- iter: 1696/2003
[A[ATraining Step: 1188  | total loss: [1m[32m0.08455[0m[0m | time: 60.298s
[2K
| RMSProp | epoch: 019 | loss: 0.08455 - acc: 0.9770 -- iter: 1728/2003
[A[ATraining Step: 1189  | total loss: [1m[32m0.07838[0m[0m | time: 61.458s
[2K
| RMSProp | epoch: 019 | loss: 0.07838 - acc: 0.9793 -- iter: 1760/2003
[A[ATraining Step: 1190  | total loss: [1m[32m0.07665[0m[0m | time: 62.751s
[2K
| RMSProp | epoch: 019 | loss: 0.07665 - acc: 0.9782 -- iter: 1792/2003
[A[ATraining Step: 1191  | total loss: [1m[32m0.07117[0m[0m | time: 64.051s
[2K
| RMSProp | epoch: 019 | loss: 0.07117 - acc: 0.9804 -- iter: 1824/2003
[A[ATraining Step: 1192  | total loss: [1m[32m0.06588[0m[0m | time: 65.330s
[2K
| RMSProp | epoch: 019 | loss: 0.06588 - acc: 0.9824 -- iter: 1856/2003
[A[ATraining Step: 1193  | total loss: [1m[32m0.06036[0m[0m | time: 66.531s
[2K
| RMSProp | epoch: 019 | loss: 0.06036 - acc: 0.9841 -- iter: 1888/2003
[A[ATraining Step: 1194  | total loss: [1m[32m0.07185[0m[0m | time: 67.864s
[2K
| RMSProp | epoch: 019 | loss: 0.07185 - acc: 0.9826 -- iter: 1920/2003
[A[ATraining Step: 1195  | total loss: [1m[32m0.06531[0m[0m | time: 69.214s
[2K
| RMSProp | epoch: 019 | loss: 0.06531 - acc: 0.9843 -- iter: 1952/2003
[A[ATraining Step: 1196  | total loss: [1m[32m0.05950[0m[0m | time: 70.304s
[2K
| RMSProp | epoch: 019 | loss: 0.05950 - acc: 0.9859 -- iter: 1984/2003
[A[ATraining Step: 1197  | total loss: [1m[32m0.05426[0m[0m | time: 76.486s
[2K
| RMSProp | epoch: 019 | loss: 0.05426 - acc: 0.9873 | val_loss: 0.89087 - val_acc: 0.7460 -- iter: 2003/2003
--
Training Step: 1198  | total loss: [1m[32m0.06309[0m[0m | time: 1.152s
[2K
| RMSProp | epoch: 020 | loss: 0.06309 - acc: 0.9855 -- iter: 0032/2003
[A[ATraining Step: 1199  | total loss: [1m[32m0.12493[0m[0m | time: 2.430s
[2K
| RMSProp | epoch: 020 | loss: 0.12493 - acc: 0.9650 -- iter: 0064/2003
[A[ATraining Step: 1200  | total loss: [1m[32m0.14325[0m[0m | time: 8.710s
[2K
| RMSProp | epoch: 020 | loss: 0.14325 - acc: 0.9592 | val_loss: 0.46131 - val_acc: 0.8450 -- iter: 0096/2003
--
Training Step: 1201  | total loss: [1m[32m0.13019[0m[0m | time: 10.162s
[2K
| RMSProp | epoch: 020 | loss: 0.13019 - acc: 0.9632 -- iter: 0128/2003
[A[ATraining Step: 1202  | total loss: [1m[32m0.12007[0m[0m | time: 11.510s
[2K
| RMSProp | epoch: 020 | loss: 0.12007 - acc: 0.9669 -- iter: 0160/2003
[A[ATraining Step: 1203  | total loss: [1m[32m0.11369[0m[0m | time: 12.696s
[2K
| RMSProp | epoch: 020 | loss: 0.11369 - acc: 0.9671 -- iter: 0192/2003
[A[ATraining Step: 1204  | total loss: [1m[32m0.10338[0m[0m | time: 13.790s
[2K
| RMSProp | epoch: 020 | loss: 0.10338 - acc: 0.9704 -- iter: 0224/2003
[A[ATraining Step: 1205  | total loss: [1m[32m0.10939[0m[0m | time: 14.988s
[2K
| RMSProp | epoch: 020 | loss: 0.10939 - acc: 0.9702 -- iter: 0256/2003
[A[ATraining Step: 1206  | total loss: [1m[32m0.09993[0m[0m | time: 16.178s
[2K
| RMSProp | epoch: 020 | loss: 0.09993 - acc: 0.9732 -- iter: 0288/2003
[A[ATraining Step: 1207  | total loss: [1m[32m0.09112[0m[0m | time: 17.514s
[2K
| RMSProp | epoch: 020 | loss: 0.09112 - acc: 0.9759 -- iter: 0320/2003
[A[ATraining Step: 1208  | total loss: [1m[32m0.08280[0m[0m | time: 18.922s
[2K
| RMSProp | epoch: 020 | loss: 0.08280 - acc: 0.9783 -- iter: 0352/2003
[A[ATraining Step: 1209  | total loss: [1m[32m0.07547[0m[0m | time: 20.142s
[2K
| RMSProp | epoch: 020 | loss: 0.07547 - acc: 0.9805 -- iter: 0384/2003
[A[ATraining Step: 1210  | total loss: [1m[32m0.06824[0m[0m | time: 21.468s
[2K
| RMSProp | epoch: 020 | loss: 0.06824 - acc: 0.9824 -- iter: 0416/2003
[A[ATraining Step: 1211  | total loss: [1m[32m0.06154[0m[0m | time: 22.702s
[2K
| RMSProp | epoch: 020 | loss: 0.06154 - acc: 0.9842 -- iter: 0448/2003
[A[ATraining Step: 1212  | total loss: [1m[32m0.05552[0m[0m | time: 23.920s
[2K
| RMSProp | epoch: 020 | loss: 0.05552 - acc: 0.9858 -- iter: 0480/2003
[A[ATraining Step: 1213  | total loss: [1m[32m0.05211[0m[0m | time: 25.270s
[2K
| RMSProp | epoch: 020 | loss: 0.05211 - acc: 0.9872 -- iter: 0512/2003
[A[ATraining Step: 1214  | total loss: [1m[32m0.05054[0m[0m | time: 26.661s
[2K
| RMSProp | epoch: 020 | loss: 0.05054 - acc: 0.9853 -- iter: 0544/2003
[A[ATraining Step: 1215  | total loss: [1m[32m0.04576[0m[0m | time: 27.368s
[2K
| RMSProp | epoch: 020 | loss: 0.04576 - acc: 0.9868 -- iter: 0576/2003
[A[ATraining Step: 1216  | total loss: [1m[32m0.04502[0m[0m | time: 29.392s
[2K
| RMSProp | epoch: 020 | loss: 0.04502 - acc: 0.9829 -- iter: 0608/2003
[A[ATraining Step: 1217  | total loss: [1m[32m0.08509[0m[0m | time: 30.433s
[2K
| RMSProp | epoch: 020 | loss: 0.08509 - acc: 0.9688 -- iter: 0640/2003
[A[ATraining Step: 1218  | total loss: [1m[32m0.11408[0m[0m | time: 31.658s
[2K
| RMSProp | epoch: 020 | loss: 0.11408 - acc: 0.9594 -- iter: 0672/2003
[A[ATraining Step: 1219  | total loss: [1m[32m0.14886[0m[0m | time: 32.966s
[2K
| RMSProp | epoch: 020 | loss: 0.14886 - acc: 0.9510 -- iter: 0704/2003
[A[ATraining Step: 1220  | total loss: [1m[32m0.15604[0m[0m | time: 34.233s
[2K
| RMSProp | epoch: 020 | loss: 0.15604 - acc: 0.9434 -- iter: 0736/2003
[A[ATraining Step: 1221  | total loss: [1m[32m0.14625[0m[0m | time: 35.572s
[2K
| RMSProp | epoch: 020 | loss: 0.14625 - acc: 0.9490 -- iter: 0768/2003
[A[ATraining Step: 1222  | total loss: [1m[32m0.14945[0m[0m | time: 37.063s
[2K
| RMSProp | epoch: 020 | loss: 0.14945 - acc: 0.9479 -- iter: 0800/2003
[A[ATraining Step: 1223  | total loss: [1m[32m0.14455[0m[0m | time: 38.359s
[2K
| RMSProp | epoch: 020 | loss: 0.14455 - acc: 0.9437 -- iter: 0832/2003
[A[ATraining Step: 1224  | total loss: [1m[32m0.13532[0m[0m | time: 39.601s
[2K
| RMSProp | epoch: 020 | loss: 0.13532 - acc: 0.9462 -- iter: 0864/2003
[A[ATraining Step: 1225  | total loss: [1m[32m0.13146[0m[0m | time: 40.870s
[2K
| RMSProp | epoch: 020 | loss: 0.13146 - acc: 0.9485 -- iter: 0896/2003
[A[ATraining Step: 1226  | total loss: [1m[32m0.12422[0m[0m | time: 42.049s
[2K
| RMSProp | epoch: 020 | loss: 0.12422 - acc: 0.9505 -- iter: 0928/2003
[A[ATraining Step: 1227  | total loss: [1m[32m0.11476[0m[0m | time: 43.262s
[2K
| RMSProp | epoch: 020 | loss: 0.11476 - acc: 0.9555 -- iter: 0960/2003
[A[ATraining Step: 1228  | total loss: [1m[32m0.10441[0m[0m | time: 44.513s
[2K
| RMSProp | epoch: 020 | loss: 0.10441 - acc: 0.9599 -- iter: 0992/2003
[A[ATraining Step: 1229  | total loss: [1m[32m0.09449[0m[0m | time: 45.692s
[2K
| RMSProp | epoch: 020 | loss: 0.09449 - acc: 0.9639 -- iter: 1024/2003
[A[ATraining Step: 1230  | total loss: [1m[32m0.08577[0m[0m | time: 46.754s
[2K
| RMSProp | epoch: 020 | loss: 0.08577 - acc: 0.9675 -- iter: 1056/2003
[A[ATraining Step: 1231  | total loss: [1m[32m0.09562[0m[0m | time: 48.065s
[2K
| RMSProp | epoch: 020 | loss: 0.09562 - acc: 0.9645 -- iter: 1088/2003
[A[ATraining Step: 1232  | total loss: [1m[32m0.09932[0m[0m | time: 49.300s
[2K
| RMSProp | epoch: 020 | loss: 0.09932 - acc: 0.9649 -- iter: 1120/2003
[A[ATraining Step: 1233  | total loss: [1m[32m0.09401[0m[0m | time: 50.517s
[2K
| RMSProp | epoch: 020 | loss: 0.09401 - acc: 0.9684 -- iter: 1152/2003
[A[ATraining Step: 1234  | total loss: [1m[32m0.08607[0m[0m | time: 51.847s
[2K
| RMSProp | epoch: 020 | loss: 0.08607 - acc: 0.9716 -- iter: 1184/2003
[A[ATraining Step: 1235  | total loss: [1m[32m0.07828[0m[0m | time: 53.155s
[2K
| RMSProp | epoch: 020 | loss: 0.07828 - acc: 0.9744 -- iter: 1216/2003
[A[ATraining Step: 1236  | total loss: [1m[32m0.07085[0m[0m | time: 54.377s
[2K
| RMSProp | epoch: 020 | loss: 0.07085 - acc: 0.9770 -- iter: 1248/2003
[A[ATraining Step: 1237  | total loss: [1m[32m0.06518[0m[0m | time: 55.667s
[2K
| RMSProp | epoch: 020 | loss: 0.06518 - acc: 0.9793 -- iter: 1280/2003
[A[ATraining Step: 1238  | total loss: [1m[32m0.06059[0m[0m | time: 56.976s
[2K
| RMSProp | epoch: 020 | loss: 0.06059 - acc: 0.9814 -- iter: 1312/2003
[A[ATraining Step: 1239  | total loss: [1m[32m0.05511[0m[0m | time: 58.002s
[2K
| RMSProp | epoch: 020 | loss: 0.05511 - acc: 0.9832 -- iter: 1344/2003
[A[ATraining Step: 1240  | total loss: [1m[32m0.06041[0m[0m | time: 59.226s
[2K
| RMSProp | epoch: 020 | loss: 0.06041 - acc: 0.9818 -- iter: 1376/2003
[A[ATraining Step: 1241  | total loss: [1m[32m0.08193[0m[0m | time: 60.591s
[2K
| RMSProp | epoch: 020 | loss: 0.08193 - acc: 0.9774 -- iter: 1408/2003
[A[ATraining Step: 1242  | total loss: [1m[32m0.07950[0m[0m | time: 61.827s
[2K
| RMSProp | epoch: 020 | loss: 0.07950 - acc: 0.9796 -- iter: 1440/2003
[A[ATraining Step: 1243  | total loss: [1m[32m0.11950[0m[0m | time: 62.924s
[2K
| RMSProp | epoch: 020 | loss: 0.11950 - acc: 0.9692 -- iter: 1472/2003
[A[ATraining Step: 1244  | total loss: [1m[32m0.14255[0m[0m | time: 64.162s
[2K
| RMSProp | epoch: 020 | loss: 0.14255 - acc: 0.9566 -- iter: 1504/2003
[A[ATraining Step: 1245  | total loss: [1m[32m0.15722[0m[0m | time: 65.526s
[2K
| RMSProp | epoch: 020 | loss: 0.15722 - acc: 0.9485 -- iter: 1536/2003
[A[ATraining Step: 1246  | total loss: [1m[32m0.14516[0m[0m | time: 66.656s
[2K
| RMSProp | epoch: 020 | loss: 0.14516 - acc: 0.9536 -- iter: 1568/2003
[A[ATraining Step: 1247  | total loss: [1m[32m0.13181[0m[0m | time: 68.022s
[2K
| RMSProp | epoch: 020 | loss: 0.13181 - acc: 0.9582 -- iter: 1600/2003
[A[ATraining Step: 1248  | total loss: [1m[32m0.31773[0m[0m | time: 69.289s
[2K
| RMSProp | epoch: 020 | loss: 0.31773 - acc: 0.9249 -- iter: 1632/2003
[A[ATraining Step: 1249  | total loss: [1m[32m0.31479[0m[0m | time: 70.671s
[2K
| RMSProp | epoch: 020 | loss: 0.31479 - acc: 0.9137 -- iter: 1664/2003
[A[ATraining Step: 1250  | total loss: [1m[32m0.28846[0m[0m | time: 71.783s
[2K
| RMSProp | epoch: 020 | loss: 0.28846 - acc: 0.9223 -- iter: 1696/2003
[A[ATraining Step: 1251  | total loss: [1m[32m0.26397[0m[0m | time: 73.066s
[2K
| RMSProp | epoch: 020 | loss: 0.26397 - acc: 0.9301 -- iter: 1728/2003
[A[ATraining Step: 1252  | total loss: [1m[32m0.24689[0m[0m | time: 74.314s
[2K
| RMSProp | epoch: 020 | loss: 0.24689 - acc: 0.9339 -- iter: 1760/2003
[A[ATraining Step: 1253  | total loss: [1m[32m0.22510[0m[0m | time: 75.595s
[2K
| RMSProp | epoch: 020 | loss: 0.22510 - acc: 0.9406 -- iter: 1792/2003
[A[ATraining Step: 1254  | total loss: [1m[32m0.22024[0m[0m | time: 76.928s
[2K
| RMSProp | epoch: 020 | loss: 0.22024 - acc: 0.9402 -- iter: 1824/2003
[A[ATraining Step: 1255  | total loss: [1m[32m0.20478[0m[0m | time: 78.287s
[2K
| RMSProp | epoch: 020 | loss: 0.20478 - acc: 0.9431 -- iter: 1856/2003
[A[ATraining Step: 1256  | total loss: [1m[32m0.18621[0m[0m | time: 79.426s
[2K
| RMSProp | epoch: 020 | loss: 0.18621 - acc: 0.9488 -- iter: 1888/2003
[A[ATraining Step: 1257  | total loss: [1m[32m0.16879[0m[0m | time: 80.652s
[2K
| RMSProp | epoch: 020 | loss: 0.16879 - acc: 0.9539 -- iter: 1920/2003
[A[ATraining Step: 1258  | total loss: [1m[32m0.15215[0m[0m | time: 81.771s
[2K
| RMSProp | epoch: 020 | loss: 0.15215 - acc: 0.9585 -- iter: 1952/2003
[A[ATraining Step: 1259  | total loss: [1m[32m0.13737[0m[0m | time: 83.103s
[2K
| RMSProp | epoch: 020 | loss: 0.13737 - acc: 0.9627 -- iter: 1984/2003
[A[ATraining Step: 1260  | total loss: [1m[32m0.12453[0m[0m | time: 88.649s
[2K
| RMSProp | epoch: 020 | loss: 0.12453 - acc: 0.9664 | val_loss: 0.54014 - val_acc: 0.8482 -- iter: 2003/2003
--
Training Step: 1261  | total loss: [1m[32m0.11288[0m[0m | time: 1.220s
[2K
| RMSProp | epoch: 021 | loss: 0.11288 - acc: 0.9698 -- iter: 0032/2003
[A[ATraining Step: 1262  | total loss: [1m[32m0.10293[0m[0m | time: 2.370s
[2K
| RMSProp | epoch: 021 | loss: 0.10293 - acc: 0.9728 -- iter: 0064/2003
[A[ATraining Step: 1263  | total loss: [1m[32m0.09345[0m[0m | time: 3.764s
[2K
| RMSProp | epoch: 021 | loss: 0.09345 - acc: 0.9755 -- iter: 0096/2003
[A[ATraining Step: 1264  | total loss: [1m[32m0.09709[0m[0m | time: 5.116s
[2K
| RMSProp | epoch: 021 | loss: 0.09709 - acc: 0.9748 -- iter: 0128/2003
[A[ATraining Step: 1265  | total loss: [1m[32m0.13009[0m[0m | time: 6.298s
[2K
| RMSProp | epoch: 021 | loss: 0.13009 - acc: 0.9648 -- iter: 0160/2003
[A[ATraining Step: 1266  | total loss: [1m[32m0.12483[0m[0m | time: 7.337s
[2K
| RMSProp | epoch: 021 | loss: 0.12483 - acc: 0.9684 -- iter: 0192/2003
[A[ATraining Step: 1267  | total loss: [1m[32m0.13717[0m[0m | time: 8.616s
[2K
| RMSProp | epoch: 021 | loss: 0.13717 - acc: 0.9684 -- iter: 0224/2003
[A[ATraining Step: 1268  | total loss: [1m[32m0.12466[0m[0m | time: 9.965s
[2K
| RMSProp | epoch: 021 | loss: 0.12466 - acc: 0.9716 -- iter: 0256/2003
[A[ATraining Step: 1269  | total loss: [1m[32m0.11374[0m[0m | time: 11.111s
[2K
| RMSProp | epoch: 021 | loss: 0.11374 - acc: 0.9744 -- iter: 0288/2003
[A[ATraining Step: 1270  | total loss: [1m[32m0.10616[0m[0m | time: 12.400s
[2K
| RMSProp | epoch: 021 | loss: 0.10616 - acc: 0.9770 -- iter: 0320/2003
[A[ATraining Step: 1271  | total loss: [1m[32m0.09671[0m[0m | time: 13.580s
[2K
| RMSProp | epoch: 021 | loss: 0.09671 - acc: 0.9793 -- iter: 0352/2003
[A[ATraining Step: 1272  | total loss: [1m[32m0.08791[0m[0m | time: 14.799s
[2K
| RMSProp | epoch: 021 | loss: 0.08791 - acc: 0.9813 -- iter: 0384/2003
[A[ATraining Step: 1273  | total loss: [1m[32m0.08044[0m[0m | time: 16.151s
[2K
| RMSProp | epoch: 021 | loss: 0.08044 - acc: 0.9832 -- iter: 0416/2003
[A[ATraining Step: 1274  | total loss: [1m[32m0.07584[0m[0m | time: 17.486s
[2K
| RMSProp | epoch: 021 | loss: 0.07584 - acc: 0.9818 -- iter: 0448/2003
[A[ATraining Step: 1275  | total loss: [1m[32m0.07628[0m[0m | time: 18.674s
[2K
| RMSProp | epoch: 021 | loss: 0.07628 - acc: 0.9773 -- iter: 0480/2003
[A[ATraining Step: 1276  | total loss: [1m[32m0.08993[0m[0m | time: 19.978s
[2K
| RMSProp | epoch: 021 | loss: 0.08993 - acc: 0.9734 -- iter: 0512/2003
[A[ATraining Step: 1277  | total loss: [1m[32m0.08996[0m[0m | time: 21.250s
[2K
| RMSProp | epoch: 021 | loss: 0.08996 - acc: 0.9729 -- iter: 0544/2003
[A[ATraining Step: 1278  | total loss: [1m[32m0.09244[0m[0m | time: 22.573s
[2K
| RMSProp | epoch: 021 | loss: 0.09244 - acc: 0.9694 -- iter: 0576/2003
[A[ATraining Step: 1279  | total loss: [1m[32m0.08931[0m[0m | time: 23.417s
[2K
| RMSProp | epoch: 021 | loss: 0.08931 - acc: 0.9693 -- iter: 0608/2003
[A[ATraining Step: 1280  | total loss: [1m[32m0.08274[0m[0m | time: 24.157s
[2K
| RMSProp | epoch: 021 | loss: 0.08274 - acc: 0.9724 -- iter: 0640/2003
[A[ATraining Step: 1281  | total loss: [1m[32m0.07489[0m[0m | time: 25.133s
[2K
| RMSProp | epoch: 021 | loss: 0.07489 - acc: 0.9751 -- iter: 0672/2003
[A[ATraining Step: 1282  | total loss: [1m[32m0.07146[0m[0m | time: 26.295s
[2K
| RMSProp | epoch: 021 | loss: 0.07146 - acc: 0.9776 -- iter: 0704/2003
[A[ATraining Step: 1283  | total loss: [1m[32m0.08822[0m[0m | time: 27.451s
[2K
| RMSProp | epoch: 021 | loss: 0.08822 - acc: 0.9674 -- iter: 0736/2003
[A[ATraining Step: 1284  | total loss: [1m[32m0.08793[0m[0m | time: 28.592s
[2K
| RMSProp | epoch: 021 | loss: 0.08793 - acc: 0.9644 -- iter: 0768/2003
[A[ATraining Step: 1285  | total loss: [1m[32m0.07999[0m[0m | time: 29.881s
[2K
| RMSProp | epoch: 021 | loss: 0.07999 - acc: 0.9679 -- iter: 0800/2003
[A[ATraining Step: 1286  | total loss: [1m[32m0.07323[0m[0m | time: 31.319s
[2K
| RMSProp | epoch: 021 | loss: 0.07323 - acc: 0.9711 -- iter: 0832/2003
[A[ATraining Step: 1287  | total loss: [1m[32m0.06630[0m[0m | time: 32.635s
[2K
| RMSProp | epoch: 021 | loss: 0.06630 - acc: 0.9740 -- iter: 0864/2003
[A[ATraining Step: 1288  | total loss: [1m[32m0.06347[0m[0m | time: 34.034s
[2K
| RMSProp | epoch: 021 | loss: 0.06347 - acc: 0.9735 -- iter: 0896/2003
[A[ATraining Step: 1289  | total loss: [1m[32m0.06205[0m[0m | time: 35.451s
[2K
| RMSProp | epoch: 021 | loss: 0.06205 - acc: 0.9730 -- iter: 0928/2003
[A[ATraining Step: 1290  | total loss: [1m[32m0.05658[0m[0m | time: 36.754s
[2K
| RMSProp | epoch: 021 | loss: 0.05658 - acc: 0.9757 -- iter: 0960/2003
[A[ATraining Step: 1291  | total loss: [1m[32m0.06300[0m[0m | time: 38.218s
[2K
| RMSProp | epoch: 021 | loss: 0.06300 - acc: 0.9750 -- iter: 0992/2003
[A[ATraining Step: 1292  | total loss: [1m[32m0.11091[0m[0m | time: 39.416s
[2K
| RMSProp | epoch: 021 | loss: 0.11091 - acc: 0.9650 -- iter: 1024/2003
[A[ATraining Step: 1293  | total loss: [1m[32m0.10484[0m[0m | time: 40.563s
[2K
| RMSProp | epoch: 021 | loss: 0.10484 - acc: 0.9654 -- iter: 1056/2003
[A[ATraining Step: 1294  | total loss: [1m[32m0.09670[0m[0m | time: 41.773s
[2K
| RMSProp | epoch: 021 | loss: 0.09670 - acc: 0.9689 -- iter: 1088/2003
[A[ATraining Step: 1295  | total loss: [1m[32m0.08898[0m[0m | time: 42.931s
[2K
| RMSProp | epoch: 021 | loss: 0.08898 - acc: 0.9720 -- iter: 1120/2003
[A[ATraining Step: 1296  | total loss: [1m[32m0.08593[0m[0m | time: 44.130s
[2K
| RMSProp | epoch: 021 | loss: 0.08593 - acc: 0.9716 -- iter: 1152/2003
[A[ATraining Step: 1297  | total loss: [1m[32m0.08368[0m[0m | time: 45.465s
[2K
| RMSProp | epoch: 021 | loss: 0.08368 - acc: 0.9714 -- iter: 1184/2003
[A[ATraining Step: 1298  | total loss: [1m[32m0.08986[0m[0m | time: 46.875s
[2K
| RMSProp | epoch: 021 | loss: 0.08986 - acc: 0.9680 -- iter: 1216/2003
[A[ATraining Step: 1299  | total loss: [1m[32m0.08682[0m[0m | time: 48.177s
[2K
| RMSProp | epoch: 021 | loss: 0.08682 - acc: 0.9680 -- iter: 1248/2003
[A[ATraining Step: 1300  | total loss: [1m[32m0.08637[0m[0m | time: 49.577s
[2K
| RMSProp | epoch: 021 | loss: 0.08637 - acc: 0.9681 -- iter: 1280/2003
[A[ATraining Step: 1301  | total loss: [1m[32m0.07865[0m[0m | time: 50.988s
[2K
| RMSProp | epoch: 021 | loss: 0.07865 - acc: 0.9713 -- iter: 1312/2003
[A[ATraining Step: 1302  | total loss: [1m[32m0.07123[0m[0m | time: 52.143s
[2K
| RMSProp | epoch: 021 | loss: 0.07123 - acc: 0.9742 -- iter: 1344/2003
[A[ATraining Step: 1303  | total loss: [1m[32m0.06565[0m[0m | time: 53.313s
[2K
| RMSProp | epoch: 021 | loss: 0.06565 - acc: 0.9768 -- iter: 1376/2003
[A[ATraining Step: 1304  | total loss: [1m[32m0.06302[0m[0m | time: 54.555s
[2K
| RMSProp | epoch: 021 | loss: 0.06302 - acc: 0.9760 -- iter: 1408/2003
[A[ATraining Step: 1305  | total loss: [1m[32m0.06562[0m[0m | time: 55.983s
[2K
| RMSProp | epoch: 021 | loss: 0.06562 - acc: 0.9752 -- iter: 1440/2003
[A[ATraining Step: 1306  | total loss: [1m[32m0.09844[0m[0m | time: 56.995s
[2K
| RMSProp | epoch: 021 | loss: 0.09844 - acc: 0.9715 -- iter: 1472/2003
[A[ATraining Step: 1307  | total loss: [1m[32m0.10518[0m[0m | time: 58.271s
[2K
| RMSProp | epoch: 021 | loss: 0.10518 - acc: 0.9649 -- iter: 1504/2003
[A[ATraining Step: 1308  | total loss: [1m[32m0.09858[0m[0m | time: 59.537s
[2K
| RMSProp | epoch: 021 | loss: 0.09858 - acc: 0.9684 -- iter: 1536/2003
[A[ATraining Step: 1309  | total loss: [1m[32m0.11174[0m[0m | time: 60.756s
[2K
| RMSProp | epoch: 021 | loss: 0.11174 - acc: 0.9654 -- iter: 1568/2003
[A[ATraining Step: 1310  | total loss: [1m[32m0.10612[0m[0m | time: 61.947s
[2K
| RMSProp | epoch: 021 | loss: 0.10612 - acc: 0.9688 -- iter: 1600/2003
[A[ATraining Step: 1311  | total loss: [1m[32m0.09934[0m[0m | time: 63.149s
[2K
| RMSProp | epoch: 021 | loss: 0.09934 - acc: 0.9719 -- iter: 1632/2003
[A[ATraining Step: 1312  | total loss: [1m[32m0.11733[0m[0m | time: 64.177s
[2K
| RMSProp | epoch: 021 | loss: 0.11733 - acc: 0.9716 -- iter: 1664/2003
[A[ATraining Step: 1313  | total loss: [1m[32m0.11734[0m[0m | time: 65.324s
[2K
| RMSProp | epoch: 021 | loss: 0.11734 - acc: 0.9713 -- iter: 1696/2003
[A[ATraining Step: 1314  | total loss: [1m[32m0.10685[0m[0m | time: 66.499s
[2K
| RMSProp | epoch: 021 | loss: 0.10685 - acc: 0.9742 -- iter: 1728/2003
[A[ATraining Step: 1315  | total loss: [1m[32m0.09748[0m[0m | time: 67.599s
[2K
| RMSProp | epoch: 021 | loss: 0.09748 - acc: 0.9768 -- iter: 1760/2003
[A[ATraining Step: 1316  | total loss: [1m[32m0.09369[0m[0m | time: 68.878s
[2K
| RMSProp | epoch: 021 | loss: 0.09369 - acc: 0.9729 -- iter: 1792/2003
[A[ATraining Step: 1317  | total loss: [1m[32m0.08486[0m[0m | time: 70.106s
[2K
| RMSProp | epoch: 021 | loss: 0.08486 - acc: 0.9756 -- iter: 1824/2003
[A[ATraining Step: 1318  | total loss: [1m[32m0.07715[0m[0m | time: 71.211s
[2K
| RMSProp | epoch: 021 | loss: 0.07715 - acc: 0.9780 -- iter: 1856/2003
[A[ATraining Step: 1319  | total loss: [1m[32m0.06967[0m[0m | time: 72.222s
[2K
| RMSProp | epoch: 021 | loss: 0.06967 - acc: 0.9802 -- iter: 1888/2003
[A[ATraining Step: 1320  | total loss: [1m[32m0.06380[0m[0m | time: 73.584s
[2K
| RMSProp | epoch: 021 | loss: 0.06380 - acc: 0.9822 -- iter: 1920/2003
[A[ATraining Step: 1321  | total loss: [1m[32m0.05854[0m[0m | time: 74.798s
[2K
| RMSProp | epoch: 021 | loss: 0.05854 - acc: 0.9840 -- iter: 1952/2003
[A[ATraining Step: 1322  | total loss: [1m[32m0.06187[0m[0m | time: 76.031s
[2K
| RMSProp | epoch: 021 | loss: 0.06187 - acc: 0.9824 -- iter: 1984/2003
[A[ATraining Step: 1323  | total loss: [1m[32m0.06314[0m[0m | time: 81.939s
[2K
| RMSProp | epoch: 021 | loss: 0.06314 - acc: 0.9811 | val_loss: 0.46777 - val_acc: 0.8642 -- iter: 2003/2003
--
Training Step: 1324  | total loss: [1m[32m0.10310[0m[0m | time: 1.247s
[2K
| RMSProp | epoch: 022 | loss: 0.10310 - acc: 0.9705 -- iter: 0032/2003
[A[ATraining Step: 1325  | total loss: [1m[32m0.09541[0m[0m | time: 2.638s
[2K
| RMSProp | epoch: 022 | loss: 0.09541 - acc: 0.9734 -- iter: 0064/2003
[A[ATraining Step: 1326  | total loss: [1m[32m0.08618[0m[0m | time: 3.935s
[2K
| RMSProp | epoch: 022 | loss: 0.08618 - acc: 0.9761 -- iter: 0096/2003
[A[ATraining Step: 1327  | total loss: [1m[32m0.07791[0m[0m | time: 5.101s
[2K
| RMSProp | epoch: 022 | loss: 0.07791 - acc: 0.9785 -- iter: 0128/2003
[A[ATraining Step: 1328  | total loss: [1m[32m0.07057[0m[0m | time: 6.193s
[2K
| RMSProp | epoch: 022 | loss: 0.07057 - acc: 0.9806 -- iter: 0160/2003
[A[ATraining Step: 1329  | total loss: [1m[32m0.06400[0m[0m | time: 7.360s
[2K
| RMSProp | epoch: 022 | loss: 0.06400 - acc: 0.9826 -- iter: 0192/2003
[A[ATraining Step: 1330  | total loss: [1m[32m0.05779[0m[0m | time: 8.556s
[2K
| RMSProp | epoch: 022 | loss: 0.05779 - acc: 0.9843 -- iter: 0224/2003
[A[ATraining Step: 1331  | total loss: [1m[32m0.05245[0m[0m | time: 9.818s
[2K
| RMSProp | epoch: 022 | loss: 0.05245 - acc: 0.9859 -- iter: 0256/2003
[A[ATraining Step: 1332  | total loss: [1m[32m0.04756[0m[0m | time: 11.016s
[2K
| RMSProp | epoch: 022 | loss: 0.04756 - acc: 0.9873 -- iter: 0288/2003
[A[ATraining Step: 1333  | total loss: [1m[32m0.04291[0m[0m | time: 12.337s
[2K
| RMSProp | epoch: 022 | loss: 0.04291 - acc: 0.9886 -- iter: 0320/2003
[A[ATraining Step: 1334  | total loss: [1m[32m0.03909[0m[0m | time: 13.654s
[2K
| RMSProp | epoch: 022 | loss: 0.03909 - acc: 0.9897 -- iter: 0352/2003
[A[ATraining Step: 1335  | total loss: [1m[32m0.04445[0m[0m | time: 14.974s
[2K
| RMSProp | epoch: 022 | loss: 0.04445 - acc: 0.9845 -- iter: 0384/2003
[A[ATraining Step: 1336  | total loss: [1m[32m0.13012[0m[0m | time: 16.143s
[2K
| RMSProp | epoch: 022 | loss: 0.13012 - acc: 0.9673 -- iter: 0416/2003
[A[ATraining Step: 1337  | total loss: [1m[32m0.12485[0m[0m | time: 17.135s
[2K
| RMSProp | epoch: 022 | loss: 0.12485 - acc: 0.9674 -- iter: 0448/2003
[A[ATraining Step: 1338  | total loss: [1m[32m0.12009[0m[0m | time: 18.181s
[2K
| RMSProp | epoch: 022 | loss: 0.12009 - acc: 0.9644 -- iter: 0480/2003
[A[ATraining Step: 1339  | total loss: [1m[32m0.12462[0m[0m | time: 19.195s
[2K
| RMSProp | epoch: 022 | loss: 0.12462 - acc: 0.9617 -- iter: 0512/2003
[A[ATraining Step: 1340  | total loss: [1m[32m0.12584[0m[0m | time: 20.229s
[2K
| RMSProp | epoch: 022 | loss: 0.12584 - acc: 0.9593 -- iter: 0544/2003
[A[ATraining Step: 1341  | total loss: [1m[32m0.11585[0m[0m | time: 20.988s
[2K
| RMSProp | epoch: 022 | loss: 0.11585 - acc: 0.9634 -- iter: 0576/2003
[A[ATraining Step: 1342  | total loss: [1m[32m0.10601[0m[0m | time: 21.933s
[2K
| RMSProp | epoch: 022 | loss: 0.10601 - acc: 0.9670 -- iter: 0608/2003
[A[ATraining Step: 1343  | total loss: [1m[32m0.09655[0m[0m | time: 22.542s
[2K
| RMSProp | epoch: 022 | loss: 0.09655 - acc: 0.9703 -- iter: 0640/2003
[A[ATraining Step: 1344  | total loss: [1m[32m0.12815[0m[0m | time: 23.147s
[2K
| RMSProp | epoch: 022 | loss: 0.12815 - acc: 0.9680 -- iter: 0672/2003
[A[ATraining Step: 1345  | total loss: [1m[32m0.11705[0m[0m | time: 24.150s
[2K
| RMSProp | epoch: 022 | loss: 0.11705 - acc: 0.9712 -- iter: 0704/2003
[A[ATraining Step: 1346  | total loss: [1m[32m0.11678[0m[0m | time: 25.099s
[2K
| RMSProp | epoch: 022 | loss: 0.11678 - acc: 0.9710 -- iter: 0736/2003
[A[ATraining Step: 1347  | total loss: [1m[32m0.10803[0m[0m | time: 26.066s
[2K
| RMSProp | epoch: 022 | loss: 0.10803 - acc: 0.9739 -- iter: 0768/2003
[A[ATraining Step: 1348  | total loss: [1m[32m0.09938[0m[0m | time: 27.114s
[2K
| RMSProp | epoch: 022 | loss: 0.09938 - acc: 0.9765 -- iter: 0800/2003
[A[ATraining Step: 1349  | total loss: [1m[32m0.10098[0m[0m | time: 28.032s
[2K
| RMSProp | epoch: 022 | loss: 0.10098 - acc: 0.9757 -- iter: 0832/2003
[A[ATraining Step: 1350  | total loss: [1m[32m0.09434[0m[0m | time: 29.119s
[2K
| RMSProp | epoch: 022 | loss: 0.09434 - acc: 0.9782 -- iter: 0864/2003
[A[ATraining Step: 1351  | total loss: [1m[32m0.09035[0m[0m | time: 30.214s
[2K
| RMSProp | epoch: 022 | loss: 0.09035 - acc: 0.9772 -- iter: 0896/2003
[A[ATraining Step: 1352  | total loss: [1m[32m0.08161[0m[0m | time: 30.969s
[2K
| RMSProp | epoch: 022 | loss: 0.08161 - acc: 0.9795 -- iter: 0928/2003
[A[ATraining Step: 1353  | total loss: [1m[32m0.07502[0m[0m | time: 31.857s
[2K
| RMSProp | epoch: 022 | loss: 0.07502 - acc: 0.9815 -- iter: 0960/2003
[A[ATraining Step: 1354  | total loss: [1m[32m0.06970[0m[0m | time: 32.804s
[2K
| RMSProp | epoch: 022 | loss: 0.06970 - acc: 0.9834 -- iter: 0992/2003
[A[ATraining Step: 1355  | total loss: [1m[32m0.06799[0m[0m | time: 33.772s
[2K
| RMSProp | epoch: 022 | loss: 0.06799 - acc: 0.9788 -- iter: 1024/2003
[A[ATraining Step: 1356  | total loss: [1m[32m0.08304[0m[0m | time: 34.812s
[2K
| RMSProp | epoch: 022 | loss: 0.08304 - acc: 0.9715 -- iter: 1056/2003
[A[ATraining Step: 1357  | total loss: [1m[32m0.07850[0m[0m | time: 35.854s
[2K
| RMSProp | epoch: 022 | loss: 0.07850 - acc: 0.9744 -- iter: 1088/2003
[A[ATraining Step: 1358  | total loss: [1m[32m0.07361[0m[0m | time: 36.813s
[2K
| RMSProp | epoch: 022 | loss: 0.07361 - acc: 0.9738 -- iter: 1120/2003
[A[ATraining Step: 1359  | total loss: [1m[32m0.06903[0m[0m | time: 37.698s
[2K
| RMSProp | epoch: 022 | loss: 0.06903 - acc: 0.9764 -- iter: 1152/2003
[A[ATraining Step: 1360  | total loss: [1m[32m0.07591[0m[0m | time: 38.731s
[2K
| RMSProp | epoch: 022 | loss: 0.07591 - acc: 0.9725 -- iter: 1184/2003
[A[ATraining Step: 1361  | total loss: [1m[32m0.07747[0m[0m | time: 39.774s
[2K
| RMSProp | epoch: 022 | loss: 0.07747 - acc: 0.9690 -- iter: 1216/2003
[A[ATraining Step: 1362  | total loss: [1m[32m0.07496[0m[0m | time: 40.739s
[2K
| RMSProp | epoch: 022 | loss: 0.07496 - acc: 0.9690 -- iter: 1248/2003
[A[ATraining Step: 1363  | total loss: [1m[32m0.06981[0m[0m | time: 41.444s
[2K
| RMSProp | epoch: 022 | loss: 0.06981 - acc: 0.9721 -- iter: 1280/2003
[A[ATraining Step: 1364  | total loss: [1m[32m0.06672[0m[0m | time: 42.336s
[2K
| RMSProp | epoch: 022 | loss: 0.06672 - acc: 0.9718 -- iter: 1312/2003
[A[ATraining Step: 1365  | total loss: [1m[32m0.09443[0m[0m | time: 43.270s
[2K
| RMSProp | epoch: 022 | loss: 0.09443 - acc: 0.9652 -- iter: 1344/2003
[A[ATraining Step: 1366  | total loss: [1m[32m0.11573[0m[0m | time: 44.200s
[2K
| RMSProp | epoch: 022 | loss: 0.11573 - acc: 0.9593 -- iter: 1376/2003
[A[ATraining Step: 1367  | total loss: [1m[32m0.10581[0m[0m | time: 45.122s
[2K
| RMSProp | epoch: 022 | loss: 0.10581 - acc: 0.9634 -- iter: 1408/2003
[A[ATraining Step: 1368  | total loss: [1m[32m0.09693[0m[0m | time: 46.109s
[2K
| RMSProp | epoch: 022 | loss: 0.09693 - acc: 0.9671 -- iter: 1440/2003
[A[ATraining Step: 1369  | total loss: [1m[32m0.08956[0m[0m | time: 47.089s
[2K
| RMSProp | epoch: 022 | loss: 0.08956 - acc: 0.9703 -- iter: 1472/2003
[A[ATraining Step: 1370  | total loss: [1m[32m0.08096[0m[0m | time: 48.027s
[2K
| RMSProp | epoch: 022 | loss: 0.08096 - acc: 0.9733 -- iter: 1504/2003
[A[ATraining Step: 1371  | total loss: [1m[32m0.07614[0m[0m | time: 49.438s
[2K
| RMSProp | epoch: 022 | loss: 0.07614 - acc: 0.9760 -- iter: 1536/2003
[A[ATraining Step: 1372  | total loss: [1m[32m0.06974[0m[0m | time: 50.887s
[2K
| RMSProp | epoch: 022 | loss: 0.06974 - acc: 0.9784 -- iter: 1568/2003
[A[ATraining Step: 1373  | total loss: [1m[32m0.06329[0m[0m | time: 52.274s
[2K
| RMSProp | epoch: 022 | loss: 0.06329 - acc: 0.9805 -- iter: 1600/2003
[A[ATraining Step: 1374  | total loss: [1m[32m0.05722[0m[0m | time: 53.781s
[2K
| RMSProp | epoch: 022 | loss: 0.05722 - acc: 0.9825 -- iter: 1632/2003
[A[ATraining Step: 1375  | total loss: [1m[32m0.05473[0m[0m | time: 55.014s
[2K
| RMSProp | epoch: 022 | loss: 0.05473 - acc: 0.9811 -- iter: 1664/2003
[A[ATraining Step: 1376  | total loss: [1m[32m0.10261[0m[0m | time: 56.262s
[2K
| RMSProp | epoch: 022 | loss: 0.10261 - acc: 0.9768 -- iter: 1696/2003
[A[ATraining Step: 1377  | total loss: [1m[32m0.09566[0m[0m | time: 57.619s
[2K
| RMSProp | epoch: 022 | loss: 0.09566 - acc: 0.9791 -- iter: 1728/2003
[A[ATraining Step: 1378  | total loss: [1m[32m0.08765[0m[0m | time: 59.128s
[2K
| RMSProp | epoch: 022 | loss: 0.08765 - acc: 0.9812 -- iter: 1760/2003
[A[ATraining Step: 1379  | total loss: [1m[32m0.07962[0m[0m | time: 60.408s
[2K
| RMSProp | epoch: 022 | loss: 0.07962 - acc: 0.9831 -- iter: 1792/2003
[A[ATraining Step: 1380  | total loss: [1m[32m0.07363[0m[0m | time: 61.788s
[2K
| RMSProp | epoch: 022 | loss: 0.07363 - acc: 0.9847 -- iter: 1824/2003
[A[ATraining Step: 1381  | total loss: [1m[32m0.06679[0m[0m | time: 63.363s
[2K
| RMSProp | epoch: 022 | loss: 0.06679 - acc: 0.9863 -- iter: 1856/2003
[A[ATraining Step: 1382  | total loss: [1m[32m0.06035[0m[0m | time: 64.842s
[2K
| RMSProp | epoch: 022 | loss: 0.06035 - acc: 0.9876 -- iter: 1888/2003
[A[ATraining Step: 1383  | total loss: [1m[32m0.05439[0m[0m | time: 66.380s
[2K
| RMSProp | epoch: 022 | loss: 0.05439 - acc: 0.9889 -- iter: 1920/2003
[A[ATraining Step: 1384  | total loss: [1m[32m0.04931[0m[0m | time: 67.895s
[2K
| RMSProp | epoch: 022 | loss: 0.04931 - acc: 0.9900 -- iter: 1952/2003
[A[ATraining Step: 1385  | total loss: [1m[32m0.04455[0m[0m | time: 68.928s
[2K
| RMSProp | epoch: 022 | loss: 0.04455 - acc: 0.9910 -- iter: 1984/2003
[A[ATraining Step: 1386  | total loss: [1m[32m0.04038[0m[0m | time: 73.173s
[2K
| RMSProp | epoch: 022 | loss: 0.04038 - acc: 0.9919 | val_loss: 0.63194 - val_acc: 0.8658 -- iter: 2003/2003
--
Training Step: 1387  | total loss: [1m[32m0.03725[0m[0m | time: 0.902s
[2K
| RMSProp | epoch: 023 | loss: 0.03725 - acc: 0.9927 -- iter: 0032/2003
[A[ATraining Step: 1388  | total loss: [1m[32m0.03365[0m[0m | time: 1.803s
[2K
| RMSProp | epoch: 023 | loss: 0.03365 - acc: 0.9934 -- iter: 0064/2003
[A[ATraining Step: 1389  | total loss: [1m[32m0.03038[0m[0m | time: 2.689s
[2K
| RMSProp | epoch: 023 | loss: 0.03038 - acc: 0.9941 -- iter: 0096/2003
[A[ATraining Step: 1390  | total loss: [1m[32m0.02746[0m[0m | time: 3.614s
[2K
| RMSProp | epoch: 023 | loss: 0.02746 - acc: 0.9947 -- iter: 0128/2003
[A[ATraining Step: 1391  | total loss: [1m[32m0.03527[0m[0m | time: 4.500s
[2K
| RMSProp | epoch: 023 | loss: 0.03527 - acc: 0.9921 -- iter: 0160/2003
[A[ATraining Step: 1392  | total loss: [1m[32m0.04861[0m[0m | time: 5.468s
[2K
| RMSProp | epoch: 023 | loss: 0.04861 - acc: 0.9835 -- iter: 0192/2003
[A[ATraining Step: 1393  | total loss: [1m[32m0.05163[0m[0m | time: 6.418s
[2K
| RMSProp | epoch: 023 | loss: 0.05163 - acc: 0.9789 -- iter: 0224/2003
[A[ATraining Step: 1394  | total loss: [1m[32m0.05676[0m[0m | time: 7.354s
[2K
| RMSProp | epoch: 023 | loss: 0.05676 - acc: 0.9748 -- iter: 0256/2003
[A[ATraining Step: 1395  | total loss: [1m[32m0.08178[0m[0m | time: 8.534s
[2K
| RMSProp | epoch: 023 | loss: 0.08178 - acc: 0.9648 -- iter: 0288/2003
[A[ATraining Step: 1396  | total loss: [1m[32m0.08046[0m[0m | time: 9.503s
[2K
| RMSProp | epoch: 023 | loss: 0.08046 - acc: 0.9652 -- iter: 0320/2003
[A[ATraining Step: 1397  | total loss: [1m[32m0.07690[0m[0m | time: 10.235s
[2K
| RMSProp | epoch: 023 | loss: 0.07690 - acc: 0.9655 -- iter: 0352/2003
[A[ATraining Step: 1398  | total loss: [1m[32m0.07030[0m[0m | time: 11.073s
[2K
| RMSProp | epoch: 023 | loss: 0.07030 - acc: 0.9690 -- iter: 0384/2003
[A[ATraining Step: 1399  | total loss: [1m[32m0.06341[0m[0m | time: 12.044s
[2K
| RMSProp | epoch: 023 | loss: 0.06341 - acc: 0.9721 -- iter: 0416/2003
[A[ATraining Step: 1400  | total loss: [1m[32m0.05743[0m[0m | time: 16.196s
[2K
| RMSProp | epoch: 023 | loss: 0.05743 - acc: 0.9749 | val_loss: 0.53437 - val_acc: 0.8834 -- iter: 0448/2003
--
Training Step: 1401  | total loss: [1m[32m0.05184[0m[0m | time: 17.031s
[2K
| RMSProp | epoch: 023 | loss: 0.05184 - acc: 0.9774 -- iter: 0480/2003
[A[ATraining Step: 1402  | total loss: [1m[32m0.04677[0m[0m | time: 18.038s
[2K
| RMSProp | epoch: 023 | loss: 0.04677 - acc: 0.9797 -- iter: 0512/2003
[A[ATraining Step: 1403  | total loss: [1m[32m0.04276[0m[0m | time: 19.115s
[2K
| RMSProp | epoch: 023 | loss: 0.04276 - acc: 0.9817 -- iter: 0544/2003
[A[ATraining Step: 1404  | total loss: [1m[32m0.03887[0m[0m | time: 20.139s
[2K
| RMSProp | epoch: 023 | loss: 0.03887 - acc: 0.9835 -- iter: 0576/2003
[A[ATraining Step: 1405  | total loss: [1m[32m0.03506[0m[0m | time: 20.935s
[2K
| RMSProp | epoch: 023 | loss: 0.03506 - acc: 0.9852 -- iter: 0608/2003
[A[ATraining Step: 1406  | total loss: [1m[32m0.03238[0m[0m | time: 21.905s
[2K
| RMSProp | epoch: 023 | loss: 0.03238 - acc: 0.9866 -- iter: 0640/2003
[A[ATraining Step: 1407  | total loss: [1m[32m0.02935[0m[0m | time: 22.469s
[2K
| RMSProp | epoch: 023 | loss: 0.02935 - acc: 0.9880 -- iter: 0672/2003
[A[ATraining Step: 1408  | total loss: [1m[32m0.02647[0m[0m | time: 22.993s
[2K
| RMSProp | epoch: 023 | loss: 0.02647 - acc: 0.9892 -- iter: 0704/2003
[A[ATraining Step: 1409  | total loss: [1m[32m0.02386[0m[0m | time: 23.836s
[2K
| RMSProp | epoch: 023 | loss: 0.02386 - acc: 0.9903 -- iter: 0736/2003
[A[ATraining Step: 1410  | total loss: [1m[32m0.02150[0m[0m | time: 24.836s
[2K
| RMSProp | epoch: 023 | loss: 0.02150 - acc: 0.9912 -- iter: 0768/2003
[A[ATraining Step: 1411  | total loss: [1m[32m0.01949[0m[0m | time: 25.749s
[2K
| RMSProp | epoch: 023 | loss: 0.01949 - acc: 0.9921 -- iter: 0800/2003
[A[ATraining Step: 1412  | total loss: [1m[32m0.01854[0m[0m | time: 26.715s
[2K
| RMSProp | epoch: 023 | loss: 0.01854 - acc: 0.9929 -- iter: 0832/2003
[A[ATraining Step: 1413  | total loss: [1m[32m0.02594[0m[0m | time: 27.602s
[2K
| RMSProp | epoch: 023 | loss: 0.02594 - acc: 0.9874 -- iter: 0864/2003
[A[ATraining Step: 1414  | total loss: [1m[32m0.04071[0m[0m | time: 28.787s
[2K
| RMSProp | epoch: 023 | loss: 0.04071 - acc: 0.9761 -- iter: 0896/2003
[A[ATraining Step: 1415  | total loss: [1m[32m0.04660[0m[0m | time: 29.871s
[2K
| RMSProp | epoch: 023 | loss: 0.04660 - acc: 0.9691 -- iter: 0928/2003
[A[ATraining Step: 1416  | total loss: [1m[32m0.04474[0m[0m | time: 30.647s
[2K
| RMSProp | epoch: 023 | loss: 0.04474 - acc: 0.9722 -- iter: 0960/2003
[A[ATraining Step: 1417  | total loss: [1m[32m0.04970[0m[0m | time: 31.573s
[2K
| RMSProp | epoch: 023 | loss: 0.04970 - acc: 0.9719 -- iter: 0992/2003
[A[ATraining Step: 1418  | total loss: [1m[32m0.04754[0m[0m | time: 32.539s
[2K
| RMSProp | epoch: 023 | loss: 0.04754 - acc: 0.9747 -- iter: 1024/2003
[A[ATraining Step: 1419  | total loss: [1m[32m0.05538[0m[0m | time: 33.432s
[2K
| RMSProp | epoch: 023 | loss: 0.05538 - acc: 0.9741 -- iter: 1056/2003
[A[ATraining Step: 1420  | total loss: [1m[32m0.06739[0m[0m | time: 34.350s
[2K
| RMSProp | epoch: 023 | loss: 0.06739 - acc: 0.9736 -- iter: 1088/2003
[A[ATraining Step: 1421  | total loss: [1m[32m0.06477[0m[0m | time: 35.301s
[2K
| RMSProp | epoch: 023 | loss: 0.06477 - acc: 0.9731 -- iter: 1120/2003
[A[ATraining Step: 1422  | total loss: [1m[32m0.07038[0m[0m | time: 36.333s
[2K
| RMSProp | epoch: 023 | loss: 0.07038 - acc: 0.9726 -- iter: 1152/2003
[A[ATraining Step: 1423  | total loss: [1m[32m0.10792[0m[0m | time: 37.271s
[2K
| RMSProp | epoch: 023 | loss: 0.10792 - acc: 0.9629 -- iter: 1184/2003
[A[ATraining Step: 1424  | total loss: [1m[32m0.09822[0m[0m | time: 38.207s
[2K
| RMSProp | epoch: 023 | loss: 0.09822 - acc: 0.9666 -- iter: 1216/2003
[A[ATraining Step: 1425  | total loss: [1m[32m0.10934[0m[0m | time: 39.243s
[2K
| RMSProp | epoch: 023 | loss: 0.10934 - acc: 0.9668 -- iter: 1248/2003
[A[ATraining Step: 1426  | total loss: [1m[32m0.10104[0m[0m | time: 40.354s
[2K
| RMSProp | epoch: 023 | loss: 0.10104 - acc: 0.9701 -- iter: 1280/2003
[A[ATraining Step: 1427  | total loss: [1m[32m0.09179[0m[0m | time: 41.128s
[2K
| RMSProp | epoch: 023 | loss: 0.09179 - acc: 0.9731 -- iter: 1312/2003
[A[ATraining Step: 1428  | total loss: [1m[32m0.08334[0m[0m | time: 41.988s
[2K
| RMSProp | epoch: 023 | loss: 0.08334 - acc: 0.9758 -- iter: 1344/2003
[A[ATraining Step: 1429  | total loss: [1m[32m0.07557[0m[0m | time: 42.911s
[2K
| RMSProp | epoch: 023 | loss: 0.07557 - acc: 0.9782 -- iter: 1376/2003
[A[ATraining Step: 1430  | total loss: [1m[32m0.06860[0m[0m | time: 43.856s
[2K
| RMSProp | epoch: 023 | loss: 0.06860 - acc: 0.9804 -- iter: 1408/2003
[A[ATraining Step: 1431  | total loss: [1m[32m0.06223[0m[0m | time: 44.842s
[2K
| RMSProp | epoch: 023 | loss: 0.06223 - acc: 0.9824 -- iter: 1440/2003
[A[ATraining Step: 1432  | total loss: [1m[32m0.05722[0m[0m | time: 45.731s
[2K
| RMSProp | epoch: 023 | loss: 0.05722 - acc: 0.9841 -- iter: 1472/2003
[A[ATraining Step: 1433  | total loss: [1m[32m0.05184[0m[0m | time: 46.650s
[2K
| RMSProp | epoch: 023 | loss: 0.05184 - acc: 0.9857 -- iter: 1504/2003
[A[ATraining Step: 1434  | total loss: [1m[32m0.06071[0m[0m | time: 47.610s
[2K
| RMSProp | epoch: 023 | loss: 0.06071 - acc: 0.9840 -- iter: 1536/2003
[A[ATraining Step: 1435  | total loss: [1m[32m0.06686[0m[0m | time: 48.527s
[2K
| RMSProp | epoch: 023 | loss: 0.06686 - acc: 0.9794 -- iter: 1568/2003
[A[ATraining Step: 1436  | total loss: [1m[32m0.06669[0m[0m | time: 49.683s
[2K
| RMSProp | epoch: 023 | loss: 0.06669 - acc: 0.9783 -- iter: 1600/2003
[A[ATraining Step: 1437  | total loss: [1m[32m0.06416[0m[0m | time: 50.714s
[2K
| RMSProp | epoch: 023 | loss: 0.06416 - acc: 0.9773 -- iter: 1632/2003
[A[ATraining Step: 1438  | total loss: [1m[32m0.07852[0m[0m | time: 51.523s
[2K
| RMSProp | epoch: 023 | loss: 0.07852 - acc: 0.9671 -- iter: 1664/2003
[A[ATraining Step: 1439  | total loss: [1m[32m0.07086[0m[0m | time: 52.529s
[2K
| RMSProp | epoch: 023 | loss: 0.07086 - acc: 0.9704 -- iter: 1696/2003
[A[ATraining Step: 1440  | total loss: [1m[32m0.17338[0m[0m | time: 53.459s
[2K
| RMSProp | epoch: 023 | loss: 0.17338 - acc: 0.9640 -- iter: 1728/2003
[A[ATraining Step: 1441  | total loss: [1m[32m0.16463[0m[0m | time: 54.459s
[2K
| RMSProp | epoch: 023 | loss: 0.16463 - acc: 0.9645 -- iter: 1760/2003
[A[ATraining Step: 1442  | total loss: [1m[32m0.15116[0m[0m | time: 55.326s
[2K
| RMSProp | epoch: 023 | loss: 0.15116 - acc: 0.9680 -- iter: 1792/2003
[A[ATraining Step: 1443  | total loss: [1m[32m0.13935[0m[0m | time: 56.383s
[2K
| RMSProp | epoch: 023 | loss: 0.13935 - acc: 0.9712 -- iter: 1824/2003
[A[ATraining Step: 1444  | total loss: [1m[32m0.12759[0m[0m | time: 57.383s
[2K
| RMSProp | epoch: 023 | loss: 0.12759 - acc: 0.9741 -- iter: 1856/2003
[A[ATraining Step: 1445  | total loss: [1m[32m0.11529[0m[0m | time: 58.242s
[2K
| RMSProp | epoch: 023 | loss: 0.11529 - acc: 0.9767 -- iter: 1888/2003
[A[ATraining Step: 1446  | total loss: [1m[32m0.12397[0m[0m | time: 59.188s
[2K
| RMSProp | epoch: 023 | loss: 0.12397 - acc: 0.9759 -- iter: 1920/2003
[A[ATraining Step: 1447  | total loss: [1m[32m0.11233[0m[0m | time: 60.251s
[2K
| RMSProp | epoch: 023 | loss: 0.11233 - acc: 0.9783 -- iter: 1952/2003
[A[ATraining Step: 1448  | total loss: [1m[32m0.10712[0m[0m | time: 61.301s
[2K
| RMSProp | epoch: 023 | loss: 0.10712 - acc: 0.9773 -- iter: 1984/2003
[A[ATraining Step: 1449  | total loss: [1m[32m0.11234[0m[0m | time: 65.116s
[2K
| RMSProp | epoch: 023 | loss: 0.11234 - acc: 0.9734 | val_loss: 0.45464 - val_acc: 0.8419 -- iter: 2003/2003
--
Training Step: 1450  | total loss: [1m[32m0.11989[0m[0m | time: 0.992s
[2K
| RMSProp | epoch: 024 | loss: 0.11989 - acc: 0.9698 -- iter: 0032/2003
[A[ATraining Step: 1451  | total loss: [1m[32m0.12140[0m[0m | time: 1.957s
[2K
| RMSProp | epoch: 024 | loss: 0.12140 - acc: 0.9665 -- iter: 0064/2003
[A[ATraining Step: 1452  | total loss: [1m[32m0.11746[0m[0m | time: 2.935s
[2K
| RMSProp | epoch: 024 | loss: 0.11746 - acc: 0.9668 -- iter: 0096/2003
[A[ATraining Step: 1453  | total loss: [1m[32m0.11213[0m[0m | time: 3.888s
[2K
| RMSProp | epoch: 024 | loss: 0.11213 - acc: 0.9670 -- iter: 0128/2003
[A[ATraining Step: 1454  | total loss: [1m[32m0.10406[0m[0m | time: 4.789s
[2K
| RMSProp | epoch: 024 | loss: 0.10406 - acc: 0.9703 -- iter: 0160/2003
[A[ATraining Step: 1455  | total loss: [1m[32m0.10011[0m[0m | time: 5.865s
[2K
| RMSProp | epoch: 024 | loss: 0.10011 - acc: 0.9701 -- iter: 0192/2003
[A[ATraining Step: 1456  | total loss: [1m[32m0.09263[0m[0m | time: 6.885s
[2K
| RMSProp | epoch: 024 | loss: 0.09263 - acc: 0.9731 -- iter: 0224/2003
[A[ATraining Step: 1457  | total loss: [1m[32m0.08685[0m[0m | time: 7.684s
[2K
| RMSProp | epoch: 024 | loss: 0.08685 - acc: 0.9727 -- iter: 0256/2003
[A[ATraining Step: 1458  | total loss: [1m[32m0.07955[0m[0m | time: 8.642s
[2K
| RMSProp | epoch: 024 | loss: 0.07955 - acc: 0.9754 -- iter: 0288/2003
[A[ATraining Step: 1459  | total loss: [1m[32m0.07178[0m[0m | time: 9.526s
[2K
| RMSProp | epoch: 024 | loss: 0.07178 - acc: 0.9779 -- iter: 0320/2003
[A[ATraining Step: 1460  | total loss: [1m[32m0.06496[0m[0m | time: 10.434s
[2K
| RMSProp | epoch: 024 | loss: 0.06496 - acc: 0.9801 -- iter: 0352/2003
[A[ATraining Step: 1461  | total loss: [1m[32m0.05868[0m[0m | time: 11.389s
[2K
| RMSProp | epoch: 024 | loss: 0.05868 - acc: 0.9821 -- iter: 0384/2003
[A[ATraining Step: 1462  | total loss: [1m[32m0.05383[0m[0m | time: 12.381s
[2K
| RMSProp | epoch: 024 | loss: 0.05383 - acc: 0.9839 -- iter: 0416/2003
[A[ATraining Step: 1463  | total loss: [1m[32m0.04883[0m[0m | time: 13.255s
[2K
| RMSProp | epoch: 024 | loss: 0.04883 - acc: 0.9855 -- iter: 0448/2003
[A[ATraining Step: 1464  | total loss: [1m[32m0.04414[0m[0m | time: 14.156s
[2K
| RMSProp | epoch: 024 | loss: 0.04414 - acc: 0.9869 -- iter: 0480/2003
[A[ATraining Step: 1465  | total loss: [1m[32m0.03979[0m[0m | time: 15.111s
[2K
| RMSProp | epoch: 024 | loss: 0.03979 - acc: 0.9882 -- iter: 0512/2003
[A[ATraining Step: 1466  | total loss: [1m[32m0.03625[0m[0m | time: 16.206s
[2K
| RMSProp | epoch: 024 | loss: 0.03625 - acc: 0.9894 -- iter: 0544/2003
[A[ATraining Step: 1467  | total loss: [1m[32m0.03369[0m[0m | time: 17.323s
[2K
| RMSProp | epoch: 024 | loss: 0.03369 - acc: 0.9905 -- iter: 0576/2003
[A[ATraining Step: 1468  | total loss: [1m[32m0.03042[0m[0m | time: 18.073s
[2K
| RMSProp | epoch: 024 | loss: 0.03042 - acc: 0.9914 -- iter: 0608/2003
[A[ATraining Step: 1469  | total loss: [1m[32m0.05157[0m[0m | time: 18.938s
[2K
| RMSProp | epoch: 024 | loss: 0.05157 - acc: 0.9892 -- iter: 0640/2003
[A[ATraining Step: 1470  | total loss: [1m[32m0.06548[0m[0m | time: 19.799s
[2K
| RMSProp | epoch: 024 | loss: 0.06548 - acc: 0.9840 -- iter: 0672/2003
[A[ATraining Step: 1471  | total loss: [1m[32m0.05960[0m[0m | time: 20.359s
[2K
| RMSProp | epoch: 024 | loss: 0.05960 - acc: 0.9856 -- iter: 0704/2003
[A[ATraining Step: 1472  | total loss: [1m[32m0.05368[0m[0m | time: 20.915s
[2K
| RMSProp | epoch: 024 | loss: 0.05368 - acc: 0.9870 -- iter: 0736/2003
[A[ATraining Step: 1473  | total loss: [1m[32m0.04836[0m[0m | time: 21.831s
[2K
| RMSProp | epoch: 024 | loss: 0.04836 - acc: 0.9883 -- iter: 0768/2003
[A[ATraining Step: 1474  | total loss: [1m[32m0.04380[0m[0m | time: 22.799s
[2K
| RMSProp | epoch: 024 | loss: 0.04380 - acc: 0.9895 -- iter: 0800/2003
[A[ATraining Step: 1475  | total loss: [1m[32m0.03973[0m[0m | time: 23.735s
[2K
| RMSProp | epoch: 024 | loss: 0.03973 - acc: 0.9905 -- iter: 0832/2003
[A[ATraining Step: 1476  | total loss: [1m[32m0.03658[0m[0m | time: 24.653s
[2K
| RMSProp | epoch: 024 | loss: 0.03658 - acc: 0.9915 -- iter: 0864/2003
[A[ATraining Step: 1477  | total loss: [1m[32m0.03304[0m[0m | time: 25.567s
[2K
| RMSProp | epoch: 024 | loss: 0.03304 - acc: 0.9923 -- iter: 0896/2003
[A[ATraining Step: 1478  | total loss: [1m[32m0.03740[0m[0m | time: 26.577s
[2K
| RMSProp | epoch: 024 | loss: 0.03740 - acc: 0.9900 -- iter: 0928/2003
[A[ATraining Step: 1479  | total loss: [1m[32m0.03783[0m[0m | time: 27.719s
[2K
| RMSProp | epoch: 024 | loss: 0.03783 - acc: 0.9879 -- iter: 0960/2003
[A[ATraining Step: 1480  | total loss: [1m[32m0.03459[0m[0m | time: 28.549s
[2K
| RMSProp | epoch: 024 | loss: 0.03459 - acc: 0.9891 -- iter: 0992/2003
[A[ATraining Step: 1481  | total loss: [1m[32m0.03136[0m[0m | time: 29.458s
[2K
| RMSProp | epoch: 024 | loss: 0.03136 - acc: 0.9902 -- iter: 1024/2003
[A[ATraining Step: 1482  | total loss: [1m[32m0.02831[0m[0m | time: 30.407s
[2K
| RMSProp | epoch: 024 | loss: 0.02831 - acc: 0.9912 -- iter: 1056/2003
[A[ATraining Step: 1483  | total loss: [1m[32m0.02560[0m[0m | time: 31.323s
[2K
| RMSProp | epoch: 024 | loss: 0.02560 - acc: 0.9920 -- iter: 1088/2003
[A[ATraining Step: 1484  | total loss: [1m[32m0.02328[0m[0m | time: 32.318s
[2K
| RMSProp | epoch: 024 | loss: 0.02328 - acc: 0.9928 -- iter: 1120/2003
[A[ATraining Step: 1485  | total loss: [1m[32m0.02110[0m[0m | time: 33.325s
[2K
| RMSProp | epoch: 024 | loss: 0.02110 - acc: 0.9935 -- iter: 1152/2003
[A[ATraining Step: 1486  | total loss: [1m[32m0.01927[0m[0m | time: 34.244s
[2K
| RMSProp | epoch: 024 | loss: 0.01927 - acc: 0.9942 -- iter: 1184/2003
[A[ATraining Step: 1487  | total loss: [1m[32m0.01751[0m[0m | time: 35.106s
[2K
| RMSProp | epoch: 024 | loss: 0.01751 - acc: 0.9948 -- iter: 1216/2003
[A[ATraining Step: 1488  | total loss: [1m[32m0.01583[0m[0m | time: 36.075s
[2K
| RMSProp | epoch: 024 | loss: 0.01583 - acc: 0.9953 -- iter: 1248/2003
[A[ATraining Step: 1489  | total loss: [1m[32m0.01429[0m[0m | time: 37.225s
[2K
| RMSProp | epoch: 024 | loss: 0.01429 - acc: 0.9958 -- iter: 1280/2003
[A[ATraining Step: 1490  | total loss: [1m[32m0.01350[0m[0m | time: 38.227s
[2K
| RMSProp | epoch: 024 | loss: 0.01350 - acc: 0.9962 -- iter: 1312/2003
[A[ATraining Step: 1491  | total loss: [1m[32m0.01218[0m[0m | time: 38.956s
[2K
| RMSProp | epoch: 024 | loss: 0.01218 - acc: 0.9966 -- iter: 1344/2003
[A[ATraining Step: 1492  | total loss: [1m[32m0.01103[0m[0m | time: 39.873s
[2K
| RMSProp | epoch: 024 | loss: 0.01103 - acc: 0.9969 -- iter: 1376/2003
[A[ATraining Step: 1493  | total loss: [1m[32m0.01025[0m[0m | time: 40.794s
[2K
| RMSProp | epoch: 024 | loss: 0.01025 - acc: 0.9972 -- iter: 1408/2003
[A[ATraining Step: 1494  | total loss: [1m[32m0.00941[0m[0m | time: 41.737s
[2K
| RMSProp | epoch: 024 | loss: 0.00941 - acc: 0.9975 -- iter: 1440/2003
[A[ATraining Step: 1495  | total loss: [1m[32m0.02222[0m[0m | time: 42.567s
[2K
| RMSProp | epoch: 024 | loss: 0.02222 - acc: 0.9946 -- iter: 1472/2003
[A[ATraining Step: 1496  | total loss: [1m[32m0.13149[0m[0m | time: 43.463s
[2K
| RMSProp | epoch: 024 | loss: 0.13149 - acc: 0.9795 -- iter: 1504/2003
[A[ATraining Step: 1497  | total loss: [1m[32m0.16691[0m[0m | time: 44.512s
[2K
| RMSProp | epoch: 024 | loss: 0.16691 - acc: 0.9660 -- iter: 1536/2003
[A[ATraining Step: 1498  | total loss: [1m[32m0.16622[0m[0m | time: 45.494s
[2K
| RMSProp | epoch: 024 | loss: 0.16622 - acc: 0.9600 -- iter: 1568/2003
[A[ATraining Step: 1499  | total loss: [1m[32m0.15156[0m[0m | time: 46.356s
[2K
| RMSProp | epoch: 024 | loss: 0.15156 - acc: 0.9640 -- iter: 1600/2003
[A[ATraining Step: 1500  | total loss: [1m[32m0.13890[0m[0m | time: 47.402s
[2K
| RMSProp | epoch: 024 | loss: 0.13890 - acc: 0.9676 -- iter: 1632/2003
[A[ATraining Step: 1501  | total loss: [1m[32m0.12645[0m[0m | time: 48.518s
[2K
| RMSProp | epoch: 024 | loss: 0.12645 - acc: 0.9708 -- iter: 1664/2003
[A[ATraining Step: 1502  | total loss: [1m[32m0.13652[0m[0m | time: 49.414s
[2K
| RMSProp | epoch: 024 | loss: 0.13652 - acc: 0.9644 -- iter: 1696/2003
[A[ATraining Step: 1503  | total loss: [1m[32m0.15274[0m[0m | time: 50.248s
[2K
| RMSProp | epoch: 024 | loss: 0.15274 - acc: 0.9617 -- iter: 1728/2003
[A[ATraining Step: 1504  | total loss: [1m[32m0.16726[0m[0m | time: 51.128s
[2K
| RMSProp | epoch: 024 | loss: 0.16726 - acc: 0.9561 -- iter: 1760/2003
[A[ATraining Step: 1505  | total loss: [1m[32m0.16644[0m[0m | time: 52.049s
[2K
| RMSProp | epoch: 024 | loss: 0.16644 - acc: 0.9543 -- iter: 1792/2003
[A[ATraining Step: 1506  | total loss: [1m[32m0.15256[0m[0m | time: 52.919s
[2K
| RMSProp | epoch: 024 | loss: 0.15256 - acc: 0.9589 -- iter: 1824/2003
[A[ATraining Step: 1507  | total loss: [1m[32m0.13865[0m[0m | time: 53.862s
[2K
| RMSProp | epoch: 024 | loss: 0.13865 - acc: 0.9630 -- iter: 1856/2003
[A[ATraining Step: 1508  | total loss: [1m[32m0.13297[0m[0m | time: 54.884s
[2K
| RMSProp | epoch: 024 | loss: 0.13297 - acc: 0.9635 -- iter: 1888/2003
[A[ATraining Step: 1509  | total loss: [1m[32m0.12627[0m[0m | time: 55.822s
[2K
| RMSProp | epoch: 024 | loss: 0.12627 - acc: 0.9641 -- iter: 1920/2003
[A[ATraining Step: 1510  | total loss: [1m[32m0.11660[0m[0m | time: 56.735s
[2K
| RMSProp | epoch: 024 | loss: 0.11660 - acc: 0.9677 -- iter: 1952/2003
[A[ATraining Step: 1511  | total loss: [1m[32m0.10636[0m[0m | time: 57.685s
[2K
| RMSProp | epoch: 024 | loss: 0.10636 - acc: 0.9709 -- iter: 1984/2003
[A[ATraining Step: 1512  | total loss: [1m[32m0.09602[0m[0m | time: 61.684s
[2K
| RMSProp | epoch: 024 | loss: 0.09602 - acc: 0.9738 | val_loss: 0.54092 - val_acc: 0.8802 -- iter: 2003/2003
--
Training Step: 1513  | total loss: [1m[32m0.08707[0m[0m | time: 0.994s
[2K
| RMSProp | epoch: 025 | loss: 0.08707 - acc: 0.9764 -- iter: 0032/2003
[A[ATraining Step: 1514  | total loss: [1m[32m0.07887[0m[0m | time: 1.971s
[2K
| RMSProp | epoch: 025 | loss: 0.07887 - acc: 0.9788 -- iter: 0064/2003
[A[ATraining Step: 1515  | total loss: [1m[32m0.07120[0m[0m | time: 2.888s
[2K
| RMSProp | epoch: 025 | loss: 0.07120 - acc: 0.9809 -- iter: 0096/2003
[A[ATraining Step: 1516  | total loss: [1m[32m0.06420[0m[0m | time: 3.926s
[2K
| RMSProp | epoch: 025 | loss: 0.06420 - acc: 0.9828 -- iter: 0128/2003
[A[ATraining Step: 1517  | total loss: [1m[32m0.05781[0m[0m | time: 4.836s
[2K
| RMSProp | epoch: 025 | loss: 0.05781 - acc: 0.9845 -- iter: 0160/2003
[A[ATraining Step: 1518  | total loss: [1m[32m0.05209[0m[0m | time: 5.711s
[2K
| RMSProp | epoch: 025 | loss: 0.05209 - acc: 0.9861 -- iter: 0192/2003
[A[ATraining Step: 1519  | total loss: [1m[32m0.04701[0m[0m | time: 6.706s
[2K
| RMSProp | epoch: 025 | loss: 0.04701 - acc: 0.9875 -- iter: 0224/2003
[A[ATraining Step: 1520  | total loss: [1m[32m0.04237[0m[0m | time: 7.767s
[2K
| RMSProp | epoch: 025 | loss: 0.04237 - acc: 0.9887 -- iter: 0256/2003
[A[ATraining Step: 1521  | total loss: [1m[32m0.04007[0m[0m | time: 8.824s
[2K
| RMSProp | epoch: 025 | loss: 0.04007 - acc: 0.9899 -- iter: 0288/2003
[A[ATraining Step: 1522  | total loss: [1m[32m0.03620[0m[0m | time: 9.575s
[2K
| RMSProp | epoch: 025 | loss: 0.03620 - acc: 0.9909 -- iter: 0320/2003
[A[ATraining Step: 1523  | total loss: [1m[32m0.03696[0m[0m | time: 10.542s
[2K
| RMSProp | epoch: 025 | loss: 0.03696 - acc: 0.9887 -- iter: 0352/2003
[A[ATraining Step: 1524  | total loss: [1m[32m0.04662[0m[0m | time: 11.541s
[2K
| RMSProp | epoch: 025 | loss: 0.04662 - acc: 0.9867 -- iter: 0384/2003
[A[ATraining Step: 1525  | total loss: [1m[32m0.04479[0m[0m | time: 12.495s
[2K
| RMSProp | epoch: 025 | loss: 0.04479 - acc: 0.9880 -- iter: 0416/2003
[A[ATraining Step: 1526  | total loss: [1m[32m0.04335[0m[0m | time: 13.416s
[2K
| RMSProp | epoch: 025 | loss: 0.04335 - acc: 0.9892 -- iter: 0448/2003
[A[ATraining Step: 1527  | total loss: [1m[32m0.05765[0m[0m | time: 14.437s
[2K
| RMSProp | epoch: 025 | loss: 0.05765 - acc: 0.9872 -- iter: 0480/2003
[A[ATraining Step: 1528  | total loss: [1m[32m0.05358[0m[0m | time: 15.304s
[2K
| RMSProp | epoch: 025 | loss: 0.05358 - acc: 0.9884 -- iter: 0512/2003
[A[ATraining Step: 1529  | total loss: [1m[32m0.04828[0m[0m | time: 16.186s
[2K
| RMSProp | epoch: 025 | loss: 0.04828 - acc: 0.9896 -- iter: 0544/2003
[A[ATraining Step: 1530  | total loss: [1m[32m0.06752[0m[0m | time: 17.263s
[2K
| RMSProp | epoch: 025 | loss: 0.06752 - acc: 0.9875 -- iter: 0576/2003
[A[ATraining Step: 1531  | total loss: [1m[32m0.08159[0m[0m | time: 18.364s
[2K
| RMSProp | epoch: 025 | loss: 0.08159 - acc: 0.9856 -- iter: 0608/2003
[A[ATraining Step: 1532  | total loss: [1m[32m0.07525[0m[0m | time: 19.217s
[2K
| RMSProp | epoch: 025 | loss: 0.07525 - acc: 0.9871 -- iter: 0640/2003
[A[ATraining Step: 1533  | total loss: [1m[32m0.06823[0m[0m | time: 20.048s
[2K
| RMSProp | epoch: 025 | loss: 0.06823 - acc: 0.9884 -- iter: 0672/2003
[A[ATraining Step: 1534  | total loss: [1m[32m0.06185[0m[0m | time: 20.989s
[2K
| RMSProp | epoch: 025 | loss: 0.06185 - acc: 0.9895 -- iter: 0704/2003
[A[ATraining Step: 1535  | total loss: [1m[32m0.05778[0m[0m | time: 21.593s
[2K
| RMSProp | epoch: 025 | loss: 0.05778 - acc: 0.9906 -- iter: 0736/2003
[A[ATraining Step: 1536  | total loss: [1m[32m0.06175[0m[0m | time: 22.189s
[2K
| RMSProp | epoch: 025 | loss: 0.06175 - acc: 0.9863 -- iter: 0768/2003
[A[ATraining Step: 1537  | total loss: [1m[32m0.05579[0m[0m | time: 23.091s
[2K
| RMSProp | epoch: 025 | loss: 0.05579 - acc: 0.9876 -- iter: 0800/2003
[A[ATraining Step: 1538  | total loss: [1m[32m0.05919[0m[0m | time: 24.112s
[2K
| RMSProp | epoch: 025 | loss: 0.05919 - acc: 0.9857 -- iter: 0832/2003
[A[ATraining Step: 1539  | total loss: [1m[32m0.05357[0m[0m | time: 25.083s
[2K
| RMSProp | epoch: 025 | loss: 0.05357 - acc: 0.9872 -- iter: 0864/2003
[A[ATraining Step: 1540  | total loss: [1m[32m0.09073[0m[0m | time: 25.987s
[2K
| RMSProp | epoch: 025 | loss: 0.09073 - acc: 0.9822 -- iter: 0896/2003
[A[ATraining Step: 1541  | total loss: [1m[32m0.08620[0m[0m | time: 26.846s
[2K
| RMSProp | epoch: 025 | loss: 0.08620 - acc: 0.9809 -- iter: 0928/2003
[A[ATraining Step: 1542  | total loss: [1m[32m0.07831[0m[0m | time: 27.899s
[2K
| RMSProp | epoch: 025 | loss: 0.07831 - acc: 0.9828 -- iter: 0960/2003
[A[ATraining Step: 1543  | total loss: [1m[32m0.07518[0m[0m | time: 28.976s
[2K
| RMSProp | epoch: 025 | loss: 0.07518 - acc: 0.9845 -- iter: 0992/2003
[A[ATraining Step: 1544  | total loss: [1m[32m0.06777[0m[0m | time: 29.707s
[2K
| RMSProp | epoch: 025 | loss: 0.06777 - acc: 0.9860 -- iter: 1024/2003
[A[ATraining Step: 1545  | total loss: [1m[32m0.06110[0m[0m | time: 30.625s
[2K
| RMSProp | epoch: 025 | loss: 0.06110 - acc: 0.9874 -- iter: 1056/2003
[A[ATraining Step: 1546  | total loss: [1m[32m0.07694[0m[0m | time: 31.521s
[2K
| RMSProp | epoch: 025 | loss: 0.07694 - acc: 0.9824 -- iter: 1088/2003
[A[ATraining Step: 1547  | total loss: [1m[32m0.07357[0m[0m | time: 32.470s
[2K
| RMSProp | epoch: 025 | loss: 0.07357 - acc: 0.9811 -- iter: 1120/2003
[A[ATraining Step: 1548  | total loss: [1m[32m0.07245[0m[0m | time: 33.347s
[2K
| RMSProp | epoch: 025 | loss: 0.07245 - acc: 0.9798 -- iter: 1152/2003
[A[ATraining Step: 1549  | total loss: [1m[32m0.06859[0m[0m | time: 34.296s
[2K
| RMSProp | epoch: 025 | loss: 0.06859 - acc: 0.9819 -- iter: 1184/2003
[A[ATraining Step: 1550  | total loss: [1m[32m0.06281[0m[0m | time: 35.305s
[2K
| RMSProp | epoch: 025 | loss: 0.06281 - acc: 0.9837 -- iter: 1216/2003
[A[ATraining Step: 1551  | total loss: [1m[32m0.05680[0m[0m | time: 36.178s
[2K
| RMSProp | epoch: 025 | loss: 0.05680 - acc: 0.9853 -- iter: 1248/2003
[A[ATraining Step: 1552  | total loss: [1m[32m0.05239[0m[0m | time: 37.092s
[2K
| RMSProp | epoch: 025 | loss: 0.05239 - acc: 0.9868 -- iter: 1280/2003
[A[ATraining Step: 1553  | total loss: [1m[32m0.04778[0m[0m | time: 38.190s
[2K
| RMSProp | epoch: 025 | loss: 0.04778 - acc: 0.9881 -- iter: 1312/2003
[A[ATraining Step: 1554  | total loss: [1m[32m0.04344[0m[0m | time: 39.282s
[2K
| RMSProp | epoch: 025 | loss: 0.04344 - acc: 0.9893 -- iter: 1344/2003
[A[ATraining Step: 1555  | total loss: [1m[32m0.03956[0m[0m | time: 40.043s
[2K
| RMSProp | epoch: 025 | loss: 0.03956 - acc: 0.9904 -- iter: 1376/2003
[A[ATraining Step: 1556  | total loss: [1m[32m0.05413[0m[0m | time: 40.913s
[2K
| RMSProp | epoch: 025 | loss: 0.05413 - acc: 0.9882 -- iter: 1408/2003
[A[ATraining Step: 1557  | total loss: [1m[32m0.05366[0m[0m | time: 41.840s
[2K
| RMSProp | epoch: 025 | loss: 0.05366 - acc: 0.9863 -- iter: 1440/2003
[A[ATraining Step: 1558  | total loss: [1m[32m0.05097[0m[0m | time: 42.716s
[2K
| RMSProp | epoch: 025 | loss: 0.05097 - acc: 0.9876 -- iter: 1472/2003
[A[ATraining Step: 1559  | total loss: [1m[32m0.04674[0m[0m | time: 43.684s
[2K
| RMSProp | epoch: 025 | loss: 0.04674 - acc: 0.9889 -- iter: 1504/2003
[A[ATraining Step: 1560  | total loss: [1m[32m0.04215[0m[0m | time: 44.743s
[2K
| RMSProp | epoch: 025 | loss: 0.04215 - acc: 0.9900 -- iter: 1536/2003
[A[ATraining Step: 1561  | total loss: [1m[32m0.03828[0m[0m | time: 45.678s
[2K
| RMSProp | epoch: 025 | loss: 0.03828 - acc: 0.9910 -- iter: 1568/2003
[A[ATraining Step: 1562  | total loss: [1m[32m0.03467[0m[0m | time: 46.578s
[2K
| RMSProp | epoch: 025 | loss: 0.03467 - acc: 0.9919 -- iter: 1600/2003
[A[ATraining Step: 1563  | total loss: [1m[32m0.03123[0m[0m | time: 47.511s
[2K
| RMSProp | epoch: 025 | loss: 0.03123 - acc: 0.9927 -- iter: 1632/2003
[A[ATraining Step: 1564  | total loss: [1m[32m0.02936[0m[0m | time: 48.543s
[2K
| RMSProp | epoch: 025 | loss: 0.02936 - acc: 0.9934 -- iter: 1664/2003
[A[ATraining Step: 1565  | total loss: [1m[32m0.02952[0m[0m | time: 49.529s
[2K
| RMSProp | epoch: 025 | loss: 0.02952 - acc: 0.9941 -- iter: 1696/2003
[A[ATraining Step: 1566  | total loss: [1m[32m0.14345[0m[0m | time: 50.224s
[2K
| RMSProp | epoch: 025 | loss: 0.14345 - acc: 0.9759 -- iter: 1728/2003
[A[ATraining Step: 1567  | total loss: [1m[32m0.13976[0m[0m | time: 51.118s
[2K
| RMSProp | epoch: 025 | loss: 0.13976 - acc: 0.9721 -- iter: 1760/2003
[A[ATraining Step: 1568  | total loss: [1m[32m0.13735[0m[0m | time: 51.973s
[2K
| RMSProp | epoch: 025 | loss: 0.13735 - acc: 0.9686 -- iter: 1792/2003
[A[ATraining Step: 1569  | total loss: [1m[32m0.12673[0m[0m | time: 52.951s
[2K
| RMSProp | epoch: 025 | loss: 0.12673 - acc: 0.9686 -- iter: 1824/2003
[A[ATraining Step: 1570  | total loss: [1m[32m0.11848[0m[0m | time: 53.869s
[2K
| RMSProp | epoch: 025 | loss: 0.11848 - acc: 0.9686 -- iter: 1856/2003
[A[ATraining Step: 1571  | total loss: [1m[32m0.11535[0m[0m | time: 54.881s
[2K
| RMSProp | epoch: 025 | loss: 0.11535 - acc: 0.9687 -- iter: 1888/2003
[A[ATraining Step: 1572  | total loss: [1m[32m0.10498[0m[0m | time: 55.756s
[2K
| RMSProp | epoch: 025 | loss: 0.10498 - acc: 0.9718 -- iter: 1920/2003
[A[ATraining Step: 1573  | total loss: [1m[32m0.10227[0m[0m | time: 56.660s
[2K
| RMSProp | epoch: 025 | loss: 0.10227 - acc: 0.9715 -- iter: 1952/2003
[A[ATraining Step: 1574  | total loss: [1m[32m0.10358[0m[0m | time: 57.480s
[2K
| RMSProp | epoch: 025 | loss: 0.10358 - acc: 0.9681 -- iter: 1984/2003
[A[ATraining Step: 1575  | total loss: [1m[32m0.10113[0m[0m | time: 61.680s
[2K
| RMSProp | epoch: 025 | loss: 0.10113 - acc: 0.9682 | val_loss: 0.59306 - val_acc: 0.8339 -- iter: 2003/2003
--
Training Step: 1576  | total loss: [1m[32m0.09620[0m[0m | time: 0.910s
[2K
| RMSProp | epoch: 026 | loss: 0.09620 - acc: 0.9682 -- iter: 0032/2003
[A[ATraining Step: 1577  | total loss: [1m[32m0.09258[0m[0m | time: 1.779s
[2K
| RMSProp | epoch: 026 | loss: 0.09258 - acc: 0.9683 -- iter: 0064/2003
[A[ATraining Step: 1578  | total loss: [1m[32m0.08616[0m[0m | time: 2.721s
[2K
| RMSProp | epoch: 026 | loss: 0.08616 - acc: 0.9714 -- iter: 0096/2003
[A[ATraining Step: 1579  | total loss: [1m[32m0.07800[0m[0m | time: 3.715s
[2K
| RMSProp | epoch: 026 | loss: 0.07800 - acc: 0.9743 -- iter: 0128/2003
[A[ATraining Step: 1580  | total loss: [1m[32m0.07137[0m[0m | time: 4.682s
[2K
| RMSProp | epoch: 026 | loss: 0.07137 - acc: 0.9769 -- iter: 0160/2003
[A[ATraining Step: 1581  | total loss: [1m[32m0.06499[0m[0m | time: 5.647s
[2K
| RMSProp | epoch: 026 | loss: 0.06499 - acc: 0.9792 -- iter: 0192/2003
[A[ATraining Step: 1582  | total loss: [1m[32m0.05960[0m[0m | time: 6.519s
[2K
| RMSProp | epoch: 026 | loss: 0.05960 - acc: 0.9813 -- iter: 0224/2003
[A[ATraining Step: 1583  | total loss: [1m[32m0.05381[0m[0m | time: 7.602s
[2K
| RMSProp | epoch: 026 | loss: 0.05381 - acc: 0.9831 -- iter: 0256/2003
[A[ATraining Step: 1584  | total loss: [1m[32m0.04848[0m[0m | time: 8.777s
[2K
| RMSProp | epoch: 026 | loss: 0.04848 - acc: 0.9848 -- iter: 0288/2003
[A[ATraining Step: 1585  | total loss: [1m[32m0.04397[0m[0m | time: 9.591s
[2K
| RMSProp | epoch: 026 | loss: 0.04397 - acc: 0.9863 -- iter: 0320/2003
[A[ATraining Step: 1586  | total loss: [1m[32m0.03993[0m[0m | time: 10.376s
[2K
| RMSProp | epoch: 026 | loss: 0.03993 - acc: 0.9877 -- iter: 0352/2003
[A[ATraining Step: 1587  | total loss: [1m[32m0.03599[0m[0m | time: 11.273s
[2K
| RMSProp | epoch: 026 | loss: 0.03599 - acc: 0.9889 -- iter: 0384/2003
[A[ATraining Step: 1588  | total loss: [1m[32m0.03355[0m[0m | time: 12.206s
[2K
| RMSProp | epoch: 026 | loss: 0.03355 - acc: 0.9900 -- iter: 0416/2003
[A[ATraining Step: 1589  | total loss: [1m[32m0.04787[0m[0m | time: 13.105s
[2K
| RMSProp | epoch: 026 | loss: 0.04787 - acc: 0.9879 -- iter: 0448/2003
[A[ATraining Step: 1590  | total loss: [1m[32m0.05815[0m[0m | time: 14.069s
[2K
| RMSProp | epoch: 026 | loss: 0.05815 - acc: 0.9860 -- iter: 0480/2003
[A[ATraining Step: 1591  | total loss: [1m[32m0.05685[0m[0m | time: 15.089s
[2K
| RMSProp | epoch: 026 | loss: 0.05685 - acc: 0.9843 -- iter: 0512/2003
[A[ATraining Step: 1592  | total loss: [1m[32m0.05412[0m[0m | time: 16.061s
[2K
| RMSProp | epoch: 026 | loss: 0.05412 - acc: 0.9827 -- iter: 0544/2003
[A[ATraining Step: 1593  | total loss: [1m[32m0.04879[0m[0m | time: 16.931s
[2K
| RMSProp | epoch: 026 | loss: 0.04879 - acc: 0.9844 -- iter: 0576/2003
[A[ATraining Step: 1594  | total loss: [1m[32m0.04595[0m[0m | time: 18.022s
[2K
| RMSProp | epoch: 026 | loss: 0.04595 - acc: 0.9860 -- iter: 0608/2003
[A[ATraining Step: 1595  | total loss: [1m[32m0.04140[0m[0m | time: 19.093s
[2K
| RMSProp | epoch: 026 | loss: 0.04140 - acc: 0.9874 -- iter: 0640/2003
[A[ATraining Step: 1596  | total loss: [1m[32m0.03751[0m[0m | time: 19.853s
[2K
| RMSProp | epoch: 026 | loss: 0.03751 - acc: 0.9887 -- iter: 0672/2003
[A[ATraining Step: 1597  | total loss: [1m[32m0.03380[0m[0m | time: 20.659s
[2K
| RMSProp | epoch: 026 | loss: 0.03380 - acc: 0.9898 -- iter: 0704/2003
[A[ATraining Step: 1598  | total loss: [1m[32m0.03043[0m[0m | time: 21.561s
[2K
| RMSProp | epoch: 026 | loss: 0.03043 - acc: 0.9908 -- iter: 0736/2003
[A[ATraining Step: 1599  | total loss: [1m[32m0.05155[0m[0m | time: 22.154s
[2K
| RMSProp | epoch: 026 | loss: 0.05155 - acc: 0.9886 -- iter: 0768/2003
[A[ATraining Step: 1600  | total loss: [1m[32m0.04714[0m[0m | time: 26.005s
[2K
| RMSProp | epoch: 026 | loss: 0.04714 - acc: 0.9897 | val_loss: 0.58168 - val_acc: 0.8578 -- iter: 0800/2003
--
Training Step: 1601  | total loss: [1m[32m0.04265[0m[0m | time: 26.836s
[2K
| RMSProp | epoch: 026 | loss: 0.04265 - acc: 0.9908 -- iter: 0832/2003
[A[ATraining Step: 1602  | total loss: [1m[32m0.04059[0m[0m | time: 27.709s
[2K
| RMSProp | epoch: 026 | loss: 0.04059 - acc: 0.9917 -- iter: 0864/2003
[A[ATraining Step: 1603  | total loss: [1m[32m0.03776[0m[0m | time: 28.807s
[2K
| RMSProp | epoch: 026 | loss: 0.03776 - acc: 0.9925 -- iter: 0896/2003
[A[ATraining Step: 1604  | total loss: [1m[32m0.03439[0m[0m | time: 29.898s
[2K
| RMSProp | epoch: 026 | loss: 0.03439 - acc: 0.9933 -- iter: 0928/2003
[A[ATraining Step: 1605  | total loss: [1m[32m0.03139[0m[0m | time: 30.678s
[2K
| RMSProp | epoch: 026 | loss: 0.03139 - acc: 0.9939 -- iter: 0960/2003
[A[ATraining Step: 1606  | total loss: [1m[32m0.02839[0m[0m | time: 31.539s
[2K
| RMSProp | epoch: 026 | loss: 0.02839 - acc: 0.9946 -- iter: 0992/2003
[A[ATraining Step: 1607  | total loss: [1m[32m0.02558[0m[0m | time: 32.458s
[2K
| RMSProp | epoch: 026 | loss: 0.02558 - acc: 0.9951 -- iter: 1024/2003
[A[ATraining Step: 1608  | total loss: [1m[32m0.02305[0m[0m | time: 33.381s
[2K
| RMSProp | epoch: 026 | loss: 0.02305 - acc: 0.9956 -- iter: 1056/2003
[A[ATraining Step: 1609  | total loss: [1m[32m0.02076[0m[0m | time: 34.238s
[2K
| RMSProp | epoch: 026 | loss: 0.02076 - acc: 0.9960 -- iter: 1088/2003
[A[ATraining Step: 1610  | total loss: [1m[32m0.01879[0m[0m | time: 35.243s
[2K
| RMSProp | epoch: 026 | loss: 0.01879 - acc: 0.9964 -- iter: 1120/2003
[A[ATraining Step: 1611  | total loss: [1m[32m0.01776[0m[0m | time: 36.218s
[2K
| RMSProp | epoch: 026 | loss: 0.01776 - acc: 0.9968 -- iter: 1152/2003
[A[ATraining Step: 1612  | total loss: [1m[32m0.04668[0m[0m | time: 37.068s
[2K
| RMSProp | epoch: 026 | loss: 0.04668 - acc: 0.9909 -- iter: 1184/2003
[A[ATraining Step: 1613  | total loss: [1m[32m0.07770[0m[0m | time: 38.021s
[2K
| RMSProp | epoch: 026 | loss: 0.07770 - acc: 0.9793 -- iter: 1216/2003
[A[ATraining Step: 1614  | total loss: [1m[32m0.08546[0m[0m | time: 39.066s
[2K
| RMSProp | epoch: 026 | loss: 0.08546 - acc: 0.9751 -- iter: 1248/2003
[A[ATraining Step: 1615  | total loss: [1m[32m0.08397[0m[0m | time: 40.175s
[2K
| RMSProp | epoch: 026 | loss: 0.08397 - acc: 0.9745 -- iter: 1280/2003
[A[ATraining Step: 1616  | total loss: [1m[32m0.07630[0m[0m | time: 40.890s
[2K
| RMSProp | epoch: 026 | loss: 0.07630 - acc: 0.9770 -- iter: 1312/2003
[A[ATraining Step: 1617  | total loss: [1m[32m0.07282[0m[0m | time: 41.752s
[2K
| RMSProp | epoch: 026 | loss: 0.07282 - acc: 0.9793 -- iter: 1344/2003
[A[ATraining Step: 1618  | total loss: [1m[32m0.06570[0m[0m | time: 42.694s
[2K
| RMSProp | epoch: 026 | loss: 0.06570 - acc: 0.9814 -- iter: 1376/2003
[A[ATraining Step: 1619  | total loss: [1m[32m0.06201[0m[0m | time: 43.650s
[2K
| RMSProp | epoch: 026 | loss: 0.06201 - acc: 0.9832 -- iter: 1408/2003
[A[ATraining Step: 1620  | total loss: [1m[32m0.07074[0m[0m | time: 44.519s
[2K
| RMSProp | epoch: 026 | loss: 0.07074 - acc: 0.9818 -- iter: 1440/2003
[A[ATraining Step: 1621  | total loss: [1m[32m0.07588[0m[0m | time: 45.414s
[2K
| RMSProp | epoch: 026 | loss: 0.07588 - acc: 0.9742 -- iter: 1472/2003
[A[ATraining Step: 1622  | total loss: [1m[32m0.07000[0m[0m | time: 46.347s
[2K
| RMSProp | epoch: 026 | loss: 0.07000 - acc: 0.9768 -- iter: 1504/2003
[A[ATraining Step: 1623  | total loss: [1m[32m0.06401[0m[0m | time: 47.255s
[2K
| RMSProp | epoch: 026 | loss: 0.06401 - acc: 0.9791 -- iter: 1536/2003
[A[ATraining Step: 1624  | total loss: [1m[32m0.05803[0m[0m | time: 48.049s
[2K
| RMSProp | epoch: 026 | loss: 0.05803 - acc: 0.9812 -- iter: 1568/2003
[A[ATraining Step: 1625  | total loss: [1m[32m0.06111[0m[0m | time: 49.028s
[2K
| RMSProp | epoch: 026 | loss: 0.06111 - acc: 0.9800 -- iter: 1600/2003
[A[ATraining Step: 1626  | total loss: [1m[32m0.05539[0m[0m | time: 50.026s
[2K
| RMSProp | epoch: 026 | loss: 0.05539 - acc: 0.9820 -- iter: 1632/2003
[A[ATraining Step: 1627  | total loss: [1m[32m0.05205[0m[0m | time: 50.953s
[2K
| RMSProp | epoch: 026 | loss: 0.05205 - acc: 0.9838 -- iter: 1664/2003
[A[ATraining Step: 1628  | total loss: [1m[32m0.04776[0m[0m | time: 51.818s
[2K
| RMSProp | epoch: 026 | loss: 0.04776 - acc: 0.9854 -- iter: 1696/2003
[A[ATraining Step: 1629  | total loss: [1m[32m0.07473[0m[0m | time: 52.735s
[2K
| RMSProp | epoch: 026 | loss: 0.07473 - acc: 0.9806 -- iter: 1728/2003
[A[ATraining Step: 1630  | total loss: [1m[32m0.07704[0m[0m | time: 53.700s
[2K
| RMSProp | epoch: 026 | loss: 0.07704 - acc: 0.9794 -- iter: 1760/2003
[A[ATraining Step: 1631  | total loss: [1m[32m0.08379[0m[0m | time: 54.561s
[2K
| RMSProp | epoch: 026 | loss: 0.08379 - acc: 0.9784 -- iter: 1792/2003
[A[ATraining Step: 1632  | total loss: [1m[32m0.12281[0m[0m | time: 55.422s
[2K
| RMSProp | epoch: 026 | loss: 0.12281 - acc: 0.9711 -- iter: 1824/2003
[A[ATraining Step: 1633  | total loss: [1m[32m0.12732[0m[0m | time: 56.437s
[2K
| RMSProp | epoch: 026 | loss: 0.12732 - acc: 0.9647 -- iter: 1856/2003
[A[ATraining Step: 1634  | total loss: [1m[32m0.11792[0m[0m | time: 57.384s
[2K
| RMSProp | epoch: 026 | loss: 0.11792 - acc: 0.9682 -- iter: 1888/2003
[A[ATraining Step: 1635  | total loss: [1m[32m0.10786[0m[0m | time: 58.274s
[2K
| RMSProp | epoch: 026 | loss: 0.10786 - acc: 0.9714 -- iter: 1920/2003
[A[ATraining Step: 1636  | total loss: [1m[32m0.09749[0m[0m | time: 59.319s
[2K
| RMSProp | epoch: 026 | loss: 0.09749 - acc: 0.9742 -- iter: 1952/2003
[A[ATraining Step: 1637  | total loss: [1m[32m0.08821[0m[0m | time: 60.451s
[2K
| RMSProp | epoch: 026 | loss: 0.08821 - acc: 0.9768 -- iter: 1984/2003
[A[ATraining Step: 1638  | total loss: [1m[32m0.08093[0m[0m | time: 64.275s
[2K
| RMSProp | epoch: 026 | loss: 0.08093 - acc: 0.9791 | val_loss: 0.51319 - val_acc: 0.8786 -- iter: 2003/2003
--
Training Step: 1639  | total loss: [1m[32m0.07347[0m[0m | time: 0.946s
[2K
| RMSProp | epoch: 027 | loss: 0.07347 - acc: 0.9812 -- iter: 0032/2003
[A[ATraining Step: 1640  | total loss: [1m[32m0.06800[0m[0m | time: 1.902s
[2K
| RMSProp | epoch: 027 | loss: 0.06800 - acc: 0.9831 -- iter: 0064/2003
[A[ATraining Step: 1641  | total loss: [1m[32m0.06768[0m[0m | time: 2.764s
[2K
| RMSProp | epoch: 027 | loss: 0.06768 - acc: 0.9817 -- iter: 0096/2003
[A[ATraining Step: 1642  | total loss: [1m[32m0.07142[0m[0m | time: 3.641s
[2K
| RMSProp | epoch: 027 | loss: 0.07142 - acc: 0.9772 -- iter: 0128/2003
[A[ATraining Step: 1643  | total loss: [1m[32m0.06862[0m[0m | time: 4.584s
[2K
| RMSProp | epoch: 027 | loss: 0.06862 - acc: 0.9764 -- iter: 0160/2003
[A[ATraining Step: 1644  | total loss: [1m[32m0.06188[0m[0m | time: 5.769s
[2K
| RMSProp | epoch: 027 | loss: 0.06188 - acc: 0.9788 -- iter: 0192/2003
[A[ATraining Step: 1645  | total loss: [1m[32m0.05960[0m[0m | time: 6.853s
[2K
| RMSProp | epoch: 027 | loss: 0.05960 - acc: 0.9778 -- iter: 0224/2003
[A[ATraining Step: 1646  | total loss: [1m[32m0.05390[0m[0m | time: 7.673s
[2K
| RMSProp | epoch: 027 | loss: 0.05390 - acc: 0.9800 -- iter: 0256/2003
[A[ATraining Step: 1647  | total loss: [1m[32m0.06234[0m[0m | time: 8.598s
[2K
| RMSProp | epoch: 027 | loss: 0.06234 - acc: 0.9789 -- iter: 0288/2003
[A[ATraining Step: 1648  | total loss: [1m[32m0.06367[0m[0m | time: 9.593s
[2K
| RMSProp | epoch: 027 | loss: 0.06367 - acc: 0.9778 -- iter: 0320/2003
[A[ATraining Step: 1649  | total loss: [1m[32m0.06406[0m[0m | time: 10.501s
[2K
| RMSProp | epoch: 027 | loss: 0.06406 - acc: 0.9769 -- iter: 0352/2003
[A[ATraining Step: 1650  | total loss: [1m[32m0.05919[0m[0m | time: 11.424s
[2K
| RMSProp | epoch: 027 | loss: 0.05919 - acc: 0.9792 -- iter: 0384/2003
[A[ATraining Step: 1651  | total loss: [1m[32m0.05465[0m[0m | time: 12.366s
[2K
| RMSProp | epoch: 027 | loss: 0.05465 - acc: 0.9813 -- iter: 0416/2003
[A[ATraining Step: 1652  | total loss: [1m[32m0.05016[0m[0m | time: 13.313s
[2K
| RMSProp | epoch: 027 | loss: 0.05016 - acc: 0.9832 -- iter: 0448/2003
[A[ATraining Step: 1653  | total loss: [1m[32m0.04537[0m[0m | time: 14.249s
[2K
| RMSProp | epoch: 027 | loss: 0.04537 - acc: 0.9849 -- iter: 0480/2003
[A[ATraining Step: 1654  | total loss: [1m[32m0.04095[0m[0m | time: 15.122s
[2K
| RMSProp | epoch: 027 | loss: 0.04095 - acc: 0.9864 -- iter: 0512/2003
[A[ATraining Step: 1655  | total loss: [1m[32m0.03720[0m[0m | time: 16.160s
[2K
| RMSProp | epoch: 027 | loss: 0.03720 - acc: 0.9877 -- iter: 0544/2003
[A[ATraining Step: 1656  | total loss: [1m[32m0.03524[0m[0m | time: 17.187s
[2K
| RMSProp | epoch: 027 | loss: 0.03524 - acc: 0.9890 -- iter: 0576/2003
[A[ATraining Step: 1657  | total loss: [1m[32m0.03195[0m[0m | time: 17.938s
[2K
| RMSProp | epoch: 027 | loss: 0.03195 - acc: 0.9901 -- iter: 0608/2003
[A[ATraining Step: 1658  | total loss: [1m[32m0.02910[0m[0m | time: 18.787s
[2K
| RMSProp | epoch: 027 | loss: 0.02910 - acc: 0.9911 -- iter: 0640/2003
[A[ATraining Step: 1659  | total loss: [1m[32m0.02632[0m[0m | time: 19.720s
[2K
| RMSProp | epoch: 027 | loss: 0.02632 - acc: 0.9920 -- iter: 0672/2003
[A[ATraining Step: 1660  | total loss: [1m[32m0.03418[0m[0m | time: 20.575s
[2K
| RMSProp | epoch: 027 | loss: 0.03418 - acc: 0.9896 -- iter: 0704/2003
[A[ATraining Step: 1661  | total loss: [1m[32m0.04495[0m[0m | time: 21.496s
[2K
| RMSProp | epoch: 027 | loss: 0.04495 - acc: 0.9844 -- iter: 0736/2003
[A[ATraining Step: 1662  | total loss: [1m[32m0.04283[0m[0m | time: 22.387s
[2K
| RMSProp | epoch: 027 | loss: 0.04283 - acc: 0.9829 -- iter: 0768/2003
[A[ATraining Step: 1663  | total loss: [1m[32m0.04416[0m[0m | time: 22.973s
[2K
| RMSProp | epoch: 027 | loss: 0.04416 - acc: 0.9814 -- iter: 0800/2003
[A[ATraining Step: 1664  | total loss: [1m[32m0.05226[0m[0m | time: 23.529s
[2K
| RMSProp | epoch: 027 | loss: 0.05226 - acc: 0.9780 -- iter: 0832/2003
[A[ATraining Step: 1665  | total loss: [1m[32m0.04725[0m[0m | time: 24.540s
[2K
| RMSProp | epoch: 027 | loss: 0.04725 - acc: 0.9802 -- iter: 0864/2003
[A[ATraining Step: 1666  | total loss: [1m[32m0.04473[0m[0m | time: 25.355s
[2K
| RMSProp | epoch: 027 | loss: 0.04473 - acc: 0.9822 -- iter: 0896/2003
[A[ATraining Step: 1667  | total loss: [1m[32m0.05523[0m[0m | time: 26.365s
[2K
| RMSProp | epoch: 027 | loss: 0.05523 - acc: 0.9777 -- iter: 0928/2003
[A[ATraining Step: 1668  | total loss: [1m[32m0.05566[0m[0m | time: 27.447s
[2K
| RMSProp | epoch: 027 | loss: 0.05566 - acc: 0.9768 -- iter: 0960/2003
[A[ATraining Step: 1669  | total loss: [1m[32m0.05832[0m[0m | time: 28.243s
[2K
| RMSProp | epoch: 027 | loss: 0.05832 - acc: 0.9729 -- iter: 0992/2003
[A[ATraining Step: 1670  | total loss: [1m[32m0.05314[0m[0m | time: 29.058s
[2K
| RMSProp | epoch: 027 | loss: 0.05314 - acc: 0.9756 -- iter: 1024/2003
[A[ATraining Step: 1671  | total loss: [1m[32m0.05032[0m[0m | time: 30.028s
[2K
| RMSProp | epoch: 027 | loss: 0.05032 - acc: 0.9781 -- iter: 1056/2003
[A[ATraining Step: 1672  | total loss: [1m[32m0.04651[0m[0m | time: 30.872s
[2K
| RMSProp | epoch: 027 | loss: 0.04651 - acc: 0.9802 -- iter: 1088/2003
[A[ATraining Step: 1673  | total loss: [1m[32m0.06664[0m[0m | time: 31.895s
[2K
| RMSProp | epoch: 027 | loss: 0.06664 - acc: 0.9760 -- iter: 1120/2003
[A[ATraining Step: 1674  | total loss: [1m[32m0.06140[0m[0m | time: 32.767s
[2K
| RMSProp | epoch: 027 | loss: 0.06140 - acc: 0.9784 -- iter: 1152/2003
[A[ATraining Step: 1675  | total loss: [1m[32m0.05535[0m[0m | time: 33.686s
[2K
| RMSProp | epoch: 027 | loss: 0.05535 - acc: 0.9805 -- iter: 1184/2003
[A[ATraining Step: 1676  | total loss: [1m[32m0.05014[0m[0m | time: 34.686s
[2K
| RMSProp | epoch: 027 | loss: 0.05014 - acc: 0.9825 -- iter: 1216/2003
[A[ATraining Step: 1677  | total loss: [1m[32m0.04548[0m[0m | time: 35.518s
[2K
| RMSProp | epoch: 027 | loss: 0.04548 - acc: 0.9842 -- iter: 1248/2003
[A[ATraining Step: 1678  | total loss: [1m[32m0.05069[0m[0m | time: 36.451s
[2K
| RMSProp | epoch: 027 | loss: 0.05069 - acc: 0.9827 -- iter: 1280/2003
[A[ATraining Step: 1679  | total loss: [1m[32m0.04618[0m[0m | time: 37.604s
[2K
| RMSProp | epoch: 027 | loss: 0.04618 - acc: 0.9844 -- iter: 1312/2003
[A[ATraining Step: 1680  | total loss: [1m[32m0.04276[0m[0m | time: 38.575s
[2K
| RMSProp | epoch: 027 | loss: 0.04276 - acc: 0.9860 -- iter: 1344/2003
[A[ATraining Step: 1681  | total loss: [1m[32m0.03867[0m[0m | time: 39.334s
[2K
| RMSProp | epoch: 027 | loss: 0.03867 - acc: 0.9874 -- iter: 1376/2003
[A[ATraining Step: 1682  | total loss: [1m[32m0.03489[0m[0m | time: 40.211s
[2K
| RMSProp | epoch: 027 | loss: 0.03489 - acc: 0.9886 -- iter: 1408/2003
[A[ATraining Step: 1683  | total loss: [1m[32m0.03143[0m[0m | time: 41.089s
[2K
| RMSProp | epoch: 027 | loss: 0.03143 - acc: 0.9898 -- iter: 1440/2003
[A[ATraining Step: 1684  | total loss: [1m[32m0.02835[0m[0m | time: 41.996s
[2K
| RMSProp | epoch: 027 | loss: 0.02835 - acc: 0.9908 -- iter: 1472/2003
[A[ATraining Step: 1685  | total loss: [1m[32m0.02607[0m[0m | time: 42.905s
[2K
| RMSProp | epoch: 027 | loss: 0.02607 - acc: 0.9917 -- iter: 1504/2003
[A[ATraining Step: 1686  | total loss: [1m[32m0.02577[0m[0m | time: 43.827s
[2K
| RMSProp | epoch: 027 | loss: 0.02577 - acc: 0.9894 -- iter: 1536/2003
[A[ATraining Step: 1687  | total loss: [1m[32m0.03839[0m[0m | time: 44.684s
[2K
| RMSProp | epoch: 027 | loss: 0.03839 - acc: 0.9842 -- iter: 1568/2003
[A[ATraining Step: 1688  | total loss: [1m[32m0.05603[0m[0m | time: 45.597s
[2K
| RMSProp | epoch: 027 | loss: 0.05603 - acc: 0.9764 -- iter: 1600/2003
[A[ATraining Step: 1689  | total loss: [1m[32m0.05070[0m[0m | time: 46.504s
[2K
| RMSProp | epoch: 027 | loss: 0.05070 - acc: 0.9788 -- iter: 1632/2003
[A[ATraining Step: 1690  | total loss: [1m[32m0.05289[0m[0m | time: 47.547s
[2K
| RMSProp | epoch: 027 | loss: 0.05289 - acc: 0.9778 -- iter: 1664/2003
[A[ATraining Step: 1691  | total loss: [1m[32m0.05108[0m[0m | time: 48.545s
[2K
| RMSProp | epoch: 027 | loss: 0.05108 - acc: 0.9769 -- iter: 1696/2003
[A[ATraining Step: 1692  | total loss: [1m[32m0.04608[0m[0m | time: 49.313s
[2K
| RMSProp | epoch: 027 | loss: 0.04608 - acc: 0.9792 -- iter: 1728/2003
[A[ATraining Step: 1693  | total loss: [1m[32m0.04196[0m[0m | time: 50.209s
[2K
| RMSProp | epoch: 027 | loss: 0.04196 - acc: 0.9813 -- iter: 1760/2003
[A[ATraining Step: 1694  | total loss: [1m[32m0.03788[0m[0m | time: 51.140s
[2K
| RMSProp | epoch: 027 | loss: 0.03788 - acc: 0.9831 -- iter: 1792/2003
[A[ATraining Step: 1695  | total loss: [1m[32m0.03422[0m[0m | time: 52.038s
[2K
| RMSProp | epoch: 027 | loss: 0.03422 - acc: 0.9848 -- iter: 1824/2003
[A[ATraining Step: 1696  | total loss: [1m[32m0.06466[0m[0m | time: 52.926s
[2K
| RMSProp | epoch: 027 | loss: 0.06466 - acc: 0.9832 -- iter: 1856/2003
[A[ATraining Step: 1697  | total loss: [1m[32m0.06043[0m[0m | time: 53.875s
[2K
| RMSProp | epoch: 027 | loss: 0.06043 - acc: 0.9849 -- iter: 1888/2003
[A[ATraining Step: 1698  | total loss: [1m[32m0.05510[0m[0m | time: 54.878s
[2K
| RMSProp | epoch: 027 | loss: 0.05510 - acc: 0.9864 -- iter: 1920/2003
[A[ATraining Step: 1699  | total loss: [1m[32m0.04991[0m[0m | time: 55.864s
[2K
| RMSProp | epoch: 027 | loss: 0.04991 - acc: 0.9878 -- iter: 1952/2003
[A[ATraining Step: 1700  | total loss: [1m[32m0.04538[0m[0m | time: 56.811s
[2K
| RMSProp | epoch: 027 | loss: 0.04538 - acc: 0.9890 -- iter: 1984/2003
[A[ATraining Step: 1701  | total loss: [1m[32m0.04583[0m[0m | time: 61.071s
[2K
| RMSProp | epoch: 027 | loss: 0.04583 - acc: 0.9870 | val_loss: 0.70393 - val_acc: 0.8179 -- iter: 2003/2003
--
Training Step: 1702  | total loss: [1m[32m0.04954[0m[0m | time: 0.867s
[2K
| RMSProp | epoch: 028 | loss: 0.04954 - acc: 0.9851 -- iter: 0032/2003
[A[ATraining Step: 1703  | total loss: [1m[32m0.06726[0m[0m | time: 1.848s
[2K
| RMSProp | epoch: 028 | loss: 0.06726 - acc: 0.9835 -- iter: 0064/2003
[A[ATraining Step: 1704  | total loss: [1m[32m0.06091[0m[0m | time: 2.786s
[2K
| RMSProp | epoch: 028 | loss: 0.06091 - acc: 0.9852 -- iter: 0096/2003
[A[ATraining Step: 1705  | total loss: [1m[32m0.05571[0m[0m | time: 3.778s
[2K
| RMSProp | epoch: 028 | loss: 0.05571 - acc: 0.9866 -- iter: 0128/2003
[A[ATraining Step: 1706  | total loss: [1m[32m0.05071[0m[0m | time: 4.783s
[2K
| RMSProp | epoch: 028 | loss: 0.05071 - acc: 0.9880 -- iter: 0160/2003
[A[ATraining Step: 1707  | total loss: [1m[32m0.04736[0m[0m | time: 5.735s
[2K
| RMSProp | epoch: 028 | loss: 0.04736 - acc: 0.9892 -- iter: 0192/2003
[A[ATraining Step: 1708  | total loss: [1m[32m0.04400[0m[0m | time: 6.778s
[2K
| RMSProp | epoch: 028 | loss: 0.04400 - acc: 0.9903 -- iter: 0224/2003
[A[ATraining Step: 1709  | total loss: [1m[32m0.04202[0m[0m | time: 7.955s
[2K
| RMSProp | epoch: 028 | loss: 0.04202 - acc: 0.9912 -- iter: 0256/2003
[A[ATraining Step: 1710  | total loss: [1m[32m0.07811[0m[0m | time: 9.056s
[2K
| RMSProp | epoch: 028 | loss: 0.07811 - acc: 0.9827 -- iter: 0288/2003
[A[ATraining Step: 1711  | total loss: [1m[32m0.07877[0m[0m | time: 9.839s
[2K
| RMSProp | epoch: 028 | loss: 0.07877 - acc: 0.9813 -- iter: 0320/2003
[A[ATraining Step: 1712  | total loss: [1m[32m0.07101[0m[0m | time: 10.653s
[2K
| RMSProp | epoch: 028 | loss: 0.07101 - acc: 0.9832 -- iter: 0352/2003
[A[ATraining Step: 1713  | total loss: [1m[32m0.06400[0m[0m | time: 11.579s
[2K
| RMSProp | epoch: 028 | loss: 0.06400 - acc: 0.9849 -- iter: 0384/2003
[A[ATraining Step: 1714  | total loss: [1m[32m0.05829[0m[0m | time: 12.530s
[2K
| RMSProp | epoch: 028 | loss: 0.05829 - acc: 0.9864 -- iter: 0416/2003
[A[ATraining Step: 1715  | total loss: [1m[32m0.05254[0m[0m | time: 13.477s
[2K
| RMSProp | epoch: 028 | loss: 0.05254 - acc: 0.9878 -- iter: 0448/2003
[A[ATraining Step: 1716  | total loss: [1m[32m0.05142[0m[0m | time: 14.439s
[2K
| RMSProp | epoch: 028 | loss: 0.05142 - acc: 0.9859 -- iter: 0480/2003
[A[ATraining Step: 1717  | total loss: [1m[32m0.07429[0m[0m | time: 15.278s
[2K
| RMSProp | epoch: 028 | loss: 0.07429 - acc: 0.9810 -- iter: 0512/2003
[A[ATraining Step: 1718  | total loss: [1m[32m0.07035[0m[0m | time: 16.279s
[2K
| RMSProp | epoch: 028 | loss: 0.07035 - acc: 0.9798 -- iter: 0544/2003
[A[ATraining Step: 1719  | total loss: [1m[32m0.07003[0m[0m | time: 17.107s
[2K
| RMSProp | epoch: 028 | loss: 0.07003 - acc: 0.9787 -- iter: 0576/2003
[A[ATraining Step: 1720  | total loss: [1m[32m0.06874[0m[0m | time: 18.284s
[2K
| RMSProp | epoch: 028 | loss: 0.06874 - acc: 0.9777 -- iter: 0608/2003
[A[ATraining Step: 1721  | total loss: [1m[32m0.06675[0m[0m | time: 19.314s
[2K
| RMSProp | epoch: 028 | loss: 0.06675 - acc: 0.9768 -- iter: 0640/2003
[A[ATraining Step: 1722  | total loss: [1m[32m0.06068[0m[0m | time: 20.069s
[2K
| RMSProp | epoch: 028 | loss: 0.06068 - acc: 0.9791 -- iter: 0672/2003
[A[ATraining Step: 1723  | total loss: [1m[32m0.05531[0m[0m | time: 21.040s
[2K
| RMSProp | epoch: 028 | loss: 0.05531 - acc: 0.9812 -- iter: 0704/2003
[A[ATraining Step: 1724  | total loss: [1m[32m0.04997[0m[0m | time: 21.887s
[2K
| RMSProp | epoch: 028 | loss: 0.04997 - acc: 0.9831 -- iter: 0736/2003
[A[ATraining Step: 1725  | total loss: [1m[32m0.04820[0m[0m | time: 22.777s
[2K
| RMSProp | epoch: 028 | loss: 0.04820 - acc: 0.9817 -- iter: 0768/2003
[A[ATraining Step: 1726  | total loss: [1m[32m0.04342[0m[0m | time: 23.712s
[2K
| RMSProp | epoch: 028 | loss: 0.04342 - acc: 0.9835 -- iter: 0800/2003
[A[ATraining Step: 1727  | total loss: [1m[32m0.04690[0m[0m | time: 24.292s
[2K
| RMSProp | epoch: 028 | loss: 0.04690 - acc: 0.9820 -- iter: 0832/2003
[A[ATraining Step: 1728  | total loss: [1m[32m0.07430[0m[0m | time: 24.929s
[2K
| RMSProp | epoch: 028 | loss: 0.07430 - acc: 0.9785 -- iter: 0864/2003
[A[ATraining Step: 1729  | total loss: [1m[32m0.07360[0m[0m | time: 25.849s
[2K
| RMSProp | epoch: 028 | loss: 0.07360 - acc: 0.9807 -- iter: 0896/2003
[A[ATraining Step: 1730  | total loss: [1m[32m0.07596[0m[0m | time: 26.742s
[2K
| RMSProp | epoch: 028 | loss: 0.07596 - acc: 0.9764 -- iter: 0928/2003
[A[ATraining Step: 1731  | total loss: [1m[32m0.08105[0m[0m | time: 27.660s
[2K
| RMSProp | epoch: 028 | loss: 0.08105 - acc: 0.9725 -- iter: 0960/2003
[A[ATraining Step: 1732  | total loss: [1m[32m0.07356[0m[0m | time: 28.658s
[2K
| RMSProp | epoch: 028 | loss: 0.07356 - acc: 0.9752 -- iter: 0992/2003
[A[ATraining Step: 1733  | total loss: [1m[32m0.07554[0m[0m | time: 29.619s
[2K
| RMSProp | epoch: 028 | loss: 0.07554 - acc: 0.9746 -- iter: 1024/2003
[A[ATraining Step: 1734  | total loss: [1m[32m0.06861[0m[0m | time: 30.422s
[2K
| RMSProp | epoch: 028 | loss: 0.06861 - acc: 0.9771 -- iter: 1056/2003
[A[ATraining Step: 1735  | total loss: [1m[32m0.06573[0m[0m | time: 31.312s
[2K
| RMSProp | epoch: 028 | loss: 0.06573 - acc: 0.9763 -- iter: 1088/2003
[A[ATraining Step: 1736  | total loss: [1m[32m0.06028[0m[0m | time: 32.208s
[2K
| RMSProp | epoch: 028 | loss: 0.06028 - acc: 0.9787 -- iter: 1120/2003
[A[ATraining Step: 1737  | total loss: [1m[32m0.05448[0m[0m | time: 33.112s
[2K
| RMSProp | epoch: 028 | loss: 0.05448 - acc: 0.9808 -- iter: 1152/2003
[A[ATraining Step: 1738  | total loss: [1m[32m0.05579[0m[0m | time: 34.047s
[2K
| RMSProp | epoch: 028 | loss: 0.05579 - acc: 0.9796 -- iter: 1184/2003
[A[ATraining Step: 1739  | total loss: [1m[32m0.07310[0m[0m | time: 34.987s
[2K
| RMSProp | epoch: 028 | loss: 0.07310 - acc: 0.9785 -- iter: 1216/2003
[A[ATraining Step: 1740  | total loss: [1m[32m0.08055[0m[0m | time: 35.869s
[2K
| RMSProp | epoch: 028 | loss: 0.08055 - acc: 0.9775 -- iter: 1248/2003
[A[ATraining Step: 1741  | total loss: [1m[32m0.07470[0m[0m | time: 36.916s
[2K
| RMSProp | epoch: 028 | loss: 0.07470 - acc: 0.9798 -- iter: 1280/2003
[A[ATraining Step: 1742  | total loss: [1m[32m0.06908[0m[0m | time: 37.844s
[2K
| RMSProp | epoch: 028 | loss: 0.06908 - acc: 0.9818 -- iter: 1312/2003
[A[ATraining Step: 1743  | total loss: [1m[32m0.06311[0m[0m | time: 38.938s
[2K
| RMSProp | epoch: 028 | loss: 0.06311 - acc: 0.9836 -- iter: 1344/2003
[A[ATraining Step: 1744  | total loss: [1m[32m0.06151[0m[0m | time: 39.946s
[2K
| RMSProp | epoch: 028 | loss: 0.06151 - acc: 0.9821 -- iter: 1376/2003
[A[ATraining Step: 1745  | total loss: [1m[32m0.05646[0m[0m | time: 40.747s
[2K
| RMSProp | epoch: 028 | loss: 0.05646 - acc: 0.9839 -- iter: 1408/2003
[A[ATraining Step: 1746  | total loss: [1m[32m0.05294[0m[0m | time: 41.623s
[2K
| RMSProp | epoch: 028 | loss: 0.05294 - acc: 0.9855 -- iter: 1440/2003
[A[ATraining Step: 1747  | total loss: [1m[32m0.04782[0m[0m | time: 42.480s
[2K
| RMSProp | epoch: 028 | loss: 0.04782 - acc: 0.9870 -- iter: 1472/2003
[A[ATraining Step: 1748  | total loss: [1m[32m0.04624[0m[0m | time: 43.335s
[2K
| RMSProp | epoch: 028 | loss: 0.04624 - acc: 0.9883 -- iter: 1504/2003
[A[ATraining Step: 1749  | total loss: [1m[32m0.06418[0m[0m | time: 44.168s
[2K
| RMSProp | epoch: 028 | loss: 0.06418 - acc: 0.9832 -- iter: 1536/2003
[A[ATraining Step: 1750  | total loss: [1m[32m0.05860[0m[0m | time: 45.067s
[2K
| RMSProp | epoch: 028 | loss: 0.05860 - acc: 0.9849 -- iter: 1568/2003
[A[ATraining Step: 1751  | total loss: [1m[32m0.05382[0m[0m | time: 46.010s
[2K
| RMSProp | epoch: 028 | loss: 0.05382 - acc: 0.9864 -- iter: 1600/2003
[A[ATraining Step: 1752  | total loss: [1m[32m0.04881[0m[0m | time: 46.897s
[2K
| RMSProp | epoch: 028 | loss: 0.04881 - acc: 0.9878 -- iter: 1632/2003
[A[ATraining Step: 1753  | total loss: [1m[32m0.07155[0m[0m | time: 47.715s
[2K
| RMSProp | epoch: 028 | loss: 0.07155 - acc: 0.9859 -- iter: 1664/2003
[A[ATraining Step: 1754  | total loss: [1m[32m0.07143[0m[0m | time: 48.644s
[2K
| RMSProp | epoch: 028 | loss: 0.07143 - acc: 0.9841 -- iter: 1696/2003
[A[ATraining Step: 1755  | total loss: [1m[32m0.06494[0m[0m | time: 49.645s
[2K
| RMSProp | epoch: 028 | loss: 0.06494 - acc: 0.9857 -- iter: 1728/2003
[A[ATraining Step: 1756  | total loss: [1m[32m0.06227[0m[0m | time: 50.715s
[2K
| RMSProp | epoch: 028 | loss: 0.06227 - acc: 0.9840 -- iter: 1760/2003
[A[ATraining Step: 1757  | total loss: [1m[32m0.05684[0m[0m | time: 51.458s
[2K
| RMSProp | epoch: 028 | loss: 0.05684 - acc: 0.9856 -- iter: 1792/2003
[A[ATraining Step: 1758  | total loss: [1m[32m0.05303[0m[0m | time: 52.348s
[2K
| RMSProp | epoch: 028 | loss: 0.05303 - acc: 0.9871 -- iter: 1824/2003
[A[ATraining Step: 1759  | total loss: [1m[32m0.05374[0m[0m | time: 53.195s
[2K
| RMSProp | epoch: 028 | loss: 0.05374 - acc: 0.9821 -- iter: 1856/2003
[A[ATraining Step: 1760  | total loss: [1m[32m0.09039[0m[0m | time: 54.121s
[2K
| RMSProp | epoch: 028 | loss: 0.09039 - acc: 0.9714 -- iter: 1888/2003
[A[ATraining Step: 1761  | total loss: [1m[32m0.08221[0m[0m | time: 55.058s
[2K
| RMSProp | epoch: 028 | loss: 0.08221 - acc: 0.9743 -- iter: 1920/2003
[A[ATraining Step: 1762  | total loss: [1m[32m0.07487[0m[0m | time: 56.067s
[2K
| RMSProp | epoch: 028 | loss: 0.07487 - acc: 0.9768 -- iter: 1952/2003
[A[ATraining Step: 1763  | total loss: [1m[32m0.06756[0m[0m | time: 57.077s
[2K
| RMSProp | epoch: 028 | loss: 0.06756 - acc: 0.9791 -- iter: 1984/2003
[A[ATraining Step: 1764  | total loss: [1m[32m0.06089[0m[0m | time: 61.490s
[2K
| RMSProp | epoch: 028 | loss: 0.06089 - acc: 0.9812 | val_loss: 0.56851 - val_acc: 0.8706 -- iter: 2003/2003
--
Training Step: 1765  | total loss: [1m[32m0.05493[0m[0m | time: 0.768s
[2K
| RMSProp | epoch: 029 | loss: 0.05493 - acc: 0.9831 -- iter: 0032/2003
[A[ATraining Step: 1766  | total loss: [1m[32m0.04953[0m[0m | time: 1.678s
[2K
| RMSProp | epoch: 029 | loss: 0.04953 - acc: 0.9848 -- iter: 0064/2003
[A[ATraining Step: 1767  | total loss: [1m[32m0.04471[0m[0m | time: 2.578s
[2K
| RMSProp | epoch: 029 | loss: 0.04471 - acc: 0.9863 -- iter: 0096/2003
[A[ATraining Step: 1768  | total loss: [1m[32m0.04031[0m[0m | time: 3.492s
[2K
| RMSProp | epoch: 029 | loss: 0.04031 - acc: 0.9877 -- iter: 0128/2003
[A[ATraining Step: 1769  | total loss: [1m[32m0.03656[0m[0m | time: 4.433s
[2K
| RMSProp | epoch: 029 | loss: 0.03656 - acc: 0.9889 -- iter: 0160/2003
[A[ATraining Step: 1770  | total loss: [1m[32m0.04563[0m[0m | time: 5.342s
[2K
| RMSProp | epoch: 029 | loss: 0.04563 - acc: 0.9869 -- iter: 0192/2003
[A[ATraining Step: 1771  | total loss: [1m[32m0.04433[0m[0m | time: 6.258s
[2K
| RMSProp | epoch: 029 | loss: 0.04433 - acc: 0.9882 -- iter: 0224/2003
[A[ATraining Step: 1772  | total loss: [1m[32m0.04038[0m[0m | time: 7.168s
[2K
| RMSProp | epoch: 029 | loss: 0.04038 - acc: 0.9894 -- iter: 0256/2003
[A[ATraining Step: 1773  | total loss: [1m[32m0.03859[0m[0m | time: 7.959s
[2K
| RMSProp | epoch: 029 | loss: 0.03859 - acc: 0.9905 -- iter: 0288/2003
[A[ATraining Step: 1774  | total loss: [1m[32m0.04851[0m[0m | time: 9.109s
[2K
| RMSProp | epoch: 029 | loss: 0.04851 - acc: 0.9883 -- iter: 0320/2003
[A[ATraining Step: 1775  | total loss: [1m[32m0.04641[0m[0m | time: 10.186s
[2K
| RMSProp | epoch: 029 | loss: 0.04641 - acc: 0.9863 -- iter: 0352/2003
[A[ATraining Step: 1776  | total loss: [1m[32m0.04208[0m[0m | time: 11.020s
[2K
| RMSProp | epoch: 029 | loss: 0.04208 - acc: 0.9877 -- iter: 0384/2003
[A[ATraining Step: 1777  | total loss: [1m[32m0.03844[0m[0m | time: 11.966s
[2K
| RMSProp | epoch: 029 | loss: 0.03844 - acc: 0.9889 -- iter: 0416/2003
[A[ATraining Step: 1778  | total loss: [1m[32m0.03471[0m[0m | time: 12.821s
[2K
| RMSProp | epoch: 029 | loss: 0.03471 - acc: 0.9900 -- iter: 0448/2003
[A[ATraining Step: 1779  | total loss: [1m[32m0.03168[0m[0m | time: 13.797s
[2K
| RMSProp | epoch: 029 | loss: 0.03168 - acc: 0.9910 -- iter: 0480/2003
[A[ATraining Step: 1780  | total loss: [1m[32m0.02857[0m[0m | time: 14.682s
[2K
| RMSProp | epoch: 029 | loss: 0.02857 - acc: 0.9919 -- iter: 0512/2003
[A[ATraining Step: 1781  | total loss: [1m[32m0.02573[0m[0m | time: 15.677s
[2K
| RMSProp | epoch: 029 | loss: 0.02573 - acc: 0.9927 -- iter: 0544/2003
[A[ATraining Step: 1782  | total loss: [1m[32m0.02368[0m[0m | time: 16.587s
[2K
| RMSProp | epoch: 029 | loss: 0.02368 - acc: 0.9935 -- iter: 0576/2003
[A[ATraining Step: 1783  | total loss: [1m[32m0.02184[0m[0m | time: 17.433s
[2K
| RMSProp | epoch: 029 | loss: 0.02184 - acc: 0.9941 -- iter: 0608/2003
[A[ATraining Step: 1784  | total loss: [1m[32m0.02005[0m[0m | time: 18.272s
[2K
| RMSProp | epoch: 029 | loss: 0.02005 - acc: 0.9947 -- iter: 0640/2003
[A[ATraining Step: 1785  | total loss: [1m[32m0.01807[0m[0m | time: 19.251s
[2K
| RMSProp | epoch: 029 | loss: 0.01807 - acc: 0.9952 -- iter: 0672/2003
[A[ATraining Step: 1786  | total loss: [1m[32m0.01628[0m[0m | time: 20.372s
[2K
| RMSProp | epoch: 029 | loss: 0.01628 - acc: 0.9957 -- iter: 0704/2003
[A[ATraining Step: 1787  | total loss: [1m[32m0.01478[0m[0m | time: 21.322s
[2K
| RMSProp | epoch: 029 | loss: 0.01478 - acc: 0.9961 -- iter: 0736/2003
[A[ATraining Step: 1788  | total loss: [1m[32m0.01615[0m[0m | time: 22.190s
[2K
| RMSProp | epoch: 029 | loss: 0.01615 - acc: 0.9934 -- iter: 0768/2003
[A[ATraining Step: 1789  | total loss: [1m[32m0.05130[0m[0m | time: 23.068s
[2K
| RMSProp | epoch: 029 | loss: 0.05130 - acc: 0.9816 -- iter: 0800/2003
[A[ATraining Step: 1790  | total loss: [1m[32m0.05935[0m[0m | time: 23.931s
[2K
| RMSProp | epoch: 029 | loss: 0.05935 - acc: 0.9772 -- iter: 0832/2003
[A[ATraining Step: 1791  | total loss: [1m[32m0.05895[0m[0m | time: 24.568s
[2K
| RMSProp | epoch: 029 | loss: 0.05895 - acc: 0.9763 -- iter: 0864/2003
[A[ATraining Step: 1792  | total loss: [1m[32m0.05344[0m[0m | time: 25.207s
[2K
| RMSProp | epoch: 029 | loss: 0.05344 - acc: 0.9787 -- iter: 0896/2003
[A[ATraining Step: 1793  | total loss: [1m[32m0.04828[0m[0m | time: 26.175s
[2K
| RMSProp | epoch: 029 | loss: 0.04828 - acc: 0.9808 -- iter: 0928/2003
[A[ATraining Step: 1794  | total loss: [1m[32m0.04398[0m[0m | time: 27.202s
[2K
| RMSProp | epoch: 029 | loss: 0.04398 - acc: 0.9827 -- iter: 0960/2003
[A[ATraining Step: 1795  | total loss: [1m[32m0.03963[0m[0m | time: 28.178s
[2K
| RMSProp | epoch: 029 | loss: 0.03963 - acc: 0.9845 -- iter: 0992/2003
[A[ATraining Step: 1796  | total loss: [1m[32m0.03570[0m[0m | time: 29.083s
[2K
| RMSProp | epoch: 029 | loss: 0.03570 - acc: 0.9860 -- iter: 1024/2003
[A[ATraining Step: 1797  | total loss: [1m[32m0.03216[0m[0m | time: 30.070s
[2K
| RMSProp | epoch: 029 | loss: 0.03216 - acc: 0.9874 -- iter: 1056/2003
[A[ATraining Step: 1798  | total loss: [1m[32m0.03068[0m[0m | time: 31.109s
[2K
| RMSProp | epoch: 029 | loss: 0.03068 - acc: 0.9887 -- iter: 1088/2003
[A[ATraining Step: 1799  | total loss: [1m[32m0.02895[0m[0m | time: 31.899s
[2K
| RMSProp | epoch: 029 | loss: 0.02895 - acc: 0.9898 -- iter: 1120/2003
[A[ATraining Step: 1800  | total loss: [1m[32m0.02631[0m[0m | time: 35.632s
[2K
| RMSProp | epoch: 029 | loss: 0.02631 - acc: 0.9908 | val_loss: 0.71969 - val_acc: 0.8658 -- iter: 1152/2003
--
Training Step: 1801  | total loss: [1m[32m0.02382[0m[0m | time: 36.567s
[2K
| RMSProp | epoch: 029 | loss: 0.02382 - acc: 0.9917 -- iter: 1184/2003
[A[ATraining Step: 1802  | total loss: [1m[32m0.02173[0m[0m | time: 37.582s
[2K
| RMSProp | epoch: 029 | loss: 0.02173 - acc: 0.9926 -- iter: 1216/2003
[A[ATraining Step: 1803  | total loss: [1m[32m0.01962[0m[0m | time: 38.458s
[2K
| RMSProp | epoch: 029 | loss: 0.01962 - acc: 0.9933 -- iter: 1248/2003
[A[ATraining Step: 1804  | total loss: [1m[32m0.03099[0m[0m | time: 39.351s
[2K
| RMSProp | epoch: 029 | loss: 0.03099 - acc: 0.9877 -- iter: 1280/2003
[A[ATraining Step: 1805  | total loss: [1m[32m0.05770[0m[0m | time: 40.200s
[2K
| RMSProp | epoch: 029 | loss: 0.05770 - acc: 0.9827 -- iter: 1312/2003
[A[ATraining Step: 1806  | total loss: [1m[32m0.05606[0m[0m | time: 41.308s
[2K
| RMSProp | epoch: 029 | loss: 0.05606 - acc: 0.9813 -- iter: 1344/2003
[A[ATraining Step: 1807  | total loss: [1m[32m0.05308[0m[0m | time: 42.328s
[2K
| RMSProp | epoch: 029 | loss: 0.05308 - acc: 0.9832 -- iter: 1376/2003
[A[ATraining Step: 1808  | total loss: [1m[32m0.04827[0m[0m | time: 43.040s
[2K
| RMSProp | epoch: 029 | loss: 0.04827 - acc: 0.9849 -- iter: 1408/2003
[A[ATraining Step: 1809  | total loss: [1m[32m0.04379[0m[0m | time: 43.970s
[2K
| RMSProp | epoch: 029 | loss: 0.04379 - acc: 0.9864 -- iter: 1440/2003
[A[ATraining Step: 1810  | total loss: [1m[32m0.03979[0m[0m | time: 44.938s
[2K
| RMSProp | epoch: 029 | loss: 0.03979 - acc: 0.9877 -- iter: 1472/2003
[A[ATraining Step: 1811  | total loss: [1m[32m0.03585[0m[0m | time: 45.810s
[2K
| RMSProp | epoch: 029 | loss: 0.03585 - acc: 0.9890 -- iter: 1504/2003
[A[ATraining Step: 1812  | total loss: [1m[32m0.03263[0m[0m | time: 46.636s
[2K
| RMSProp | epoch: 029 | loss: 0.03263 - acc: 0.9901 -- iter: 1536/2003
[A[ATraining Step: 1813  | total loss: [1m[32m0.04271[0m[0m | time: 47.626s
[2K
| RMSProp | epoch: 029 | loss: 0.04271 - acc: 0.9879 -- iter: 1568/2003
[A[ATraining Step: 1814  | total loss: [1m[32m0.05389[0m[0m | time: 48.529s
[2K
| RMSProp | epoch: 029 | loss: 0.05389 - acc: 0.9860 -- iter: 1600/2003
[A[ATraining Step: 1815  | total loss: [1m[32m0.04904[0m[0m | time: 49.468s
[2K
| RMSProp | epoch: 029 | loss: 0.04904 - acc: 0.9874 -- iter: 1632/2003
[A[ATraining Step: 1816  | total loss: [1m[32m0.04555[0m[0m | time: 50.334s
[2K
| RMSProp | epoch: 029 | loss: 0.04555 - acc: 0.9887 -- iter: 1664/2003
[A[ATraining Step: 1817  | total loss: [1m[32m0.04126[0m[0m | time: 51.478s
[2K
| RMSProp | epoch: 029 | loss: 0.04126 - acc: 0.9898 -- iter: 1696/2003
[A[ATraining Step: 1818  | total loss: [1m[32m0.04998[0m[0m | time: 52.544s
[2K
| RMSProp | epoch: 029 | loss: 0.04998 - acc: 0.9877 -- iter: 1728/2003
[A[ATraining Step: 1819  | total loss: [1m[32m0.04858[0m[0m | time: 53.333s
[2K
| RMSProp | epoch: 029 | loss: 0.04858 - acc: 0.9858 -- iter: 1760/2003
[A[ATraining Step: 1820  | total loss: [1m[32m0.06455[0m[0m | time: 54.423s
[2K
| RMSProp | epoch: 029 | loss: 0.06455 - acc: 0.9779 -- iter: 1792/2003
[A[ATraining Step: 1821  | total loss: [1m[32m0.06174[0m[0m | time: 55.368s
[2K
| RMSProp | epoch: 029 | loss: 0.06174 - acc: 0.9769 -- iter: 1824/2003
[A[ATraining Step: 1822  | total loss: [1m[32m0.05870[0m[0m | time: 56.289s
[2K
| RMSProp | epoch: 029 | loss: 0.05870 - acc: 0.9761 -- iter: 1856/2003
[A[ATraining Step: 1823  | total loss: [1m[32m0.05794[0m[0m | time: 57.308s
[2K
| RMSProp | epoch: 029 | loss: 0.05794 - acc: 0.9754 -- iter: 1888/2003
[A[ATraining Step: 1824  | total loss: [1m[32m0.06501[0m[0m | time: 58.356s
[2K
| RMSProp | epoch: 029 | loss: 0.06501 - acc: 0.9747 -- iter: 1920/2003
[A[ATraining Step: 1825  | total loss: [1m[32m0.06699[0m[0m | time: 59.298s
[2K
| RMSProp | epoch: 029 | loss: 0.06699 - acc: 0.9741 -- iter: 1952/2003
[A[ATraining Step: 1826  | total loss: [1m[32m0.10085[0m[0m | time: 60.170s
[2K
| RMSProp | epoch: 029 | loss: 0.10085 - acc: 0.9736 -- iter: 1984/2003
[A[ATraining Step: 1827  | total loss: [1m[32m0.09325[0m[0m | time: 64.308s
[2K
| RMSProp | epoch: 029 | loss: 0.09325 - acc: 0.9762 | val_loss: 0.41330 - val_acc: 0.8786 -- iter: 2003/2003
--
Training Step: 1828  | total loss: [1m[32m0.08610[0m[0m | time: 0.952s
[2K
| RMSProp | epoch: 030 | loss: 0.08610 - acc: 0.9786 -- iter: 0032/2003
[A[ATraining Step: 1829  | total loss: [1m[32m0.07830[0m[0m | time: 1.862s
[2K
| RMSProp | epoch: 030 | loss: 0.07830 - acc: 0.9807 -- iter: 0064/2003
[A[ATraining Step: 1830  | total loss: [1m[32m0.07167[0m[0m | time: 2.776s
[2K
| RMSProp | epoch: 030 | loss: 0.07167 - acc: 0.9827 -- iter: 0096/2003
[A[ATraining Step: 1831  | total loss: [1m[32m0.06781[0m[0m | time: 3.630s
[2K
| RMSProp | epoch: 030 | loss: 0.06781 - acc: 0.9844 -- iter: 0128/2003
[A[ATraining Step: 1832  | total loss: [1m[32m0.06146[0m[0m | time: 4.527s
[2K
| RMSProp | epoch: 030 | loss: 0.06146 - acc: 0.9860 -- iter: 0160/2003
[A[ATraining Step: 1833  | total loss: [1m[32m0.06207[0m[0m | time: 5.377s
[2K
| RMSProp | epoch: 030 | loss: 0.06207 - acc: 0.9842 -- iter: 0192/2003
[A[ATraining Step: 1834  | total loss: [1m[32m0.06861[0m[0m | time: 6.291s
[2K
| RMSProp | epoch: 030 | loss: 0.06861 - acc: 0.9827 -- iter: 0224/2003
[A[ATraining Step: 1835  | total loss: [1m[32m0.06222[0m[0m | time: 7.153s
[2K
| RMSProp | epoch: 030 | loss: 0.06222 - acc: 0.9844 -- iter: 0256/2003
[A[ATraining Step: 1836  | total loss: [1m[32m0.05680[0m[0m | time: 8.154s
[2K
| RMSProp | epoch: 030 | loss: 0.05680 - acc: 0.9860 -- iter: 0288/2003
[A[ATraining Step: 1837  | total loss: [1m[32m0.05164[0m[0m | time: 9.207s
[2K
| RMSProp | epoch: 030 | loss: 0.05164 - acc: 0.9874 -- iter: 0320/2003
[A[ATraining Step: 1838  | total loss: [1m[32m0.04742[0m[0m | time: 10.112s
[2K
| RMSProp | epoch: 030 | loss: 0.04742 - acc: 0.9886 -- iter: 0352/2003
[A[ATraining Step: 1839  | total loss: [1m[32m0.04289[0m[0m | time: 10.937s
[2K
| RMSProp | epoch: 030 | loss: 0.04289 - acc: 0.9898 -- iter: 0384/2003
[A[ATraining Step: 1840  | total loss: [1m[32m0.03898[0m[0m | time: 11.754s
[2K
| RMSProp | epoch: 030 | loss: 0.03898 - acc: 0.9908 -- iter: 0416/2003
[A[ATraining Step: 1841  | total loss: [1m[32m0.03517[0m[0m | time: 12.592s
[2K
| RMSProp | epoch: 030 | loss: 0.03517 - acc: 0.9917 -- iter: 0448/2003
[A[ATraining Step: 1842  | total loss: [1m[32m0.03175[0m[0m | time: 13.571s
[2K
| RMSProp | epoch: 030 | loss: 0.03175 - acc: 0.9925 -- iter: 0480/2003
[A[ATraining Step: 1843  | total loss: [1m[32m0.02864[0m[0m | time: 14.486s
[2K
| RMSProp | epoch: 030 | loss: 0.02864 - acc: 0.9933 -- iter: 0512/2003
[A[ATraining Step: 1844  | total loss: [1m[32m0.03188[0m[0m | time: 15.343s
[2K
| RMSProp | epoch: 030 | loss: 0.03188 - acc: 0.9908 -- iter: 0544/2003
[A[ATraining Step: 1845  | total loss: [1m[32m0.03958[0m[0m | time: 16.284s
[2K
| RMSProp | epoch: 030 | loss: 0.03958 - acc: 0.9855 -- iter: 0576/2003
[A[ATraining Step: 1846  | total loss: [1m[32m0.03988[0m[0m | time: 17.155s
[2K
| RMSProp | epoch: 030 | loss: 0.03988 - acc: 0.9870 -- iter: 0608/2003
[A[ATraining Step: 1847  | total loss: [1m[32m0.04179[0m[0m | time: 17.964s
[2K
| RMSProp | epoch: 030 | loss: 0.04179 - acc: 0.9851 -- iter: 0640/2003
[A[ATraining Step: 1848  | total loss: [1m[32m0.03796[0m[0m | time: 18.972s
[2K
| RMSProp | epoch: 030 | loss: 0.03796 - acc: 0.9866 -- iter: 0672/2003
[A[ATraining Step: 1849  | total loss: [1m[32m0.03630[0m[0m | time: 20.004s
[2K
| RMSProp | epoch: 030 | loss: 0.03630 - acc: 0.9880 -- iter: 0704/2003
[A[ATraining Step: 1850  | total loss: [1m[32m0.04198[0m[0m | time: 20.959s
[2K
| RMSProp | epoch: 030 | loss: 0.04198 - acc: 0.9860 -- iter: 0736/2003
[A[ATraining Step: 1851  | total loss: [1m[32m0.04691[0m[0m | time: 21.936s
[2K
| RMSProp | epoch: 030 | loss: 0.04691 - acc: 0.9812 -- iter: 0768/2003
[A[ATraining Step: 1852  | total loss: [1m[32m0.04285[0m[0m | time: 22.903s
[2K
| RMSProp | epoch: 030 | loss: 0.04285 - acc: 0.9831 -- iter: 0800/2003
[A[ATraining Step: 1853  | total loss: [1m[32m0.03882[0m[0m | time: 23.913s
[2K
| RMSProp | epoch: 030 | loss: 0.03882 - acc: 0.9848 -- iter: 0832/2003
[A[ATraining Step: 1854  | total loss: [1m[32m0.03514[0m[0m | time: 24.948s
[2K
| RMSProp | epoch: 030 | loss: 0.03514 - acc: 0.9863 -- iter: 0864/2003
[A[ATraining Step: 1855  | total loss: [1m[32m0.03181[0m[0m | time: 25.568s
[2K
| RMSProp | epoch: 030 | loss: 0.03181 - acc: 0.9877 -- iter: 0896/2003
[A[ATraining Step: 1856  | total loss: [1m[32m0.02891[0m[0m | time: 26.202s
[2K
| RMSProp | epoch: 030 | loss: 0.02891 - acc: 0.9889 -- iter: 0928/2003
[A[ATraining Step: 1857  | total loss: [1m[32m0.02611[0m[0m | time: 27.295s
[2K
| RMSProp | epoch: 030 | loss: 0.02611 - acc: 0.9900 -- iter: 0960/2003
[A[ATraining Step: 1858  | total loss: [1m[32m0.02359[0m[0m | time: 28.359s
[2K
| RMSProp | epoch: 030 | loss: 0.02359 - acc: 0.9910 -- iter: 0992/2003
[A[ATraining Step: 1859  | total loss: [1m[32m0.02218[0m[0m | time: 29.354s
[2K
| RMSProp | epoch: 030 | loss: 0.02218 - acc: 0.9919 -- iter: 1024/2003
[A[ATraining Step: 1860  | total loss: [1m[32m0.02519[0m[0m | time: 30.377s
[2K
| RMSProp | epoch: 030 | loss: 0.02519 - acc: 0.9896 -- iter: 1056/2003
[A[ATraining Step: 1861  | total loss: [1m[32m0.03146[0m[0m | time: 31.318s
[2K
| RMSProp | epoch: 030 | loss: 0.03146 - acc: 0.9844 -- iter: 1088/2003
[A[ATraining Step: 1862  | total loss: [1m[32m0.03029[0m[0m | time: 32.300s
[2K
| RMSProp | epoch: 030 | loss: 0.03029 - acc: 0.9859 -- iter: 1120/2003
[A[ATraining Step: 1863  | total loss: [1m[32m0.02828[0m[0m | time: 33.232s
[2K
| RMSProp | epoch: 030 | loss: 0.02828 - acc: 0.9873 -- iter: 1152/2003
[A[ATraining Step: 1864  | total loss: [1m[32m0.02796[0m[0m | time: 34.153s
[2K
| RMSProp | epoch: 030 | loss: 0.02796 - acc: 0.9886 -- iter: 1184/2003
[A[ATraining Step: 1865  | total loss: [1m[32m0.03186[0m[0m | time: 35.225s
[2K
| RMSProp | epoch: 030 | loss: 0.03186 - acc: 0.9866 -- iter: 1216/2003
[A[ATraining Step: 1866  | total loss: [1m[32m0.02922[0m[0m | time: 36.256s
[2K
| RMSProp | epoch: 030 | loss: 0.02922 - acc: 0.9880 -- iter: 1248/2003
[A[ATraining Step: 1867  | total loss: [1m[32m0.03312[0m[0m | time: 37.325s
[2K
| RMSProp | epoch: 030 | loss: 0.03312 - acc: 0.9860 -- iter: 1280/2003
[A[ATraining Step: 1868  | total loss: [1m[32m0.03825[0m[0m | time: 38.268s
[2K
| RMSProp | epoch: 030 | loss: 0.03825 - acc: 0.9843 -- iter: 1312/2003
[A[ATraining Step: 1869  | total loss: [1m[32m0.03455[0m[0m | time: 39.313s
[2K
| RMSProp | epoch: 030 | loss: 0.03455 - acc: 0.9859 -- iter: 1344/2003
[A[ATraining Step: 1870  | total loss: [1m[32m0.03299[0m[0m | time: 40.288s
[2K
| RMSProp | epoch: 030 | loss: 0.03299 - acc: 0.9873 -- iter: 1376/2003
[A[ATraining Step: 1871  | total loss: [1m[32m0.03104[0m[0m | time: 41.243s
[2K
| RMSProp | epoch: 030 | loss: 0.03104 - acc: 0.9886 -- iter: 1408/2003
[A[ATraining Step: 1872  | total loss: [1m[32m0.02890[0m[0m | time: 42.260s
[2K
| RMSProp | epoch: 030 | loss: 0.02890 - acc: 0.9897 -- iter: 1440/2003
[A[ATraining Step: 1873  | total loss: [1m[32m0.02622[0m[0m | time: 43.276s
[2K
| RMSProp | epoch: 030 | loss: 0.02622 - acc: 0.9907 -- iter: 1472/2003
[A[ATraining Step: 1874  | total loss: [1m[32m0.02678[0m[0m | time: 44.335s
[2K
| RMSProp | epoch: 030 | loss: 0.02678 - acc: 0.9917 -- iter: 1504/2003
[A[ATraining Step: 1875  | total loss: [1m[32m0.02606[0m[0m | time: 45.333s
[2K
| RMSProp | epoch: 030 | loss: 0.02606 - acc: 0.9925 -- iter: 1536/2003
[A[ATraining Step: 1876  | total loss: [1m[32m0.02374[0m[0m | time: 46.409s
[2K
| RMSProp | epoch: 030 | loss: 0.02374 - acc: 0.9932 -- iter: 1568/2003
[A[ATraining Step: 1877  | total loss: [1m[32m0.02147[0m[0m | time: 47.423s
[2K
| RMSProp | epoch: 030 | loss: 0.02147 - acc: 0.9939 -- iter: 1600/2003
[A[ATraining Step: 1878  | total loss: [1m[32m0.02989[0m[0m | time: 48.408s
[2K
| RMSProp | epoch: 030 | loss: 0.02989 - acc: 0.9914 -- iter: 1632/2003
[A[ATraining Step: 1879  | total loss: [1m[32m0.04777[0m[0m | time: 49.361s
[2K
| RMSProp | epoch: 030 | loss: 0.04777 - acc: 0.9860 -- iter: 1664/2003
[A[ATraining Step: 1880  | total loss: [1m[32m0.05227[0m[0m | time: 50.471s
[2K
| RMSProp | epoch: 030 | loss: 0.05227 - acc: 0.9843 -- iter: 1696/2003
[A[ATraining Step: 1881  | total loss: [1m[32m0.04943[0m[0m | time: 51.510s
[2K
| RMSProp | epoch: 030 | loss: 0.04943 - acc: 0.9859 -- iter: 1728/2003
[A[ATraining Step: 1882  | total loss: [1m[32m0.04480[0m[0m | time: 52.427s
[2K
| RMSProp | epoch: 030 | loss: 0.04480 - acc: 0.9873 -- iter: 1760/2003
[A[ATraining Step: 1883  | total loss: [1m[32m0.04088[0m[0m | time: 53.419s
[2K
| RMSProp | epoch: 030 | loss: 0.04088 - acc: 0.9885 -- iter: 1792/2003
[A[ATraining Step: 1884  | total loss: [1m[32m0.03728[0m[0m | time: 54.329s
[2K
| RMSProp | epoch: 030 | loss: 0.03728 - acc: 0.9897 -- iter: 1824/2003
[A[ATraining Step: 1885  | total loss: [1m[32m0.03493[0m[0m | time: 55.313s
[2K
| RMSProp | epoch: 030 | loss: 0.03493 - acc: 0.9907 -- iter: 1856/2003
[A[ATraining Step: 1886  | total loss: [1m[32m0.03159[0m[0m | time: 56.306s
[2K
| RMSProp | epoch: 030 | loss: 0.03159 - acc: 0.9916 -- iter: 1888/2003
[A[ATraining Step: 1887  | total loss: [1m[32m0.02863[0m[0m | time: 57.230s
[2K
| RMSProp | epoch: 030 | loss: 0.02863 - acc: 0.9925 -- iter: 1920/2003
[A[ATraining Step: 1888  | total loss: [1m[32m0.02585[0m[0m | time: 58.216s
[2K
| RMSProp | epoch: 030 | loss: 0.02585 - acc: 0.9932 -- iter: 1952/2003
[A[ATraining Step: 1889  | total loss: [1m[32m0.02346[0m[0m | time: 59.193s
[2K
| RMSProp | epoch: 030 | loss: 0.02346 - acc: 0.9939 -- iter: 1984/2003
[A[ATraining Step: 1890  | total loss: [1m[32m0.02113[0m[0m | time: 63.516s
[2K
| RMSProp | epoch: 030 | loss: 0.02113 - acc: 0.9945 | val_loss: 0.64907 - val_acc: 0.8834 -- iter: 2003/2003
--
2018-08-02 04:53:54.926799: W tensorflow/core/framework/allocator.cc:101] Allocation of 5669536768 exceeds 10% of system memory.
2018-08-02 04:53:57.146119: W tensorflow/core/framework/allocator.cc:101] Allocation of 5669536768 exceeds 10% of system memory.
Validation AUC:0.9376716734910601
Validation AUPRC:0.9483696700378075
Test AUC:0.9432603480834992
Test AUPRC:0.9507278629066209
BestTestF1Score	0.89	0.79	0.9	0.93	0.86	271	21	290	44	0.46
BestTestMCCScore	0.88	0.79	0.89	0.94	0.83	262	16	295	53	0.88
BestTestAccuracyScore	0.88	0.79	0.89	0.94	0.83	262	16	295	53	0.88
BestValidationF1Score	0.88	0.77	0.89	0.91	0.85	261	25	294	46	0.46
BestValidationMCC	0.88	0.78	0.89	0.95	0.81	250	14	305	57	0.88
BestValidationAccuracy	0.88	0.78	0.89	0.95	0.81	250	14	305	57	0.88
TestPredictions (Threshold:0.88)
CHEMBL68534,TN,INACT,0.0	CHEMBL3656543,TP,ACT,1.0	CHEMBL309625,TN,INACT,0.0	CHEMBL3656733,TP,ACT,1.0	CHEMBL1809052,FN,ACT,0.8600000143051147	CHEMBL3656621,TP,ACT,1.0	CHEMBL557525,TN,INACT,0.0	CHEMBL201511,FN,ACT,0.0	CHEMBL558347,FN,ACT,0.17000000178813934	CHEMBL1290072,FN,ACT,0.5400000214576721	CHEMBL287306,TN,INACT,0.0	CHEMBL559882,TN,INACT,0.0	CHEMBL3786668,TP,ACT,1.0	CHEMBL2316152,TN,INACT,0.0	CHEMBL1796186,TN,INACT,0.0	CHEMBL1240677,FN,ACT,0.0	CHEMBL482102,TP,ACT,1.0	CHEMBL3656613,TP,ACT,1.0	CHEMBL1077107,TN,INACT,0.0	CHEMBL394135,TP,ACT,1.0	CHEMBL453336,TN,INACT,0.0	CHEMBL3237941,TN,INACT,0.0	CHEMBL1428762,TN,INACT,0.0	CHEMBL77262,TN,INACT,0.0	CHEMBL3679684,TP,ACT,1.0	CHEMBL3660913,TP,ACT,1.0	CHEMBL3655736,TP,ACT,1.0	CHEMBL3656691,TP,ACT,0.9399999976158142	CHEMBL3656590,TP,ACT,1.0	CHEMBL1667948,TP,ACT,1.0	CHEMBL56671,TN,INACT,0.0	CHEMBL3656610,TP,ACT,1.0	CHEMBL2382019,TN,INACT,0.0	CHEMBL3691604,TN,INACT,0.0	CHEMBL1809070,TP,ACT,1.0	CHEMBL3656592,TP,ACT,1.0	CHEMBL3037916,TP,ACT,1.0	CHEMBL1667943,TP,ACT,1.0	CHEMBL1241679,TN,INACT,0.0	CHEMBL2032031,TP,ACT,1.0	CHEMBL3398115,FN,ACT,0.20000000298023224	CHEMBL113985,TN,INACT,0.0	CHEMBL80915,TN,INACT,0.0	CHEMBL1096717,TP,ACT,1.0	CHEMBL1171273,TN,INACT,0.0	CHEMBL3656675,TP,ACT,0.9900000095367432	CHEMBL288999,TN,INACT,0.0	CHEMBL557725,FN,ACT,0.0	CHEMBL392589,FN,ACT,0.0	CHEMBL149603,TN,INACT,0.0	CHEMBL13690,TN,INACT,0.0	CHEMBL318300,TN,INACT,0.0	CHEMBL3398108,TP,ACT,1.0	CHEMBL1809051,TP,ACT,1.0	CHEMBL1242117,TN,INACT,0.0	CHEMBL89483,TN,INACT,0.0	CHEMBL1372905,FP,INACT,0.9900000095367432	CHEMBL1809078,TP,ACT,1.0	CHEMBL3785519,FN,ACT,0.05000000074505806	CHEMBL455114,FP,INACT,1.0	CHEMBL2088097,TP,ACT,1.0	CHEMBL2047251,TN,INACT,0.0	CHEMBL3656586,TP,ACT,1.0	CHEMBL1160317,TN,INACT,0.0	CHEMBL483108,TN,INACT,0.0	CHEMBL3656556,TP,ACT,1.0	CHEMBL1796181,FN,ACT,0.019999999552965164	CHEMBL201638,TN,INACT,0.20000000298023224	CHEMBL154080,TN,INACT,0.009999999776482582	CHEMBL1078892,TP,ACT,1.0	CHEMBL551031,TN,INACT,0.0	CHEMBL48718,TN,INACT,0.0	CHEMBL1760039,TP,ACT,1.0	CHEMBL1478,TN,INACT,0.0	CHEMBL575669,TP,ACT,1.0	CHEMBL3098319,TN,INACT,0.0	CHEMBL3652859,TP,ACT,1.0	CHEMBL3824290,TP,ACT,1.0	CHEMBL398399,TP,ACT,1.0	CHEMBL1088931,TP,ACT,0.9599999785423279	CHEMBL1933542,TN,INACT,0.0	CHEMBL465577,TP,ACT,1.0	CHEMBL3656716,TP,ACT,1.0	CHEMBL1242845,TN,INACT,0.0	CHEMBL3652835,TP,ACT,1.0	CHEMBL230042,TP,ACT,1.0	CHEMBL1968515,TN,INACT,0.0	CHEMBL3104852,FN,ACT,0.0	CHEMBL426622,TN,INACT,0.0	CHEMBL3781566,TN,INACT,0.0	CHEMBL1253862,FN,ACT,0.019999999552965164	CHEMBL232542,TN,INACT,0.019999999552965164	CHEMBL1256435,TP,ACT,1.0	CHEMBL551064,TP,ACT,1.0	CHEMBL511765,TP,ACT,1.0	CHEMBL77032,TN,INACT,0.009999999776482582	CHEMBL248093,TP,ACT,1.0	CHEMBL3656605,TP,ACT,1.0	CHEMBL3652840,TP,ACT,1.0	CHEMBL3823296,TP,ACT,1.0	CHEMBL233958,TN,INACT,0.09000000357627869	CHEMBL407143,TN,INACT,0.0	CHEMBL251292,TP,ACT,1.0	CHEMBL1668417,TN,INACT,0.0	CHEMBL3656614,TP,ACT,1.0	CHEMBL3133910,TN,INACT,0.0	CHEMBL467136,TP,ACT,0.9599999785423279	CHEMBL1922211,TN,INACT,0.0	CHEMBL1641996,TN,INACT,0.0	CHEMBL2205390,TN,INACT,0.0	CHEMBL248086,TP,ACT,1.0	CHEMBL367442,TN,INACT,0.0	CHEMBL3822976,TP,ACT,1.0	CHEMBL1796179,TN,INACT,0.0	CHEMBL1254309,TN,INACT,0.0	CHEMBL559069,TN,INACT,0.0	CHEMBL1078542,TN,INACT,0.0	CHEMBL2112638,TN,INACT,0.0	CHEMBL318485,TN,INACT,0.0	CHEMBL3824308,TP,ACT,1.0	CHEMBL556670,TN,INACT,0.0	CHEMBL591504,FN,ACT,0.009999999776482582	CHEMBL154911,TN,INACT,0.0	CHEMBL311740,TN,INACT,0.0	CHEMBL572878,FN,ACT,0.0	CHEMBL456964,TN,INACT,0.0	CHEMBL2426289,FP,INACT,1.0	CHEMBL558557,TN,INACT,0.0	CHEMBL92812,TN,INACT,0.0	CHEMBL3652839,FN,ACT,0.18000000715255737	CHEMBL429472,TP,ACT,1.0	CHEMBL457401,TN,INACT,0.0	CHEMBL1094825,TP,ACT,0.9900000095367432	CHEMBL400457,TP,ACT,1.0	CHEMBL1097189,TP,ACT,1.0	CHEMBL120719,TN,INACT,0.0	CHEMBL504408,TN,INACT,0.0	CHEMBL3629605,TN,INACT,0.0	CHEMBL3656731,TP,ACT,0.9800000190734863	CHEMBL2337364,TN,INACT,0.0	CHEMBL3679688,TP,ACT,1.0	CHEMBL3125862,FP,INACT,1.0	CHEMBL1929314,TN,INACT,0.0	CHEMBL1809056,TP,ACT,1.0	CHEMBL416444,TN,INACT,0.0	CHEMBL245769,TN,INACT,0.0	CHEMBL373882,TP,ACT,1.0	CHEMBL560016,TP,ACT,0.9900000095367432	CHEMBL3656672,TP,ACT,1.0	CHEMBL3660970,TP,ACT,0.9800000190734863	CHEMBL2088099,FP,INACT,0.9900000095367432	CHEMBL361660,TN,INACT,0.0	CHEMBL3128226,TN,INACT,0.07000000029802322	CHEMBL270342,TN,INACT,0.009999999776482582	CHEMBL3104855,FN,ACT,0.0	CHEMBL1809053,TP,ACT,1.0	CHEMBL2088096,FN,ACT,0.0	CHEMBL3652854,FN,ACT,0.009999999776482582	CHEMBL3409737,TP,ACT,1.0	CHEMBL602471,TN,INACT,0.0	CHEMBL543600,TN,INACT,0.0	CHEMBL3824296,TP,ACT,1.0	CHEMBL3660895,TP,ACT,0.9700000286102295	CHEMBL3656673,TP,ACT,1.0	CHEMBL388206,TP,ACT,0.949999988079071	CHEMBL3786564,TP,ACT,1.0	CHEMBL102765,TN,INACT,0.0	CHEMBL1762178,TP,ACT,1.0	CHEMBL75680,TN,INACT,0.0	CHEMBL73829,TN,INACT,0.0	CHEMBL402625,TP,ACT,1.0	CHEMBL116893,TN,INACT,0.0	CHEMBL3661000,TP,ACT,1.0	CHEMBL3656573,TP,ACT,1.0	CHEMBL310193,TN,INACT,0.0	CHEMBL277430,FP,INACT,1.0	CHEMBL3786634,TP,ACT,1.0	CHEMBL292866,TN,INACT,0.0	CHEMBL1956888,TN,INACT,0.0	CHEMBL3421963,FP,INACT,0.9700000286102295	CHEMBL458415,TP,ACT,1.0	CHEMBL1090355,TP,ACT,0.9900000095367432	CHEMBL551263,FN,ACT,0.14000000059604645	CHEMBL3655761,TP,ACT,1.0	CHEMBL3660983,TP,ACT,1.0	CHEMBL3660947,TP,ACT,1.0	CHEMBL3421967,TP,ACT,0.9700000286102295	CHEMBL3656606,TP,ACT,1.0	CHEMBL80510,TN,INACT,0.0	CHEMBL542008,FN,ACT,0.0	CHEMBL304271,TN,INACT,0.009999999776482582	CHEMBL584355,TP,ACT,1.0	CHEMBL1933734,TN,INACT,0.0	CHEMBL3656588,TP,ACT,0.9700000286102295	CHEMBL450383,TN,INACT,0.0	CHEMBL205572,TN,INACT,0.10999999940395355	CHEMBL480965,TP,ACT,1.0	CHEMBL3787474,FN,ACT,0.0	CHEMBL308134,TN,INACT,0.0	CHEMBL3656576,TP,ACT,1.0	CHEMBL404939,TN,INACT,0.029999999329447746	CHEMBL1809074,TP,ACT,1.0	CHEMBL390002,TP,ACT,1.0	CHEMBL2023550,TP,ACT,1.0	CHEMBL100312,TN,INACT,0.0	CHEMBL1762170,FN,ACT,0.0	CHEMBL1390871,TN,INACT,0.0	CHEMBL316009,TN,INACT,0.0	CHEMBL460815,TP,ACT,1.0	CHEMBL3746387,TN,INACT,0.0	CHEMBL1078891,TP,ACT,1.0	CHEMBL3785927,TP,ACT,1.0	CHEMBL602472,TN,INACT,0.0	CHEMBL483234,TN,INACT,0.0	CHEMBL2047244,TN,INACT,0.6700000166893005	CHEMBL3652865,TP,ACT,1.0	CHEMBL428414,TP,ACT,1.0	CHEMBL398597,TP,ACT,1.0	CHEMBL71884,TN,INACT,0.0	CHEMBL1916951,TN,INACT,0.0	CHEMBL489627,TN,INACT,0.0	CHEMBL53463,TN,INACT,0.0	CHEMBL1095404,TP,ACT,1.0	CHEMBL134030,TN,INACT,0.0	CHEMBL3652819,TP,ACT,1.0	CHEMBL383030,TN,INACT,0.029999999329447746	CHEMBL1910762,TN,INACT,0.0	CHEMBL245366,TP,ACT,1.0	CHEMBL1809044,TP,ACT,1.0	CHEMBL514016,FN,ACT,0.0	CHEMBL202831,TN,INACT,0.0	CHEMBL261146,TP,ACT,1.0	CHEMBL3398110,TP,ACT,1.0	CHEMBL7699,TN,INACT,0.0	CHEMBL245407,FN,ACT,0.0	CHEMBL284362,TN,INACT,0.009999999776482582	CHEMBL3656661,TP,ACT,1.0	CHEMBL520515,TN,INACT,0.0	CHEMBL77412,TN,INACT,0.019999999552965164	CHEMBL1078965,TN,INACT,0.0	CHEMBL422240,TN,INACT,0.0	CHEMBL518354,FN,ACT,0.75	CHEMBL490251,TN,INACT,0.0	CHEMBL1242288,TN,INACT,0.0	CHEMBL501964,TP,ACT,1.0	CHEMBL393313,TP,ACT,1.0	CHEMBL3656594,TP,ACT,1.0	CHEMBL456796,TN,INACT,0.0	CHEMBL510360,TN,INACT,0.0	CHEMBL315546,TN,INACT,0.0	CHEMBL14326,TN,INACT,0.0	CHEMBL3660937,TP,ACT,0.9900000095367432	CHEMBL1922224,TP,ACT,0.9800000190734863	CHEMBL2031911,TP,ACT,1.0	CHEMBL3652827,TP,ACT,1.0	CHEMBL456760,TN,INACT,0.0	CHEMBL554350,FN,ACT,0.4099999964237213	CHEMBL3656569,TP,ACT,1.0	CHEMBL3655722,TP,ACT,0.9700000286102295	CHEMBL309016,TN,INACT,0.0	CHEMBL131256,TN,INACT,0.0	CHEMBL1809060,TP,ACT,1.0	CHEMBL3358980,TN,INACT,0.0	CHEMBL3421973,TP,ACT,0.9900000095367432	CHEMBL502423,TP,ACT,1.0	CHEMBL3656746,FN,ACT,0.019999999552965164	CHEMBL3656754,FN,ACT,0.550000011920929	CHEMBL1160318,TN,INACT,0.0	CHEMBL1760042,TP,ACT,1.0	CHEMBL311119,TN,INACT,0.0	CHEMBL3652832,TP,ACT,1.0	CHEMBL83228,TN,INACT,0.0	CHEMBL3656537,TP,ACT,1.0	CHEMBL3656721,TP,ACT,0.9900000095367432	CHEMBL442420,FN,ACT,0.0	CHEMBL3656692,TP,ACT,1.0	CHEMBL1277990,TN,INACT,0.0	CHEMBL66127,TN,INACT,0.03999999910593033	CHEMBL1088741,TP,ACT,1.0	CHEMBL2179821,FP,INACT,1.0	CHEMBL344319,TN,INACT,0.0	CHEMBL456112,TN,INACT,0.0	CHEMBL324399,TN,INACT,0.009999999776482582	CHEMBL1688363,TP,ACT,1.0	CHEMBL276711,TN,INACT,0.0	CHEMBL3660974,TP,ACT,1.0	CHEMBL3652841,TP,ACT,0.9900000095367432	CHEMBL1933751,TN,INACT,0.0	CHEMBL2036868,TN,INACT,0.029999999329447746	CHEMBL474046,TN,INACT,0.0	CHEMBL456797,TN,INACT,0.0	CHEMBL87325,TN,INACT,0.0	CHEMBL1165417,TN,INACT,0.0	CHEMBL2086760,FN,ACT,0.8299999833106995	CHEMBL1760044,TP,ACT,1.0	CHEMBL499067,TN,INACT,0.0	CHEMBL549932,TP,ACT,1.0	CHEMBL564417,TP,ACT,1.0	CHEMBL3781192,TN,INACT,0.0	CHEMBL2382017,TN,INACT,0.0	CHEMBL1964269,TP,ACT,1.0	CHEMBL270754,TN,INACT,0.05000000074505806	CHEMBL445590,TN,INACT,0.0	CHEMBL3656653,TP,ACT,1.0	CHEMBL403218,TP,ACT,0.9900000095367432	CHEMBL575448,TP,ACT,1.0	CHEMBL3799956,TN,INACT,0.0	CHEMBL162157,TN,INACT,0.0	CHEMBL444031,FN,ACT,0.07999999821186066	CHEMBL293731,TN,INACT,0.0	CHEMBL211998,TN,INACT,0.0	CHEMBL590521,TN,INACT,0.0	CHEMBL19562,TN,INACT,0.009999999776482582	CHEMBL1642294,TN,INACT,0.0	CHEMBL3133906,TN,INACT,0.0	CHEMBL1791367,TN,INACT,0.0	CHEMBL2032021,TP,ACT,0.9599999785423279	CHEMBL2409602,TN,INACT,0.0	CHEMBL557050,TN,INACT,0.0	CHEMBL106840,TN,INACT,0.0	CHEMBL3656609,TP,ACT,1.0	CHEMBL1090357,TP,ACT,0.9399999976158142	CHEMBL1809079,TP,ACT,0.949999988079071	CHEMBL233373,TN,INACT,0.0	CHEMBL3660956,TP,ACT,1.0	CHEMBL3652846,TP,ACT,1.0	CHEMBL3037913,FN,ACT,0.07999999821186066	CHEMBL80333,TN,INACT,0.009999999776482582	CHEMBL460377,TP,ACT,0.9900000095367432	CHEMBL3656608,TP,ACT,1.0	CHEMBL473354,TP,ACT,1.0	CHEMBL3800448,TN,INACT,0.0	CHEMBL3656729,TP,ACT,1.0	CHEMBL199298,TN,INACT,0.5299999713897705	CHEMBL3629608,TN,INACT,0.0	CHEMBL3660993,TP,ACT,1.0	CHEMBL3656597,TP,ACT,1.0	CHEMBL477069,FN,ACT,0.2800000011920929	CHEMBL132006,TN,INACT,0.0	CHEMBL2335019,TN,INACT,0.0	CHEMBL501256,TN,INACT,0.0	CHEMBL2311808,FP,INACT,1.0	CHEMBL230232,TN,INACT,0.0	CHEMBL76962,TN,INACT,0.0	CHEMBL3660949,TP,ACT,1.0	CHEMBL499587,TN,INACT,0.18000000715255737	CHEMBL3660906,TP,ACT,1.0	CHEMBL2325087,TN,INACT,0.0	CHEMBL1241775,TN,INACT,0.0	CHEMBL3655731,TP,ACT,1.0	CHEMBL1256436,TP,ACT,1.0	CHEMBL132007,TN,INACT,0.0	CHEMBL3656620,TP,ACT,1.0	CHEMBL94581,TN,INACT,0.0	CHEMBL3325480,TN,INACT,0.0	CHEMBL514828,TP,ACT,1.0	CHEMBL513602,TN,INACT,0.0	CHEMBL506169,TP,ACT,1.0	CHEMBL399192,TP,ACT,1.0	CHEMBL443098,TP,ACT,0.9800000190734863	CHEMBL1828882,TN,INACT,0.0	CHEMBL245513,TP,ACT,1.0	CHEMBL3661022,TP,ACT,1.0	CHEMBL87746,TN,INACT,0.0	CHEMBL502198,TP,ACT,1.0	CHEMBL246356,TN,INACT,0.7699999809265137	CHEMBL474015,FP,INACT,0.949999988079071	CHEMBL77869,TN,INACT,0.0	CHEMBL245212,TN,INACT,0.0	CHEMBL69863,TN,INACT,0.0	CHEMBL99801,TN,INACT,0.0	CHEMBL3652811,TP,ACT,0.9900000095367432	CHEMBL564235,TN,INACT,0.0	CHEMBL3656709,TP,ACT,1.0	CHEMBL1916949,TN,INACT,0.0	CHEMBL311135,TN,INACT,0.0	CHEMBL3398118,FN,ACT,0.6200000047683716	CHEMBL445799,TP,ACT,1.0	CHEMBL103667,FP,INACT,1.0	CHEMBL245306,TP,ACT,1.0	CHEMBL1094195,TP,ACT,1.0	CHEMBL3115321,TN,INACT,0.029999999329447746	CHEMBL2409603,TN,INACT,0.0	CHEMBL347854,TN,INACT,0.0	CHEMBL3582442,TP,ACT,0.9900000095367432	CHEMBL304340,FP,INACT,0.9900000095367432	CHEMBL491686,FN,ACT,0.0	CHEMBL560245,TN,INACT,0.0	CHEMBL462162,FN,ACT,0.0	CHEMBL539942,TN,INACT,0.0	CHEMBL286678,TN,INACT,0.009999999776482582	CHEMBL3261191,TN,INACT,0.0	CHEMBL3628816,TN,INACT,0.0	CHEMBL394154,TP,ACT,1.0	CHEMBL3661017,TP,ACT,1.0	CHEMBL529217,TN,INACT,0.0	CHEMBL1242033,TN,INACT,0.2800000011920929	CHEMBL2393373,TN,INACT,0.0	CHEMBL397666,TP,ACT,1.0	CHEMBL519132,TP,ACT,1.0	CHEMBL3652848,TP,ACT,1.0	CHEMBL3655750,TP,ACT,1.0	CHEMBL474208,FP,INACT,0.9800000190734863	CHEMBL189514,TN,INACT,0.0	CHEMBL134042,TN,INACT,0.0	CHEMBL27085,TN,INACT,0.0	CHEMBL1933756,TN,INACT,0.0	CHEMBL3785995,TP,ACT,1.0	CHEMBL3092769,TN,INACT,0.0	CHEMBL3586444,TN,INACT,0.0	CHEMBL1256421,TP,ACT,1.0	CHEMBL1760046,TP,ACT,1.0	CHEMBL394998,TP,ACT,1.0	CHEMBL245445,TP,ACT,1.0	CHEMBL3798556,TN,INACT,0.0	CHEMBL150504,TN,INACT,0.0	CHEMBL1098711,TP,ACT,1.0	CHEMBL505538,TN,INACT,0.11999999731779099	CHEMBL174634,TN,INACT,0.0	CHEMBL483081,TN,INACT,0.0	CHEMBL457179,TN,INACT,0.0	CHEMBL2436978,FN,ACT,0.0	CHEMBL261805,TP,ACT,1.0	CHEMBL1809205,TP,ACT,1.0	CHEMBL1929312,TN,INACT,0.0	CHEMBL269871,TN,INACT,0.0	CHEMBL2372113,FP,INACT,1.0	CHEMBL464552,TP,ACT,0.9300000071525574	CHEMBL1087055,TN,INACT,0.0	CHEMBL495758,TN,INACT,0.0	CHEMBL387794,TP,ACT,1.0	CHEMBL3237857,TN,INACT,0.009999999776482582	CHEMBL1910756,TN,INACT,0.0	CHEMBL7137,TN,INACT,0.0	CHEMBL3787567,TP,ACT,1.0	CHEMBL339077,TN,INACT,0.0	CHEMBL2409596,TN,INACT,0.0	CHEMBL3216699,TP,ACT,1.0	CHEMBL150177,TN,INACT,0.0	CHEMBL3133826,TN,INACT,0.019999999552965164	CHEMBL1809048,TP,ACT,1.0	CHEMBL2031908,TP,ACT,1.0	CHEMBL513227,FN,ACT,0.0	CHEMBL3660980,TP,ACT,1.0	CHEMBL505519,TP,ACT,0.949999988079071	CHEMBL3823577,TP,ACT,1.0	CHEMBL3656563,TP,ACT,1.0	CHEMBL2032020,TP,ACT,1.0	CHEMBL2023555,TP,ACT,1.0	CHEMBL551523,TP,ACT,1.0	CHEMBL2031902,TP,ACT,1.0	CHEMBL3656560,TP,ACT,1.0	CHEMBL515258,TN,INACT,0.0	CHEMBL86771,TN,INACT,0.0	CHEMBL518732,TN,INACT,0.009999999776482582	CHEMBL3660953,TP,ACT,1.0	CHEMBL1667950,TP,ACT,1.0	CHEMBL1241947,TN,INACT,0.0	CHEMBL589064,TN,INACT,0.0	CHEMBL279003,TN,INACT,0.0	CHEMBL251326,TP,ACT,1.0	CHEMBL231609,TP,ACT,1.0	CHEMBL1762110,FN,ACT,0.0	CHEMBL395427,TN,INACT,0.0	CHEMBL1929238,FP,INACT,0.9300000071525574	CHEMBL549303,TN,INACT,0.0	CHEMBL3655738,TP,ACT,1.0	CHEMBL3656738,FN,ACT,0.8799999952316284	CHEMBL335966,TN,INACT,0.029999999329447746	CHEMBL392082,TP,ACT,1.0	CHEMBL322857,TP,ACT,1.0	CHEMBL3335244,TN,INACT,0.0	CHEMBL461015,TP,ACT,1.0	CHEMBL48760,FN,ACT,0.0	CHEMBL1641997,TN,INACT,0.009999999776482582	CHEMBL130106,TN,INACT,0.0	CHEMBL299763,TN,INACT,0.0	CHEMBL404178,TN,INACT,0.0	CHEMBL3656723,TP,ACT,1.0	CHEMBL3037918,TP,ACT,0.9900000095367432	CHEMBL3629607,TN,INACT,0.0	CHEMBL2023992,TP,ACT,1.0	CHEMBL3805002,TP,ACT,1.0	CHEMBL3358966,TN,INACT,0.0	CHEMBL3656634,FN,ACT,0.009999999776482582	CHEMBL86921,TN,INACT,0.0	CHEMBL3823278,TP,ACT,1.0	CHEMBL1809066,TP,ACT,1.0	CHEMBL53898,TN,INACT,0.0	CHEMBL3661016,TP,ACT,1.0	CHEMBL1683952,TN,INACT,0.0	CHEMBL26128,TN,INACT,0.0	CHEMBL3786567,TP,ACT,1.0	CHEMBL3799345,TN,INACT,0.0	CHEMBL1258231,TP,ACT,1.0	CHEMBL551264,TP,ACT,1.0	CHEMBL413828,TP,ACT,1.0	CHEMBL3656580,TP,ACT,1.0	CHEMBL3660982,TP,ACT,1.0	CHEMBL1090744,TP,ACT,1.0	CHEMBL576179,TP,ACT,1.0	CHEMBL1098712,TP,ACT,1.0	CHEMBL52654,TN,INACT,0.0	CHEMBL2023547,FN,ACT,0.0	CHEMBL430206,TN,INACT,0.4399999976158142	CHEMBL1257888,TP,ACT,1.0	CHEMBL102622,TN,INACT,0.0	CHEMBL1944641,TN,INACT,0.0	CHEMBL3656641,TP,ACT,1.0	CHEMBL33175,TN,INACT,0.0	CHEMBL3421966,TP,ACT,1.0	CHEMBL1092013,TN,INACT,0.0	CHEMBL428221,TP,ACT,1.0	CHEMBL76642,TN,INACT,0.0	CHEMBL1809206,TP,ACT,1.0	CHEMBL3652818,FN,ACT,0.0	CHEMBL133477,TN,INACT,0.0	CHEMBL3133834,TN,INACT,0.0	CHEMBL278287,TN,INACT,0.0	CHEMBL2032014,TP,ACT,1.0	CHEMBL3823045,TP,ACT,1.0	CHEMBL3660927,TP,ACT,1.0	CHEMBL475251,FN,ACT,0.0	CHEMBL3612749,TN,INACT,0.0	CHEMBL128000,TN,INACT,0.0	CHEMBL3660915,TP,ACT,1.0	CHEMBL485502,TN,INACT,0.0	CHEMBL1791364,TN,INACT,0.0	CHEMBL3824246,TP,ACT,1.0	CHEMBL405681,TN,INACT,0.0	CHEMBL2036726,TN,INACT,0.05000000074505806	CHEMBL3656741,TP,ACT,1.0	CHEMBL551023,TP,ACT,0.9800000190734863	CHEMBL1289977,FN,ACT,0.0	CHEMBL1241492,TN,INACT,0.0	CHEMBL592210,TN,INACT,0.0	CHEMBL318248,TN,INACT,0.0	CHEMBL3189172,TN,INACT,0.0	CHEMBL3421971,FN,ACT,0.5799999833106995	CHEMBL3409723,FN,ACT,0.10999999940395355	CHEMBL499661,TP,ACT,1.0	CHEMBL1809054,TP,ACT,1.0	CHEMBL3660891,TP,ACT,0.9800000190734863	CHEMBL342280,TN,INACT,0.0	CHEMBL1642288,TN,INACT,0.0	CHEMBL443706,TN,INACT,0.0	CHEMBL3780912,TN,INACT,0.0	CHEMBL3691629,TN,INACT,0.0	CHEMBL1809061,TP,ACT,1.0	CHEMBL2426288,FN,ACT,0.0	CHEMBL55994,TN,INACT,0.0	CHEMBL1240545,TN,INACT,0.0	CHEMBL1079428,TN,INACT,0.0	CHEMBL300974,TN,INACT,0.0	CHEMBL74799,TN,INACT,0.0	CHEMBL3660946,TP,ACT,1.0	CHEMBL450361,TN,INACT,0.0	CHEMBL3656699,TP,ACT,1.0	CHEMBL21108,FP,INACT,0.9300000071525574	CHEMBL3098318,TN,INACT,0.0	CHEMBL3787399,TP,ACT,1.0	CHEMBL231509,TP,ACT,1.0	CHEMBL2147259,TN,INACT,0.0	CHEMBL435831,TN,INACT,0.0	CHEMBL1222711,TP,ACT,1.0	CHEMBL3656686,TP,ACT,1.0	CHEMBL524820,TN,INACT,0.0	CHEMBL346611,TN,INACT,0.0	CHEMBL3656595,TP,ACT,1.0	CHEMBL1796187,TN,INACT,0.0	CHEMBL392641,TP,ACT,1.0	CHEMBL1667944,TP,ACT,1.0	CHEMBL240884,TP,ACT,1.0	CHEMBL3655732,TP,ACT,1.0	CHEMBL1241864,TN,INACT,0.0	CHEMBL540923,TP,ACT,0.9800000190734863	CHEMBL3335238,TN,INACT,0.0	CHEMBL3421972,FN,ACT,0.8700000047683716	CHEMBL539152,TP,ACT,1.0	CHEMBL3660888,TP,ACT,1.0	CHEMBL565235,TP,ACT,1.0	CHEMBL150581,TN,INACT,0.0	CHEMBL404307,TP,ACT,1.0	CHEMBL62701,TN,INACT,0.0	CHEMBL486437,FN,ACT,0.0	CHEMBL1667935,TP,ACT,1.0	CHEMBL3655748,TP,ACT,1.0	CHEMBL230308,TP,ACT,1.0	CHEMBL482088,TP,ACT,1.0	CHEMBL3660957,TP,ACT,1.0	CHEMBL1241858,TN,INACT,0.0	CHEMBL79704,TN,INACT,0.0	CHEMBL3409731,TP,ACT,0.9300000071525574	CHEMBL117369,TN,INACT,0.0	CHEMBL1910760,TN,INACT,0.6899999976158142	CHEMBL559683,TN,INACT,0.0	CHEMBL187007,TN,INACT,0.009999999776482582	CHEMBL1627106,TN,INACT,0.0	CHEMBL1083527,TP,ACT,0.8899999856948853	CHEMBL3679687,FN,ACT,0.0	CHEMBL67655,TN,INACT,0.009999999776482582	CHEMBL1807518,TN,INACT,0.0	CHEMBL1910275,TN,INACT,0.8799999952316284	CHEMBL62699,TN,INACT,0.0	CHEMBL67747,TN,INACT,0.0	CHEMBL1760038,FN,ACT,0.009999999776482582	CHEMBL3656717,TP,ACT,1.0	CHEMBL488101,TN,INACT,0.0	CHEMBL113996,TN,INACT,0.18000000715255737	CHEMBL447951,TP,ACT,1.0	CHEMBL1641991,TN,INACT,0.0	CHEMBL3823017,TP,ACT,1.0	CHEMBL240672,TP,ACT,1.0	CHEMBL559628,TN,INACT,0.0	CHEMBL1080362,TN,INACT,0.0	CHEMBL3652856,TP,ACT,1.0	CHEMBL3652860,TP,ACT,1.0	CHEMBL2032013,TP,ACT,1.0	CHEMBL3660903,TP,ACT,1.0	CHEMBL460468,TP,ACT,1.0	CHEMBL541988,TN,INACT,0.0	CHEMBL2064400,TN,INACT,0.0	CHEMBL1080222,TN,INACT,0.0	CHEMBL563948,TN,INACT,0.0	CHEMBL1809073,TP,ACT,1.0	CHEMBL401766,TP,ACT,1.0	

