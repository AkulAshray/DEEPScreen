CNNModel CHEMBL3401 adam 0.0005 30 32 0 0.6 False True
Number of active compounds :	108
Number of inactive compounds :	108
---------------------------------
Run id: CNNModel_CHEMBL3401_adam_0.0005_30_32_0_0.6_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL3401_adam_0.0005_30_32_0.6_True/
---------------------------------
Training samples: 136
Validation samples: 43
--
Training Step: 1  | time: 1.204s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/136
[A[ATraining Step: 2  | total loss: [1m[32m0.62403[0m[0m | time: 2.077s
[2K
| Adam | epoch: 001 | loss: 0.62403 - acc: 0.4219 -- iter: 064/136
[A[ATraining Step: 3  | total loss: [1m[32m0.68193[0m[0m | time: 2.979s
[2K
| Adam | epoch: 001 | loss: 0.68193 - acc: 0.4091 -- iter: 096/136
[A[ATraining Step: 4  | total loss: [1m[32m0.69023[0m[0m | time: 3.804s
[2K
| Adam | epoch: 001 | loss: 0.69023 - acc: 0.5007 -- iter: 128/136
[A[ATraining Step: 5  | total loss: [1m[32m0.69200[0m[0m | time: 5.127s
[2K
| Adam | epoch: 001 | loss: 0.69200 - acc: 0.5435 | val_loss: 0.69318 - val_acc: 0.4651 -- iter: 136/136
--
Training Step: 6  | total loss: [1m[32m0.69416[0m[0m | time: 0.279s
[2K
| Adam | epoch: 002 | loss: 0.69416 - acc: 0.3548 -- iter: 032/136
[A[ATraining Step: 7  | total loss: [1m[32m0.69361[0m[0m | time: 1.170s
[2K
| Adam | epoch: 002 | loss: 0.69361 - acc: 0.3669 -- iter: 064/136
[A[ATraining Step: 8  | total loss: [1m[32m0.69407[0m[0m | time: 2.062s
[2K
| Adam | epoch: 002 | loss: 0.69407 - acc: 0.3890 -- iter: 096/136
[A[ATraining Step: 9  | total loss: [1m[32m0.69317[0m[0m | time: 2.925s
[2K
| Adam | epoch: 002 | loss: 0.69317 - acc: 0.4643 -- iter: 128/136
[A[ATraining Step: 10  | total loss: [1m[32m0.69240[0m[0m | time: 4.786s
[2K
| Adam | epoch: 002 | loss: 0.69240 - acc: 0.5134 | val_loss: 0.69447 - val_acc: 0.4651 -- iter: 136/136
--
Training Step: 11  | total loss: [1m[32m0.69309[0m[0m | time: 0.312s
[2K
| Adam | epoch: 003 | loss: 0.69309 - acc: 0.4923 -- iter: 032/136
[A[ATraining Step: 12  | total loss: [1m[32m0.69817[0m[0m | time: 0.600s
[2K
| Adam | epoch: 003 | loss: 0.69817 - acc: 0.3832 -- iter: 064/136
[A[ATraining Step: 13  | total loss: [1m[32m0.69976[0m[0m | time: 1.521s
[2K
| Adam | epoch: 003 | loss: 0.69976 - acc: 0.3261 -- iter: 096/136
[A[ATraining Step: 14  | total loss: [1m[32m0.69723[0m[0m | time: 2.166s
[2K
| Adam | epoch: 003 | loss: 0.69723 - acc: 0.3845 -- iter: 128/136
[A[ATraining Step: 15  | total loss: [1m[32m0.69566[0m[0m | time: 3.953s
[2K
| Adam | epoch: 003 | loss: 0.69566 - acc: 0.4297 | val_loss: 0.69317 - val_acc: 0.4651 -- iter: 136/136
--
Training Step: 16  | total loss: [1m[32m0.69504[0m[0m | time: 0.852s
[2K
| Adam | epoch: 004 | loss: 0.69504 - acc: 0.3975 -- iter: 032/136
[A[ATraining Step: 17  | total loss: [1m[32m0.69437[0m[0m | time: 1.091s
[2K
| Adam | epoch: 004 | loss: 0.69437 - acc: 0.4231 -- iter: 064/136
[A[ATraining Step: 18  | total loss: [1m[32m0.69386[0m[0m | time: 1.359s
[2K
| Adam | epoch: 004 | loss: 0.69386 - acc: 0.4930 -- iter: 096/136
[A[ATraining Step: 19  | total loss: [1m[32m0.69369[0m[0m | time: 2.134s
[2K
| Adam | epoch: 004 | loss: 0.69369 - acc: 0.4953 -- iter: 128/136
[A[ATraining Step: 20  | total loss: [1m[32m0.69360[0m[0m | time: 4.051s
[2K
| Adam | epoch: 004 | loss: 0.69360 - acc: 0.4767 | val_loss: 0.69297 - val_acc: 0.5349 -- iter: 136/136
--
Training Step: 21  | total loss: [1m[32m0.69340[0m[0m | time: 0.877s
[2K
| Adam | epoch: 005 | loss: 0.69340 - acc: 0.4937 -- iter: 032/136
[A[ATraining Step: 22  | total loss: [1m[32m0.69332[0m[0m | time: 1.769s
[2K
| Adam | epoch: 005 | loss: 0.69332 - acc: 0.4956 -- iter: 064/136
[A[ATraining Step: 23  | total loss: [1m[32m0.69314[0m[0m | time: 2.054s
[2K
| Adam | epoch: 005 | loss: 0.69314 - acc: 0.5150 -- iter: 096/136
[A[ATraining Step: 24  | total loss: [1m[32m0.69368[0m[0m | time: 2.288s
[2K
| Adam | epoch: 005 | loss: 0.69368 - acc: 0.4405 -- iter: 128/136
[A[ATraining Step: 25  | total loss: [1m[32m0.69384[0m[0m | time: 4.131s
[2K
| Adam | epoch: 005 | loss: 0.69384 - acc: 0.3885 | val_loss: 0.69314 - val_acc: 0.4651 -- iter: 136/136
--
Training Step: 26  | total loss: [1m[32m0.69366[0m[0m | time: 0.895s
[2K
| Adam | epoch: 006 | loss: 0.69366 - acc: 0.4098 -- iter: 032/136
[A[ATraining Step: 27  | total loss: [1m[32m0.69355[0m[0m | time: 1.784s
[2K
| Adam | epoch: 006 | loss: 0.69355 - acc: 0.4169 -- iter: 064/136
[A[ATraining Step: 28  | total loss: [1m[32m0.69344[0m[0m | time: 2.614s
[2K
| Adam | epoch: 006 | loss: 0.69344 - acc: 0.4377 -- iter: 096/136
[A[ATraining Step: 29  | total loss: [1m[32m0.69341[0m[0m | time: 2.871s
[2K
| Adam | epoch: 006 | loss: 0.69341 - acc: 0.4300 -- iter: 128/136
[A[ATraining Step: 30  | total loss: [1m[32m0.69320[0m[0m | time: 4.136s
[2K
| Adam | epoch: 006 | loss: 0.69320 - acc: 0.5058 | val_loss: 0.69340 - val_acc: 0.4651 -- iter: 136/136
--
Training Step: 31  | total loss: [1m[32m0.69287[0m[0m | time: 0.791s
[2K
| Adam | epoch: 007 | loss: 0.69287 - acc: 0.5622 -- iter: 032/136
[A[ATraining Step: 32  | total loss: [1m[32m0.69296[0m[0m | time: 1.662s
[2K
| Adam | epoch: 007 | loss: 0.69296 - acc: 0.5411 -- iter: 064/136
[A[ATraining Step: 33  | total loss: [1m[32m0.69298[0m[0m | time: 2.583s
[2K
| Adam | epoch: 007 | loss: 0.69298 - acc: 0.5321 -- iter: 096/136
[A[ATraining Step: 34  | total loss: [1m[32m0.69293[0m[0m | time: 3.517s
[2K
| Adam | epoch: 007 | loss: 0.69293 - acc: 0.5319 -- iter: 128/136
[A[ATraining Step: 35  | total loss: [1m[32m0.69331[0m[0m | time: 4.772s
[2K
| Adam | epoch: 007 | loss: 0.69331 - acc: 0.5056 | val_loss: 0.69387 - val_acc: 0.4651 -- iter: 136/136
--
Training Step: 36  | total loss: [1m[32m0.69177[0m[0m | time: 0.193s
[2K
| Adam | epoch: 008 | loss: 0.69177 - acc: 0.5812 -- iter: 032/136
[A[ATraining Step: 37  | total loss: [1m[32m0.69028[0m[0m | time: 0.825s
[2K
| Adam | epoch: 008 | loss: 0.69028 - acc: 0.6399 -- iter: 064/136
[A[ATraining Step: 38  | total loss: [1m[32m0.69105[0m[0m | time: 1.438s
[2K
| Adam | epoch: 008 | loss: 0.69105 - acc: 0.6064 -- iter: 096/136
[A[ATraining Step: 39  | total loss: [1m[32m0.69200[0m[0m | time: 2.052s
[2K
| Adam | epoch: 008 | loss: 0.69200 - acc: 0.5741 -- iter: 128/136
[A[ATraining Step: 40  | total loss: [1m[32m0.69218[0m[0m | time: 3.665s
[2K
| Adam | epoch: 008 | loss: 0.69218 - acc: 0.5602 | val_loss: 0.69541 - val_acc: 0.4651 -- iter: 136/136
--
Training Step: 41  | total loss: [1m[32m0.69169[0m[0m | time: 0.179s
[2K
| Adam | epoch: 009 | loss: 0.69169 - acc: 0.5606 -- iter: 032/136
[A[ATraining Step: 42  | total loss: [1m[32m0.69176[0m[0m | time: 0.372s
[2K
| Adam | epoch: 009 | loss: 0.69176 - acc: 0.5497 -- iter: 064/136
[A[ATraining Step: 43  | total loss: [1m[32m0.69193[0m[0m | time: 1.003s
[2K
| Adam | epoch: 009 | loss: 0.69193 - acc: 0.5409 -- iter: 096/136
[A[ATraining Step: 44  | total loss: [1m[32m0.69321[0m[0m | time: 1.619s
[2K
| Adam | epoch: 009 | loss: 0.69321 - acc: 0.5230 -- iter: 128/136
[A[ATraining Step: 45  | total loss: [1m[32m0.69404[0m[0m | time: 3.244s
[2K
| Adam | epoch: 009 | loss: 0.69404 - acc: 0.5085 | val_loss: 0.69570 - val_acc: 0.4651 -- iter: 136/136
--
Training Step: 46  | total loss: [1m[32m0.69295[0m[0m | time: 0.626s
[2K
| Adam | epoch: 010 | loss: 0.69295 - acc: 0.5175 -- iter: 032/136
[A[ATraining Step: 47  | total loss: [1m[32m0.69295[0m[0m | time: 0.817s
[2K
| Adam | epoch: 010 | loss: 0.69295 - acc: 0.5146 -- iter: 064/136
[A[ATraining Step: 48  | total loss: [1m[32m0.69108[0m[0m | time: 1.017s
[2K
| Adam | epoch: 010 | loss: 0.69108 - acc: 0.5324 -- iter: 096/136
[A[ATraining Step: 49  | total loss: [1m[32m0.68925[0m[0m | time: 1.630s
[2K
| Adam | epoch: 010 | loss: 0.68925 - acc: 0.5470 -- iter: 128/136
[A[ATraining Step: 50  | total loss: [1m[32m0.69111[0m[0m | time: 3.258s
[2K
| Adam | epoch: 010 | loss: 0.69111 - acc: 0.5300 | val_loss: 0.69645 - val_acc: 0.4651 -- iter: 136/136
--
Training Step: 51  | total loss: [1m[32m0.69314[0m[0m | time: 0.615s
[2K
| Adam | epoch: 011 | loss: 0.69314 - acc: 0.5111 -- iter: 032/136
[A[ATraining Step: 52  | total loss: [1m[32m0.69064[0m[0m | time: 1.238s
[2K
| Adam | epoch: 011 | loss: 0.69064 - acc: 0.5282 -- iter: 064/136
[A[ATraining Step: 53  | total loss: [1m[32m0.69135[0m[0m | time: 1.422s
[2K
| Adam | epoch: 011 | loss: 0.69135 - acc: 0.5194 -- iter: 096/136
[A[ATraining Step: 54  | total loss: [1m[32m0.68848[0m[0m | time: 1.607s
[2K
| Adam | epoch: 011 | loss: 0.68848 - acc: 0.5348 -- iter: 128/136
[A[ATraining Step: 55  | total loss: [1m[32m0.68555[0m[0m | time: 3.276s
[2K
| Adam | epoch: 011 | loss: 0.68555 - acc: 0.5477 | val_loss: 0.69837 - val_acc: 0.4651 -- iter: 136/136
--
Training Step: 56  | total loss: [1m[32m0.69028[0m[0m | time: 0.636s
[2K
| Adam | epoch: 012 | loss: 0.69028 - acc: 0.5234 -- iter: 032/136
[A[ATraining Step: 57  | total loss: [1m[32m0.69224[0m[0m | time: 1.269s
[2K
| Adam | epoch: 012 | loss: 0.69224 - acc: 0.5115 -- iter: 064/136
[A[ATraining Step: 58  | total loss: [1m[32m0.68961[0m[0m | time: 1.881s
[2K
| Adam | epoch: 012 | loss: 0.68961 - acc: 0.5227 -- iter: 096/136
[A[ATraining Step: 59  | total loss: [1m[32m0.69506[0m[0m | time: 2.071s
[2K
| Adam | epoch: 012 | loss: 0.69506 - acc: 0.4903 -- iter: 128/136
[A[ATraining Step: 60  | total loss: [1m[32m0.70075[0m[0m | time: 3.259s
[2K
| Adam | epoch: 012 | loss: 0.70075 - acc: 0.4419 | val_loss: 0.69213 - val_acc: 0.4651 -- iter: 136/136
--
Training Step: 61  | total loss: [1m[32m0.70346[0m[0m | time: 0.917s
[2K
| Adam | epoch: 013 | loss: 0.70346 - acc: 0.4006 -- iter: 032/136
[A[ATraining Step: 62  | total loss: [1m[32m0.70155[0m[0m | time: 1.909s
[2K
| Adam | epoch: 013 | loss: 0.70155 - acc: 0.4174 -- iter: 064/136
[A[ATraining Step: 63  | total loss: [1m[32m0.69967[0m[0m | time: 2.884s
[2K
| Adam | epoch: 013 | loss: 0.69967 - acc: 0.4437 -- iter: 096/136
[A[ATraining Step: 64  | total loss: [1m[32m0.69845[0m[0m | time: 3.543s
[2K
| Adam | epoch: 013 | loss: 0.69845 - acc: 0.4586 -- iter: 128/136
[A[ATraining Step: 65  | total loss: [1m[32m0.69761[0m[0m | time: 4.783s
[2K
| Adam | epoch: 013 | loss: 0.69761 - acc: 0.4675 | val_loss: 0.69309 - val_acc: 0.4651 -- iter: 136/136
--
Training Step: 66  | total loss: [1m[32m0.69697[0m[0m | time: 0.258s
[2K
| Adam | epoch: 014 | loss: 0.69697 - acc: 0.4715 -- iter: 032/136
[A[ATraining Step: 67  | total loss: [1m[32m0.69639[0m[0m | time: 1.081s
[2K
| Adam | epoch: 014 | loss: 0.69639 - acc: 0.4749 -- iter: 064/136
[A[ATraining Step: 68  | total loss: [1m[32m0.69629[0m[0m | time: 1.973s
[2K
| Adam | epoch: 014 | loss: 0.69629 - acc: 0.4557 -- iter: 096/136
[A[ATraining Step: 69  | total loss: [1m[32m0.69582[0m[0m | time: 2.876s
[2K
| Adam | epoch: 014 | loss: 0.69582 - acc: 0.4645 -- iter: 128/136
[A[ATraining Step: 70  | total loss: [1m[32m0.69544[0m[0m | time: 4.707s
[2K
| Adam | epoch: 014 | loss: 0.69544 - acc: 0.4722 | val_loss: 0.69227 - val_acc: 0.4651 -- iter: 136/136
--
Training Step: 71  | total loss: [1m[32m0.69510[0m[0m | time: 0.260s
[2K
| Adam | epoch: 015 | loss: 0.69510 - acc: 0.4718 -- iter: 032/136
[A[ATraining Step: 72  | total loss: [1m[32m0.69481[0m[0m | time: 0.523s
[2K
| Adam | epoch: 015 | loss: 0.69481 - acc: 0.4609 -- iter: 064/136
[A[ATraining Step: 73  | total loss: [1m[32m0.69447[0m[0m | time: 1.392s
[2K
| Adam | epoch: 015 | loss: 0.69447 - acc: 0.4514 -- iter: 096/136
[A[ATraining Step: 74  | total loss: [1m[32m0.69416[0m[0m | time: 2.278s
[2K
| Adam | epoch: 015 | loss: 0.69416 - acc: 0.4910 -- iter: 128/136
[A[ATraining Step: 75  | total loss: [1m[32m0.69389[0m[0m | time: 4.188s
[2K
| Adam | epoch: 015 | loss: 0.69389 - acc: 0.5157 | val_loss: 0.69044 - val_acc: 0.7674 -- iter: 136/136
--
Training Step: 76  | total loss: [1m[32m0.69340[0m[0m | time: 0.882s
[2K
| Adam | epoch: 016 | loss: 0.69340 - acc: 0.5508 -- iter: 032/136
[A[ATraining Step: 77  | total loss: [1m[32m0.69299[0m[0m | time: 1.139s
[2K
| Adam | epoch: 016 | loss: 0.69299 - acc: 0.5786 -- iter: 064/136
[A[ATraining Step: 78  | total loss: [1m[32m0.69276[0m[0m | time: 1.396s
[2K
| Adam | epoch: 016 | loss: 0.69276 - acc: 0.5965 -- iter: 096/136
[A[ATraining Step: 79  | total loss: [1m[32m0.69249[0m[0m | time: 2.270s
[2K
| Adam | epoch: 016 | loss: 0.69249 - acc: 0.6124 -- iter: 128/136
[A[ATraining Step: 80  | total loss: [1m[32m0.69208[0m[0m | time: 4.168s
[2K
| Adam | epoch: 016 | loss: 0.69208 - acc: 0.6233 | val_loss: 0.68550 - val_acc: 0.6977 -- iter: 136/136
--
Training Step: 81  | total loss: [1m[32m0.69180[0m[0m | time: 0.905s
[2K
| Adam | epoch: 017 | loss: 0.69180 - acc: 0.6266 -- iter: 032/136
[A[ATraining Step: 82  | total loss: [1m[32m0.69063[0m[0m | time: 1.882s
[2K
| Adam | epoch: 017 | loss: 0.69063 - acc: 0.6608 -- iter: 064/136
[A[ATraining Step: 83  | total loss: [1m[32m0.69008[0m[0m | time: 2.165s
[2K
| Adam | epoch: 017 | loss: 0.69008 - acc: 0.6666 -- iter: 096/136
[A[ATraining Step: 84  | total loss: [1m[32m0.68810[0m[0m | time: 2.442s
[2K
| Adam | epoch: 017 | loss: 0.68810 - acc: 0.6999 -- iter: 128/136
[A[ATraining Step: 85  | total loss: [1m[32m0.68547[0m[0m | time: 4.153s
[2K
| Adam | epoch: 017 | loss: 0.68547 - acc: 0.7299 | val_loss: 0.66879 - val_acc: 0.7209 -- iter: 136/136
--
Training Step: 86  | total loss: [1m[32m0.68453[0m[0m | time: 0.959s
[2K
| Adam | epoch: 018 | loss: 0.68453 - acc: 0.7257 -- iter: 032/136
[A[ATraining Step: 87  | total loss: [1m[32m0.68382[0m[0m | time: 1.912s
[2K
| Adam | epoch: 018 | loss: 0.68382 - acc: 0.7188 -- iter: 064/136
[A[ATraining Step: 88  | total loss: [1m[32m0.68228[0m[0m | time: 2.768s
[2K
| Adam | epoch: 018 | loss: 0.68228 - acc: 0.7156 -- iter: 096/136
[A[ATraining Step: 89  | total loss: [1m[32m0.67722[0m[0m | time: 3.024s
[2K
| Adam | epoch: 018 | loss: 0.67722 - acc: 0.7347 -- iter: 128/136
[A[ATraining Step: 90  | total loss: [1m[32m0.67976[0m[0m | time: 4.273s
[2K
| Adam | epoch: 018 | loss: 0.67976 - acc: 0.7237 | val_loss: 0.64518 - val_acc: 0.6512 -- iter: 136/136
--
Training Step: 91  | total loss: [1m[32m0.67446[0m[0m | time: 0.831s
[2K
| Adam | epoch: 019 | loss: 0.67446 - acc: 0.7264 -- iter: 032/136
[A[ATraining Step: 92  | total loss: [1m[32m0.66934[0m[0m | time: 1.706s
[2K
| Adam | epoch: 019 | loss: 0.66934 - acc: 0.7162 -- iter: 064/136
[A[ATraining Step: 93  | total loss: [1m[32m0.67083[0m[0m | time: 2.548s
[2K
| Adam | epoch: 019 | loss: 0.67083 - acc: 0.6915 -- iter: 096/136
[A[ATraining Step: 94  | total loss: [1m[32m0.66872[0m[0m | time: 3.455s
[2K
| Adam | epoch: 019 | loss: 0.66872 - acc: 0.6754 -- iter: 128/136
[A[ATraining Step: 95  | total loss: [1m[32m0.67166[0m[0m | time: 4.719s
[2K
| Adam | epoch: 019 | loss: 0.67166 - acc: 0.6610 | val_loss: 0.58236 - val_acc: 0.7674 -- iter: 136/136
--
Training Step: 96  | total loss: [1m[32m0.65745[0m[0m | time: 0.293s
[2K
| Adam | epoch: 020 | loss: 0.65745 - acc: 0.6824 -- iter: 032/136
[A[ATraining Step: 97  | total loss: [1m[32m0.64083[0m[0m | time: 1.211s
[2K
| Adam | epoch: 020 | loss: 0.64083 - acc: 0.7017 -- iter: 064/136
[A[ATraining Step: 98  | total loss: [1m[32m0.63119[0m[0m | time: 1.908s
[2K
| Adam | epoch: 020 | loss: 0.63119 - acc: 0.7190 -- iter: 096/136
[A[ATraining Step: 99  | total loss: [1m[32m0.62524[0m[0m | time: 2.771s
[2K
| Adam | epoch: 020 | loss: 0.62524 - acc: 0.7221 -- iter: 128/136
[A[ATraining Step: 100  | total loss: [1m[32m0.62094[0m[0m | time: 4.692s
[2K
| Adam | epoch: 020 | loss: 0.62094 - acc: 0.7249 | val_loss: 0.50318 - val_acc: 0.7674 -- iter: 136/136
--
Training Step: 101  | total loss: [1m[32m0.60571[0m[0m | time: 0.247s
[2K
| Adam | epoch: 021 | loss: 0.60571 - acc: 0.7305 -- iter: 032/136
[A[ATraining Step: 102  | total loss: [1m[32m0.58436[0m[0m | time: 0.481s
[2K
| Adam | epoch: 021 | loss: 0.58436 - acc: 0.7450 -- iter: 064/136
[A[ATraining Step: 103  | total loss: [1m[32m0.56280[0m[0m | time: 1.342s
[2K
| Adam | epoch: 021 | loss: 0.56280 - acc: 0.7580 -- iter: 096/136
[A[ATraining Step: 104  | total loss: [1m[32m0.55155[0m[0m | time: 2.367s
[2K
| Adam | epoch: 021 | loss: 0.55155 - acc: 0.7603 -- iter: 128/136
[A[ATraining Step: 105  | total loss: [1m[32m0.54390[0m[0m | time: 4.344s
[2K
| Adam | epoch: 021 | loss: 0.54390 - acc: 0.7655 | val_loss: 0.59065 - val_acc: 0.6977 -- iter: 136/136
--
Training Step: 106  | total loss: [1m[32m0.59773[0m[0m | time: 0.859s
[2K
| Adam | epoch: 022 | loss: 0.59773 - acc: 0.7452 -- iter: 032/136
[A[ATraining Step: 107  | total loss: [1m[32m0.59300[0m[0m | time: 1.140s
[2K
| Adam | epoch: 022 | loss: 0.59300 - acc: 0.7426 -- iter: 064/136
[A[ATraining Step: 108  | total loss: [1m[32m0.64090[0m[0m | time: 1.414s
[2K
| Adam | epoch: 022 | loss: 0.64090 - acc: 0.7183 -- iter: 096/136
[A[ATraining Step: 109  | total loss: [1m[32m0.68012[0m[0m | time: 2.178s
[2K
| Adam | epoch: 022 | loss: 0.68012 - acc: 0.7090 -- iter: 128/136
[A[ATraining Step: 110  | total loss: [1m[32m0.65696[0m[0m | time: 3.995s
[2K
| Adam | epoch: 022 | loss: 0.65696 - acc: 0.7131 | val_loss: 0.50139 - val_acc: 0.7442 -- iter: 136/136
--
Training Step: 111  | total loss: [1m[32m0.62661[0m[0m | time: 0.863s
[2K
| Adam | epoch: 023 | loss: 0.62661 - acc: 0.7293 -- iter: 032/136
[A[ATraining Step: 112  | total loss: [1m[32m0.60894[0m[0m | time: 1.727s
[2K
| Adam | epoch: 023 | loss: 0.60894 - acc: 0.7470 -- iter: 064/136
[A[ATraining Step: 113  | total loss: [1m[32m0.59415[0m[0m | time: 1.990s
[2K
| Adam | epoch: 023 | loss: 0.59415 - acc: 0.7535 -- iter: 096/136
[A[ATraining Step: 114  | total loss: [1m[32m0.59175[0m[0m | time: 2.239s
[2K
| Adam | epoch: 023 | loss: 0.59175 - acc: 0.7532 -- iter: 128/136
[A[ATraining Step: 115  | total loss: [1m[32m0.58879[0m[0m | time: 4.101s
[2K
| Adam | epoch: 023 | loss: 0.58879 - acc: 0.7529 | val_loss: 0.48886 - val_acc: 0.8140 -- iter: 136/136
--
Training Step: 116  | total loss: [1m[32m0.57528[0m[0m | time: 0.871s
[2K
| Adam | epoch: 024 | loss: 0.57528 - acc: 0.7588 -- iter: 032/136
[A[ATraining Step: 117  | total loss: [1m[32m0.55869[0m[0m | time: 1.746s
[2K
| Adam | epoch: 024 | loss: 0.55869 - acc: 0.7704 -- iter: 064/136
[A[ATraining Step: 118  | total loss: [1m[32m0.55356[0m[0m | time: 2.582s
[2K
| Adam | epoch: 024 | loss: 0.55356 - acc: 0.7684 -- iter: 096/136
[A[ATraining Step: 119  | total loss: [1m[32m0.54032[0m[0m | time: 2.822s
[2K
| Adam | epoch: 024 | loss: 0.54032 - acc: 0.7791 -- iter: 128/136
[A[ATraining Step: 120  | total loss: [1m[32m0.54097[0m[0m | time: 4.100s
[2K
| Adam | epoch: 024 | loss: 0.54097 - acc: 0.7762 | val_loss: 0.49209 - val_acc: 0.7674 -- iter: 136/136
--
Training Step: 121  | total loss: [1m[32m0.53803[0m[0m | time: 0.999s
[2K
| Adam | epoch: 025 | loss: 0.53803 - acc: 0.7735 -- iter: 032/136
[A[ATraining Step: 122  | total loss: [1m[32m0.52645[0m[0m | time: 1.833s
[2K
| Adam | epoch: 025 | loss: 0.52645 - acc: 0.7837 -- iter: 064/136
[A[ATraining Step: 123  | total loss: [1m[32m0.51317[0m[0m | time: 2.581s
[2K
| Adam | epoch: 025 | loss: 0.51317 - acc: 0.7897 -- iter: 096/136
[A[ATraining Step: 124  | total loss: [1m[32m0.51764[0m[0m | time: 3.430s
[2K
| Adam | epoch: 025 | loss: 0.51764 - acc: 0.7857 -- iter: 128/136
[A[ATraining Step: 125  | total loss: [1m[32m0.51216[0m[0m | time: 4.690s
[2K
| Adam | epoch: 025 | loss: 0.51216 - acc: 0.7884 | val_loss: 0.44331 - val_acc: 0.8372 -- iter: 136/136
--
Training Step: 126  | total loss: [1m[32m0.48780[0m[0m | time: 0.317s
[2K
| Adam | epoch: 026 | loss: 0.48780 - acc: 0.8096 -- iter: 032/136
[A[ATraining Step: 127  | total loss: [1m[32m0.46630[0m[0m | time: 1.260s
[2K
| Adam | epoch: 026 | loss: 0.46630 - acc: 0.8286 -- iter: 064/136
[A[ATraining Step: 128  | total loss: [1m[32m0.45122[0m[0m | time: 2.229s
[2K
| Adam | epoch: 026 | loss: 0.45122 - acc: 0.8395 -- iter: 096/136
[A[ATraining Step: 129  | total loss: [1m[32m0.44409[0m[0m | time: 2.946s
[2K
| Adam | epoch: 026 | loss: 0.44409 - acc: 0.8430 -- iter: 128/136
[A[ATraining Step: 130  | total loss: [1m[32m0.44681[0m[0m | time: 4.806s
[2K
| Adam | epoch: 026 | loss: 0.44681 - acc: 0.8369 | val_loss: 0.44030 - val_acc: 0.8140 -- iter: 136/136
--
Training Step: 131  | total loss: [1m[32m0.43454[0m[0m | time: 0.253s
[2K
| Adam | epoch: 027 | loss: 0.43454 - acc: 0.8438 -- iter: 032/136
[A[ATraining Step: 132  | total loss: [1m[32m0.40420[0m[0m | time: 0.494s
[2K
| Adam | epoch: 027 | loss: 0.40420 - acc: 0.8594 -- iter: 064/136
[A[ATraining Step: 133  | total loss: [1m[32m0.37452[0m[0m | time: 1.363s
[2K
| Adam | epoch: 027 | loss: 0.37452 - acc: 0.8735 -- iter: 096/136
[A[ATraining Step: 134  | total loss: [1m[32m0.36674[0m[0m | time: 2.197s
[2K
| Adam | epoch: 027 | loss: 0.36674 - acc: 0.8768 -- iter: 128/136
[A[ATraining Step: 135  | total loss: [1m[32m0.37233[0m[0m | time: 4.013s
[2K
| Adam | epoch: 027 | loss: 0.37233 - acc: 0.8735 | val_loss: 0.47850 - val_acc: 0.7674 -- iter: 136/136
--
Training Step: 136  | total loss: [1m[32m0.37922[0m[0m | time: 0.872s
[2K
| Adam | epoch: 028 | loss: 0.37922 - acc: 0.8705 -- iter: 032/136
[A[ATraining Step: 137  | total loss: [1m[32m0.36359[0m[0m | time: 1.143s
[2K
| Adam | epoch: 028 | loss: 0.36359 - acc: 0.8741 -- iter: 064/136
[A[ATraining Step: 138  | total loss: [1m[32m0.35934[0m[0m | time: 1.388s
[2K
| Adam | epoch: 028 | loss: 0.35934 - acc: 0.8742 -- iter: 096/136
[A[ATraining Step: 139  | total loss: [1m[32m0.35569[0m[0m | time: 2.244s
[2K
| Adam | epoch: 028 | loss: 0.35569 - acc: 0.8742 -- iter: 128/136
[A[ATraining Step: 140  | total loss: [1m[32m0.35454[0m[0m | time: 4.158s
[2K
| Adam | epoch: 028 | loss: 0.35454 - acc: 0.8712 | val_loss: 0.46711 - val_acc: 0.8372 -- iter: 136/136
--
Training Step: 141  | total loss: [1m[32m0.33679[0m[0m | time: 0.917s
[2K
| Adam | epoch: 029 | loss: 0.33679 - acc: 0.8778 -- iter: 032/136
[A[ATraining Step: 142  | total loss: [1m[32m0.32300[0m[0m | time: 1.853s
[2K
| Adam | epoch: 029 | loss: 0.32300 - acc: 0.8807 -- iter: 064/136
[A[ATraining Step: 143  | total loss: [1m[32m0.30764[0m[0m | time: 2.116s
[2K
| Adam | epoch: 029 | loss: 0.30764 - acc: 0.8832 -- iter: 096/136
[A[ATraining Step: 144  | total loss: [1m[32m0.29567[0m[0m | time: 2.403s
[2K
| Adam | epoch: 029 | loss: 0.29567 - acc: 0.8824 -- iter: 128/136
[A[ATraining Step: 145  | total loss: [1m[32m0.27870[0m[0m | time: 4.286s
[2K
| Adam | epoch: 029 | loss: 0.27870 - acc: 0.8942 | val_loss: 0.67401 - val_acc: 0.6512 -- iter: 136/136
--
Training Step: 146  | total loss: [1m[32m0.31292[0m[0m | time: 0.630s
[2K
| Adam | epoch: 030 | loss: 0.31292 - acc: 0.8829 -- iter: 032/136
[A[ATraining Step: 147  | total loss: [1m[32m0.31939[0m[0m | time: 1.267s
[2K
| Adam | epoch: 030 | loss: 0.31939 - acc: 0.8758 -- iter: 064/136
[A[ATraining Step: 148  | total loss: [1m[32m0.32147[0m[0m | time: 1.882s
[2K
| Adam | epoch: 030 | loss: 0.32147 - acc: 0.8789 -- iter: 096/136
[A[ATraining Step: 149  | total loss: [1m[32m0.31711[0m[0m | time: 2.058s
[2K
| Adam | epoch: 030 | loss: 0.31711 - acc: 0.8754 -- iter: 128/136
[A[ATraining Step: 150  | total loss: [1m[32m0.31731[0m[0m | time: 3.256s
[2K
| Adam | epoch: 030 | loss: 0.31731 - acc: 0.8753 | val_loss: 0.45788 - val_acc: 0.8605 -- iter: 136/136
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.8782608695652174
Validation AUPRC:0.8454834649224034
Test AUC:0.8614718614718615
Test AUPRC:0.8471902269464433
BestTestF1Score	0.78	0.59	0.79	0.84	0.73	16	3	18	6	0.61
BestTestMCCScore	0.78	0.59	0.79	0.84	0.73	16	3	18	6	0.61
BestTestAccuracyScore	0.78	0.59	0.79	0.84	0.73	16	3	18	6	0.61
BestValidationF1Score	0.86	0.74	0.86	0.79	0.95	19	5	18	1	0.61
BestValidationMCC	0.86	0.74	0.86	0.79	0.95	19	5	18	1	0.61
BestValidationAccuracy	0.86	0.74	0.86	0.79	0.95	19	5	18	1	0.61
TestPredictions (Threshold:0.61)
CHEMBL487611,TN,INACT,0.07000000029802322	CHEMBL2107430,TP,ACT,0.9200000166893005	CHEMBL491237,TN,INACT,0.20999999344348907	CHEMBL2348912,TN,INACT,0.23999999463558197	CHEMBL237823,TN,INACT,0.019999999552965164	CHEMBL515181,TN,INACT,0.009999999776482582	CHEMBL468714,FN,ACT,0.28999999165534973	CHEMBL1956358,TN,INACT,0.33000001311302185	CHEMBL1358,FN,ACT,0.009999999776482582	CHEMBL2204687,TN,INACT,0.019999999552965164	CHEMBL237611,TN,INACT,0.2199999988079071	CHEMBL520305,TN,INACT,0.05999999865889549	CHEMBL3092206,TP,ACT,0.9200000166893005	CHEMBL3632931,TP,ACT,0.7599999904632568	CHEMBL235113,TN,INACT,0.009999999776482582	CHEMBL2164575,TP,ACT,0.8199999928474426	CHEMBL484264,TN,INACT,0.4300000071525574	CHEMBL2398716,FN,ACT,0.5400000214576721	CHEMBL237605,TN,INACT,0.009999999776482582	CHEMBL3126839,TP,ACT,0.9100000262260437	CHEMBL486176,TN,INACT,0.20000000298023224	CHEMBL672,TN,INACT,0.03999999910593033	CHEMBL2164581,TP,ACT,0.9800000190734863	CHEMBL1617,TP,ACT,0.949999988079071	CHEMBL501711,TP,ACT,0.9599999785423279	CHEMBL2398724,TP,ACT,0.949999988079071	CHEMBL394678,TN,INACT,0.1599999964237213	CHEMBL3598052,TP,ACT,0.6700000166893005	CHEMBL458767,FN,ACT,0.5199999809265137	CHEMBL463678,TP,ACT,0.6700000166893005	CHEMBL3334727,FN,ACT,0.28999999165534973	CHEMBL1643728,TP,ACT,0.9800000190734863	CHEMBL1800133,FN,ACT,0.36000001430511475	CHEMBL2398722,TP,ACT,0.949999988079071	CHEMBL492046,TN,INACT,0.09000000357627869	CHEMBL3263246,FP,INACT,0.9800000190734863	CHEMBL295416,TN,INACT,0.4699999988079071	CHEMBL237241,TN,INACT,0.009999999776482582	CHEMBL2348886,FP,INACT,0.6899999976158142	CHEMBL242527,TP,ACT,0.9800000190734863	CHEMBL2348891,FP,INACT,0.699999988079071	CHEMBL3105681,TP,ACT,0.7799999713897705	CHEMBL3598048,TP,ACT,0.6100000143051147	

