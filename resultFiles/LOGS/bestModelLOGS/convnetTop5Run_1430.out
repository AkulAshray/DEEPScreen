CNNModel CHEMBL320 adam 0.0005 15 128 0 0.8 False True
Number of active compounds :	184
Number of inactive compounds :	184
---------------------------------
Run id: CNNModel_CHEMBL320_adam_0.0005_15_128_0_0.8_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL320_adam_0.0005_15_128_0.8_True/
---------------------------------
Training samples: 201
Validation samples: 63
--
Training Step: 1  | time: 1.319s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/201
[A[ATraining Step: 2  | total loss: [1m[32m0.62386[0m[0m | time: 2.300s
[2K
| Adam | epoch: 001 | loss: 0.62386 - acc: 0.3937 -- iter: 064/201
[A[ATraining Step: 3  | total loss: [1m[32m0.67914[0m[0m | time: 3.419s
[2K
| Adam | epoch: 001 | loss: 0.67914 - acc: 0.5574 -- iter: 096/201
[A[ATraining Step: 4  | total loss: [1m[32m0.69029[0m[0m | time: 4.489s
[2K
| Adam | epoch: 001 | loss: 0.69029 - acc: 0.5143 -- iter: 128/201
[A[ATraining Step: 5  | total loss: [1m[32m0.69068[0m[0m | time: 5.667s
[2K
| Adam | epoch: 001 | loss: 0.69068 - acc: 0.5260 -- iter: 160/201
[A[ATraining Step: 6  | total loss: [1m[32m0.69581[0m[0m | time: 6.709s
[2K
| Adam | epoch: 001 | loss: 0.69581 - acc: 0.4892 -- iter: 192/201
[A[ATraining Step: 7  | total loss: [1m[32m0.70503[0m[0m | time: 8.125s
[2K
| Adam | epoch: 001 | loss: 0.70503 - acc: 0.3832 | val_loss: 0.69385 - val_acc: 0.4603 -- iter: 201/201
--
Training Step: 8  | total loss: [1m[32m0.70275[0m[0m | time: 0.455s
[2K
| Adam | epoch: 002 | loss: 0.70275 - acc: 0.3551 -- iter: 032/201
[A[ATraining Step: 9  | total loss: [1m[32m0.69665[0m[0m | time: 1.688s
[2K
| Adam | epoch: 002 | loss: 0.69665 - acc: 0.4612 -- iter: 064/201
[A[ATraining Step: 10  | total loss: [1m[32m0.69491[0m[0m | time: 2.827s
[2K
| Adam | epoch: 002 | loss: 0.69491 - acc: 0.4806 -- iter: 096/201
[A[ATraining Step: 11  | total loss: [1m[32m0.69410[0m[0m | time: 3.829s
[2K
| Adam | epoch: 002 | loss: 0.69410 - acc: 0.4898 -- iter: 128/201
[A[ATraining Step: 12  | total loss: [1m[32m0.69361[0m[0m | time: 4.812s
[2K
| Adam | epoch: 002 | loss: 0.69361 - acc: 0.5225 -- iter: 160/201
[A[ATraining Step: 13  | total loss: [1m[32m0.69354[0m[0m | time: 5.761s
[2K
| Adam | epoch: 002 | loss: 0.69354 - acc: 0.4727 -- iter: 192/201
[A[ATraining Step: 14  | total loss: [1m[32m0.69335[0m[0m | time: 7.462s
[2K
| Adam | epoch: 002 | loss: 0.69335 - acc: 0.4966 | val_loss: 0.69285 - val_acc: 0.5397 -- iter: 201/201
--
Training Step: 15  | total loss: [1m[32m0.69327[0m[0m | time: 0.198s
[2K
| Adam | epoch: 003 | loss: 0.69327 - acc: 0.4857 -- iter: 032/201
[A[ATraining Step: 16  | total loss: [1m[32m0.69310[0m[0m | time: 0.406s
[2K
| Adam | epoch: 003 | loss: 0.69310 - acc: 0.5119 -- iter: 064/201
[A[ATraining Step: 17  | total loss: [1m[32m0.69294[0m[0m | time: 1.018s
[2K
| Adam | epoch: 003 | loss: 0.69294 - acc: 0.5276 -- iter: 096/201
[A[ATraining Step: 18  | total loss: [1m[32m0.69227[0m[0m | time: 1.622s
[2K
| Adam | epoch: 003 | loss: 0.69227 - acc: 0.5721 -- iter: 128/201
[A[ATraining Step: 19  | total loss: [1m[32m0.69291[0m[0m | time: 2.276s
[2K
| Adam | epoch: 003 | loss: 0.69291 - acc: 0.5377 -- iter: 160/201
[A[ATraining Step: 20  | total loss: [1m[32m0.69245[0m[0m | time: 2.885s
[2K
| Adam | epoch: 003 | loss: 0.69245 - acc: 0.5457 -- iter: 192/201
[A[ATraining Step: 21  | total loss: [1m[32m0.69362[0m[0m | time: 4.485s
[2K
| Adam | epoch: 003 | loss: 0.69362 - acc: 0.5024 | val_loss: 0.69197 - val_acc: 0.5397 -- iter: 201/201
--
Training Step: 22  | total loss: [1m[32m0.69420[0m[0m | time: 0.734s
[2K
| Adam | epoch: 004 | loss: 0.69420 - acc: 0.4829 -- iter: 032/201
[A[ATraining Step: 23  | total loss: [1m[32m0.69392[0m[0m | time: 0.934s
[2K
| Adam | epoch: 004 | loss: 0.69392 - acc: 0.4879 -- iter: 064/201
[A[ATraining Step: 24  | total loss: [1m[32m0.69331[0m[0m | time: 1.157s
[2K
| Adam | epoch: 004 | loss: 0.69331 - acc: 0.5069 -- iter: 096/201
[A[ATraining Step: 25  | total loss: [1m[32m0.69275[0m[0m | time: 1.760s
[2K
| Adam | epoch: 004 | loss: 0.69275 - acc: 0.5202 -- iter: 128/201
[A[ATraining Step: 26  | total loss: [1m[32m0.69172[0m[0m | time: 2.365s
[2K
| Adam | epoch: 004 | loss: 0.69172 - acc: 0.5479 -- iter: 160/201
[A[ATraining Step: 27  | total loss: [1m[32m0.69201[0m[0m | time: 2.977s
[2K
| Adam | epoch: 004 | loss: 0.69201 - acc: 0.5356 -- iter: 192/201
[A[ATraining Step: 28  | total loss: [1m[32m0.69402[0m[0m | time: 4.594s
[2K
| Adam | epoch: 004 | loss: 0.69402 - acc: 0.4876 | val_loss: 0.69176 - val_acc: 0.5397 -- iter: 201/201
--
Training Step: 29  | total loss: [1m[32m0.69451[0m[0m | time: 0.599s
[2K
| Adam | epoch: 005 | loss: 0.69451 - acc: 0.4754 -- iter: 032/201
[A[ATraining Step: 30  | total loss: [1m[32m0.69426[0m[0m | time: 1.396s
[2K
| Adam | epoch: 005 | loss: 0.69426 - acc: 0.4813 -- iter: 064/201
[A[ATraining Step: 31  | total loss: [1m[32m0.69300[0m[0m | time: 1.727s
[2K
| Adam | epoch: 005 | loss: 0.69300 - acc: 0.5144 -- iter: 096/201
[A[ATraining Step: 32  | total loss: [1m[32m0.69172[0m[0m | time: 2.098s
[2K
| Adam | epoch: 005 | loss: 0.69172 - acc: 0.5487 -- iter: 128/201
[A[ATraining Step: 33  | total loss: [1m[32m0.69058[0m[0m | time: 3.217s
[2K
| Adam | epoch: 005 | loss: 0.69058 - acc: 0.5746 -- iter: 160/201
[A[ATraining Step: 34  | total loss: [1m[32m0.69081[0m[0m | time: 4.159s
[2K
| Adam | epoch: 005 | loss: 0.69081 - acc: 0.5653 -- iter: 192/201
[A[ATraining Step: 35  | total loss: [1m[32m0.69159[0m[0m | time: 6.179s
[2K
| Adam | epoch: 005 | loss: 0.69159 - acc: 0.5451 | val_loss: 0.69043 - val_acc: 0.5397 -- iter: 201/201
--
Training Step: 36  | total loss: [1m[32m0.69014[0m[0m | time: 1.264s
[2K
| Adam | epoch: 006 | loss: 0.69014 - acc: 0.5614 -- iter: 032/201
[A[ATraining Step: 37  | total loss: [1m[32m0.69324[0m[0m | time: 2.169s
[2K
| Adam | epoch: 006 | loss: 0.69324 - acc: 0.5241 -- iter: 064/201
[A[ATraining Step: 38  | total loss: [1m[32m0.69454[0m[0m | time: 3.289s
[2K
| Adam | epoch: 006 | loss: 0.69454 - acc: 0.5072 -- iter: 096/201
[A[ATraining Step: 39  | total loss: [1m[32m0.69341[0m[0m | time: 3.656s
[2K
| Adam | epoch: 006 | loss: 0.69341 - acc: 0.5178 -- iter: 128/201
[A[ATraining Step: 40  | total loss: [1m[32m0.69248[0m[0m | time: 4.015s
[2K
| Adam | epoch: 006 | loss: 0.69248 - acc: 0.5249 -- iter: 160/201
[A[ATraining Step: 41  | total loss: [1m[32m0.69175[0m[0m | time: 5.065s
[2K
| Adam | epoch: 006 | loss: 0.69175 - acc: 0.5305 -- iter: 192/201
[A[ATraining Step: 42  | total loss: [1m[32m0.69259[0m[0m | time: 7.032s
[2K
| Adam | epoch: 006 | loss: 0.69259 - acc: 0.5194 | val_loss: 0.69034 - val_acc: 0.5397 -- iter: 201/201
--
Training Step: 43  | total loss: [1m[32m0.69141[0m[0m | time: 1.139s
[2K
| Adam | epoch: 007 | loss: 0.69141 - acc: 0.5325 -- iter: 032/201
[A[ATraining Step: 44  | total loss: [1m[32m0.69073[0m[0m | time: 1.983s
[2K
| Adam | epoch: 007 | loss: 0.69073 - acc: 0.5377 -- iter: 064/201
[A[ATraining Step: 45  | total loss: [1m[32m0.69247[0m[0m | time: 3.080s
[2K
| Adam | epoch: 007 | loss: 0.69247 - acc: 0.5207 -- iter: 096/201
[A[ATraining Step: 46  | total loss: [1m[32m0.69288[0m[0m | time: 4.170s
[2K
| Adam | epoch: 007 | loss: 0.69288 - acc: 0.5172 -- iter: 128/201
[A[ATraining Step: 47  | total loss: [1m[32m0.69330[0m[0m | time: 4.578s
[2K
| Adam | epoch: 007 | loss: 0.69330 - acc: 0.5093 -- iter: 160/201
[A[ATraining Step: 48  | total loss: [1m[32m0.69081[0m[0m | time: 4.978s
[2K
| Adam | epoch: 007 | loss: 0.69081 - acc: 0.5346 -- iter: 192/201
[A[ATraining Step: 49  | total loss: [1m[32m0.68843[0m[0m | time: 6.899s
[2K
| Adam | epoch: 007 | loss: 0.68843 - acc: 0.5554 | val_loss: 0.68967 - val_acc: 0.5397 -- iter: 201/201
--
Training Step: 50  | total loss: [1m[32m0.68972[0m[0m | time: 0.914s
[2K
| Adam | epoch: 008 | loss: 0.68972 - acc: 0.5420 -- iter: 032/201
[A[ATraining Step: 51  | total loss: [1m[32m0.68963[0m[0m | time: 1.805s
[2K
| Adam | epoch: 008 | loss: 0.68963 - acc: 0.5404 -- iter: 064/201
[A[ATraining Step: 52  | total loss: [1m[32m0.69181[0m[0m | time: 2.657s
[2K
| Adam | epoch: 008 | loss: 0.69181 - acc: 0.5249 -- iter: 096/201
[A[ATraining Step: 53  | total loss: [1m[32m0.69219[0m[0m | time: 3.696s
[2K
| Adam | epoch: 008 | loss: 0.69219 - acc: 0.5212 -- iter: 128/201
[A[ATraining Step: 54  | total loss: [1m[32m0.68919[0m[0m | time: 4.627s
[2K
| Adam | epoch: 008 | loss: 0.68919 - acc: 0.5408 -- iter: 160/201
[A[ATraining Step: 55  | total loss: [1m[32m0.69225[0m[0m | time: 4.948s
[2K
| Adam | epoch: 008 | loss: 0.69225 - acc: 0.5216 -- iter: 192/201
[A[ATraining Step: 56  | total loss: [1m[32m0.68909[0m[0m | time: 6.252s
[2K
| Adam | epoch: 008 | loss: 0.68909 - acc: 0.5420 | val_loss: 0.68936 - val_acc: 0.5397 -- iter: 201/201
--
Training Step: 57  | total loss: [1m[32m0.68606[0m[0m | time: 0.998s
[2K
| Adam | epoch: 009 | loss: 0.68606 - acc: 0.5593 -- iter: 032/201
[A[ATraining Step: 58  | total loss: [1m[32m0.68495[0m[0m | time: 1.941s
[2K
| Adam | epoch: 009 | loss: 0.68495 - acc: 0.5640 -- iter: 064/201
[A[ATraining Step: 59  | total loss: [1m[32m0.68973[0m[0m | time: 2.848s
[2K
| Adam | epoch: 009 | loss: 0.68973 - acc: 0.5428 -- iter: 096/201
[A[ATraining Step: 60  | total loss: [1m[32m0.69157[0m[0m | time: 3.479s
[2K
| Adam | epoch: 009 | loss: 0.69157 - acc: 0.5330 -- iter: 128/201
[A[ATraining Step: 61  | total loss: [1m[32m0.69051[0m[0m | time: 4.226s
[2K
| Adam | epoch: 009 | loss: 0.69051 - acc: 0.5368 -- iter: 160/201
[A[ATraining Step: 62  | total loss: [1m[32m0.69059[0m[0m | time: 4.960s
[2K
| Adam | epoch: 009 | loss: 0.69059 - acc: 0.5361 -- iter: 192/201
[A[ATraining Step: 63  | total loss: [1m[32m0.69235[0m[0m | time: 6.187s
[2K
| Adam | epoch: 009 | loss: 0.69235 - acc: 0.5236 | val_loss: 0.68889 - val_acc: 0.5397 -- iter: 201/201
--
Training Step: 64  | total loss: [1m[32m0.69181[0m[0m | time: 0.207s
[2K
| Adam | epoch: 010 | loss: 0.69181 - acc: 0.5276 -- iter: 032/201
[A[ATraining Step: 65  | total loss: [1m[32m0.69108[0m[0m | time: 0.860s
[2K
| Adam | epoch: 010 | loss: 0.69108 - acc: 0.5311 -- iter: 064/201
[A[ATraining Step: 66  | total loss: [1m[32m0.68984[0m[0m | time: 1.475s
[2K
| Adam | epoch: 010 | loss: 0.68984 - acc: 0.5387 -- iter: 096/201
[A[ATraining Step: 67  | total loss: [1m[32m0.69003[0m[0m | time: 2.085s
[2K
| Adam | epoch: 010 | loss: 0.69003 - acc: 0.5340 -- iter: 128/201
[A[ATraining Step: 68  | total loss: [1m[32m0.69068[0m[0m | time: 2.708s
[2K
| Adam | epoch: 010 | loss: 0.69068 - acc: 0.5263 -- iter: 160/201
[A[ATraining Step: 69  | total loss: [1m[32m0.69160[0m[0m | time: 3.343s
[2K
| Adam | epoch: 010 | loss: 0.69160 - acc: 0.5159 -- iter: 192/201
[A[ATraining Step: 70  | total loss: [1m[32m0.69168[0m[0m | time: 5.163s
[2K
| Adam | epoch: 010 | loss: 0.69168 - acc: 0.5141 | val_loss: 0.68929 - val_acc: 0.5397 -- iter: 201/201
--
Training Step: 71  | total loss: [1m[32m0.69114[0m[0m | time: 0.204s
[2K
| Adam | epoch: 011 | loss: 0.69114 - acc: 0.5160 -- iter: 032/201
[A[ATraining Step: 72  | total loss: [1m[32m0.69200[0m[0m | time: 0.423s
[2K
| Adam | epoch: 011 | loss: 0.69200 - acc: 0.5080 -- iter: 064/201
[A[ATraining Step: 73  | total loss: [1m[32m0.69279[0m[0m | time: 1.036s
[2K
| Adam | epoch: 011 | loss: 0.69279 - acc: 0.5009 -- iter: 096/201
[A[ATraining Step: 74  | total loss: [1m[32m0.69290[0m[0m | time: 1.639s
[2K
| Adam | epoch: 011 | loss: 0.69290 - acc: 0.4940 -- iter: 128/201
[A[ATraining Step: 75  | total loss: [1m[32m0.69213[0m[0m | time: 2.239s
[2K
| Adam | epoch: 011 | loss: 0.69213 - acc: 0.5048 -- iter: 160/201
[A[ATraining Step: 76  | total loss: [1m[32m0.69171[0m[0m | time: 3.008s
[2K
| Adam | epoch: 011 | loss: 0.69171 - acc: 0.5110 -- iter: 192/201
[A[ATraining Step: 77  | total loss: [1m[32m0.69143[0m[0m | time: 4.779s
[2K
| Adam | epoch: 011 | loss: 0.69143 - acc: 0.5131 | val_loss: 0.68980 - val_acc: 0.5397 -- iter: 201/201
--
Training Step: 78  | total loss: [1m[32m0.69147[0m[0m | time: 0.646s
[2K
| Adam | epoch: 012 | loss: 0.69147 - acc: 0.5085 -- iter: 032/201
[A[ATraining Step: 79  | total loss: [1m[32m0.69138[0m[0m | time: 0.858s
[2K
| Adam | epoch: 012 | loss: 0.69138 - acc: 0.5044 -- iter: 064/201
[A[ATraining Step: 80  | total loss: [1m[32m0.68955[0m[0m | time: 1.072s
[2K
| Adam | epoch: 012 | loss: 0.68955 - acc: 0.5323 -- iter: 096/201
[A[ATraining Step: 81  | total loss: [1m[32m0.68748[0m[0m | time: 1.693s
[2K
| Adam | epoch: 012 | loss: 0.68748 - acc: 0.5571 -- iter: 128/201
[A[ATraining Step: 82  | total loss: [1m[32m0.68756[0m[0m | time: 2.294s
[2K
| Adam | epoch: 012 | loss: 0.68756 - acc: 0.5514 -- iter: 160/201
[A[ATraining Step: 83  | total loss: [1m[32m0.68614[0m[0m | time: 2.902s
[2K
| Adam | epoch: 012 | loss: 0.68614 - acc: 0.5588 -- iter: 192/201
[A[ATraining Step: 84  | total loss: [1m[32m0.68494[0m[0m | time: 4.693s
[2K
| Adam | epoch: 012 | loss: 0.68494 - acc: 0.5623 | val_loss: 0.68543 - val_acc: 0.5397 -- iter: 201/201
--
Training Step: 85  | total loss: [1m[32m0.68901[0m[0m | time: 0.667s
[2K
| Adam | epoch: 013 | loss: 0.68901 - acc: 0.5404 -- iter: 032/201
[A[ATraining Step: 86  | total loss: [1m[32m0.68972[0m[0m | time: 1.755s
[2K
| Adam | epoch: 013 | loss: 0.68972 - acc: 0.5333 -- iter: 064/201
[A[ATraining Step: 87  | total loss: [1m[32m0.69077[0m[0m | time: 2.120s
[2K
| Adam | epoch: 013 | loss: 0.69077 - acc: 0.5237 -- iter: 096/201
[A[ATraining Step: 88  | total loss: [1m[32m0.68951[0m[0m | time: 2.469s
[2K
| Adam | epoch: 013 | loss: 0.68951 - acc: 0.5269 -- iter: 128/201
[A[ATraining Step: 89  | total loss: [1m[32m0.68778[0m[0m | time: 3.519s
[2K
| Adam | epoch: 013 | loss: 0.68778 - acc: 0.5297 -- iter: 160/201
[A[ATraining Step: 90  | total loss: [1m[32m0.68612[0m[0m | time: 4.478s
[2K
| Adam | epoch: 013 | loss: 0.68612 - acc: 0.5330 -- iter: 192/201
[A[ATraining Step: 91  | total loss: [1m[32m0.68648[0m[0m | time: 6.498s
[2K
| Adam | epoch: 013 | loss: 0.68648 - acc: 0.5235 | val_loss: 0.67964 - val_acc: 0.5397 -- iter: 201/201
--
Training Step: 92  | total loss: [1m[32m0.68573[0m[0m | time: 1.033s
[2K
| Adam | epoch: 014 | loss: 0.68573 - acc: 0.5242 -- iter: 032/201
[A[ATraining Step: 93  | total loss: [1m[32m0.68497[0m[0m | time: 1.906s
[2K
| Adam | epoch: 014 | loss: 0.68497 - acc: 0.5218 -- iter: 064/201
[A[ATraining Step: 94  | total loss: [1m[32m0.68447[0m[0m | time: 3.025s
[2K
| Adam | epoch: 014 | loss: 0.68447 - acc: 0.5134 -- iter: 096/201
[A[ATraining Step: 95  | total loss: [1m[32m0.68274[0m[0m | time: 3.442s
[2K
| Adam | epoch: 014 | loss: 0.68274 - acc: 0.5183 -- iter: 128/201
[A[ATraining Step: 96  | total loss: [1m[32m0.68113[0m[0m | time: 3.868s
[2K
| Adam | epoch: 014 | loss: 0.68113 - acc: 0.5220 -- iter: 160/201
[A[ATraining Step: 97  | total loss: [1m[32m0.67839[0m[0m | time: 4.958s
[2K
| Adam | epoch: 014 | loss: 0.67839 - acc: 0.5254 -- iter: 192/201
[A[ATraining Step: 98  | total loss: [1m[32m0.68019[0m[0m | time: 6.891s
[2K
| Adam | epoch: 014 | loss: 0.68019 - acc: 0.5135 | val_loss: 0.65844 - val_acc: 0.5397 -- iter: 201/201
--
Training Step: 99  | total loss: [1m[32m0.67268[0m[0m | time: 1.000s
[2K
| Adam | epoch: 015 | loss: 0.67268 - acc: 0.5246 -- iter: 032/201
[A[ATraining Step: 100  | total loss: [1m[32m0.67135[0m[0m | time: 1.886s
[2K
| Adam | epoch: 015 | loss: 0.67135 - acc: 0.5222 -- iter: 064/201
[A[ATraining Step: 101  | total loss: [1m[32m0.66678[0m[0m | time: 2.892s
[2K
| Adam | epoch: 015 | loss: 0.66678 - acc: 0.5231 -- iter: 096/201
[A[ATraining Step: 102  | total loss: [1m[32m0.65883[0m[0m | time: 4.090s
[2K
| Adam | epoch: 015 | loss: 0.65883 - acc: 0.5489 -- iter: 128/201
[A[ATraining Step: 103  | total loss: [1m[32m0.66341[0m[0m | time: 4.489s
[2K
| Adam | epoch: 015 | loss: 0.66341 - acc: 0.5315 -- iter: 160/201
[A[ATraining Step: 104  | total loss: [1m[32m0.64870[0m[0m | time: 4.888s
[2K
| Adam | epoch: 015 | loss: 0.64870 - acc: 0.5561 -- iter: 192/201
[A[ATraining Step: 105  | total loss: [1m[32m0.62623[0m[0m | time: 7.018s
[2K
| Adam | epoch: 015 | loss: 0.62623 - acc: 0.5894 | val_loss: 0.71264 - val_acc: 0.5397 -- iter: 201/201
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.7931034482758621
Validation AUPRC:0.7892649869033876
Test AUC:0.8286004056795132
Test AUPRC:0.8600119728252664
BestTestF1Score	0.77	0.56	0.78	0.74	0.79	23	8	26	6	0.76
BestTestMCCScore	0.78	0.59	0.79	0.77	0.79	23	7	27	6	0.77
BestTestAccuracyScore	0.78	0.59	0.79	0.77	0.79	23	7	27	6	0.77
BestValidationF1Score	0.77	0.53	0.76	0.81	0.74	25	6	23	9	0.76
BestValidationMCC	0.76	0.53	0.76	0.83	0.71	24	5	24	10	0.77
BestValidationAccuracy	0.76	0.53	0.76	0.83	0.71	24	5	24	10	0.77
TestPredictions (Threshold:0.77)
CHEMBL451335,TN,INACT,0.6800000071525574	CHEMBL3545181,TP,ACT,0.8100000023841858	CHEMBL522460,TP,ACT,0.8600000143051147	CHEMBL281232,TN,INACT,0.6200000047683716	CHEMBL64000,FN,ACT,0.699999988079071	CHEMBL296365,TP,ACT,0.8100000023841858	CHEMBL110808,TP,ACT,0.8500000238418579	CHEMBL308243,TN,INACT,0.699999988079071	CHEMBL602269,FP,INACT,0.800000011920929	CHEMBL70003,TP,ACT,0.8600000143051147	CHEMBL1202131,FN,ACT,0.6700000166893005	CHEMBL462650,TN,INACT,0.6700000166893005	CHEMBL297215,TN,INACT,0.7200000286102295	CHEMBL372983,TP,ACT,0.8799999952316284	CHEMBL606904,TP,ACT,0.8700000047683716	CHEMBL553783,TP,ACT,0.8500000238418579	CHEMBL351183,FP,INACT,0.7900000214576721	CHEMBL69458,TP,ACT,0.8500000238418579	CHEMBL308924,TN,INACT,0.6000000238418579	CHEMBL407975,TP,ACT,0.8399999737739563	CHEMBL2321893,FP,INACT,0.7799999713897705	CHEMBL553155,TN,INACT,0.7599999904632568	CHEMBL1231,TP,ACT,0.8299999833106995	CHEMBL404557,TN,INACT,0.5600000023841858	CHEMBL294919,TP,ACT,0.7900000214576721	CHEMBL410928,TP,ACT,0.7900000214576721	CHEMBL248614,TP,ACT,0.8700000047683716	CHEMBL319910,TN,INACT,0.7300000190734863	CHEMBL2391356,TN,INACT,0.699999988079071	CHEMBL312266,FP,INACT,0.7799999713897705	CHEMBL297173,TN,INACT,0.6100000143051147	CHEMBL107680,TN,INACT,0.7200000286102295	CHEMBL296245,TN,INACT,0.6100000143051147	CHEMBL48448,FP,INACT,0.8100000023841858	CHEMBL296317,TP,ACT,0.7799999713897705	CHEMBL408493,TN,INACT,0.5799999833106995	CHEMBL175698,TN,INACT,0.7300000190734863	CHEMBL3298599,FN,ACT,0.6700000166893005	CHEMBL9967,TP,ACT,0.8500000238418579	CHEMBL2113072,TN,INACT,0.6899999976158142	CHEMBL128360,TN,INACT,0.6299999952316284	CHEMBL42359,TN,INACT,0.6899999976158142	CHEMBL553066,TP,ACT,0.8600000143051147	CHEMBL296405,FN,ACT,0.699999988079071	CHEMBL195169,TP,ACT,0.8500000238418579	CHEMBL249006,TP,ACT,0.8500000238418579	CHEMBL7303,FN,ACT,0.5299999713897705	CHEMBL2042551,TN,INACT,0.75	CHEMBL352779,FP,INACT,0.8299999833106995	CHEMBL253978,FN,ACT,0.6899999976158142	CHEMBL169553,TN,INACT,0.5400000214576721	CHEMBL2093084,TN,INACT,0.7200000286102295	CHEMBL174463,TN,INACT,0.6600000262260437	CHEMBL168632,FP,INACT,0.8100000023841858	CHEMBL2449003,TP,ACT,0.7900000214576721	CHEMBL594376,TN,INACT,0.6499999761581421	CHEMBL67700,TN,INACT,0.75	CHEMBL3298334,TP,ACT,0.800000011920929	CHEMBL192537,TP,ACT,0.8199999928474426	CHEMBL603858,TN,INACT,0.7699999809265137	CHEMBL302038,TN,INACT,0.7400000095367432	CHEMBL1201994,TP,ACT,0.8799999952316284	CHEMBL413040,TN,INACT,0.6700000166893005	

