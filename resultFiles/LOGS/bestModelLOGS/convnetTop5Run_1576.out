ImageNetInceptionV2 CHEMBL2003 adam 0.0005 15 0 0 0.6 False True
Number of active compounds :	174
Number of inactive compounds :	174
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL2003_adam_0.0005_15_0_0_0.6_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL2003_adam_0.0005_15_0.6/
---------------------------------
Training samples: 222
Validation samples: 70
--
Training Step: 1  | time: 46.272s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/222
[A[ATraining Step: 2  | total loss: [1m[32m0.57605[0m[0m | time: 58.124s
[2K
| Adam | epoch: 001 | loss: 0.57605 - acc: 0.5625 -- iter: 064/222
[A[ATraining Step: 3  | total loss: [1m[32m0.72490[0m[0m | time: 69.314s
[2K
| Adam | epoch: 001 | loss: 0.72490 - acc: 0.6648 -- iter: 096/222
[A[ATraining Step: 4  | total loss: [1m[32m0.49155[0m[0m | time: 80.516s
[2K
| Adam | epoch: 001 | loss: 0.49155 - acc: 0.7990 -- iter: 128/222
[A[ATraining Step: 5  | total loss: [1m[32m0.53601[0m[0m | time: 92.577s
[2K
| Adam | epoch: 001 | loss: 0.53601 - acc: 0.7651 -- iter: 160/222
[A[ATraining Step: 6  | total loss: [1m[32m0.59143[0m[0m | time: 103.219s
[2K
| Adam | epoch: 001 | loss: 0.59143 - acc: 0.7152 -- iter: 192/222
[A[ATraining Step: 7  | total loss: [1m[32m0.43695[0m[0m | time: 125.574s
[2K
| Adam | epoch: 001 | loss: 0.43695 - acc: 0.8298 | val_loss: 0.78494 - val_acc: 0.5143 -- iter: 222/222
--
Training Step: 8  | total loss: [1m[32m0.38074[0m[0m | time: 9.826s
[2K
| Adam | epoch: 002 | loss: 0.38074 - acc: 0.8506 -- iter: 032/222
[A[ATraining Step: 9  | total loss: [1m[32m0.25185[0m[0m | time: 20.885s
[2K
| Adam | epoch: 002 | loss: 0.25185 - acc: 0.9120 -- iter: 064/222
[A[ATraining Step: 10  | total loss: [1m[32m0.19509[0m[0m | time: 33.096s
[2K
| Adam | epoch: 002 | loss: 0.19509 - acc: 0.9404 -- iter: 096/222
[A[ATraining Step: 11  | total loss: [1m[32m0.17121[0m[0m | time: 48.884s
[2K
| Adam | epoch: 002 | loss: 0.17121 - acc: 0.9538 -- iter: 128/222
[A[ATraining Step: 12  | total loss: [1m[32m0.20740[0m[0m | time: 60.281s
[2K
| Adam | epoch: 002 | loss: 0.20740 - acc: 0.9324 -- iter: 160/222
[A[ATraining Step: 13  | total loss: [1m[32m0.17905[0m[0m | time: 71.123s
[2K
| Adam | epoch: 002 | loss: 0.17905 - acc: 0.9346 -- iter: 192/222
[A[ATraining Step: 14  | total loss: [1m[32m0.24462[0m[0m | time: 85.872s
[2K
| Adam | epoch: 002 | loss: 0.24462 - acc: 0.9230 | val_loss: 1.36491 - val_acc: 0.4857 -- iter: 222/222
--
Training Step: 15  | total loss: [1m[32m0.18658[0m[0m | time: 10.093s
[2K
| Adam | epoch: 003 | loss: 0.18658 - acc: 0.9287 -- iter: 032/222
[A[ATraining Step: 16  | total loss: [1m[32m0.13360[0m[0m | time: 20.003s
[2K
| Adam | epoch: 003 | loss: 0.13360 - acc: 0.9554 -- iter: 064/222
[A[ATraining Step: 17  | total loss: [1m[32m0.09932[0m[0m | time: 30.808s
[2K
| Adam | epoch: 003 | loss: 0.09932 - acc: 0.9715 -- iter: 096/222
[A[ATraining Step: 18  | total loss: [1m[32m0.10606[0m[0m | time: 42.168s
[2K
| Adam | epoch: 003 | loss: 0.10606 - acc: 0.9705 -- iter: 128/222
[A[ATraining Step: 19  | total loss: [1m[32m0.12629[0m[0m | time: 53.329s
[2K
| Adam | epoch: 003 | loss: 0.12629 - acc: 0.9491 -- iter: 160/222
[A[ATraining Step: 20  | total loss: [1m[32m0.14090[0m[0m | time: 63.498s
[2K
| Adam | epoch: 003 | loss: 0.14090 - acc: 0.9554 -- iter: 192/222
[A[ATraining Step: 21  | total loss: [1m[32m0.10276[0m[0m | time: 77.972s
[2K
| Adam | epoch: 003 | loss: 0.10276 - acc: 0.9693 | val_loss: 1.39332 - val_acc: 0.4857 -- iter: 222/222
--
Training Step: 22  | total loss: [1m[32m0.13629[0m[0m | time: 10.670s
[2K
| Adam | epoch: 004 | loss: 0.13629 - acc: 0.9504 -- iter: 032/222
[A[ATraining Step: 23  | total loss: [1m[32m0.13996[0m[0m | time: 20.462s
[2K
| Adam | epoch: 004 | loss: 0.13996 - acc: 0.9375 -- iter: 064/222
[A[ATraining Step: 24  | total loss: [1m[32m0.48466[0m[0m | time: 30.743s
[2K
| Adam | epoch: 004 | loss: 0.48466 - acc: 0.8707 -- iter: 096/222
[A[ATraining Step: 25  | total loss: [1m[32m0.39774[0m[0m | time: 41.021s
[2K
| Adam | epoch: 004 | loss: 0.39774 - acc: 0.8969 -- iter: 128/222
[A[ATraining Step: 26  | total loss: [1m[32m0.30911[0m[0m | time: 52.038s
[2K
| Adam | epoch: 004 | loss: 0.30911 - acc: 0.9242 -- iter: 160/222
[A[ATraining Step: 27  | total loss: [1m[32m0.27231[0m[0m | time: 62.752s
[2K
| Adam | epoch: 004 | loss: 0.27231 - acc: 0.9276 -- iter: 192/222
[A[ATraining Step: 28  | total loss: [1m[32m0.22703[0m[0m | time: 77.273s
[2K
| Adam | epoch: 004 | loss: 0.22703 - acc: 0.9379 | val_loss: 1.40324 - val_acc: 0.4857 -- iter: 222/222
--
Training Step: 29  | total loss: [1m[32m0.22349[0m[0m | time: 29.264s
[2K
| Adam | epoch: 005 | loss: 0.22349 - acc: 0.9302 -- iter: 032/222
[A[ATraining Step: 30  | total loss: [1m[32m0.17983[0m[0m | time: 37.308s
[2K
| Adam | epoch: 005 | loss: 0.17983 - acc: 0.9467 -- iter: 064/222
[A[ATraining Step: 31  | total loss: [1m[32m0.17998[0m[0m | time: 45.055s
[2K
| Adam | epoch: 005 | loss: 0.17998 - acc: 0.9374 -- iter: 096/222
[A[ATraining Step: 32  | total loss: [1m[32m0.31471[0m[0m | time: 52.876s
[2K
| Adam | epoch: 005 | loss: 0.31471 - acc: 0.9215 -- iter: 128/222
[A[ATraining Step: 33  | total loss: [1m[32m0.26021[0m[0m | time: 60.995s
[2K
| Adam | epoch: 005 | loss: 0.26021 - acc: 0.9314 -- iter: 160/222
[A[ATraining Step: 34  | total loss: [1m[32m0.22163[0m[0m | time: 69.222s
[2K
| Adam | epoch: 005 | loss: 0.22163 - acc: 0.9394 -- iter: 192/222
[A[ATraining Step: 35  | total loss: [1m[32m0.18465[0m[0m | time: 80.421s
[2K
| Adam | epoch: 005 | loss: 0.18465 - acc: 0.9455 | val_loss: 1.16769 - val_acc: 0.4286 -- iter: 222/222
--
Training Step: 36  | total loss: [1m[32m0.16404[0m[0m | time: 8.037s
[2K
| Adam | epoch: 006 | loss: 0.16404 - acc: 0.9439 -- iter: 032/222
[A[ATraining Step: 37  | total loss: [1m[32m0.13783[0m[0m | time: 16.201s
[2K
| Adam | epoch: 006 | loss: 0.13783 - acc: 0.9551 -- iter: 064/222
[A[ATraining Step: 38  | total loss: [1m[32m0.14447[0m[0m | time: 24.355s
[2K
| Adam | epoch: 006 | loss: 0.14447 - acc: 0.9517 -- iter: 096/222
[A[ATraining Step: 39  | total loss: [1m[32m0.11870[0m[0m | time: 31.989s
[2K
| Adam | epoch: 006 | loss: 0.11870 - acc: 0.9609 -- iter: 128/222
[A[ATraining Step: 40  | total loss: [1m[32m0.13722[0m[0m | time: 39.820s
[2K
| Adam | epoch: 006 | loss: 0.13722 - acc: 0.9620 -- iter: 160/222
[A[ATraining Step: 41  | total loss: [1m[32m0.11347[0m[0m | time: 48.016s
[2K
| Adam | epoch: 006 | loss: 0.11347 - acc: 0.9690 -- iter: 192/222
[A[ATraining Step: 42  | total loss: [1m[32m0.09528[0m[0m | time: 59.552s
[2K
| Adam | epoch: 006 | loss: 0.09528 - acc: 0.9746 | val_loss: 3.10471 - val_acc: 0.5143 -- iter: 222/222
--
Training Step: 43  | total loss: [1m[32m0.08334[0m[0m | time: 8.072s
[2K
| Adam | epoch: 007 | loss: 0.08334 - acc: 0.9791 -- iter: 032/222
[A[ATraining Step: 44  | total loss: [1m[32m0.08017[0m[0m | time: 16.321s
[2K
| Adam | epoch: 007 | loss: 0.08017 - acc: 0.9719 -- iter: 064/222
[A[ATraining Step: 45  | total loss: [1m[32m0.06742[0m[0m | time: 24.541s
[2K
| Adam | epoch: 007 | loss: 0.06742 - acc: 0.9766 -- iter: 096/222
[A[ATraining Step: 46  | total loss: [1m[32m0.05931[0m[0m | time: 32.801s
[2K
| Adam | epoch: 007 | loss: 0.05931 - acc: 0.9805 -- iter: 128/222
[A[ATraining Step: 47  | total loss: [1m[32m0.06890[0m[0m | time: 40.538s
[2K
| Adam | epoch: 007 | loss: 0.06890 - acc: 0.9735 -- iter: 160/222
[A[ATraining Step: 48  | total loss: [1m[32m0.05917[0m[0m | time: 48.167s
[2K
| Adam | epoch: 007 | loss: 0.05917 - acc: 0.9778 -- iter: 192/222
[A[ATraining Step: 49  | total loss: [1m[32m0.05031[0m[0m | time: 59.485s
[2K
| Adam | epoch: 007 | loss: 0.05031 - acc: 0.9813 | val_loss: 3.92658 - val_acc: 0.5143 -- iter: 222/222
--
Training Step: 50  | total loss: [1m[32m0.04566[0m[0m | time: 8.143s
[2K
| Adam | epoch: 008 | loss: 0.04566 - acc: 0.9842 -- iter: 032/222
[A[ATraining Step: 51  | total loss: [1m[32m0.04533[0m[0m | time: 16.369s
[2K
| Adam | epoch: 008 | loss: 0.04533 - acc: 0.9866 -- iter: 064/222
[A[ATraining Step: 52  | total loss: [1m[32m0.03933[0m[0m | time: 24.508s
[2K
| Adam | epoch: 008 | loss: 0.03933 - acc: 0.9886 -- iter: 096/222
[A[ATraining Step: 53  | total loss: [1m[32m0.04051[0m[0m | time: 32.498s
[2K
| Adam | epoch: 008 | loss: 0.04051 - acc: 0.9903 -- iter: 128/222
[A[ATraining Step: 54  | total loss: [1m[32m0.04684[0m[0m | time: 40.598s
[2K
| Adam | epoch: 008 | loss: 0.04684 - acc: 0.9917 -- iter: 160/222
[A[ATraining Step: 55  | total loss: [1m[32m0.04078[0m[0m | time: 48.696s
[2K
| Adam | epoch: 008 | loss: 0.04078 - acc: 0.9929 -- iter: 192/222
[A[ATraining Step: 56  | total loss: [1m[32m0.06061[0m[0m | time: 59.662s
[2K
| Adam | epoch: 008 | loss: 0.06061 - acc: 0.9892 | val_loss: 3.46134 - val_acc: 0.5143 -- iter: 222/222
--
Training Step: 57  | total loss: [1m[32m0.05290[0m[0m | time: 7.888s
[2K
| Adam | epoch: 009 | loss: 0.05290 - acc: 0.9907 -- iter: 032/222
[A[ATraining Step: 58  | total loss: [1m[32m0.06436[0m[0m | time: 16.105s
[2K
| Adam | epoch: 009 | loss: 0.06436 - acc: 0.9877 -- iter: 064/222
[A[ATraining Step: 59  | total loss: [1m[32m0.07293[0m[0m | time: 24.151s
[2K
| Adam | epoch: 009 | loss: 0.07293 - acc: 0.9810 -- iter: 096/222
[A[ATraining Step: 60  | total loss: [1m[32m0.07135[0m[0m | time: 32.338s
[2K
| Adam | epoch: 009 | loss: 0.07135 - acc: 0.9793 -- iter: 128/222
[A[ATraining Step: 61  | total loss: [1m[32m0.06271[0m[0m | time: 40.495s
[2K
| Adam | epoch: 009 | loss: 0.06271 - acc: 0.9820 -- iter: 160/222
[A[ATraining Step: 62  | total loss: [1m[32m0.05489[0m[0m | time: 48.640s
[2K
| Adam | epoch: 009 | loss: 0.05489 - acc: 0.9843 -- iter: 192/222
[A[ATraining Step: 63  | total loss: [1m[32m0.04960[0m[0m | time: 59.383s
[2K
| Adam | epoch: 009 | loss: 0.04960 - acc: 0.9863 | val_loss: 0.31320 - val_acc: 0.8429 -- iter: 222/222
--
Training Step: 64  | total loss: [1m[32m0.04473[0m[0m | time: 7.558s
[2K
| Adam | epoch: 010 | loss: 0.04473 - acc: 0.9880 -- iter: 032/222
[A[ATraining Step: 65  | total loss: [1m[32m0.04052[0m[0m | time: 15.830s
[2K
| Adam | epoch: 010 | loss: 0.04052 - acc: 0.9895 -- iter: 064/222
[A[ATraining Step: 66  | total loss: [1m[32m0.04076[0m[0m | time: 23.983s
[2K
| Adam | epoch: 010 | loss: 0.04076 - acc: 0.9908 -- iter: 096/222
[A[ATraining Step: 67  | total loss: [1m[32m0.04116[0m[0m | time: 32.256s
[2K
| Adam | epoch: 010 | loss: 0.04116 - acc: 0.9919 -- iter: 128/222
[A[ATraining Step: 68  | total loss: [1m[32m0.04897[0m[0m | time: 40.213s
[2K
| Adam | epoch: 010 | loss: 0.04897 - acc: 0.9892 -- iter: 160/222
[A[ATraining Step: 69  | total loss: [1m[32m0.04388[0m[0m | time: 48.237s
[2K
| Adam | epoch: 010 | loss: 0.04388 - acc: 0.9904 -- iter: 192/222
[A[ATraining Step: 70  | total loss: [1m[32m0.03971[0m[0m | time: 59.591s
[2K
| Adam | epoch: 010 | loss: 0.03971 - acc: 0.9915 | val_loss: 1.16109 - val_acc: 0.8000 -- iter: 222/222
--
Training Step: 71  | total loss: [1m[32m0.03795[0m[0m | time: 7.574s
[2K
| Adam | epoch: 011 | loss: 0.03795 - acc: 0.9925 -- iter: 032/222
[A[ATraining Step: 72  | total loss: [1m[32m0.06440[0m[0m | time: 15.040s
[2K
| Adam | epoch: 011 | loss: 0.06440 - acc: 0.9858 -- iter: 064/222
[A[ATraining Step: 73  | total loss: [1m[32m0.05789[0m[0m | time: 23.108s
[2K
| Adam | epoch: 011 | loss: 0.05789 - acc: 0.9874 -- iter: 096/222
[A[ATraining Step: 74  | total loss: [1m[32m0.05200[0m[0m | time: 31.057s
[2K
| Adam | epoch: 011 | loss: 0.05200 - acc: 0.9888 -- iter: 128/222
[A[ATraining Step: 75  | total loss: [1m[32m0.04771[0m[0m | time: 38.928s
[2K
| Adam | epoch: 011 | loss: 0.04771 - acc: 0.9900 -- iter: 160/222
[A[ATraining Step: 76  | total loss: [1m[32m0.04298[0m[0m | time: 47.066s
[2K
| Adam | epoch: 011 | loss: 0.04298 - acc: 0.9911 -- iter: 192/222
[A[ATraining Step: 77  | total loss: [1m[32m0.04431[0m[0m | time: 58.301s
[2K
| Adam | epoch: 011 | loss: 0.04431 - acc: 0.9920 | val_loss: 0.25544 - val_acc: 0.9286 -- iter: 222/222
--
Training Step: 78  | total loss: [1m[32m0.04002[0m[0m | time: 8.056s
[2K
| Adam | epoch: 012 | loss: 0.04002 - acc: 0.9929 -- iter: 032/222
[A[ATraining Step: 79  | total loss: [1m[32m0.03625[0m[0m | time: 15.729s
[2K
| Adam | epoch: 012 | loss: 0.03625 - acc: 0.9936 -- iter: 064/222
[A[ATraining Step: 80  | total loss: [1m[32m0.05787[0m[0m | time: 23.425s
[2K
| Adam | epoch: 012 | loss: 0.05787 - acc: 0.9908 -- iter: 096/222
[A[ATraining Step: 81  | total loss: [1m[32m0.05241[0m[0m | time: 32.167s
[2K
| Adam | epoch: 012 | loss: 0.05241 - acc: 0.9918 -- iter: 128/222
[A[ATraining Step: 82  | total loss: [1m[32m0.04732[0m[0m | time: 42.530s
[2K
| Adam | epoch: 012 | loss: 0.04732 - acc: 0.9926 -- iter: 160/222
[A[ATraining Step: 83  | total loss: [1m[32m0.04313[0m[0m | time: 52.704s
[2K
| Adam | epoch: 012 | loss: 0.04313 - acc: 0.9933 -- iter: 192/222
[A[ATraining Step: 84  | total loss: [1m[32m0.04183[0m[0m | time: 66.569s
[2K
| Adam | epoch: 012 | loss: 0.04183 - acc: 0.9909 | val_loss: 0.21653 - val_acc: 0.9000 -- iter: 222/222
--
Training Step: 85  | total loss: [1m[32m0.03818[0m[0m | time: 9.767s
[2K
| Adam | epoch: 013 | loss: 0.03818 - acc: 0.9918 -- iter: 032/222
[A[ATraining Step: 86  | total loss: [1m[32m0.03487[0m[0m | time: 19.889s
[2K
| Adam | epoch: 013 | loss: 0.03487 - acc: 0.9926 -- iter: 064/222
[A[ATraining Step: 87  | total loss: [1m[32m0.04073[0m[0m | time: 29.739s
[2K
| Adam | epoch: 013 | loss: 0.04073 - acc: 0.9902 -- iter: 096/222
[A[ATraining Step: 88  | total loss: [1m[32m0.05013[0m[0m | time: 38.613s
[2K
| Adam | epoch: 013 | loss: 0.05013 - acc: 0.9879 -- iter: 128/222
[A[ATraining Step: 89  | total loss: [1m[32m0.04572[0m[0m | time: 48.933s
[2K
| Adam | epoch: 013 | loss: 0.04572 - acc: 0.9891 -- iter: 160/222
[A[ATraining Step: 90  | total loss: [1m[32m0.04180[0m[0m | time: 59.267s
[2K
| Adam | epoch: 013 | loss: 0.04180 - acc: 0.9902 -- iter: 192/222
[A[ATraining Step: 91  | total loss: [1m[32m0.03822[0m[0m | time: 73.037s
[2K
| Adam | epoch: 013 | loss: 0.03822 - acc: 0.9912 | val_loss: 1.69044 - val_acc: 0.7143 -- iter: 222/222
--
Training Step: 92  | total loss: [1m[32m0.03495[0m[0m | time: 9.740s
[2K
| Adam | epoch: 014 | loss: 0.03495 - acc: 0.9920 -- iter: 032/222
[A[ATraining Step: 93  | total loss: [1m[32m0.04359[0m[0m | time: 20.285s
[2K
| Adam | epoch: 014 | loss: 0.04359 - acc: 0.9897 -- iter: 064/222
[A[ATraining Step: 94  | total loss: [1m[32m0.04413[0m[0m | time: 29.576s
[2K
| Adam | epoch: 014 | loss: 0.04413 - acc: 0.9876 -- iter: 096/222
[A[ATraining Step: 95  | total loss: [1m[32m0.04504[0m[0m | time: 39.580s
[2K
| Adam | epoch: 014 | loss: 0.04504 - acc: 0.9857 -- iter: 128/222
[A[ATraining Step: 96  | total loss: [1m[32m0.04277[0m[0m | time: 49.357s
[2K
| Adam | epoch: 014 | loss: 0.04277 - acc: 0.9872 -- iter: 160/222
[A[ATraining Step: 97  | total loss: [1m[32m0.04273[0m[0m | time: 58.814s
[2K
| Adam | epoch: 014 | loss: 0.04273 - acc: 0.9851 -- iter: 192/222
[A[ATraining Step: 98  | total loss: [1m[32m0.03971[0m[0m | time: 73.572s
[2K
| Adam | epoch: 014 | loss: 0.03971 - acc: 0.9866 | val_loss: 2.28815 - val_acc: 0.6571 -- iter: 222/222
--
Training Step: 99  | total loss: [1m[32m0.04805[0m[0m | time: 10.077s
[2K
| Adam | epoch: 015 | loss: 0.04805 - acc: 0.9817 -- iter: 032/222
[A[ATraining Step: 100  | total loss: [1m[32m0.05510[0m[0m | time: 19.971s
[2K
| Adam | epoch: 015 | loss: 0.05510 - acc: 0.9773 -- iter: 064/222
[A[ATraining Step: 101  | total loss: [1m[32m0.05064[0m[0m | time: 28.765s
[2K
| Adam | epoch: 015 | loss: 0.05064 - acc: 0.9795 -- iter: 096/222
[A[ATraining Step: 102  | total loss: [1m[32m0.04588[0m[0m | time: 39.136s
[2K
| Adam | epoch: 015 | loss: 0.04588 - acc: 0.9816 -- iter: 128/222
[A[ATraining Step: 103  | total loss: [1m[32m0.04638[0m[0m | time: 48.899s
[2K
| Adam | epoch: 015 | loss: 0.04638 - acc: 0.9803 -- iter: 160/222
[A[ATraining Step: 104  | total loss: [1m[32m0.08747[0m[0m | time: 57.718s
[2K
| Adam | epoch: 015 | loss: 0.08747 - acc: 0.9789 -- iter: 192/222
[A[ATraining Step: 105  | total loss: [1m[32m0.07962[0m[0m | time: 71.922s
[2K
| Adam | epoch: 015 | loss: 0.07962 - acc: 0.9810 | val_loss: 0.81507 - val_acc: 0.8000 -- iter: 222/222
--
Validation AUC:0.9926470588235294
Validation AUPRC:0.992195048047752
Test AUC:0.9743589743589743
Test AUPRC:0.9848907088017798
BestTestF1Score	0.9	0.77	0.89	0.86	0.95	37	6	25	2	1.0
BestTestMCCScore	0.9	0.77	0.89	0.86	0.95	37	6	25	2	1.0
BestTestAccuracyScore	0.9	0.77	0.89	0.86	0.95	37	6	25	2	1.0
BestValidationF1Score	0.94	0.89	0.94	0.92	0.97	33	3	33	1	1.0
BestValidationMCC	0.94	0.89	0.94	0.92	0.97	33	3	33	1	1.0
BestValidationAccuracy	0.94	0.89	0.94	0.92	0.97	33	3	33	1	1.0
TestPredictions (Threshold:1.0)
CHEMBL148231,TP,ACT,1.0	CHEMBL3753871,TN,INACT,0.009999999776482582	CHEMBL3109600,FP,INACT,1.0	CHEMBL69367,TP,ACT,1.0	CHEMBL34292,TP,ACT,1.0	CHEMBL2204699,TN,INACT,0.9700000286102295	CHEMBL430616,TP,ACT,1.0	CHEMBL21518,TP,ACT,1.0	CHEMBL1808552,TN,INACT,0.0	CHEMBL374682,TN,INACT,0.11999999731779099	CHEMBL151870,TP,ACT,1.0	CHEMBL309282,TP,ACT,1.0	CHEMBL2204701,TN,INACT,0.9800000190734863	CHEMBL165417,TP,ACT,1.0	CHEMBL491039,TN,INACT,0.6299999952316284	CHEMBL36768,TP,ACT,1.0	CHEMBL515972,FP,INACT,1.0	CHEMBL315986,TP,ACT,1.0	CHEMBL132033,TP,ACT,1.0	CHEMBL488991,FP,INACT,1.0	CHEMBL36718,TP,ACT,1.0	CHEMBL39344,TP,ACT,1.0	CHEMBL346202,TP,ACT,1.0	CHEMBL237035,TN,INACT,0.36000001430511475	CHEMBL44478,TP,ACT,1.0	CHEMBL472238,FN,ACT,0.15000000596046448	CHEMBL492047,FP,INACT,1.0	CHEMBL2403509,TN,INACT,0.05999999865889549	CHEMBL398106,TN,INACT,0.46000000834465027	CHEMBL1808555,TN,INACT,0.0	CHEMBL82716,TP,ACT,1.0	CHEMBL75527,TP,ACT,1.0	CHEMBL370054,TN,INACT,0.8399999737739563	CHEMBL313647,TP,ACT,1.0	CHEMBL2159606,TN,INACT,0.6499999761581421	CHEMBL163223,TP,ACT,1.0	CHEMBL487561,TN,INACT,0.5	CHEMBL6,FP,INACT,1.0	CHEMBL111077,TN,INACT,0.0	CHEMBL157160,TP,ACT,1.0	CHEMBL165843,TP,ACT,1.0	CHEMBL484063,TN,INACT,0.009999999776482582	CHEMBL42314,TP,ACT,1.0	CHEMBL165629,TP,ACT,1.0	CHEMBL1373918,FP,INACT,1.0	CHEMBL139536,FN,ACT,0.9399999976158142	CHEMBL115604,TN,INACT,0.2199999988079071	CHEMBL326911,TP,ACT,1.0	CHEMBL109581,TP,ACT,1.0	CHEMBL163530,TP,ACT,1.0	CHEMBL88740,TP,ACT,1.0	CHEMBL520305,TN,INACT,0.03999999910593033	CHEMBL119111,TN,INACT,0.44999998807907104	CHEMBL349935,TP,ACT,1.0	CHEMBL162393,TP,ACT,1.0	CHEMBL236339,TN,INACT,0.12999999523162842	CHEMBL330593,TP,ACT,1.0	CHEMBL118754,TP,ACT,1.0	CHEMBL2348891,TN,INACT,0.009999999776482582	CHEMBL311269,TP,ACT,1.0	CHEMBL80271,TP,ACT,1.0	CHEMBL37672,TP,ACT,1.0	CHEMBL2348872,TN,INACT,0.9900000095367432	CHEMBL92430,TP,ACT,1.0	CHEMBL3263244,TN,INACT,0.0	CHEMBL237600,TN,INACT,0.23999999463558197	CHEMBL12585,TP,ACT,1.0	CHEMBL2204687,TN,INACT,0.9900000095367432	CHEMBL275311,TP,ACT,1.0	CHEMBL2348884,TN,INACT,0.009999999776482582	

