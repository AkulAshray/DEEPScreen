ImageNetInceptionV2 CHEMBL4317 adam 0.0001 15 0 0 0.6 False True
Number of active compounds :	146
Number of inactive compounds :	146
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL4317_adam_0.0001_15_0_0_0.6_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL4317_adam_0.0001_15_0.6/
---------------------------------
Training samples: 181
Validation samples: 57
--
Training Step: 1  | time: 41.604s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/181
[A[ATraining Step: 2  | total loss: [1m[32m0.75361[0m[0m | time: 59.516s
[2K
| Adam | epoch: 001 | loss: 0.75361 - acc: 0.3656 -- iter: 064/181
[A[ATraining Step: 3  | total loss: [1m[32m0.68726[0m[0m | time: 75.994s
[2K
| Adam | epoch: 001 | loss: 0.68726 - acc: 0.4756 -- iter: 096/181
[A[ATraining Step: 4  | total loss: [1m[32m0.53992[0m[0m | time: 105.831s
[2K
| Adam | epoch: 001 | loss: 0.53992 - acc: 0.7048 -- iter: 128/181
[A[ATraining Step: 5  | total loss: [1m[32m0.52047[0m[0m | time: 129.333s
[2K
| Adam | epoch: 001 | loss: 0.52047 - acc: 0.7577 -- iter: 160/181
[A[ATraining Step: 6  | total loss: [1m[32m0.57418[0m[0m | time: 143.695s
[2K
| Adam | epoch: 001 | loss: 0.57418 - acc: 0.6925 | val_loss: 1.17968 - val_acc: 0.4912 -- iter: 181/181
--
Training Step: 7  | total loss: [1m[32m0.57768[0m[0m | time: 6.108s
[2K
| Adam | epoch: 002 | loss: 0.57768 - acc: 0.7341 -- iter: 032/181
[A[ATraining Step: 8  | total loss: [1m[32m0.48649[0m[0m | time: 45.294s
[2K
| Adam | epoch: 002 | loss: 0.48649 - acc: 0.7765 -- iter: 064/181
[A[ATraining Step: 9  | total loss: [1m[32m0.50141[0m[0m | time: 119.615s
[2K
| Adam | epoch: 002 | loss: 0.50141 - acc: 0.7625 -- iter: 096/181
[A[ATraining Step: 10  | total loss: [1m[32m0.42464[0m[0m | time: 172.606s
[2K
| Adam | epoch: 002 | loss: 0.42464 - acc: 0.8187 -- iter: 128/181
[A[ATraining Step: 11  | total loss: [1m[32m0.38994[0m[0m | time: 233.941s
[2K
| Adam | epoch: 002 | loss: 0.38994 - acc: 0.8602 -- iter: 160/181
[A[ATraining Step: 12  | total loss: [1m[32m0.34838[0m[0m | time: 267.620s
[2K
| Adam | epoch: 002 | loss: 0.34838 - acc: 0.9090 | val_loss: 2.03159 - val_acc: 0.4912 -- iter: 181/181
--
Training Step: 13  | total loss: [1m[32m0.31018[0m[0m | time: 6.001s
[2K
| Adam | epoch: 003 | loss: 0.31018 - acc: 0.9346 -- iter: 032/181
[A[ATraining Step: 14  | total loss: [1m[32m0.30749[0m[0m | time: 11.928s
[2K
| Adam | epoch: 003 | loss: 0.30749 - acc: 0.9224 -- iter: 064/181
[A[ATraining Step: 15  | total loss: [1m[32m0.27074[0m[0m | time: 32.979s
[2K
| Adam | epoch: 003 | loss: 0.27074 - acc: 0.9528 -- iter: 096/181
[A[ATraining Step: 16  | total loss: [1m[32m0.26194[0m[0m | time: 59.824s
[2K
| Adam | epoch: 003 | loss: 0.26194 - acc: 0.9470 -- iter: 128/181
[A[ATraining Step: 17  | total loss: [1m[32m0.23774[0m[0m | time: 89.623s
[2K
| Adam | epoch: 003 | loss: 0.23774 - acc: 0.9436 -- iter: 160/181
[A[ATraining Step: 18  | total loss: [1m[32m0.19224[0m[0m | time: 118.779s
[2K
| Adam | epoch: 003 | loss: 0.19224 - acc: 0.9631 | val_loss: 2.23570 - val_acc: 0.4912 -- iter: 181/181
--
Training Step: 19  | total loss: [1m[32m0.16062[0m[0m | time: 43.802s
[2K
| Adam | epoch: 004 | loss: 0.16062 - acc: 0.9754 -- iter: 032/181
[A[ATraining Step: 20  | total loss: [1m[32m0.13507[0m[0m | time: 51.120s
[2K
| Adam | epoch: 004 | loss: 0.13507 - acc: 0.9833 -- iter: 064/181
[A[ATraining Step: 21  | total loss: [1m[32m0.12059[0m[0m | time: 58.511s
[2K
| Adam | epoch: 004 | loss: 0.12059 - acc: 0.9885 -- iter: 096/181
[A[ATraining Step: 22  | total loss: [1m[32m0.10250[0m[0m | time: 71.531s
[2K
| Adam | epoch: 004 | loss: 0.10250 - acc: 0.9919 -- iter: 128/181
[A[ATraining Step: 23  | total loss: [1m[32m0.09026[0m[0m | time: 98.005s
[2K
| Adam | epoch: 004 | loss: 0.09026 - acc: 0.9943 -- iter: 160/181
[A[ATraining Step: 24  | total loss: [1m[32m0.07439[0m[0m | time: 121.176s
[2K
| Adam | epoch: 004 | loss: 0.07439 - acc: 0.9959 | val_loss: 2.28529 - val_acc: 0.4912 -- iter: 181/181
--
Training Step: 25  | total loss: [1m[32m0.06125[0m[0m | time: 29.803s
[2K
| Adam | epoch: 005 | loss: 0.06125 - acc: 0.9970 -- iter: 032/181
[A[ATraining Step: 26  | total loss: [1m[32m0.04981[0m[0m | time: 56.525s
[2K
| Adam | epoch: 005 | loss: 0.04981 - acc: 0.9978 -- iter: 064/181
[A[ATraining Step: 27  | total loss: [1m[32m0.04163[0m[0m | time: 63.584s
[2K
| Adam | epoch: 005 | loss: 0.04163 - acc: 0.9984 -- iter: 096/181
[A[ATraining Step: 28  | total loss: [1m[32m0.03893[0m[0m | time: 70.566s
[2K
| Adam | epoch: 005 | loss: 0.03893 - acc: 0.9988 -- iter: 128/181
[A[ATraining Step: 29  | total loss: [1m[32m0.03486[0m[0m | time: 93.000s
[2K
| Adam | epoch: 005 | loss: 0.03486 - acc: 0.9991 -- iter: 160/181
[A[ATraining Step: 30  | total loss: [1m[32m0.03250[0m[0m | time: 126.662s
[2K
| Adam | epoch: 005 | loss: 0.03250 - acc: 0.9993 | val_loss: 2.48854 - val_acc: 0.4912 -- iter: 181/181
--
Training Step: 31  | total loss: [1m[32m0.05725[0m[0m | time: 25.151s
[2K
| Adam | epoch: 006 | loss: 0.05725 - acc: 0.9922 -- iter: 032/181
[A[ATraining Step: 32  | total loss: [1m[32m0.04687[0m[0m | time: 71.037s
[2K
| Adam | epoch: 006 | loss: 0.04687 - acc: 0.9940 -- iter: 064/181
[A[ATraining Step: 33  | total loss: [1m[32m0.04844[0m[0m | time: 109.491s
[2K
| Adam | epoch: 006 | loss: 0.04844 - acc: 0.9884 -- iter: 096/181
[A[ATraining Step: 34  | total loss: [1m[32m0.03971[0m[0m | time: 118.129s
[2K
| Adam | epoch: 006 | loss: 0.03971 - acc: 0.9909 -- iter: 128/181
[A[ATraining Step: 35  | total loss: [1m[32m0.03224[0m[0m | time: 128.447s
[2K
| Adam | epoch: 006 | loss: 0.03224 - acc: 0.9928 -- iter: 160/181
[A[ATraining Step: 36  | total loss: [1m[32m0.02639[0m[0m | time: 180.628s
[2K
| Adam | epoch: 006 | loss: 0.02639 - acc: 0.9943 | val_loss: 2.81154 - val_acc: 0.4912 -- iter: 181/181
--
Training Step: 37  | total loss: [1m[32m0.02369[0m[0m | time: 23.422s
[2K
| Adam | epoch: 007 | loss: 0.02369 - acc: 0.9954 -- iter: 032/181
[A[ATraining Step: 38  | total loss: [1m[32m0.01971[0m[0m | time: 40.813s
[2K
| Adam | epoch: 007 | loss: 0.01971 - acc: 0.9963 -- iter: 064/181
[A[ATraining Step: 39  | total loss: [1m[32m0.01663[0m[0m | time: 65.010s
[2K
| Adam | epoch: 007 | loss: 0.01663 - acc: 0.9970 -- iter: 096/181
[A[ATraining Step: 40  | total loss: [1m[32m0.01403[0m[0m | time: 83.500s
[2K
| Adam | epoch: 007 | loss: 0.01403 - acc: 0.9976 -- iter: 128/181
[A[ATraining Step: 41  | total loss: [1m[32m0.02017[0m[0m | time: 92.035s
[2K
| Adam | epoch: 007 | loss: 0.02017 - acc: 0.9923 -- iter: 160/181
[A[ATraining Step: 42  | total loss: [1m[32m0.01728[0m[0m | time: 104.791s
[2K
| Adam | epoch: 007 | loss: 0.01728 - acc: 0.9937 | val_loss: 2.68815 - val_acc: 0.4912 -- iter: 181/181
--
Training Step: 43  | total loss: [1m[32m0.01496[0m[0m | time: 18.877s
[2K
| Adam | epoch: 008 | loss: 0.01496 - acc: 0.9948 -- iter: 032/181
[A[ATraining Step: 44  | total loss: [1m[32m0.01279[0m[0m | time: 33.151s
[2K
| Adam | epoch: 008 | loss: 0.01279 - acc: 0.9957 -- iter: 064/181
[A[ATraining Step: 45  | total loss: [1m[32m0.03622[0m[0m | time: 61.432s
[2K
| Adam | epoch: 008 | loss: 0.03622 - acc: 0.9911 -- iter: 096/181
[A[ATraining Step: 46  | total loss: [1m[32m0.03098[0m[0m | time: 73.161s
[2K
| Adam | epoch: 008 | loss: 0.03098 - acc: 0.9926 -- iter: 128/181
[A[ATraining Step: 47  | total loss: [1m[32m0.02682[0m[0m | time: 85.888s
[2K
| Adam | epoch: 008 | loss: 0.02682 - acc: 0.9938 -- iter: 160/181
[A[ATraining Step: 48  | total loss: [1m[32m0.02339[0m[0m | time: 98.576s
[2K
| Adam | epoch: 008 | loss: 0.02339 - acc: 0.9948 | val_loss: 2.18897 - val_acc: 0.4912 -- iter: 181/181
--
Training Step: 49  | total loss: [1m[32m0.02009[0m[0m | time: 7.973s
[2K
| Adam | epoch: 009 | loss: 0.02009 - acc: 0.9956 -- iter: 032/181
[A[ATraining Step: 50  | total loss: [1m[32m0.01728[0m[0m | time: 24.317s
[2K
| Adam | epoch: 009 | loss: 0.01728 - acc: 0.9963 -- iter: 064/181
[A[ATraining Step: 51  | total loss: [1m[32m0.01500[0m[0m | time: 51.243s
[2K
| Adam | epoch: 009 | loss: 0.01500 - acc: 0.9969 -- iter: 096/181
[A[ATraining Step: 52  | total loss: [1m[32m0.04374[0m[0m | time: 63.454s
[2K
| Adam | epoch: 009 | loss: 0.04374 - acc: 0.9927 -- iter: 128/181
[A[ATraining Step: 53  | total loss: [1m[32m0.07476[0m[0m | time: 93.408s
[2K
| Adam | epoch: 009 | loss: 0.07476 - acc: 0.9891 -- iter: 160/181
[A[ATraining Step: 54  | total loss: [1m[32m0.06438[0m[0m | time: 111.354s
[2K
| Adam | epoch: 009 | loss: 0.06438 - acc: 0.9907 | val_loss: 1.18708 - val_acc: 0.5614 -- iter: 181/181
--
Training Step: 55  | total loss: [1m[32m0.05739[0m[0m | time: 8.348s
[2K
| Adam | epoch: 010 | loss: 0.05739 - acc: 0.9920 -- iter: 032/181
[A[ATraining Step: 56  | total loss: [1m[32m0.06532[0m[0m | time: 17.070s
[2K
| Adam | epoch: 010 | loss: 0.06532 - acc: 0.9865 -- iter: 064/181
[A[ATraining Step: 57  | total loss: [1m[32m0.05814[0m[0m | time: 28.894s
[2K
| Adam | epoch: 010 | loss: 0.05814 - acc: 0.9883 -- iter: 096/181
[A[ATraining Step: 58  | total loss: [1m[32m0.05115[0m[0m | time: 56.541s
[2K
| Adam | epoch: 010 | loss: 0.05115 - acc: 0.9899 -- iter: 128/181
[A[ATraining Step: 59  | total loss: [1m[32m0.04532[0m[0m | time: 69.758s
[2K
| Adam | epoch: 010 | loss: 0.04532 - acc: 0.9913 -- iter: 160/181
[A[ATraining Step: 60  | total loss: [1m[32m0.03992[0m[0m | time: 85.677s
[2K
| Adam | epoch: 010 | loss: 0.03992 - acc: 0.9924 | val_loss: 1.85037 - val_acc: 0.5439 -- iter: 181/181
--
Training Step: 61  | total loss: [1m[32m0.03524[0m[0m | time: 12.229s
[2K
| Adam | epoch: 011 | loss: 0.03524 - acc: 0.9934 -- iter: 032/181
[A[ATraining Step: 62  | total loss: [1m[32m0.03481[0m[0m | time: 20.971s
[2K
| Adam | epoch: 011 | loss: 0.03481 - acc: 0.9943 -- iter: 064/181
[A[ATraining Step: 63  | total loss: [1m[32m0.03251[0m[0m | time: 30.032s
[2K
| Adam | epoch: 011 | loss: 0.03251 - acc: 0.9950 -- iter: 096/181
[A[ATraining Step: 64  | total loss: [1m[32m0.02943[0m[0m | time: 48.476s
[2K
| Adam | epoch: 011 | loss: 0.02943 - acc: 0.9956 -- iter: 128/181
[A[ATraining Step: 65  | total loss: [1m[32m0.02634[0m[0m | time: 64.868s
[2K
| Adam | epoch: 011 | loss: 0.02634 - acc: 0.9962 -- iter: 160/181
[A[ATraining Step: 66  | total loss: [1m[32m0.02375[0m[0m | time: 78.476s
[2K
| Adam | epoch: 011 | loss: 0.02375 - acc: 0.9966 | val_loss: 0.41734 - val_acc: 0.8246 -- iter: 181/181
--
Training Step: 67  | total loss: [1m[32m0.02123[0m[0m | time: 15.285s
[2K
| Adam | epoch: 012 | loss: 0.02123 - acc: 0.9970 -- iter: 032/181
[A[ATraining Step: 68  | total loss: [1m[32m0.01907[0m[0m | time: 26.947s
[2K
| Adam | epoch: 012 | loss: 0.01907 - acc: 0.9974 -- iter: 064/181
[A[ATraining Step: 69  | total loss: [1m[32m0.01722[0m[0m | time: 33.956s
[2K
| Adam | epoch: 012 | loss: 0.01722 - acc: 0.9977 -- iter: 096/181
[A[ATraining Step: 70  | total loss: [1m[32m0.01566[0m[0m | time: 41.062s
[2K
| Adam | epoch: 012 | loss: 0.01566 - acc: 0.9980 -- iter: 128/181
[A[ATraining Step: 71  | total loss: [1m[32m0.01424[0m[0m | time: 50.929s
[2K
| Adam | epoch: 012 | loss: 0.01424 - acc: 0.9982 -- iter: 160/181
[A[ATraining Step: 72  | total loss: [1m[32m0.01485[0m[0m | time: 64.417s
[2K
| Adam | epoch: 012 | loss: 0.01485 - acc: 0.9984 | val_loss: 0.76235 - val_acc: 0.6491 -- iter: 181/181
--
Training Step: 73  | total loss: [1m[32m0.09702[0m[0m | time: 9.992s
[2K
| Adam | epoch: 013 | loss: 0.09702 - acc: 0.9847 -- iter: 032/181
[A[ATraining Step: 74  | total loss: [1m[32m0.08696[0m[0m | time: 22.906s
[2K
| Adam | epoch: 013 | loss: 0.08696 - acc: 0.9864 -- iter: 064/181
[A[ATraining Step: 75  | total loss: [1m[32m0.07800[0m[0m | time: 32.595s
[2K
| Adam | epoch: 013 | loss: 0.07800 - acc: 0.9878 -- iter: 096/181
[A[ATraining Step: 76  | total loss: [1m[32m0.07087[0m[0m | time: 39.291s
[2K
| Adam | epoch: 013 | loss: 0.07087 - acc: 0.9891 -- iter: 128/181
[A[ATraining Step: 77  | total loss: [1m[32m0.06380[0m[0m | time: 46.507s
[2K
| Adam | epoch: 013 | loss: 0.06380 - acc: 0.9903 -- iter: 160/181
[A[ATraining Step: 78  | total loss: [1m[32m0.05764[0m[0m | time: 59.678s
[2K
| Adam | epoch: 013 | loss: 0.05764 - acc: 0.9913 | val_loss: 0.25760 - val_acc: 0.8596 -- iter: 181/181
--
Training Step: 79  | total loss: [1m[32m0.06987[0m[0m | time: 10.110s
[2K
| Adam | epoch: 014 | loss: 0.06987 - acc: 0.9890 -- iter: 032/181
[A[ATraining Step: 80  | total loss: [1m[32m0.09964[0m[0m | time: 20.828s
[2K
| Adam | epoch: 014 | loss: 0.09964 - acc: 0.9837 -- iter: 064/181
[A[ATraining Step: 81  | total loss: [1m[32m0.09148[0m[0m | time: 30.581s
[2K
| Adam | epoch: 014 | loss: 0.09148 - acc: 0.9854 -- iter: 096/181
[A[ATraining Step: 82  | total loss: [1m[32m0.08641[0m[0m | time: 40.285s
[2K
| Adam | epoch: 014 | loss: 0.08641 - acc: 0.9837 -- iter: 128/181
[A[ATraining Step: 83  | total loss: [1m[32m0.07820[0m[0m | time: 47.287s
[2K
| Adam | epoch: 014 | loss: 0.07820 - acc: 0.9853 -- iter: 160/181
[A[ATraining Step: 84  | total loss: [1m[32m0.07210[0m[0m | time: 57.292s
[2K
| Adam | epoch: 014 | loss: 0.07210 - acc: 0.9868 | val_loss: 0.37352 - val_acc: 0.7895 -- iter: 181/181
--
Training Step: 85  | total loss: [1m[32m0.06594[0m[0m | time: 9.872s
[2K
| Adam | epoch: 015 | loss: 0.06594 - acc: 0.9881 -- iter: 032/181
[A[ATraining Step: 86  | total loss: [1m[32m0.06276[0m[0m | time: 20.997s
[2K
| Adam | epoch: 015 | loss: 0.06276 - acc: 0.9862 -- iter: 064/181
[A[ATraining Step: 87  | total loss: [1m[32m0.05907[0m[0m | time: 30.898s
[2K
| Adam | epoch: 015 | loss: 0.05907 - acc: 0.9876 -- iter: 096/181
[A[ATraining Step: 88  | total loss: [1m[32m0.05387[0m[0m | time: 40.391s
[2K
| Adam | epoch: 015 | loss: 0.05387 - acc: 0.9888 -- iter: 128/181
[A[ATraining Step: 89  | total loss: [1m[32m0.04918[0m[0m | time: 50.219s
[2K
| Adam | epoch: 015 | loss: 0.04918 - acc: 0.9899 -- iter: 160/181
[A[ATraining Step: 90  | total loss: [1m[32m0.06433[0m[0m | time: 61.573s
[2K
| Adam | epoch: 015 | loss: 0.06433 - acc: 0.9878 | val_loss: 0.66854 - val_acc: 0.7368 -- iter: 181/181
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.9544334975369458
Validation AUPRC:0.9640844702801223
Test AUC:0.9675
Test AUPRC:0.9712142494210795
BestTestF1Score	0.91	0.79	0.89	0.91	0.91	29	3	22	3	0.07
BestTestMCCScore	0.91	0.79	0.89	0.91	0.91	29	3	22	3	0.07
BestTestAccuracyScore	0.91	0.79	0.89	0.91	0.91	29	3	22	3	0.07
BestValidationF1Score	0.91	0.82	0.91	0.93	0.89	25	2	27	3	0.07
BestValidationMCC	0.91	0.82	0.91	0.93	0.89	25	2	27	3	0.07
BestValidationAccuracy	0.91	0.82	0.91	0.93	0.89	25	2	27	3	0.07
TestPredictions (Threshold:0.07)
CHEMBL413040,TN,INACT,0.009999999776482582	CHEMBL329545,TP,ACT,0.8700000047683716	CHEMBL288199,FN,ACT,0.05999999865889549	CHEMBL83954,TP,ACT,0.3700000047683716	CHEMBL43661,TN,INACT,0.0	CHEMBL327192,TP,ACT,0.14000000059604645	CHEMBL545363,TN,INACT,0.0	CHEMBL50852,TP,ACT,0.28999999165534973	CHEMBL89259,TP,ACT,0.4399999976158142	CHEMBL86,TP,ACT,0.6399999856948853	CHEMBL42411,TN,INACT,0.0	CHEMBL3109772,TN,INACT,0.019999999552965164	CHEMBL295155,FN,ACT,0.009999999776482582	CHEMBL451335,TN,INACT,0.0	CHEMBL414570,TN,INACT,0.0	CHEMBL595022,TN,INACT,0.0	CHEMBL1926731,TP,ACT,0.6100000143051147	CHEMBL1259241,TN,INACT,0.0	CHEMBL298612,TN,INACT,0.0	CHEMBL114112,TP,ACT,0.07000000029802322	CHEMBL88867,TP,ACT,0.3499999940395355	CHEMBL158238,TP,ACT,0.9200000166893005	CHEMBL95178,TP,ACT,0.9700000286102295	CHEMBL3633656,TN,INACT,0.0	CHEMBL3759601,TP,ACT,0.9800000190734863	CHEMBL33732,TP,ACT,0.38999998569488525	CHEMBL18473,TP,ACT,0.5799999833106995	CHEMBL327798,TP,ACT,0.3799999952316284	CHEMBL298920,TP,ACT,0.07999999821186066	CHEMBL408492,TN,INACT,0.0	CHEMBL94880,TP,ACT,0.9700000286102295	CHEMBL347235,TP,ACT,0.9200000166893005	CHEMBL114478,FP,INACT,0.20000000298023224	CHEMBL594376,TN,INACT,0.0	CHEMBL148080,FN,ACT,0.029999999329447746	CHEMBL173708,TN,INACT,0.0	CHEMBL117287,TP,ACT,0.5699999928474426	CHEMBL164968,FP,INACT,0.07000000029802322	CHEMBL336081,FP,INACT,0.44999998807907104	CHEMBL2113072,TN,INACT,0.009999999776482582	CHEMBL348311,TP,ACT,0.7300000190734863	CHEMBL85251,TP,ACT,0.7099999785423279	CHEMBL407493,TP,ACT,0.8899999856948853	CHEMBL169553,TN,INACT,0.0	CHEMBL40317,TN,INACT,0.0	CHEMBL319663,TP,ACT,0.9700000286102295	CHEMBL462650,TN,INACT,0.0	CHEMBL48836,TP,ACT,0.8500000238418579	CHEMBL3758932,TP,ACT,0.9599999785423279	CHEMBL435810,TN,INACT,0.009999999776482582	CHEMBL603858,TN,INACT,0.0	CHEMBL329487,TP,ACT,0.47999998927116394	CHEMBL147827,TP,ACT,0.10000000149011612	CHEMBL1926730,TP,ACT,0.3100000023841858	CHEMBL515170,TN,INACT,0.0	CHEMBL41769,TP,ACT,0.36000001430511475	CHEMBL3780633,TN,INACT,0.0	

