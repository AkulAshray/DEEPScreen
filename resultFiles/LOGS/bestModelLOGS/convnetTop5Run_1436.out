CNNModel CHEMBL4158 adam 0.0005 15 128 0 0.6 False True
Number of active compounds :	210
Number of inactive compounds :	140
---------------------------------
Run id: CNNModel_CHEMBL4158_adam_0.0005_15_128_0_0.6_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL4158_adam_0.0005_15_128_0.6_True/
---------------------------------
Training samples: 217
Validation samples: 69
--
Training Step: 1  | time: 1.593s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/217
[A[ATraining Step: 2  | total loss: [1m[32m0.62389[0m[0m | time: 2.825s
[2K
| Adam | epoch: 001 | loss: 0.62389 - acc: 0.5062 -- iter: 064/217
[A[ATraining Step: 3  | total loss: [1m[32m0.67964[0m[0m | time: 3.542s
[2K
| Adam | epoch: 001 | loss: 0.67964 - acc: 0.5523 -- iter: 096/217
[A[ATraining Step: 4  | total loss: [1m[32m0.68722[0m[0m | time: 4.414s
[2K
| Adam | epoch: 001 | loss: 0.68722 - acc: 0.5599 -- iter: 128/217
[A[ATraining Step: 5  | total loss: [1m[32m0.68996[0m[0m | time: 5.308s
[2K
| Adam | epoch: 001 | loss: 0.68996 - acc: 0.5401 -- iter: 160/217
[A[ATraining Step: 6  | total loss: [1m[32m0.67226[0m[0m | time: 6.218s
[2K
| Adam | epoch: 001 | loss: 0.67226 - acc: 0.6348 -- iter: 192/217
[A[ATraining Step: 7  | total loss: [1m[32m0.68679[0m[0m | time: 8.001s
[2K
| Adam | epoch: 001 | loss: 0.68679 - acc: 0.5727 | val_loss: 0.69073 - val_acc: 0.5362 -- iter: 217/217
--
Training Step: 8  | total loss: [1m[32m0.71373[0m[0m | time: 0.936s
[2K
| Adam | epoch: 002 | loss: 0.71373 - acc: 0.4981 -- iter: 032/217
[A[ATraining Step: 9  | total loss: [1m[32m0.71763[0m[0m | time: 1.733s
[2K
| Adam | epoch: 002 | loss: 0.71763 - acc: 0.4673 -- iter: 064/217
[A[ATraining Step: 10  | total loss: [1m[32m0.69681[0m[0m | time: 2.643s
[2K
| Adam | epoch: 002 | loss: 0.69681 - acc: 0.5305 -- iter: 096/217
[A[ATraining Step: 11  | total loss: [1m[32m0.69082[0m[0m | time: 3.633s
[2K
| Adam | epoch: 002 | loss: 0.69082 - acc: 0.5457 -- iter: 128/217
[A[ATraining Step: 12  | total loss: [1m[32m0.68523[0m[0m | time: 4.501s
[2K
| Adam | epoch: 002 | loss: 0.68523 - acc: 0.5814 -- iter: 160/217
[A[ATraining Step: 13  | total loss: [1m[32m0.68677[0m[0m | time: 5.484s
[2K
| Adam | epoch: 002 | loss: 0.68677 - acc: 0.5599 -- iter: 192/217
[A[ATraining Step: 14  | total loss: [1m[32m0.68254[0m[0m | time: 7.498s
[2K
| Adam | epoch: 002 | loss: 0.68254 - acc: 0.5993 | val_loss: 0.68923 - val_acc: 0.5362 -- iter: 217/217
--
Training Step: 15  | total loss: [1m[32m0.67888[0m[0m | time: 0.701s
[2K
| Adam | epoch: 003 | loss: 0.67888 - acc: 0.6338 -- iter: 032/217
[A[ATraining Step: 16  | total loss: [1m[32m0.67977[0m[0m | time: 1.418s
[2K
| Adam | epoch: 003 | loss: 0.67977 - acc: 0.6211 -- iter: 064/217
[A[ATraining Step: 17  | total loss: [1m[32m0.67910[0m[0m | time: 2.342s
[2K
| Adam | epoch: 003 | loss: 0.67910 - acc: 0.6135 -- iter: 096/217
[A[ATraining Step: 18  | total loss: [1m[32m0.67937[0m[0m | time: 3.233s
[2K
| Adam | epoch: 003 | loss: 0.67937 - acc: 0.6067 -- iter: 128/217
[A[ATraining Step: 19  | total loss: [1m[32m0.68082[0m[0m | time: 4.304s
[2K
| Adam | epoch: 003 | loss: 0.68082 - acc: 0.5920 -- iter: 160/217
[A[ATraining Step: 20  | total loss: [1m[32m0.67026[0m[0m | time: 5.341s
[2K
| Adam | epoch: 003 | loss: 0.67026 - acc: 0.6227 -- iter: 192/217
[A[ATraining Step: 21  | total loss: [1m[32m0.68103[0m[0m | time: 7.168s
[2K
| Adam | epoch: 003 | loss: 0.68103 - acc: 0.5846 | val_loss: 0.69371 - val_acc: 0.5362 -- iter: 217/217
--
Training Step: 22  | total loss: [1m[32m0.68653[0m[0m | time: 0.928s
[2K
| Adam | epoch: 004 | loss: 0.68653 - acc: 0.5686 -- iter: 032/217
[A[ATraining Step: 23  | total loss: [1m[32m0.67411[0m[0m | time: 1.668s
[2K
| Adam | epoch: 004 | loss: 0.67411 - acc: 0.5940 -- iter: 064/217
[A[ATraining Step: 24  | total loss: [1m[32m0.69799[0m[0m | time: 2.428s
[2K
| Adam | epoch: 004 | loss: 0.69799 - acc: 0.5395 -- iter: 096/217
[A[ATraining Step: 25  | total loss: [1m[32m0.71064[0m[0m | time: 3.472s
[2K
| Adam | epoch: 004 | loss: 0.71064 - acc: 0.5014 -- iter: 128/217
[A[ATraining Step: 26  | total loss: [1m[32m0.69753[0m[0m | time: 4.418s
[2K
| Adam | epoch: 004 | loss: 0.69753 - acc: 0.5341 -- iter: 160/217
[A[ATraining Step: 27  | total loss: [1m[32m0.68230[0m[0m | time: 5.330s
[2K
| Adam | epoch: 004 | loss: 0.68230 - acc: 0.5816 -- iter: 192/217
[A[ATraining Step: 28  | total loss: [1m[32m0.68566[0m[0m | time: 7.401s
[2K
| Adam | epoch: 004 | loss: 0.68566 - acc: 0.5612 | val_loss: 0.68641 - val_acc: 0.5362 -- iter: 217/217
--
Training Step: 29  | total loss: [1m[32m0.68375[0m[0m | time: 1.007s
[2K
| Adam | epoch: 005 | loss: 0.68375 - acc: 0.5615 -- iter: 032/217
[A[ATraining Step: 30  | total loss: [1m[32m0.68179[0m[0m | time: 1.937s
[2K
| Adam | epoch: 005 | loss: 0.68179 - acc: 0.5692 -- iter: 064/217
[A[ATraining Step: 31  | total loss: [1m[32m0.67518[0m[0m | time: 2.632s
[2K
| Adam | epoch: 005 | loss: 0.67518 - acc: 0.5965 -- iter: 096/217
[A[ATraining Step: 32  | total loss: [1m[32m0.67623[0m[0m | time: 3.530s
[2K
| Adam | epoch: 005 | loss: 0.67623 - acc: 0.5883 -- iter: 128/217
[A[ATraining Step: 33  | total loss: [1m[32m0.67686[0m[0m | time: 4.704s
[2K
| Adam | epoch: 005 | loss: 0.67686 - acc: 0.5821 -- iter: 160/217
[A[ATraining Step: 34  | total loss: [1m[32m0.67588[0m[0m | time: 5.840s
[2K
| Adam | epoch: 005 | loss: 0.67588 - acc: 0.5846 -- iter: 192/217
[A[ATraining Step: 35  | total loss: [1m[32m0.67834[0m[0m | time: 7.709s
[2K
| Adam | epoch: 005 | loss: 0.67834 - acc: 0.5734 | val_loss: 0.68460 - val_acc: 0.5362 -- iter: 217/217
--
Training Step: 36  | total loss: [1m[32m0.67975[0m[0m | time: 0.872s
[2K
| Adam | epoch: 006 | loss: 0.67975 - acc: 0.5648 -- iter: 032/217
[A[ATraining Step: 37  | total loss: [1m[32m0.67364[0m[0m | time: 1.795s
[2K
| Adam | epoch: 006 | loss: 0.67364 - acc: 0.5768 -- iter: 064/217
[A[ATraining Step: 38  | total loss: [1m[32m0.67128[0m[0m | time: 2.845s
[2K
| Adam | epoch: 006 | loss: 0.67128 - acc: 0.5801 -- iter: 096/217
[A[ATraining Step: 39  | total loss: [1m[32m0.66103[0m[0m | time: 3.932s
[2K
| Adam | epoch: 006 | loss: 0.66103 - acc: 0.6007 -- iter: 128/217
[A[ATraining Step: 40  | total loss: [1m[32m0.66684[0m[0m | time: 4.984s
[2K
| Adam | epoch: 006 | loss: 0.66684 - acc: 0.5856 -- iter: 160/217
[A[ATraining Step: 41  | total loss: [1m[32m0.67255[0m[0m | time: 5.899s
[2K
| Adam | epoch: 006 | loss: 0.67255 - acc: 0.5735 -- iter: 192/217
[A[ATraining Step: 42  | total loss: [1m[32m0.68867[0m[0m | time: 7.813s
[2K
| Adam | epoch: 006 | loss: 0.68867 - acc: 0.5434 | val_loss: 0.68057 - val_acc: 0.5362 -- iter: 217/217
--
Training Step: 43  | total loss: [1m[32m0.67325[0m[0m | time: 1.141s
[2K
| Adam | epoch: 007 | loss: 0.67325 - acc: 0.5688 -- iter: 032/217
[A[ATraining Step: 44  | total loss: [1m[32m0.66167[0m[0m | time: 2.042s
[2K
| Adam | epoch: 007 | loss: 0.66167 - acc: 0.5948 -- iter: 064/217
[A[ATraining Step: 45  | total loss: [1m[32m0.65505[0m[0m | time: 2.903s
[2K
| Adam | epoch: 007 | loss: 0.65505 - acc: 0.6052 -- iter: 096/217
[A[ATraining Step: 46  | total loss: [1m[32m0.65908[0m[0m | time: 3.793s
[2K
| Adam | epoch: 007 | loss: 0.65908 - acc: 0.5929 -- iter: 128/217
[A[ATraining Step: 47  | total loss: [1m[32m0.65730[0m[0m | time: 4.496s
[2K
| Adam | epoch: 007 | loss: 0.65730 - acc: 0.5930 -- iter: 160/217
[A[ATraining Step: 48  | total loss: [1m[32m0.64725[0m[0m | time: 5.197s
[2K
| Adam | epoch: 007 | loss: 0.64725 - acc: 0.6070 -- iter: 192/217
[A[ATraining Step: 49  | total loss: [1m[32m0.63814[0m[0m | time: 7.135s
[2K
| Adam | epoch: 007 | loss: 0.63814 - acc: 0.6185 | val_loss: 0.70701 - val_acc: 0.5362 -- iter: 217/217
--
Training Step: 50  | total loss: [1m[32m0.63478[0m[0m | time: 0.807s
[2K
| Adam | epoch: 008 | loss: 0.63478 - acc: 0.6195 -- iter: 032/217
[A[ATraining Step: 51  | total loss: [1m[32m0.65480[0m[0m | time: 1.649s
[2K
| Adam | epoch: 008 | loss: 0.65480 - acc: 0.5918 -- iter: 064/217
[A[ATraining Step: 52  | total loss: [1m[32m0.65229[0m[0m | time: 2.577s
[2K
| Adam | epoch: 008 | loss: 0.65229 - acc: 0.5921 -- iter: 096/217
[A[ATraining Step: 53  | total loss: [1m[32m0.64096[0m[0m | time: 3.458s
[2K
| Adam | epoch: 008 | loss: 0.64096 - acc: 0.6015 -- iter: 128/217
[A[ATraining Step: 54  | total loss: [1m[32m0.64400[0m[0m | time: 4.400s
[2K
| Adam | epoch: 008 | loss: 0.64400 - acc: 0.5959 -- iter: 160/217
[A[ATraining Step: 55  | total loss: [1m[32m0.64501[0m[0m | time: 5.223s
[2K
| Adam | epoch: 008 | loss: 0.64501 - acc: 0.5956 -- iter: 192/217
[A[ATraining Step: 56  | total loss: [1m[32m0.64532[0m[0m | time: 7.067s
[2K
| Adam | epoch: 008 | loss: 0.64532 - acc: 0.5906 | val_loss: 0.65314 - val_acc: 0.5362 -- iter: 217/217
--
Training Step: 57  | total loss: [1m[32m0.64431[0m[0m | time: 0.874s
[2K
| Adam | epoch: 009 | loss: 0.64431 - acc: 0.5863 -- iter: 032/217
[A[ATraining Step: 58  | total loss: [1m[32m0.63666[0m[0m | time: 1.804s
[2K
| Adam | epoch: 009 | loss: 0.63666 - acc: 0.6044 -- iter: 064/217
[A[ATraining Step: 59  | total loss: [1m[32m0.64411[0m[0m | time: 2.816s
[2K
| Adam | epoch: 009 | loss: 0.64411 - acc: 0.5778 -- iter: 096/217
[A[ATraining Step: 60  | total loss: [1m[32m0.64148[0m[0m | time: 3.780s
[2K
| Adam | epoch: 009 | loss: 0.64148 - acc: 0.5799 -- iter: 128/217
[A[ATraining Step: 61  | total loss: [1m[32m0.63498[0m[0m | time: 4.655s
[2K
| Adam | epoch: 009 | loss: 0.63498 - acc: 0.5858 -- iter: 160/217
[A[ATraining Step: 62  | total loss: [1m[32m0.62409[0m[0m | time: 5.639s
[2K
| Adam | epoch: 009 | loss: 0.62409 - acc: 0.5989 -- iter: 192/217
[A[ATraining Step: 63  | total loss: [1m[32m0.62327[0m[0m | time: 7.505s
[2K
| Adam | epoch: 009 | loss: 0.62327 - acc: 0.5903 | val_loss: 0.65200 - val_acc: 0.5362 -- iter: 217/217
--
Training Step: 64  | total loss: [1m[32m0.61869[0m[0m | time: 0.816s
[2K
| Adam | epoch: 010 | loss: 0.61869 - acc: 0.5915 -- iter: 032/217
[A[ATraining Step: 65  | total loss: [1m[32m0.61451[0m[0m | time: 1.818s
[2K
| Adam | epoch: 010 | loss: 0.61451 - acc: 0.5925 -- iter: 064/217
[A[ATraining Step: 66  | total loss: [1m[32m0.61930[0m[0m | time: 2.857s
[2K
| Adam | epoch: 010 | loss: 0.61930 - acc: 0.5737 -- iter: 096/217
[A[ATraining Step: 67  | total loss: [1m[32m0.61326[0m[0m | time: 3.888s
[2K
| Adam | epoch: 010 | loss: 0.61326 - acc: 0.5686 -- iter: 128/217
[A[ATraining Step: 68  | total loss: [1m[32m0.60762[0m[0m | time: 4.866s
[2K
| Adam | epoch: 010 | loss: 0.60762 - acc: 0.5753 -- iter: 160/217
[A[ATraining Step: 69  | total loss: [1m[32m0.59916[0m[0m | time: 5.858s
[2K
| Adam | epoch: 010 | loss: 0.59916 - acc: 0.5884 -- iter: 192/217
[A[ATraining Step: 70  | total loss: [1m[32m0.59240[0m[0m | time: 7.859s
[2K
| Adam | epoch: 010 | loss: 0.59240 - acc: 0.6034 | val_loss: 0.63764 - val_acc: 0.5797 -- iter: 217/217
--
Training Step: 71  | total loss: [1m[32m0.58386[0m[0m | time: 0.883s
[2K
| Adam | epoch: 011 | loss: 0.58386 - acc: 0.6237 -- iter: 032/217
[A[ATraining Step: 72  | total loss: [1m[32m0.58501[0m[0m | time: 1.675s
[2K
| Adam | epoch: 011 | loss: 0.58501 - acc: 0.6120 -- iter: 064/217
[A[ATraining Step: 73  | total loss: [1m[32m0.57822[0m[0m | time: 2.755s
[2K
| Adam | epoch: 011 | loss: 0.57822 - acc: 0.6062 -- iter: 096/217
[A[ATraining Step: 74  | total loss: [1m[32m0.57267[0m[0m | time: 3.815s
[2K
| Adam | epoch: 011 | loss: 0.57267 - acc: 0.6357 -- iter: 128/217
[A[ATraining Step: 75  | total loss: [1m[32m0.56517[0m[0m | time: 4.821s
[2K
| Adam | epoch: 011 | loss: 0.56517 - acc: 0.6380 -- iter: 160/217
[A[ATraining Step: 76  | total loss: [1m[32m0.55416[0m[0m | time: 5.827s
[2K
| Adam | epoch: 011 | loss: 0.55416 - acc: 0.6567 -- iter: 192/217
[A[ATraining Step: 77  | total loss: [1m[32m0.54984[0m[0m | time: 7.784s
[2K
| Adam | epoch: 011 | loss: 0.54984 - acc: 0.6765 | val_loss: 0.59022 - val_acc: 0.7391 -- iter: 217/217
--
Training Step: 78  | total loss: [1m[32m0.54640[0m[0m | time: 1.011s
[2K
| Adam | epoch: 012 | loss: 0.54640 - acc: 0.6940 -- iter: 032/217
[A[ATraining Step: 79  | total loss: [1m[32m0.53609[0m[0m | time: 1.886s
[2K
| Adam | epoch: 012 | loss: 0.53609 - acc: 0.7062 -- iter: 064/217
[A[ATraining Step: 80  | total loss: [1m[32m0.52930[0m[0m | time: 2.695s
[2K
| Adam | epoch: 012 | loss: 0.52930 - acc: 0.7117 -- iter: 096/217
[A[ATraining Step: 81  | total loss: [1m[32m0.52382[0m[0m | time: 3.756s
[2K
| Adam | epoch: 012 | loss: 0.52382 - acc: 0.7126 -- iter: 128/217
[A[ATraining Step: 82  | total loss: [1m[32m0.51042[0m[0m | time: 4.789s
[2K
| Adam | epoch: 012 | loss: 0.51042 - acc: 0.7319 -- iter: 160/217
[A[ATraining Step: 83  | total loss: [1m[32m0.50554[0m[0m | time: 5.770s
[2K
| Adam | epoch: 012 | loss: 0.50554 - acc: 0.7400 -- iter: 192/217
[A[ATraining Step: 84  | total loss: [1m[32m0.50359[0m[0m | time: 7.736s
[2K
| Adam | epoch: 012 | loss: 0.50359 - acc: 0.7441 | val_loss: 0.54671 - val_acc: 0.6957 -- iter: 217/217
--
Training Step: 85  | total loss: [1m[32m0.50851[0m[0m | time: 1.019s
[2K
| Adam | epoch: 013 | loss: 0.50851 - acc: 0.7510 -- iter: 032/217
[A[ATraining Step: 86  | total loss: [1m[32m0.50423[0m[0m | time: 2.058s
[2K
| Adam | epoch: 013 | loss: 0.50423 - acc: 0.7634 -- iter: 064/217
[A[ATraining Step: 87  | total loss: [1m[32m0.48037[0m[0m | time: 2.830s
[2K
| Adam | epoch: 013 | loss: 0.48037 - acc: 0.7808 -- iter: 096/217
[A[ATraining Step: 88  | total loss: [1m[32m0.49079[0m[0m | time: 3.635s
[2K
| Adam | epoch: 013 | loss: 0.49079 - acc: 0.7787 -- iter: 128/217
[A[ATraining Step: 89  | total loss: [1m[32m0.49552[0m[0m | time: 4.636s
[2K
| Adam | epoch: 013 | loss: 0.49552 - acc: 0.7808 -- iter: 160/217
[A[ATraining Step: 90  | total loss: [1m[32m0.48477[0m[0m | time: 5.605s
[2K
| Adam | epoch: 013 | loss: 0.48477 - acc: 0.7871 -- iter: 192/217
[A[ATraining Step: 91  | total loss: [1m[32m0.47103[0m[0m | time: 7.571s
[2K
| Adam | epoch: 013 | loss: 0.47103 - acc: 0.7959 | val_loss: 0.48732 - val_acc: 0.8261 -- iter: 217/217
--
Training Step: 92  | total loss: [1m[32m0.45118[0m[0m | time: 1.050s
[2K
| Adam | epoch: 014 | loss: 0.45118 - acc: 0.8069 -- iter: 032/217
[A[ATraining Step: 93  | total loss: [1m[32m0.43329[0m[0m | time: 2.067s
[2K
| Adam | epoch: 014 | loss: 0.43329 - acc: 0.8200 -- iter: 064/217
[A[ATraining Step: 94  | total loss: [1m[32m0.41554[0m[0m | time: 3.072s
[2K
| Adam | epoch: 014 | loss: 0.41554 - acc: 0.8286 -- iter: 096/217
[A[ATraining Step: 95  | total loss: [1m[32m0.40778[0m[0m | time: 3.891s
[2K
| Adam | epoch: 014 | loss: 0.40778 - acc: 0.8301 -- iter: 128/217
[A[ATraining Step: 96  | total loss: [1m[32m0.40611[0m[0m | time: 4.682s
[2K
| Adam | epoch: 014 | loss: 0.40611 - acc: 0.8311 -- iter: 160/217
[A[ATraining Step: 97  | total loss: [1m[32m0.38533[0m[0m | time: 5.719s
[2K
| Adam | epoch: 014 | loss: 0.38533 - acc: 0.8440 -- iter: 192/217
[A[ATraining Step: 98  | total loss: [1m[32m0.37479[0m[0m | time: 7.684s
[2K
| Adam | epoch: 014 | loss: 0.37479 - acc: 0.8502 | val_loss: 0.55910 - val_acc: 0.7101 -- iter: 217/217
--
Training Step: 99  | total loss: [1m[32m0.36413[0m[0m | time: 0.619s
[2K
| Adam | epoch: 015 | loss: 0.36413 - acc: 0.8558 -- iter: 032/217
[A[ATraining Step: 100  | total loss: [1m[32m0.39501[0m[0m | time: 1.271s
[2K
| Adam | epoch: 015 | loss: 0.39501 - acc: 0.8390 -- iter: 064/217
[A[ATraining Step: 101  | total loss: [1m[32m0.43261[0m[0m | time: 1.923s
[2K
| Adam | epoch: 015 | loss: 0.43261 - acc: 0.8114 -- iter: 096/217
[A[ATraining Step: 102  | total loss: [1m[32m0.41660[0m[0m | time: 2.710s
[2K
| Adam | epoch: 015 | loss: 0.41660 - acc: 0.8146 -- iter: 128/217
[A[ATraining Step: 103  | total loss: [1m[32m0.38854[0m[0m | time: 3.361s
[2K
| Adam | epoch: 015 | loss: 0.38854 - acc: 0.8269 -- iter: 160/217
[A[ATraining Step: 104  | total loss: [1m[32m0.36326[0m[0m | time: 3.993s
[2K
| Adam | epoch: 015 | loss: 0.36326 - acc: 0.8402 -- iter: 192/217
[A[ATraining Step: 105  | total loss: [1m[32m0.34879[0m[0m | time: 5.760s
[2K
| Adam | epoch: 015 | loss: 0.34879 - acc: 0.8442 | val_loss: 0.42855 - val_acc: 0.8406 -- iter: 217/217
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.9087837837837838
Validation AUPRC:0.9023857282382535
Test AUC:0.9120982986767485
Test AUPRC:0.9441167509949296
BestTestF1Score	0.91	0.7	0.87	0.85	0.98	45	8	15	1	0.1
BestTestMCCScore	0.91	0.7	0.87	0.85	0.98	45	8	15	1	0.1
BestTestAccuracyScore	0.78	0.58	0.75	0.97	0.65	30	1	22	16	0.56
BestValidationF1Score	0.87	0.7	0.84	0.78	0.97	36	10	22	1	0.1
BestValidationMCC	0.87	0.7	0.84	0.78	0.97	36	10	22	1	0.1
BestValidationAccuracy	0.85	0.68	0.84	0.88	0.81	30	4	28	7	0.56
TestPredictions (Threshold:0.1)
CHEMBL1834184,TP,ACT,0.4000000059604645	CHEMBL200559,TN,INACT,0.029999999329447746	CHEMBL3661771,TP,ACT,0.9900000095367432	CHEMBL48122,TN,INACT,0.03999999910593033	CHEMBL3646810,TP,ACT,0.7900000214576721	CHEMBL3639605,TP,ACT,0.6200000047683716	CHEMBL3666548,TP,ACT,0.8100000023841858	CHEMBL3666673,TP,ACT,0.8999999761581421	CHEMBL3661674,TP,ACT,0.949999988079071	CHEMBL45844,TN,INACT,0.019999999552965164	CHEMBL3661810,TP,ACT,0.5	CHEMBL3661644,TP,ACT,0.5799999833106995	CHEMBL3666640,TP,ACT,0.9800000190734863	CHEMBL3661829,TP,ACT,0.8700000047683716	CHEMBL3661770,TP,ACT,0.9800000190734863	CHEMBL1257642,FP,INACT,0.10000000149011612	CHEMBL3661811,TP,ACT,0.38999998569488525	CHEMBL3657588,TP,ACT,0.28999999165534973	CHEMBL2322368,TP,ACT,0.44999998807907104	CHEMBL3666626,TP,ACT,0.17000000178813934	CHEMBL49774,TN,INACT,0.029999999329447746	CHEMBL3657587,TP,ACT,0.9900000095367432	CHEMBL1171546,TN,INACT,0.029999999329447746	CHEMBL3666525,TP,ACT,0.9399999976158142	CHEMBL2322364,TP,ACT,0.5799999833106995	CHEMBL3661658,FP,INACT,0.3400000035762787	CHEMBL176934,TN,INACT,0.019999999552965164	CHEMBL3666538,TP,ACT,0.9200000166893005	CHEMBL174586,TN,INACT,0.009999999776482582	CHEMBL3228380,TN,INACT,0.019999999552965164	CHEMBL3661665,TP,ACT,0.9800000190734863	CHEMBL3661702,TP,ACT,0.5199999809265137	CHEMBL3666546,TP,ACT,0.11999999731779099	CHEMBL3623463,TP,ACT,0.800000011920929	CHEMBL3666643,TP,ACT,0.4099999964237213	CHEMBL517501,FP,INACT,0.3199999928474426	CHEMBL3666637,TP,ACT,0.15000000596046448	CHEMBL3661687,TP,ACT,0.9200000166893005	CHEMBL251750,TN,INACT,0.019999999552965164	CHEMBL1834191,TP,ACT,0.11999999731779099	CHEMBL3666636,TP,ACT,0.9900000095367432	CHEMBL3661801,FP,INACT,0.9700000286102295	CHEMBL3666572,FN,ACT,0.09000000357627869	CHEMBL3666641,TP,ACT,0.9100000262260437	CHEMBL3666532,TP,ACT,0.9700000286102295	CHEMBL3661673,TP,ACT,0.20000000298023224	CHEMBL1834198,TP,ACT,0.11999999731779099	CHEMBL28636,TN,INACT,0.019999999552965164	CHEMBL3666566,FP,INACT,0.4300000071525574	CHEMBL2322369,TP,ACT,0.9800000190734863	CHEMBL478313,TN,INACT,0.019999999552965164	CHEMBL86153,FP,INACT,0.10000000149011612	CHEMBL3661686,TP,ACT,0.44999998807907104	CHEMBL3661769,TP,ACT,0.9900000095367432	CHEMBL1502,FP,INACT,0.550000011920929	CHEMBL3666657,TP,ACT,0.9900000095367432	CHEMBL3666474,TP,ACT,0.9800000190734863	CHEMBL3706850,TN,INACT,0.03999999910593033	CHEMBL215910,TP,ACT,0.18000000715255737	CHEMBL3661722,TP,ACT,0.75	CHEMBL3661754,TP,ACT,0.9800000190734863	CHEMBL3666668,TP,ACT,0.8500000238418579	CHEMBL433898,TN,INACT,0.019999999552965164	CHEMBL3646803,TP,ACT,0.699999988079071	CHEMBL45466,TN,INACT,0.03999999910593033	CHEMBL3666685,TP,ACT,0.8600000143051147	CHEMBL380595,FP,INACT,0.38999998569488525	CHEMBL3661757,TP,ACT,0.8299999833106995	CHEMBL315053,TN,INACT,0.029999999329447746	

