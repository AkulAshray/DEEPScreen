CNNModel CHEMBL5855 adam 0.0001 15 32 0 0.6 False True
Number of active compounds :	240
Number of inactive compounds :	240
---------------------------------
Run id: CNNModel_CHEMBL5855_adam_0.0001_15_32_0_0.6_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL5855_adam_0.0001_15_32_0.6_True/
---------------------------------
Training samples: 305
Validation samples: 96
--
Training Step: 1  | time: 1.615s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/305
[A[ATraining Step: 2  | total loss: [1m[32m0.62363[0m[0m | time: 3.707s
[2K
| Adam | epoch: 001 | loss: 0.62363 - acc: 0.4500 -- iter: 064/305
[A[ATraining Step: 3  | total loss: [1m[32m0.68022[0m[0m | time: 4.650s
[2K
| Adam | epoch: 001 | loss: 0.68022 - acc: 0.5420 -- iter: 096/305
[A[ATraining Step: 4  | total loss: [1m[32m0.68951[0m[0m | time: 5.673s
[2K
| Adam | epoch: 001 | loss: 0.68951 - acc: 0.5339 -- iter: 128/305
[A[ATraining Step: 5  | total loss: [1m[32m0.69264[0m[0m | time: 6.594s
[2K
| Adam | epoch: 001 | loss: 0.69264 - acc: 0.4888 -- iter: 160/305
[A[ATraining Step: 6  | total loss: [1m[32m0.69454[0m[0m | time: 7.660s
[2K
| Adam | epoch: 001 | loss: 0.69454 - acc: 0.4357 -- iter: 192/305
[A[ATraining Step: 7  | total loss: [1m[32m0.69470[0m[0m | time: 8.672s
[2K
| Adam | epoch: 001 | loss: 0.69470 - acc: 0.4368 -- iter: 224/305
[A[ATraining Step: 8  | total loss: [1m[32m0.69400[0m[0m | time: 9.481s
[2K
| Adam | epoch: 001 | loss: 0.69400 - acc: 0.4723 -- iter: 256/305
[A[ATraining Step: 9  | total loss: [1m[32m0.69503[0m[0m | time: 10.592s
[2K
| Adam | epoch: 001 | loss: 0.69503 - acc: 0.4043 -- iter: 288/305
[A[ATraining Step: 10  | total loss: [1m[32m0.69426[0m[0m | time: 12.481s
[2K
| Adam | epoch: 001 | loss: 0.69426 - acc: 0.4521 | val_loss: 0.69279 - val_acc: 0.5312 -- iter: 305/305
--
Training Step: 11  | total loss: [1m[32m0.69334[0m[0m | time: 0.547s
[2K
| Adam | epoch: 002 | loss: 0.69334 - acc: 0.4887 -- iter: 032/305
[A[ATraining Step: 12  | total loss: [1m[32m0.69305[0m[0m | time: 1.501s
[2K
| Adam | epoch: 002 | loss: 0.69305 - acc: 0.5335 -- iter: 064/305
[A[ATraining Step: 13  | total loss: [1m[32m0.69352[0m[0m | time: 2.513s
[2K
| Adam | epoch: 002 | loss: 0.69352 - acc: 0.4924 -- iter: 096/305
[A[ATraining Step: 14  | total loss: [1m[32m0.69298[0m[0m | time: 3.469s
[2K
| Adam | epoch: 002 | loss: 0.69298 - acc: 0.5083 -- iter: 128/305
[A[ATraining Step: 15  | total loss: [1m[32m0.69290[0m[0m | time: 4.294s
[2K
| Adam | epoch: 002 | loss: 0.69290 - acc: 0.5173 -- iter: 160/305
[A[ATraining Step: 16  | total loss: [1m[32m0.69346[0m[0m | time: 5.360s
[2K
| Adam | epoch: 002 | loss: 0.69346 - acc: 0.4522 -- iter: 192/305
[A[ATraining Step: 17  | total loss: [1m[32m0.69335[0m[0m | time: 6.736s
[2K
| Adam | epoch: 002 | loss: 0.69335 - acc: 0.4582 -- iter: 224/305
[A[ATraining Step: 18  | total loss: [1m[32m0.69332[0m[0m | time: 7.879s
[2K
| Adam | epoch: 002 | loss: 0.69332 - acc: 0.4726 -- iter: 256/305
[A[ATraining Step: 19  | total loss: [1m[32m0.69299[0m[0m | time: 8.690s
[2K
| Adam | epoch: 002 | loss: 0.69299 - acc: 0.5026 -- iter: 288/305
[A[ATraining Step: 20  | total loss: [1m[32m0.69283[0m[0m | time: 10.658s
[2K
| Adam | epoch: 002 | loss: 0.69283 - acc: 0.5218 | val_loss: 0.69282 - val_acc: 0.5312 -- iter: 305/305
--
Training Step: 21  | total loss: [1m[32m0.69292[0m[0m | time: 0.452s
[2K
| Adam | epoch: 003 | loss: 0.69292 - acc: 0.5248 -- iter: 032/305
[A[ATraining Step: 22  | total loss: [1m[32m0.69291[0m[0m | time: 0.993s
[2K
| Adam | epoch: 003 | loss: 0.69291 - acc: 0.5438 -- iter: 064/305
[A[ATraining Step: 23  | total loss: [1m[32m0.69277[0m[0m | time: 2.079s
[2K
| Adam | epoch: 003 | loss: 0.69277 - acc: 0.5738 -- iter: 096/305
[A[ATraining Step: 24  | total loss: [1m[32m0.69296[0m[0m | time: 3.419s
[2K
| Adam | epoch: 003 | loss: 0.69296 - acc: 0.5355 -- iter: 128/305
[A[ATraining Step: 25  | total loss: [1m[32m0.69307[0m[0m | time: 4.450s
[2K
| Adam | epoch: 003 | loss: 0.69307 - acc: 0.5002 -- iter: 160/305
[A[ATraining Step: 26  | total loss: [1m[32m0.69290[0m[0m | time: 5.308s
[2K
| Adam | epoch: 003 | loss: 0.69290 - acc: 0.5332 -- iter: 192/305
[A[ATraining Step: 27  | total loss: [1m[32m0.69276[0m[0m | time: 6.271s
[2K
| Adam | epoch: 003 | loss: 0.69276 - acc: 0.5408 -- iter: 224/305
[A[ATraining Step: 28  | total loss: [1m[32m0.69296[0m[0m | time: 7.221s
[2K
| Adam | epoch: 003 | loss: 0.69296 - acc: 0.5071 -- iter: 256/305
[A[ATraining Step: 29  | total loss: [1m[32m0.69294[0m[0m | time: 8.159s
[2K
| Adam | epoch: 003 | loss: 0.69294 - acc: 0.5206 -- iter: 288/305
[A[ATraining Step: 30  | total loss: [1m[32m0.69303[0m[0m | time: 10.204s
[2K
| Adam | epoch: 003 | loss: 0.69303 - acc: 0.4861 | val_loss: 0.69251 - val_acc: 0.5312 -- iter: 305/305
--
Training Step: 31  | total loss: [1m[32m0.69300[0m[0m | time: 0.898s
[2K
| Adam | epoch: 004 | loss: 0.69300 - acc: 0.4749 -- iter: 032/305
[A[ATraining Step: 32  | total loss: [1m[32m0.69284[0m[0m | time: 1.448s
[2K
| Adam | epoch: 004 | loss: 0.69284 - acc: 0.5016 -- iter: 064/305
[A[ATraining Step: 33  | total loss: [1m[32m0.69270[0m[0m | time: 2.035s
[2K
| Adam | epoch: 004 | loss: 0.69270 - acc: 0.5206 -- iter: 096/305
[A[ATraining Step: 34  | total loss: [1m[32m0.69255[0m[0m | time: 2.914s
[2K
| Adam | epoch: 004 | loss: 0.69255 - acc: 0.5477 -- iter: 128/305
[A[ATraining Step: 35  | total loss: [1m[32m0.69251[0m[0m | time: 3.557s
[2K
| Adam | epoch: 004 | loss: 0.69251 - acc: 0.5508 -- iter: 160/305
[A[ATraining Step: 36  | total loss: [1m[32m0.69239[0m[0m | time: 4.226s
[2K
| Adam | epoch: 004 | loss: 0.69239 - acc: 0.5596 -- iter: 192/305
[A[ATraining Step: 37  | total loss: [1m[32m0.69268[0m[0m | time: 4.828s
[2K
| Adam | epoch: 004 | loss: 0.69268 - acc: 0.5289 -- iter: 224/305
[A[ATraining Step: 38  | total loss: [1m[32m0.69237[0m[0m | time: 5.472s
[2K
| Adam | epoch: 004 | loss: 0.69237 - acc: 0.5416 -- iter: 256/305
[A[ATraining Step: 39  | total loss: [1m[32m0.69221[0m[0m | time: 6.097s
[2K
| Adam | epoch: 004 | loss: 0.69221 - acc: 0.5456 -- iter: 288/305
[A[ATraining Step: 40  | total loss: [1m[32m0.69254[0m[0m | time: 7.751s
[2K
| Adam | epoch: 004 | loss: 0.69254 - acc: 0.5253 | val_loss: 0.69146 - val_acc: 0.5312 -- iter: 305/305
--
Training Step: 41  | total loss: [1m[32m0.69256[0m[0m | time: 1.066s
[2K
| Adam | epoch: 005 | loss: 0.69256 - acc: 0.5149 -- iter: 032/305
[A[ATraining Step: 42  | total loss: [1m[32m0.69308[0m[0m | time: 2.053s
[2K
| Adam | epoch: 005 | loss: 0.69308 - acc: 0.4898 -- iter: 064/305
[A[ATraining Step: 43  | total loss: [1m[32m0.69334[0m[0m | time: 2.610s
[2K
| Adam | epoch: 005 | loss: 0.69334 - acc: 0.4750 -- iter: 096/305
[A[ATraining Step: 44  | total loss: [1m[32m0.69277[0m[0m | time: 3.211s
[2K
| Adam | epoch: 005 | loss: 0.69277 - acc: 0.4946 -- iter: 128/305
[A[ATraining Step: 45  | total loss: [1m[32m0.69293[0m[0m | time: 4.223s
[2K
| Adam | epoch: 005 | loss: 0.69293 - acc: 0.4805 -- iter: 160/305
[A[ATraining Step: 46  | total loss: [1m[32m0.69271[0m[0m | time: 5.261s
[2K
| Adam | epoch: 005 | loss: 0.69271 - acc: 0.4838 -- iter: 192/305
[A[ATraining Step: 47  | total loss: [1m[32m0.69221[0m[0m | time: 6.282s
[2K
| Adam | epoch: 005 | loss: 0.69221 - acc: 0.5120 -- iter: 224/305
[A[ATraining Step: 48  | total loss: [1m[32m0.69224[0m[0m | time: 7.324s
[2K
| Adam | epoch: 005 | loss: 0.69224 - acc: 0.5051 -- iter: 256/305
[A[ATraining Step: 49  | total loss: [1m[32m0.69214[0m[0m | time: 8.261s
[2K
| Adam | epoch: 005 | loss: 0.69214 - acc: 0.5043 -- iter: 288/305
[A[ATraining Step: 50  | total loss: [1m[32m0.69214[0m[0m | time: 10.295s
[2K
| Adam | epoch: 005 | loss: 0.69214 - acc: 0.4987 | val_loss: 0.69042 - val_acc: 0.5312 -- iter: 305/305
--
Training Step: 51  | total loss: [1m[32m0.69210[0m[0m | time: 1.009s
[2K
| Adam | epoch: 006 | loss: 0.69210 - acc: 0.4942 -- iter: 032/305
[A[ATraining Step: 52  | total loss: [1m[32m0.69187[0m[0m | time: 1.986s
[2K
| Adam | epoch: 006 | loss: 0.69187 - acc: 0.4950 -- iter: 064/305
[A[ATraining Step: 53  | total loss: [1m[32m0.69175[0m[0m | time: 2.978s
[2K
| Adam | epoch: 006 | loss: 0.69175 - acc: 0.4912 -- iter: 096/305
[A[ATraining Step: 54  | total loss: [1m[32m0.69162[0m[0m | time: 3.636s
[2K
| Adam | epoch: 006 | loss: 0.69162 - acc: 0.4924 -- iter: 128/305
[A[ATraining Step: 55  | total loss: [1m[32m0.69157[0m[0m | time: 4.226s
[2K
| Adam | epoch: 006 | loss: 0.69157 - acc: 0.5061 -- iter: 160/305
[A[ATraining Step: 56  | total loss: [1m[32m0.69156[0m[0m | time: 5.257s
[2K
| Adam | epoch: 006 | loss: 0.69156 - acc: 0.5260 -- iter: 192/305
[A[ATraining Step: 57  | total loss: [1m[32m0.69138[0m[0m | time: 6.261s
[2K
| Adam | epoch: 006 | loss: 0.69138 - acc: 0.5700 -- iter: 224/305
[A[ATraining Step: 58  | total loss: [1m[32m0.69129[0m[0m | time: 7.271s
[2K
| Adam | epoch: 006 | loss: 0.69129 - acc: 0.6030 -- iter: 256/305
[A[ATraining Step: 59  | total loss: [1m[32m0.69107[0m[0m | time: 8.266s
[2K
| Adam | epoch: 006 | loss: 0.69107 - acc: 0.6396 -- iter: 288/305
[A[ATraining Step: 60  | total loss: [1m[32m0.69093[0m[0m | time: 10.336s
[2K
| Adam | epoch: 006 | loss: 0.69093 - acc: 0.6625 | val_loss: 0.68756 - val_acc: 0.9375 -- iter: 305/305
--
Training Step: 61  | total loss: [1m[32m0.69063[0m[0m | time: 1.074s
[2K
| Adam | epoch: 007 | loss: 0.69063 - acc: 0.6943 -- iter: 032/305
[A[ATraining Step: 62  | total loss: [1m[32m0.69021[0m[0m | time: 2.093s
[2K
| Adam | epoch: 007 | loss: 0.69021 - acc: 0.7255 -- iter: 064/305
[A[ATraining Step: 63  | total loss: [1m[32m0.69014[0m[0m | time: 3.074s
[2K
| Adam | epoch: 007 | loss: 0.69014 - acc: 0.7247 -- iter: 096/305
[A[ATraining Step: 64  | total loss: [1m[32m0.68959[0m[0m | time: 4.110s
[2K
| Adam | epoch: 007 | loss: 0.68959 - acc: 0.7200 -- iter: 128/305
[A[ATraining Step: 65  | total loss: [1m[32m0.68933[0m[0m | time: 4.688s
[2K
| Adam | epoch: 007 | loss: 0.68933 - acc: 0.6967 -- iter: 160/305
[A[ATraining Step: 66  | total loss: [1m[32m0.68956[0m[0m | time: 5.235s
[2K
| Adam | epoch: 007 | loss: 0.68956 - acc: 0.6621 -- iter: 192/305
[A[ATraining Step: 67  | total loss: [1m[32m0.68959[0m[0m | time: 6.280s
[2K
| Adam | epoch: 007 | loss: 0.68959 - acc: 0.6391 -- iter: 224/305
[A[ATraining Step: 68  | total loss: [1m[32m0.68874[0m[0m | time: 7.290s
[2K
| Adam | epoch: 007 | loss: 0.68874 - acc: 0.6707 -- iter: 256/305
[A[ATraining Step: 69  | total loss: [1m[32m0.68834[0m[0m | time: 8.296s
[2K
| Adam | epoch: 007 | loss: 0.68834 - acc: 0.6946 -- iter: 288/305
[A[ATraining Step: 70  | total loss: [1m[32m0.68791[0m[0m | time: 10.383s
[2K
| Adam | epoch: 007 | loss: 0.68791 - acc: 0.7082 | val_loss: 0.68036 - val_acc: 0.8750 -- iter: 305/305
--
Training Step: 71  | total loss: [1m[32m0.68685[0m[0m | time: 1.010s
[2K
| Adam | epoch: 008 | loss: 0.68685 - acc: 0.7415 -- iter: 032/305
[A[ATraining Step: 72  | total loss: [1m[32m0.68624[0m[0m | time: 1.993s
[2K
| Adam | epoch: 008 | loss: 0.68624 - acc: 0.7530 -- iter: 064/305
[A[ATraining Step: 73  | total loss: [1m[32m0.68600[0m[0m | time: 3.024s
[2K
| Adam | epoch: 008 | loss: 0.68600 - acc: 0.7457 -- iter: 096/305
[A[ATraining Step: 74  | total loss: [1m[32m0.68503[0m[0m | time: 4.002s
[2K
| Adam | epoch: 008 | loss: 0.68503 - acc: 0.7599 -- iter: 128/305
[A[ATraining Step: 75  | total loss: [1m[32m0.68422[0m[0m | time: 5.000s
[2K
| Adam | epoch: 008 | loss: 0.68422 - acc: 0.7656 -- iter: 160/305
[A[ATraining Step: 76  | total loss: [1m[32m0.68420[0m[0m | time: 5.563s
[2K
| Adam | epoch: 008 | loss: 0.68420 - acc: 0.7706 -- iter: 192/305
[A[ATraining Step: 77  | total loss: [1m[32m0.68386[0m[0m | time: 6.177s
[2K
| Adam | epoch: 008 | loss: 0.68386 - acc: 0.7700 -- iter: 224/305
[A[ATraining Step: 78  | total loss: [1m[32m0.68353[0m[0m | time: 7.176s
[2K
| Adam | epoch: 008 | loss: 0.68353 - acc: 0.7633 -- iter: 256/305
[A[ATraining Step: 79  | total loss: [1m[32m0.68216[0m[0m | time: 8.196s
[2K
| Adam | epoch: 008 | loss: 0.68216 - acc: 0.7716 -- iter: 288/305
[A[ATraining Step: 80  | total loss: [1m[32m0.68114[0m[0m | time: 10.241s
[2K
| Adam | epoch: 008 | loss: 0.68114 - acc: 0.7726 | val_loss: 0.66408 - val_acc: 0.8125 -- iter: 305/305
--
Training Step: 81  | total loss: [1m[32m0.68001[0m[0m | time: 0.606s
[2K
| Adam | epoch: 009 | loss: 0.68001 - acc: 0.7735 -- iter: 032/305
[A[ATraining Step: 82  | total loss: [1m[32m0.67894[0m[0m | time: 1.220s
[2K
| Adam | epoch: 009 | loss: 0.67894 - acc: 0.7742 -- iter: 064/305
[A[ATraining Step: 83  | total loss: [1m[32m0.67776[0m[0m | time: 1.830s
[2K
| Adam | epoch: 009 | loss: 0.67776 - acc: 0.7812 -- iter: 096/305
[A[ATraining Step: 84  | total loss: [1m[32m0.67542[0m[0m | time: 2.461s
[2K
| Adam | epoch: 009 | loss: 0.67542 - acc: 0.7968 -- iter: 128/305
[A[ATraining Step: 85  | total loss: [1m[32m0.67190[0m[0m | time: 3.061s
[2K
| Adam | epoch: 009 | loss: 0.67190 - acc: 0.7765 -- iter: 160/305
[A[ATraining Step: 86  | total loss: [1m[32m0.67289[0m[0m | time: 3.673s
[2K
| Adam | epoch: 009 | loss: 0.67289 - acc: 0.7395 -- iter: 192/305
[A[ATraining Step: 87  | total loss: [1m[32m0.67410[0m[0m | time: 4.018s
[2K
| Adam | epoch: 009 | loss: 0.67410 - acc: 0.7062 -- iter: 224/305
[A[ATraining Step: 88  | total loss: [1m[32m0.67459[0m[0m | time: 4.360s
[2K
| Adam | epoch: 009 | loss: 0.67459 - acc: 0.6767 -- iter: 256/305
[A[ATraining Step: 89  | total loss: [1m[32m0.67380[0m[0m | time: 5.008s
[2K
| Adam | epoch: 009 | loss: 0.67380 - acc: 0.6796 -- iter: 288/305
[A[ATraining Step: 90  | total loss: [1m[32m0.67005[0m[0m | time: 6.616s
[2K
| Adam | epoch: 009 | loss: 0.67005 - acc: 0.6992 | val_loss: 0.65605 - val_acc: 0.5208 -- iter: 305/305
--
Training Step: 91  | total loss: [1m[32m0.66844[0m[0m | time: 0.613s
[2K
| Adam | epoch: 010 | loss: 0.66844 - acc: 0.7011 -- iter: 032/305
[A[ATraining Step: 92  | total loss: [1m[32m0.66838[0m[0m | time: 1.236s
[2K
| Adam | epoch: 010 | loss: 0.66838 - acc: 0.6810 -- iter: 064/305
[A[ATraining Step: 93  | total loss: [1m[32m0.67026[0m[0m | time: 1.852s
[2K
| Adam | epoch: 010 | loss: 0.67026 - acc: 0.6692 -- iter: 096/305
[A[ATraining Step: 94  | total loss: [1m[32m0.66858[0m[0m | time: 2.462s
[2K
| Adam | epoch: 010 | loss: 0.66858 - acc: 0.6585 -- iter: 128/305
[A[ATraining Step: 95  | total loss: [1m[32m0.66500[0m[0m | time: 3.078s
[2K
| Adam | epoch: 010 | loss: 0.66500 - acc: 0.6677 -- iter: 160/305
[A[ATraining Step: 96  | total loss: [1m[32m0.66071[0m[0m | time: 3.695s
[2K
| Adam | epoch: 010 | loss: 0.66071 - acc: 0.6884 -- iter: 192/305
[A[ATraining Step: 97  | total loss: [1m[32m0.65484[0m[0m | time: 4.295s
[2K
| Adam | epoch: 010 | loss: 0.65484 - acc: 0.7070 -- iter: 224/305
[A[ATraining Step: 98  | total loss: [1m[32m0.65146[0m[0m | time: 4.655s
[2K
| Adam | epoch: 010 | loss: 0.65146 - acc: 0.7301 -- iter: 256/305
[A[ATraining Step: 99  | total loss: [1m[32m0.64877[0m[0m | time: 5.005s
[2K
| Adam | epoch: 010 | loss: 0.64877 - acc: 0.7336 -- iter: 288/305
[A[ATraining Step: 100  | total loss: [1m[32m0.64736[0m[0m | time: 6.707s
[2K
| Adam | epoch: 010 | loss: 0.64736 - acc: 0.7367 | val_loss: 0.57681 - val_acc: 0.8958 -- iter: 305/305
--
Training Step: 101  | total loss: [1m[32m0.64127[0m[0m | time: 0.634s
[2K
| Adam | epoch: 011 | loss: 0.64127 - acc: 0.7568 -- iter: 032/305
[A[ATraining Step: 102  | total loss: [1m[32m0.63488[0m[0m | time: 1.237s
[2K
| Adam | epoch: 011 | loss: 0.63488 - acc: 0.7780 -- iter: 064/305
[A[ATraining Step: 103  | total loss: [1m[32m0.63078[0m[0m | time: 1.852s
[2K
| Adam | epoch: 011 | loss: 0.63078 - acc: 0.7752 -- iter: 096/305
[A[ATraining Step: 104  | total loss: [1m[32m0.62332[0m[0m | time: 2.473s
[2K
| Adam | epoch: 011 | loss: 0.62332 - acc: 0.7789 -- iter: 128/305
[A[ATraining Step: 105  | total loss: [1m[32m0.61778[0m[0m | time: 3.163s
[2K
| Adam | epoch: 011 | loss: 0.61778 - acc: 0.7823 -- iter: 160/305
[A[ATraining Step: 106  | total loss: [1m[32m0.61514[0m[0m | time: 3.791s
[2K
| Adam | epoch: 011 | loss: 0.61514 - acc: 0.7759 -- iter: 192/305
[A[ATraining Step: 107  | total loss: [1m[32m0.61571[0m[0m | time: 4.409s
[2K
| Adam | epoch: 011 | loss: 0.61571 - acc: 0.7733 -- iter: 224/305
[A[ATraining Step: 108  | total loss: [1m[32m0.60676[0m[0m | time: 5.017s
[2K
| Adam | epoch: 011 | loss: 0.60676 - acc: 0.7804 -- iter: 256/305
[A[ATraining Step: 109  | total loss: [1m[32m0.60405[0m[0m | time: 5.361s
[2K
| Adam | epoch: 011 | loss: 0.60405 - acc: 0.7742 -- iter: 288/305
[A[ATraining Step: 110  | total loss: [1m[32m0.59247[0m[0m | time: 6.733s
[2K
| Adam | epoch: 011 | loss: 0.59247 - acc: 0.7909 | val_loss: 0.49643 - val_acc: 0.8542 -- iter: 305/305
--
Training Step: 111  | total loss: [1m[32m0.58347[0m[0m | time: 1.069s
[2K
| Adam | epoch: 012 | loss: 0.58347 - acc: 0.8000 -- iter: 032/305
[A[ATraining Step: 112  | total loss: [1m[32m0.57058[0m[0m | time: 1.835s
[2K
| Adam | epoch: 012 | loss: 0.57058 - acc: 0.8107 -- iter: 064/305
[A[ATraining Step: 113  | total loss: [1m[32m0.56649[0m[0m | time: 2.688s
[2K
| Adam | epoch: 012 | loss: 0.56649 - acc: 0.8108 -- iter: 096/305
[A[ATraining Step: 114  | total loss: [1m[32m0.56024[0m[0m | time: 3.567s
[2K
| Adam | epoch: 012 | loss: 0.56024 - acc: 0.8110 -- iter: 128/305
[A[ATraining Step: 115  | total loss: [1m[32m0.55807[0m[0m | time: 4.632s
[2K
| Adam | epoch: 012 | loss: 0.55807 - acc: 0.8143 -- iter: 160/305
[A[ATraining Step: 116  | total loss: [1m[32m0.54946[0m[0m | time: 5.546s
[2K
| Adam | epoch: 012 | loss: 0.54946 - acc: 0.8172 -- iter: 192/305
[A[ATraining Step: 117  | total loss: [1m[32m0.54117[0m[0m | time: 6.636s
[2K
| Adam | epoch: 012 | loss: 0.54117 - acc: 0.8168 -- iter: 224/305
[A[ATraining Step: 118  | total loss: [1m[32m0.53790[0m[0m | time: 7.950s
[2K
| Adam | epoch: 012 | loss: 0.53790 - acc: 0.8070 -- iter: 256/305
[A[ATraining Step: 119  | total loss: [1m[32m0.54006[0m[0m | time: 8.943s
[2K
| Adam | epoch: 012 | loss: 0.54006 - acc: 0.7981 -- iter: 288/305
[A[ATraining Step: 120  | total loss: [1m[32m0.52995[0m[0m | time: 10.475s
[2K
| Adam | epoch: 012 | loss: 0.52995 - acc: 0.7996 | val_loss: 0.35656 - val_acc: 0.9167 -- iter: 305/305
--
Training Step: 121  | total loss: [1m[32m0.51738[0m[0m | time: 0.541s
[2K
| Adam | epoch: 013 | loss: 0.51738 - acc: 0.8079 -- iter: 032/305
[A[ATraining Step: 122  | total loss: [1m[32m0.52105[0m[0m | time: 1.578s
[2K
| Adam | epoch: 013 | loss: 0.52105 - acc: 0.7977 -- iter: 064/305
[A[ATraining Step: 123  | total loss: [1m[32m0.52004[0m[0m | time: 2.535s
[2K
| Adam | epoch: 013 | loss: 0.52004 - acc: 0.7991 -- iter: 096/305
[A[ATraining Step: 124  | total loss: [1m[32m0.50915[0m[0m | time: 3.395s
[2K
| Adam | epoch: 013 | loss: 0.50915 - acc: 0.8098 -- iter: 128/305
[A[ATraining Step: 125  | total loss: [1m[32m0.49805[0m[0m | time: 4.502s
[2K
| Adam | epoch: 013 | loss: 0.49805 - acc: 0.8195 -- iter: 160/305
[A[ATraining Step: 126  | total loss: [1m[32m0.48961[0m[0m | time: 5.708s
[2K
| Adam | epoch: 013 | loss: 0.48961 - acc: 0.8313 -- iter: 192/305
[A[ATraining Step: 127  | total loss: [1m[32m0.48464[0m[0m | time: 6.857s
[2K
| Adam | epoch: 013 | loss: 0.48464 - acc: 0.8357 -- iter: 224/305
[A[ATraining Step: 128  | total loss: [1m[32m0.47696[0m[0m | time: 7.917s
[2K
| Adam | epoch: 013 | loss: 0.47696 - acc: 0.8302 -- iter: 256/305
[A[ATraining Step: 129  | total loss: [1m[32m0.45737[0m[0m | time: 8.882s
[2K
| Adam | epoch: 013 | loss: 0.45737 - acc: 0.8409 -- iter: 288/305
[A[ATraining Step: 130  | total loss: [1m[32m0.43868[0m[0m | time: 10.815s
[2K
| Adam | epoch: 013 | loss: 0.43868 - acc: 0.8537 | val_loss: 0.28388 - val_acc: 0.9062 -- iter: 305/305
--
Training Step: 131  | total loss: [1m[32m0.42564[0m[0m | time: 0.558s
[2K
| Adam | epoch: 014 | loss: 0.42564 - acc: 0.8590 -- iter: 032/305
[A[ATraining Step: 132  | total loss: [1m[32m0.42699[0m[0m | time: 1.171s
[2K
| Adam | epoch: 014 | loss: 0.42699 - acc: 0.8437 -- iter: 064/305
[A[ATraining Step: 133  | total loss: [1m[32m0.42225[0m[0m | time: 2.484s
[2K
| Adam | epoch: 014 | loss: 0.42225 - acc: 0.8358 -- iter: 096/305
[A[ATraining Step: 134  | total loss: [1m[32m0.41552[0m[0m | time: 3.622s
[2K
| Adam | epoch: 014 | loss: 0.41552 - acc: 0.8428 -- iter: 128/305
[A[ATraining Step: 135  | total loss: [1m[32m0.40449[0m[0m | time: 6.075s
[2K
| Adam | epoch: 014 | loss: 0.40449 - acc: 0.8492 -- iter: 160/305
[A[ATraining Step: 136  | total loss: [1m[32m0.40191[0m[0m | time: 6.966s
[2K
| Adam | epoch: 014 | loss: 0.40191 - acc: 0.8549 -- iter: 192/305
[A[ATraining Step: 137  | total loss: [1m[32m0.41924[0m[0m | time: 7.896s
[2K
| Adam | epoch: 014 | loss: 0.41924 - acc: 0.8475 -- iter: 224/305
[A[ATraining Step: 138  | total loss: [1m[32m0.39927[0m[0m | time: 8.880s
[2K
| Adam | epoch: 014 | loss: 0.39927 - acc: 0.8534 -- iter: 256/305
[A[ATraining Step: 139  | total loss: [1m[32m0.38029[0m[0m | time: 9.871s
[2K
| Adam | epoch: 014 | loss: 0.38029 - acc: 0.8618 -- iter: 288/305
[A[ATraining Step: 140  | total loss: [1m[32m0.39091[0m[0m | time: 11.858s
[2K
| Adam | epoch: 014 | loss: 0.39091 - acc: 0.8475 | val_loss: 0.21833 - val_acc: 0.9062 -- iter: 305/305
--
Training Step: 141  | total loss: [1m[32m0.38419[0m[0m | time: 1.015s
[2K
| Adam | epoch: 015 | loss: 0.38419 - acc: 0.8471 -- iter: 032/305
[A[ATraining Step: 142  | total loss: [1m[32m0.36433[0m[0m | time: 1.468s
[2K
| Adam | epoch: 015 | loss: 0.36433 - acc: 0.8593 -- iter: 064/305
[A[ATraining Step: 143  | total loss: [1m[32m0.35301[0m[0m | time: 1.943s
[2K
| Adam | epoch: 015 | loss: 0.35301 - acc: 0.8557 -- iter: 096/305
[A[ATraining Step: 144  | total loss: [1m[32m0.36873[0m[0m | time: 2.883s
[2K
| Adam | epoch: 015 | loss: 0.36873 - acc: 0.8584 -- iter: 128/305
[A[ATraining Step: 145  | total loss: [1m[32m0.37390[0m[0m | time: 3.828s
[2K
| Adam | epoch: 015 | loss: 0.37390 - acc: 0.8569 -- iter: 160/305
[A[ATraining Step: 146  | total loss: [1m[32m0.35963[0m[0m | time: 4.785s
[2K
| Adam | epoch: 015 | loss: 0.35963 - acc: 0.8681 -- iter: 192/305
[A[ATraining Step: 147  | total loss: [1m[32m0.35460[0m[0m | time: 5.896s
[2K
| Adam | epoch: 015 | loss: 0.35460 - acc: 0.8719 -- iter: 224/305
[A[ATraining Step: 148  | total loss: [1m[32m0.34071[0m[0m | time: 6.926s
[2K
| Adam | epoch: 015 | loss: 0.34071 - acc: 0.8753 -- iter: 256/305
[A[ATraining Step: 149  | total loss: [1m[32m0.33209[0m[0m | time: 7.756s
[2K
| Adam | epoch: 015 | loss: 0.33209 - acc: 0.8784 -- iter: 288/305
[A[ATraining Step: 150  | total loss: [1m[32m0.32034[0m[0m | time: 9.884s
[2K
| Adam | epoch: 015 | loss: 0.32034 - acc: 0.8781 | val_loss: 0.18377 - val_acc: 0.9375 -- iter: 305/305
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.9864923747276689
Validation AUPRC:0.9884886431398061
Test AUC:0.9882761615284412
Test AUPRC:0.991115571911825
BestTestF1Score	0.96	0.92	0.96	1.0	0.92	45	0	47	4	0.75
BestTestMCCScore	0.96	0.92	0.96	1.0	0.92	45	0	47	4	0.75
BestTestAccuracyScore	0.96	0.92	0.96	1.0	0.92	45	0	47	4	0.75
BestValidationF1Score	0.97	0.94	0.97	0.98	0.96	43	1	50	2	0.75
BestValidationMCC	0.97	0.94	0.97	0.98	0.96	43	1	50	2	0.75
BestValidationAccuracy	0.97	0.94	0.97	0.98	0.96	43	1	50	2	0.75
TestPredictions (Threshold:0.75)
CHEMBL3672361,TP,ACT,0.9300000071525574	CHEMBL3672241,TP,ACT,0.9200000166893005	CHEMBL498248,TN,INACT,0.3700000047683716	CHEMBL335966,TN,INACT,0.03999999910593033	CHEMBL3672274,TP,ACT,0.9399999976158142	CHEMBL3667208,TP,ACT,0.9399999976158142	CHEMBL95477,TN,INACT,0.07999999821186066	CHEMBL1910753,TN,INACT,0.18000000715255737	CHEMBL1287914,TN,INACT,0.05000000074505806	CHEMBL3609656,TN,INACT,0.4699999988079071	CHEMBL3667235,TP,ACT,0.949999988079071	CHEMBL3672237,TP,ACT,0.8999999761581421	CHEMBL559882,TN,INACT,0.05999999865889549	CHEMBL3667256,TP,ACT,0.9100000262260437	CHEMBL3672326,TP,ACT,0.9399999976158142	CHEMBL3672335,TP,ACT,0.949999988079071	CHEMBL3672259,TP,ACT,0.949999988079071	CHEMBL3672306,TP,ACT,0.9300000071525574	CHEMBL3667231,TP,ACT,0.949999988079071	CHEMBL3672244,FN,ACT,0.7300000190734863	CHEMBL3672328,TP,ACT,0.9300000071525574	CHEMBL319709,TN,INACT,0.05000000074505806	CHEMBL512658,TN,INACT,0.05000000074505806	CHEMBL3672344,TP,ACT,0.949999988079071	CHEMBL3672292,TP,ACT,0.9599999785423279	CHEMBL1287975,TN,INACT,0.05000000074505806	CHEMBL1767294,TN,INACT,0.05000000074505806	CHEMBL3667224,TP,ACT,0.949999988079071	CHEMBL549303,TN,INACT,0.05000000074505806	CHEMBL132948,TN,INACT,0.05999999865889549	CHEMBL3672278,TP,ACT,0.949999988079071	CHEMBL3672233,TP,ACT,0.949999988079071	CHEMBL2392388,TN,INACT,0.03999999910593033	CHEMBL131382,TN,INACT,0.11999999731779099	CHEMBL3672245,TP,ACT,0.8799999952316284	CHEMBL3667218,TP,ACT,0.8600000143051147	CHEMBL456113,TN,INACT,0.029999999329447746	CHEMBL1910602,TN,INACT,0.10999999940395355	CHEMBL3667229,TP,ACT,0.8100000023841858	CHEMBL1287945,TN,INACT,0.03999999910593033	CHEMBL100079,TN,INACT,0.20999999344348907	CHEMBL1784660,TN,INACT,0.47999998927116394	CHEMBL3672301,TP,ACT,0.949999988079071	CHEMBL3667240,TP,ACT,0.9399999976158142	CHEMBL3672383,TP,ACT,0.9300000071525574	CHEMBL367442,TN,INACT,0.05000000074505806	CHEMBL3672357,TP,ACT,0.949999988079071	CHEMBL1922120,TN,INACT,0.4099999964237213	CHEMBL173478,TN,INACT,0.05999999865889549	CHEMBL3667237,TP,ACT,0.9200000166893005	CHEMBL3672268,TP,ACT,0.9300000071525574	CHEMBL456378,TN,INACT,0.03999999910593033	CHEMBL3672337,TP,ACT,0.9399999976158142	CHEMBL515051,TN,INACT,0.03999999910593033	CHEMBL3672253,TP,ACT,0.949999988079071	CHEMBL133477,TN,INACT,0.07999999821186066	CHEMBL3639712,TP,ACT,0.8799999952316284	CHEMBL3672279,TP,ACT,0.949999988079071	CHEMBL3672283,TP,ACT,0.949999988079071	CHEMBL3667257,TP,ACT,0.949999988079071	CHEMBL1277019,TN,INACT,0.05000000074505806	CHEMBL1910755,TN,INACT,0.41999998688697815	CHEMBL3672238,TP,ACT,0.949999988079071	CHEMBL100670,TN,INACT,0.20000000298023224	CHEMBL2392227,TN,INACT,0.05999999865889549	CHEMBL3672262,FN,ACT,0.7300000190734863	CHEMBL3672348,TP,ACT,0.9300000071525574	CHEMBL1287853,FN,ACT,0.10999999940395355	CHEMBL131695,TN,INACT,0.03999999910593033	CHEMBL551722,TN,INACT,0.4000000059604645	CHEMBL190201,TN,INACT,0.30000001192092896	CHEMBL3672369,TP,ACT,0.9300000071525574	CHEMBL2392242,TN,INACT,0.07000000029802322	CHEMBL3672299,TP,ACT,0.949999988079071	CHEMBL469770,TN,INACT,0.05999999865889549	CHEMBL3672235,TP,ACT,0.949999988079071	CHEMBL3335362,TN,INACT,0.05000000074505806	CHEMBL3667226,TP,ACT,0.9200000166893005	CHEMBL3672240,TP,ACT,0.8399999737739563	CHEMBL379849,TN,INACT,0.07999999821186066	CHEMBL48614,TN,INACT,0.27000001072883606	CHEMBL1081198,TN,INACT,0.09000000357627869	CHEMBL558601,TN,INACT,0.05999999865889549	CHEMBL552136,TN,INACT,0.05999999865889549	CHEMBL478488,TN,INACT,0.07000000029802322	CHEMBL559683,TN,INACT,0.05999999865889549	CHEMBL524820,TN,INACT,0.27000001072883606	CHEMBL485878,TN,INACT,0.5	CHEMBL3672382,TP,ACT,0.9200000166893005	CHEMBL3672280,TP,ACT,0.9599999785423279	CHEMBL3672310,TP,ACT,0.9399999976158142	CHEMBL3672327,TP,ACT,0.9599999785423279	CHEMBL497454,TN,INACT,0.03999999910593033	CHEMBL3672269,TP,ACT,0.9399999976158142	CHEMBL550856,TN,INACT,0.10000000149011612	CHEMBL2425628,FN,ACT,0.11999999731779099	

