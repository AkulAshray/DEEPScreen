ImageNetInceptionV2 CHEMBL2716 adam 0.0001 15 0 0 0.8 False True
Number of active compounds :	118
Number of inactive compounds :	118
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL2716_adam_0.0001_15_0_0_0.8_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL2716_adam_0.0001_15_0.8/
---------------------------------
Training samples: 141
Validation samples: 45
--
Training Step: 1  | time: 35.779s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/141
[A[ATraining Step: 2  | total loss: [1m[32m0.67517[0m[0m | time: 43.666s
[2K
| Adam | epoch: 001 | loss: 0.67517 - acc: 0.3375 -- iter: 064/141
[A[ATraining Step: 3  | total loss: [1m[32m0.67439[0m[0m | time: 51.603s
[2K
| Adam | epoch: 001 | loss: 0.67439 - acc: 0.5216 -- iter: 096/141
[A[ATraining Step: 4  | total loss: [1m[32m0.62678[0m[0m | time: 59.323s
[2K
| Adam | epoch: 001 | loss: 0.62678 - acc: 0.6460 -- iter: 128/141
[A[ATraining Step: 5  | total loss: [1m[32m0.58064[0m[0m | time: 71.173s
[2K
| Adam | epoch: 001 | loss: 0.58064 - acc: 0.6964 | val_loss: 0.77126 - val_acc: 0.4222 -- iter: 141/141
--
Training Step: 6  | total loss: [1m[32m0.53056[0m[0m | time: 3.925s
[2K
| Adam | epoch: 002 | loss: 0.53056 - acc: 0.7432 -- iter: 032/141
[A[ATraining Step: 7  | total loss: [1m[32m0.39359[0m[0m | time: 11.821s
[2K
| Adam | epoch: 002 | loss: 0.39359 - acc: 0.8973 -- iter: 064/141
[A[ATraining Step: 8  | total loss: [1m[32m0.52055[0m[0m | time: 19.642s
[2K
| Adam | epoch: 002 | loss: 0.52055 - acc: 0.7265 -- iter: 096/141
[A[ATraining Step: 9  | total loss: [1m[32m0.48187[0m[0m | time: 27.630s
[2K
| Adam | epoch: 002 | loss: 0.48187 - acc: 0.7721 -- iter: 128/141
[A[ATraining Step: 10  | total loss: [1m[32m0.41669[0m[0m | time: 37.583s
[2K
| Adam | epoch: 002 | loss: 0.41669 - acc: 0.8548 | val_loss: 0.95219 - val_acc: 0.4222 -- iter: 141/141
--
Training Step: 11  | total loss: [1m[32m0.41663[0m[0m | time: 4.076s
[2K
| Adam | epoch: 003 | loss: 0.41663 - acc: 0.8496 -- iter: 032/141
[A[ATraining Step: 12  | total loss: [1m[32m0.36477[0m[0m | time: 7.989s
[2K
| Adam | epoch: 003 | loss: 0.36477 - acc: 0.9173 -- iter: 064/141
[A[ATraining Step: 13  | total loss: [1m[32m0.28514[0m[0m | time: 15.785s
[2K
| Adam | epoch: 003 | loss: 0.28514 - acc: 0.9527 -- iter: 096/141
[A[ATraining Step: 14  | total loss: [1m[32m0.29244[0m[0m | time: 23.739s
[2K
| Adam | epoch: 003 | loss: 0.29244 - acc: 0.9209 -- iter: 128/141
[A[ATraining Step: 15  | total loss: [1m[32m0.28529[0m[0m | time: 33.771s
[2K
| Adam | epoch: 003 | loss: 0.28529 - acc: 0.9396 | val_loss: 0.88343 - val_acc: 0.4222 -- iter: 141/141
--
Training Step: 16  | total loss: [1m[32m0.28561[0m[0m | time: 7.907s
[2K
| Adam | epoch: 004 | loss: 0.28561 - acc: 0.9388 -- iter: 032/141
[A[ATraining Step: 17  | total loss: [1m[32m0.23412[0m[0m | time: 11.839s
[2K
| Adam | epoch: 004 | loss: 0.23412 - acc: 0.9609 -- iter: 064/141
[A[ATraining Step: 18  | total loss: [1m[32m0.19564[0m[0m | time: 15.781s
[2K
| Adam | epoch: 004 | loss: 0.19564 - acc: 0.9744 -- iter: 096/141
[A[ATraining Step: 19  | total loss: [1m[32m0.15430[0m[0m | time: 23.813s
[2K
| Adam | epoch: 004 | loss: 0.15430 - acc: 0.9829 -- iter: 128/141
[A[ATraining Step: 20  | total loss: [1m[32m0.15117[0m[0m | time: 33.724s
[2K
| Adam | epoch: 004 | loss: 0.15117 - acc: 0.9784 | val_loss: 0.73225 - val_acc: 0.5778 -- iter: 141/141
--
Training Step: 21  | total loss: [1m[32m0.12751[0m[0m | time: 9.501s
[2K
| Adam | epoch: 005 | loss: 0.12751 - acc: 0.9851 -- iter: 032/141
[A[ATraining Step: 22  | total loss: [1m[32m0.12999[0m[0m | time: 17.424s
[2K
| Adam | epoch: 005 | loss: 0.12999 - acc: 0.9802 -- iter: 064/141
[A[ATraining Step: 23  | total loss: [1m[32m0.10504[0m[0m | time: 21.329s
[2K
| Adam | epoch: 005 | loss: 0.10504 - acc: 0.9859 -- iter: 096/141
[A[ATraining Step: 24  | total loss: [1m[32m0.11527[0m[0m | time: 25.281s
[2K
| Adam | epoch: 005 | loss: 0.11527 - acc: 0.9899 -- iter: 128/141
[A[ATraining Step: 25  | total loss: [1m[32m0.09816[0m[0m | time: 35.141s
[2K
| Adam | epoch: 005 | loss: 0.09816 - acc: 0.9926 | val_loss: 0.80056 - val_acc: 0.5778 -- iter: 141/141
--
Training Step: 26  | total loss: [1m[32m0.08538[0m[0m | time: 7.693s
[2K
| Adam | epoch: 006 | loss: 0.08538 - acc: 0.9863 -- iter: 032/141
[A[ATraining Step: 27  | total loss: [1m[32m0.07098[0m[0m | time: 15.475s
[2K
| Adam | epoch: 006 | loss: 0.07098 - acc: 0.9898 -- iter: 064/141
[A[ATraining Step: 28  | total loss: [1m[32m0.05633[0m[0m | time: 23.276s
[2K
| Adam | epoch: 006 | loss: 0.05633 - acc: 0.9924 -- iter: 096/141
[A[ATraining Step: 29  | total loss: [1m[32m0.04656[0m[0m | time: 27.221s
[2K
| Adam | epoch: 006 | loss: 0.04656 - acc: 0.9942 -- iter: 128/141
[A[ATraining Step: 30  | total loss: [1m[32m0.06537[0m[0m | time: 33.166s
[2K
| Adam | epoch: 006 | loss: 0.06537 - acc: 0.9774 | val_loss: 0.80178 - val_acc: 0.4889 -- iter: 141/141
--
Training Step: 31  | total loss: [1m[32m0.05560[0m[0m | time: 7.867s
[2K
| Adam | epoch: 007 | loss: 0.05560 - acc: 0.9826 -- iter: 032/141
[A[ATraining Step: 32  | total loss: [1m[32m0.05042[0m[0m | time: 15.807s
[2K
| Adam | epoch: 007 | loss: 0.05042 - acc: 0.9865 -- iter: 064/141
[A[ATraining Step: 33  | total loss: [1m[32m0.04105[0m[0m | time: 23.529s
[2K
| Adam | epoch: 007 | loss: 0.04105 - acc: 0.9895 -- iter: 096/141
[A[ATraining Step: 34  | total loss: [1m[32m0.03402[0m[0m | time: 31.289s
[2K
| Adam | epoch: 007 | loss: 0.03402 - acc: 0.9917 -- iter: 128/141
[A[ATraining Step: 35  | total loss: [1m[32m0.02848[0m[0m | time: 37.255s
[2K
| Adam | epoch: 007 | loss: 0.02848 - acc: 0.9935 | val_loss: 1.03822 - val_acc: 0.4222 -- iter: 141/141
--
Training Step: 36  | total loss: [1m[32m0.02410[0m[0m | time: 3.908s
[2K
| Adam | epoch: 008 | loss: 0.02410 - acc: 0.9948 -- iter: 032/141
[A[ATraining Step: 37  | total loss: [1m[32m0.02015[0m[0m | time: 11.710s
[2K
| Adam | epoch: 008 | loss: 0.02015 - acc: 0.9958 -- iter: 064/141
[A[ATraining Step: 38  | total loss: [1m[32m0.01790[0m[0m | time: 19.545s
[2K
| Adam | epoch: 008 | loss: 0.01790 - acc: 0.9967 -- iter: 096/141
[A[ATraining Step: 39  | total loss: [1m[32m0.02344[0m[0m | time: 27.200s
[2K
| Adam | epoch: 008 | loss: 0.02344 - acc: 0.9973 -- iter: 128/141
[A[ATraining Step: 40  | total loss: [1m[32m0.10722[0m[0m | time: 37.051s
[2K
| Adam | epoch: 008 | loss: 0.10722 - acc: 0.9861 | val_loss: 1.26857 - val_acc: 0.4222 -- iter: 141/141
--
Training Step: 41  | total loss: [1m[32m0.08857[0m[0m | time: 3.863s
[2K
| Adam | epoch: 009 | loss: 0.08857 - acc: 0.9886 -- iter: 032/141
[A[ATraining Step: 42  | total loss: [1m[32m0.07321[0m[0m | time: 7.648s
[2K
| Adam | epoch: 009 | loss: 0.07321 - acc: 0.9907 -- iter: 064/141
[A[ATraining Step: 43  | total loss: [1m[32m0.06093[0m[0m | time: 15.512s
[2K
| Adam | epoch: 009 | loss: 0.06093 - acc: 0.9923 -- iter: 096/141
[A[ATraining Step: 44  | total loss: [1m[32m0.05365[0m[0m | time: 23.215s
[2K
| Adam | epoch: 009 | loss: 0.05365 - acc: 0.9937 -- iter: 128/141
[A[ATraining Step: 45  | total loss: [1m[32m0.04618[0m[0m | time: 33.066s
[2K
| Adam | epoch: 009 | loss: 0.04618 - acc: 0.9947 | val_loss: 1.06845 - val_acc: 0.4222 -- iter: 141/141
--
Training Step: 46  | total loss: [1m[32m0.10449[0m[0m | time: 7.915s
[2K
| Adam | epoch: 010 | loss: 0.10449 - acc: 0.9852 -- iter: 032/141
[A[ATraining Step: 47  | total loss: [1m[32m0.09615[0m[0m | time: 11.722s
[2K
| Adam | epoch: 010 | loss: 0.09615 - acc: 0.9825 -- iter: 064/141
[A[ATraining Step: 48  | total loss: [1m[32m0.08475[0m[0m | time: 15.552s
[2K
| Adam | epoch: 010 | loss: 0.08475 - acc: 0.9853 -- iter: 096/141
[A[ATraining Step: 49  | total loss: [1m[32m0.07278[0m[0m | time: 23.467s
[2K
| Adam | epoch: 010 | loss: 0.07278 - acc: 0.9876 -- iter: 128/141
[A[ATraining Step: 50  | total loss: [1m[32m0.06317[0m[0m | time: 33.275s
[2K
| Adam | epoch: 010 | loss: 0.06317 - acc: 0.9896 | val_loss: 0.63821 - val_acc: 0.6444 -- iter: 141/141
--
Training Step: 51  | total loss: [1m[32m0.05467[0m[0m | time: 7.821s
[2K
| Adam | epoch: 011 | loss: 0.05467 - acc: 0.9911 -- iter: 032/141
[A[ATraining Step: 52  | total loss: [1m[32m0.06900[0m[0m | time: 15.575s
[2K
| Adam | epoch: 011 | loss: 0.06900 - acc: 0.9878 -- iter: 064/141
[A[ATraining Step: 53  | total loss: [1m[32m0.06101[0m[0m | time: 19.448s
[2K
| Adam | epoch: 011 | loss: 0.06101 - acc: 0.9896 -- iter: 096/141
[A[ATraining Step: 54  | total loss: [1m[32m0.07200[0m[0m | time: 23.251s
[2K
| Adam | epoch: 011 | loss: 0.07200 - acc: 0.9799 -- iter: 128/141
[A[ATraining Step: 55  | total loss: [1m[32m0.06513[0m[0m | time: 33.059s
[2K
| Adam | epoch: 011 | loss: 0.06513 - acc: 0.9828 | val_loss: 0.70011 - val_acc: 0.6889 -- iter: 141/141
--
Training Step: 56  | total loss: [1m[32m0.05825[0m[0m | time: 9.050s
[2K
| Adam | epoch: 012 | loss: 0.05825 - acc: 0.9852 -- iter: 032/141
[A[ATraining Step: 57  | total loss: [1m[32m0.06460[0m[0m | time: 18.972s
[2K
| Adam | epoch: 012 | loss: 0.06460 - acc: 0.9829 -- iter: 064/141
[A[ATraining Step: 58  | total loss: [1m[32m0.10012[0m[0m | time: 28.315s
[2K
| Adam | epoch: 012 | loss: 0.10012 - acc: 0.9810 -- iter: 096/141
[A[ATraining Step: 59  | total loss: [1m[32m0.08769[0m[0m | time: 33.087s
[2K
| Adam | epoch: 012 | loss: 0.08769 - acc: 0.9836 -- iter: 128/141
[A[ATraining Step: 60  | total loss: [1m[32m0.07686[0m[0m | time: 40.054s
[2K
| Adam | epoch: 012 | loss: 0.07686 - acc: 0.9857 | val_loss: 0.76314 - val_acc: 0.6444 -- iter: 141/141
--
Training Step: 61  | total loss: [1m[32m0.06754[0m[0m | time: 9.682s
[2K
| Adam | epoch: 013 | loss: 0.06754 - acc: 0.9876 -- iter: 032/141
[A[ATraining Step: 62  | total loss: [1m[32m0.05985[0m[0m | time: 19.294s
[2K
| Adam | epoch: 013 | loss: 0.05985 - acc: 0.9892 -- iter: 064/141
[A[ATraining Step: 63  | total loss: [1m[32m0.06289[0m[0m | time: 28.777s
[2K
| Adam | epoch: 013 | loss: 0.06289 - acc: 0.9866 -- iter: 096/141
[A[ATraining Step: 64  | total loss: [1m[32m0.07105[0m[0m | time: 38.020s
[2K
| Adam | epoch: 013 | loss: 0.07105 - acc: 0.9844 -- iter: 128/141
[A[ATraining Step: 65  | total loss: [1m[32m0.06425[0m[0m | time: 45.361s
[2K
| Adam | epoch: 013 | loss: 0.06425 - acc: 0.9863 | val_loss: 0.97723 - val_acc: 0.6222 -- iter: 141/141
--
Training Step: 66  | total loss: [1m[32m0.05737[0m[0m | time: 5.451s
[2K
| Adam | epoch: 014 | loss: 0.05737 - acc: 0.9880 -- iter: 032/141
[A[ATraining Step: 67  | total loss: [1m[32m0.05116[0m[0m | time: 17.554s
[2K
| Adam | epoch: 014 | loss: 0.05116 - acc: 0.9894 -- iter: 064/141
[A[ATraining Step: 68  | total loss: [1m[32m0.06424[0m[0m | time: 28.688s
[2K
| Adam | epoch: 014 | loss: 0.06424 - acc: 0.9870 -- iter: 096/141
[A[ATraining Step: 69  | total loss: [1m[32m0.05844[0m[0m | time: 40.639s
[2K
| Adam | epoch: 014 | loss: 0.05844 - acc: 0.9885 -- iter: 128/141
[A[ATraining Step: 70  | total loss: [1m[32m0.08411[0m[0m | time: 55.119s
[2K
| Adam | epoch: 014 | loss: 0.08411 - acc: 0.9826 | val_loss: 0.80108 - val_acc: 0.6889 -- iter: 141/141
--
Training Step: 71  | total loss: [1m[32m0.08159[0m[0m | time: 6.049s
[2K
| Adam | epoch: 015 | loss: 0.08159 - acc: 0.9810 -- iter: 032/141
[A[ATraining Step: 72  | total loss: [1m[32m0.07967[0m[0m | time: 11.626s
[2K
| Adam | epoch: 015 | loss: 0.07967 - acc: 0.9832 -- iter: 064/141
[A[ATraining Step: 73  | total loss: [1m[32m0.07469[0m[0m | time: 21.780s
[2K
| Adam | epoch: 015 | loss: 0.07469 - acc: 0.9850 -- iter: 096/141
[A[ATraining Step: 74  | total loss: [1m[32m0.07038[0m[0m | time: 31.502s
[2K
| Adam | epoch: 015 | loss: 0.07038 - acc: 0.9832 -- iter: 128/141
[A[ATraining Step: 75  | total loss: [1m[32m0.06335[0m[0m | time: 106.116s
[2K
| Adam | epoch: 015 | loss: 0.06335 - acc: 0.9851 | val_loss: 0.67994 - val_acc: 0.6667 -- iter: 141/141
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.7429149797570851
Validation AUPRC:0.8407485704861727
Test AUC:0.7429149797570851
Test AUPRC:0.8474034151187131
BestTestF1Score	0.76	0.31	0.64	0.62	1.0	26	16	3	0	0.09
BestTestMCCScore	0.63	0.52	0.69	1.0	0.46	12	0	19	14	0.91
BestTestAccuracyScore	0.67	0.37	0.67	0.79	0.58	15	4	15	11	0.8
BestValidationF1Score	0.78	0.38	0.69	0.66	0.96	25	13	6	1	0.09
BestValidationMCC	0.59	0.49	0.67	1.0	0.42	11	0	19	15	0.91
BestValidationAccuracy	0.65	0.48	0.69	0.93	0.5	13	1	18	13	0.8
TestPredictions (Threshold:0.91)
CHEMBL1767029,TN,INACT,0.05999999865889549	CHEMBL3110005,TP,ACT,1.0	CHEMBL561483,TP,ACT,0.9700000286102295	CHEMBL3355256,TN,INACT,0.6499999761581421	CHEMBL1836042,TN,INACT,0.23000000417232513	CHEMBL1914708,FN,ACT,0.20000000298023224	CHEMBL260080,TN,INACT,0.20999999344348907	CHEMBL2436595,FN,ACT,0.8399999737739563	CHEMBL1213490,TP,ACT,1.0	CHEMBL3335314,FN,ACT,0.800000011920929	CHEMBL3220924,FN,ACT,0.11999999731779099	CHEMBL2047684,TN,INACT,0.8199999928474426	CHEMBL2417786,TN,INACT,0.009999999776482582	CHEMBL3793392,FN,ACT,0.27000001072883606	CHEMBL55895,TN,INACT,0.27000001072883606	CHEMBL3617543,TN,INACT,0.800000011920929	CHEMBL1934899,TN,INACT,0.8700000047683716	CHEMBL585365,TP,ACT,0.9300000071525574	CHEMBL472345,TN,INACT,0.15000000596046448	CHEMBL3335284,TP,ACT,0.9900000095367432	CHEMBL365927,FN,ACT,0.47999998927116394	CHEMBL3670675,FN,ACT,0.699999988079071	CHEMBL3792392,TN,INACT,0.46000000834465027	CHEMBL2431862,FN,ACT,0.6499999761581421	CHEMBL479970,TN,INACT,0.6800000071525574	CHEMBL16300,TN,INACT,0.8299999833106995	CHEMBL3235787,TN,INACT,0.30000001192092896	CHEMBL3415449,TN,INACT,0.3100000023841858	CHEMBL564876,FN,ACT,0.6800000071525574	CHEMBL475099,TN,INACT,0.6800000071525574	CHEMBL472532,TN,INACT,0.18000000715255737	CHEMBL1631910,FN,ACT,0.23000000417232513	CHEMBL3335306,TP,ACT,0.9800000190734863	CHEMBL1631915,TP,ACT,0.9700000286102295	CHEMBL3109983,TP,ACT,0.9700000286102295	CHEMBL3770180,TN,INACT,0.019999999552965164	CHEMBL572545,TP,ACT,0.9900000095367432	CHEMBL3110003,TP,ACT,1.0	CHEMBL396097,TP,ACT,1.0	CHEMBL2088207,TN,INACT,0.33000001311302185	CHEMBL471042,FN,ACT,0.12999999523162842	CHEMBL490018,FN,ACT,0.1599999964237213	CHEMBL2414098,FN,ACT,0.18000000715255737	CHEMBL3335293,TP,ACT,0.9100000262260437	CHEMBL1830422,FN,ACT,0.8199999928474426	

