CNNModel CHEMBL3864 adam 0.001 30 256 0 0.6 False True
Number of active compounds :	168
Number of inactive compounds :	168
---------------------------------
Run id: CNNModel_CHEMBL3864_adam_0.001_30_256_0_0.6_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL3864_adam_0.001_30_256_0.6_True/
---------------------------------
Training samples: 197
Validation samples: 62
--
Training Step: 1  | time: 0.766s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/197
[A[ATraining Step: 2  | total loss: [1m[32m0.62246[0m[0m | time: 1.364s
[2K
| Adam | epoch: 001 | loss: 0.62246 - acc: 0.5906 -- iter: 064/197
[A[ATraining Step: 3  | total loss: [1m[32m0.66608[0m[0m | time: 1.955s
[2K
| Adam | epoch: 001 | loss: 0.66608 - acc: 0.6187 -- iter: 096/197
[A[ATraining Step: 4  | total loss: [1m[32m0.72689[0m[0m | time: 2.561s
[2K
| Adam | epoch: 001 | loss: 0.72689 - acc: 0.5297 -- iter: 128/197
[A[ATraining Step: 5  | total loss: [1m[32m0.75274[0m[0m | time: 3.176s
[2K
| Adam | epoch: 001 | loss: 0.75274 - acc: 0.3793 -- iter: 160/197
[A[ATraining Step: 6  | total loss: [1m[32m0.71240[0m[0m | time: 3.793s
[2K
| Adam | epoch: 001 | loss: 0.71240 - acc: 0.4770 -- iter: 192/197
[A[ATraining Step: 7  | total loss: [1m[32m0.69727[0m[0m | time: 4.970s
[2K
| Adam | epoch: 001 | loss: 0.69727 - acc: 0.5845 | val_loss: 0.69319 - val_acc: 0.5000 -- iter: 197/197
--
Training Step: 8  | total loss: [1m[32m0.69125[0m[0m | time: 0.157s
[2K
| Adam | epoch: 002 | loss: 0.69125 - acc: 0.7057 -- iter: 032/197
[A[ATraining Step: 9  | total loss: [1m[32m0.68906[0m[0m | time: 0.772s
[2K
| Adam | epoch: 002 | loss: 0.68906 - acc: 0.7556 -- iter: 064/197
[A[ATraining Step: 10  | total loss: [1m[32m0.69111[0m[0m | time: 1.375s
[2K
| Adam | epoch: 002 | loss: 0.69111 - acc: 0.6278 -- iter: 096/197
[A[ATraining Step: 11  | total loss: [1m[32m0.69047[0m[0m | time: 1.983s
[2K
| Adam | epoch: 002 | loss: 0.69047 - acc: 0.6265 -- iter: 128/197
[A[ATraining Step: 12  | total loss: [1m[32m0.69127[0m[0m | time: 2.602s
[2K
| Adam | epoch: 002 | loss: 0.69127 - acc: 0.5836 -- iter: 160/197
[A[ATraining Step: 13  | total loss: [1m[32m0.69357[0m[0m | time: 3.235s
[2K
| Adam | epoch: 002 | loss: 0.69357 - acc: 0.5076 -- iter: 192/197
[A[ATraining Step: 14  | total loss: [1m[32m0.69305[0m[0m | time: 4.846s
[2K
| Adam | epoch: 002 | loss: 0.69305 - acc: 0.5173 | val_loss: 0.69333 - val_acc: 0.5000 -- iter: 197/197
--
Training Step: 15  | total loss: [1m[32m0.69184[0m[0m | time: 0.144s
[2K
| Adam | epoch: 003 | loss: 0.69184 - acc: 0.5472 -- iter: 032/197
[A[ATraining Step: 16  | total loss: [1m[32m0.69397[0m[0m | time: 0.269s
[2K
| Adam | epoch: 003 | loss: 0.69397 - acc: 0.4920 -- iter: 064/197
[A[ATraining Step: 17  | total loss: [1m[32m0.69536[0m[0m | time: 0.896s
[2K
| Adam | epoch: 003 | loss: 0.69536 - acc: 0.4589 -- iter: 096/197
[A[ATraining Step: 18  | total loss: [1m[32m0.69469[0m[0m | time: 1.513s
[2K
| Adam | epoch: 003 | loss: 0.69469 - acc: 0.4731 -- iter: 128/197
[A[ATraining Step: 19  | total loss: [1m[32m0.69385[0m[0m | time: 2.117s
[2K
| Adam | epoch: 003 | loss: 0.69385 - acc: 0.4925 -- iter: 160/197
[A[ATraining Step: 20  | total loss: [1m[32m0.69200[0m[0m | time: 2.718s
[2K
| Adam | epoch: 003 | loss: 0.69200 - acc: 0.5351 -- iter: 192/197
[A[ATraining Step: 21  | total loss: [1m[32m0.69205[0m[0m | time: 4.338s
[2K
| Adam | epoch: 003 | loss: 0.69205 - acc: 0.5339 | val_loss: 0.69336 - val_acc: 0.5000 -- iter: 197/197
--
Training Step: 22  | total loss: [1m[32m0.69119[0m[0m | time: 0.608s
[2K
| Adam | epoch: 004 | loss: 0.69119 - acc: 0.5519 -- iter: 032/197
[A[ATraining Step: 23  | total loss: [1m[32m0.69218[0m[0m | time: 0.751s
[2K
| Adam | epoch: 004 | loss: 0.69218 - acc: 0.5277 -- iter: 064/197
[A[ATraining Step: 24  | total loss: [1m[32m0.69111[0m[0m | time: 0.880s
[2K
| Adam | epoch: 004 | loss: 0.69111 - acc: 0.5481 -- iter: 096/197
[A[ATraining Step: 25  | total loss: [1m[32m0.69057[0m[0m | time: 1.492s
[2K
| Adam | epoch: 004 | loss: 0.69057 - acc: 0.5622 -- iter: 128/197
[A[ATraining Step: 26  | total loss: [1m[32m0.69132[0m[0m | time: 2.100s
[2K
| Adam | epoch: 004 | loss: 0.69132 - acc: 0.5457 -- iter: 160/197
[A[ATraining Step: 27  | total loss: [1m[32m0.69323[0m[0m | time: 2.700s
[2K
| Adam | epoch: 004 | loss: 0.69323 - acc: 0.5099 -- iter: 192/197
[A[ATraining Step: 28  | total loss: [1m[32m0.69288[0m[0m | time: 4.307s
[2K
| Adam | epoch: 004 | loss: 0.69288 - acc: 0.5152 | val_loss: 0.69351 - val_acc: 0.5000 -- iter: 197/197
--
Training Step: 29  | total loss: [1m[32m0.69178[0m[0m | time: 0.615s
[2K
| Adam | epoch: 005 | loss: 0.69178 - acc: 0.5343 -- iter: 032/197
[A[ATraining Step: 30  | total loss: [1m[32m0.69145[0m[0m | time: 1.220s
[2K
| Adam | epoch: 005 | loss: 0.69145 - acc: 0.5410 -- iter: 064/197
[A[ATraining Step: 31  | total loss: [1m[32m0.69022[0m[0m | time: 1.348s
[2K
| Adam | epoch: 005 | loss: 0.69022 - acc: 0.5604 -- iter: 096/197
[A[ATraining Step: 32  | total loss: [1m[32m0.68956[0m[0m | time: 1.483s
[2K
| Adam | epoch: 005 | loss: 0.68956 - acc: 0.5693 -- iter: 128/197
[A[ATraining Step: 33  | total loss: [1m[32m0.68868[0m[0m | time: 2.103s
[2K
| Adam | epoch: 005 | loss: 0.68868 - acc: 0.5760 -- iter: 160/197
[A[ATraining Step: 34  | total loss: [1m[32m0.68818[0m[0m | time: 2.738s
[2K
| Adam | epoch: 005 | loss: 0.68818 - acc: 0.5798 -- iter: 192/197
[A[ATraining Step: 35  | total loss: [1m[32m0.68814[0m[0m | time: 4.359s
[2K
| Adam | epoch: 005 | loss: 0.68814 - acc: 0.5762 | val_loss: 0.69457 - val_acc: 0.5000 -- iter: 197/197
--
Training Step: 36  | total loss: [1m[32m0.69089[0m[0m | time: 0.616s
[2K
| Adam | epoch: 006 | loss: 0.69089 - acc: 0.5478 -- iter: 032/197
[A[ATraining Step: 37  | total loss: [1m[32m0.68886[0m[0m | time: 1.219s
[2K
| Adam | epoch: 006 | loss: 0.68886 - acc: 0.5633 -- iter: 064/197
[A[ATraining Step: 38  | total loss: [1m[32m0.69167[0m[0m | time: 1.834s
[2K
| Adam | epoch: 006 | loss: 0.69167 - acc: 0.5387 -- iter: 096/197
[A[ATraining Step: 39  | total loss: [1m[32m0.69070[0m[0m | time: 1.962s
[2K
| Adam | epoch: 006 | loss: 0.69070 - acc: 0.5432 -- iter: 128/197
[A[ATraining Step: 40  | total loss: [1m[32m0.68911[0m[0m | time: 2.099s
[2K
| Adam | epoch: 006 | loss: 0.68911 - acc: 0.5539 -- iter: 160/197
[A[ATraining Step: 41  | total loss: [1m[32m0.68774[0m[0m | time: 2.703s
[2K
| Adam | epoch: 006 | loss: 0.68774 - acc: 0.5623 -- iter: 192/197
[A[ATraining Step: 42  | total loss: [1m[32m0.69114[0m[0m | time: 4.317s
[2K
| Adam | epoch: 006 | loss: 0.69114 - acc: 0.5399 | val_loss: 0.69665 - val_acc: 0.5000 -- iter: 197/197
--
Training Step: 43  | total loss: [1m[32m0.68847[0m[0m | time: 0.616s
[2K
| Adam | epoch: 007 | loss: 0.68847 - acc: 0.5549 -- iter: 032/197
[A[ATraining Step: 44  | total loss: [1m[32m0.68887[0m[0m | time: 1.223s
[2K
| Adam | epoch: 007 | loss: 0.68887 - acc: 0.5508 -- iter: 064/197
[A[ATraining Step: 45  | total loss: [1m[32m0.68847[0m[0m | time: 1.855s
[2K
| Adam | epoch: 007 | loss: 0.68847 - acc: 0.5528 -- iter: 096/197
[A[ATraining Step: 46  | total loss: [1m[32m0.68993[0m[0m | time: 2.454s
[2K
| Adam | epoch: 007 | loss: 0.68993 - acc: 0.5440 -- iter: 128/197
[A[ATraining Step: 47  | total loss: [1m[32m0.68911[0m[0m | time: 2.595s
[2K
| Adam | epoch: 007 | loss: 0.68911 - acc: 0.5470 -- iter: 160/197
[A[ATraining Step: 48  | total loss: [1m[32m0.69479[0m[0m | time: 2.731s
[2K
| Adam | epoch: 007 | loss: 0.69479 - acc: 0.5234 -- iter: 192/197
[A[ATraining Step: 49  | total loss: [1m[32m0.69825[0m[0m | time: 4.338s
[2K
| Adam | epoch: 007 | loss: 0.69825 - acc: 0.5039 | val_loss: 0.69492 - val_acc: 0.5000 -- iter: 197/197
--
Training Step: 50  | total loss: [1m[32m0.69800[0m[0m | time: 0.610s
[2K
| Adam | epoch: 008 | loss: 0.69800 - acc: 0.5033 -- iter: 032/197
[A[ATraining Step: 51  | total loss: [1m[32m0.69483[0m[0m | time: 1.210s
[2K
| Adam | epoch: 008 | loss: 0.69483 - acc: 0.5219 -- iter: 064/197
[A[ATraining Step: 52  | total loss: [1m[32m0.69536[0m[0m | time: 1.807s
[2K
| Adam | epoch: 008 | loss: 0.69536 - acc: 0.5139 -- iter: 096/197
[A[ATraining Step: 53  | total loss: [1m[32m0.69572[0m[0m | time: 2.423s
[2K
| Adam | epoch: 008 | loss: 0.69572 - acc: 0.5072 -- iter: 128/197
[A[ATraining Step: 54  | total loss: [1m[32m0.69329[0m[0m | time: 3.047s
[2K
| Adam | epoch: 008 | loss: 0.69329 - acc: 0.5289 -- iter: 160/197
[A[ATraining Step: 55  | total loss: [1m[32m0.69296[0m[0m | time: 3.179s
[2K
| Adam | epoch: 008 | loss: 0.69296 - acc: 0.5292 -- iter: 192/197
[A[ATraining Step: 56  | total loss: [1m[32m0.68914[0m[0m | time: 4.327s
[2K
| Adam | epoch: 008 | loss: 0.68914 - acc: 0.5673 | val_loss: 0.69423 - val_acc: 0.5000 -- iter: 197/197
--
Training Step: 57  | total loss: [1m[32m0.68584[0m[0m | time: 0.602s
[2K
| Adam | epoch: 009 | loss: 0.68584 - acc: 0.5995 -- iter: 032/197
[A[ATraining Step: 58  | total loss: [1m[32m0.68659[0m[0m | time: 1.205s
[2K
| Adam | epoch: 009 | loss: 0.68659 - acc: 0.5902 -- iter: 064/197
[A[ATraining Step: 59  | total loss: [1m[32m0.68723[0m[0m | time: 1.816s
[2K
| Adam | epoch: 009 | loss: 0.68723 - acc: 0.5823 -- iter: 096/197
[A[ATraining Step: 60  | total loss: [1m[32m0.68865[0m[0m | time: 2.442s
[2K
| Adam | epoch: 009 | loss: 0.68865 - acc: 0.5673 -- iter: 128/197
[A[ATraining Step: 61  | total loss: [1m[32m0.68713[0m[0m | time: 3.045s
[2K
| Adam | epoch: 009 | loss: 0.68713 - acc: 0.5789 -- iter: 160/197
[A[ATraining Step: 62  | total loss: [1m[32m0.68855[0m[0m | time: 3.636s
[2K
| Adam | epoch: 009 | loss: 0.68855 - acc: 0.5647 -- iter: 192/197
[A[ATraining Step: 63  | total loss: [1m[32m0.68877[0m[0m | time: 4.776s
[2K
| Adam | epoch: 009 | loss: 0.68877 - acc: 0.5605 | val_loss: 0.69468 - val_acc: 0.5000 -- iter: 197/197
--
Training Step: 64  | total loss: [1m[32m0.69724[0m[0m | time: 0.148s
[2K
| Adam | epoch: 010 | loss: 0.69724 - acc: 0.4904 -- iter: 032/197
[A[ATraining Step: 65  | total loss: [1m[32m0.70421[0m[0m | time: 0.765s
[2K
| Adam | epoch: 010 | loss: 0.70421 - acc: 0.4299 -- iter: 064/197
[A[ATraining Step: 66  | total loss: [1m[32m0.70384[0m[0m | time: 1.363s
[2K
| Adam | epoch: 010 | loss: 0.70384 - acc: 0.4309 -- iter: 096/197
[A[ATraining Step: 67  | total loss: [1m[32m0.70115[0m[0m | time: 1.975s
[2K
| Adam | epoch: 010 | loss: 0.70115 - acc: 0.4542 -- iter: 128/197
[A[ATraining Step: 68  | total loss: [1m[32m0.70066[0m[0m | time: 2.589s
[2K
| Adam | epoch: 010 | loss: 0.70066 - acc: 0.4559 -- iter: 160/197
[A[ATraining Step: 69  | total loss: [1m[32m0.69835[0m[0m | time: 3.193s
[2K
| Adam | epoch: 010 | loss: 0.69835 - acc: 0.4793 -- iter: 192/197
[A[ATraining Step: 70  | total loss: [1m[32m0.69693[0m[0m | time: 4.808s
[2K
| Adam | epoch: 010 | loss: 0.69693 - acc: 0.4925 | val_loss: 0.69383 - val_acc: 0.5000 -- iter: 197/197
--
Training Step: 71  | total loss: [1m[32m0.69628[0m[0m | time: 0.147s
[2K
| Adam | epoch: 011 | loss: 0.69628 - acc: 0.4969 -- iter: 032/197
[A[ATraining Step: 72  | total loss: [1m[32m0.69514[0m[0m | time: 0.286s
[2K
| Adam | epoch: 011 | loss: 0.69514 - acc: 0.5085 -- iter: 064/197
[A[ATraining Step: 73  | total loss: [1m[32m0.69414[0m[0m | time: 0.881s
[2K
| Adam | epoch: 011 | loss: 0.69414 - acc: 0.5187 -- iter: 096/197
[A[ATraining Step: 74  | total loss: [1m[32m0.69460[0m[0m | time: 1.477s
[2K
| Adam | epoch: 011 | loss: 0.69460 - acc: 0.5098 -- iter: 128/197
[A[ATraining Step: 75  | total loss: [1m[32m0.69328[0m[0m | time: 2.074s
[2K
| Adam | epoch: 011 | loss: 0.69328 - acc: 0.5257 -- iter: 160/197
[A[ATraining Step: 76  | total loss: [1m[32m0.69433[0m[0m | time: 2.712s
[2K
| Adam | epoch: 011 | loss: 0.69433 - acc: 0.5095 -- iter: 192/197
[A[ATraining Step: 77  | total loss: [1m[32m0.69359[0m[0m | time: 4.318s
[2K
| Adam | epoch: 011 | loss: 0.69359 - acc: 0.5184 | val_loss: 0.69371 - val_acc: 0.5000 -- iter: 197/197
--
Training Step: 78  | total loss: [1m[32m0.69429[0m[0m | time: 0.617s
[2K
| Adam | epoch: 012 | loss: 0.69429 - acc: 0.5067 -- iter: 032/197
[A[ATraining Step: 79  | total loss: [1m[32m0.69242[0m[0m | time: 0.753s
[2K
| Adam | epoch: 012 | loss: 0.69242 - acc: 0.5319 -- iter: 064/197
[A[ATraining Step: 80  | total loss: [1m[32m0.69326[0m[0m | time: 0.890s
[2K
| Adam | epoch: 012 | loss: 0.69326 - acc: 0.5184 -- iter: 096/197
[A[ATraining Step: 81  | total loss: [1m[32m0.69404[0m[0m | time: 1.491s
[2K
| Adam | epoch: 012 | loss: 0.69404 - acc: 0.5064 -- iter: 128/197
[A[ATraining Step: 82  | total loss: [1m[32m0.69381[0m[0m | time: 2.095s
[2K
| Adam | epoch: 012 | loss: 0.69381 - acc: 0.5089 -- iter: 160/197
[A[ATraining Step: 83  | total loss: [1m[32m0.69380[0m[0m | time: 2.697s
[2K
| Adam | epoch: 012 | loss: 0.69380 - acc: 0.5080 -- iter: 192/197
[A[ATraining Step: 84  | total loss: [1m[32m0.69338[0m[0m | time: 4.303s
[2K
| Adam | epoch: 012 | loss: 0.69338 - acc: 0.5135 | val_loss: 0.69360 - val_acc: 0.5000 -- iter: 197/197
--
Training Step: 85  | total loss: [1m[32m0.69297[0m[0m | time: 0.635s
[2K
| Adam | epoch: 013 | loss: 0.69297 - acc: 0.5184 -- iter: 032/197
[A[ATraining Step: 86  | total loss: [1m[32m0.69242[0m[0m | time: 1.250s
[2K
| Adam | epoch: 013 | loss: 0.69242 - acc: 0.5259 -- iter: 064/197
[A[ATraining Step: 87  | total loss: [1m[32m0.69257[0m[0m | time: 1.388s
[2K
| Adam | epoch: 013 | loss: 0.69257 - acc: 0.5233 -- iter: 096/197
[A[ATraining Step: 88  | total loss: [1m[32m0.69464[0m[0m | time: 1.524s
[2K
| Adam | epoch: 013 | loss: 0.69464 - acc: 0.4910 -- iter: 128/197
[A[ATraining Step: 89  | total loss: [1m[32m0.69652[0m[0m | time: 2.121s
[2K
| Adam | epoch: 013 | loss: 0.69652 - acc: 0.4619 -- iter: 160/197
[A[ATraining Step: 90  | total loss: [1m[32m0.69547[0m[0m | time: 2.722s
[2K
| Adam | epoch: 013 | loss: 0.69547 - acc: 0.4782 -- iter: 192/197
[A[ATraining Step: 91  | total loss: [1m[32m0.69529[0m[0m | time: 4.310s
[2K
| Adam | epoch: 013 | loss: 0.69529 - acc: 0.4804 | val_loss: 0.69332 - val_acc: 0.5000 -- iter: 197/197
--
Training Step: 92  | total loss: [1m[32m0.69510[0m[0m | time: 0.618s
[2K
| Adam | epoch: 014 | loss: 0.69510 - acc: 0.4823 -- iter: 032/197
[A[ATraining Step: 93  | total loss: [1m[32m0.69508[0m[0m | time: 1.224s
[2K
| Adam | epoch: 014 | loss: 0.69508 - acc: 0.4810 -- iter: 064/197
[A[ATraining Step: 94  | total loss: [1m[32m0.69441[0m[0m | time: 1.821s
[2K
| Adam | epoch: 014 | loss: 0.69441 - acc: 0.4923 -- iter: 096/197
[A[ATraining Step: 95  | total loss: [1m[32m0.69381[0m[0m | time: 1.957s
[2K
| Adam | epoch: 014 | loss: 0.69381 - acc: 0.5024 -- iter: 128/197
[A[ATraining Step: 96  | total loss: [1m[32m0.69430[0m[0m | time: 2.083s
[2K
| Adam | epoch: 014 | loss: 0.69430 - acc: 0.4922 -- iter: 160/197
[A[ATraining Step: 97  | total loss: [1m[32m0.69469[0m[0m | time: 2.698s
[2K
| Adam | epoch: 014 | loss: 0.69469 - acc: 0.4829 -- iter: 192/197
[A[ATraining Step: 98  | total loss: [1m[32m0.69408[0m[0m | time: 4.317s
[2K
| Adam | epoch: 014 | loss: 0.69408 - acc: 0.4940 | val_loss: 0.69286 - val_acc: 0.5000 -- iter: 197/197
--
Training Step: 99  | total loss: [1m[32m0.69326[0m[0m | time: 0.614s
[2K
| Adam | epoch: 015 | loss: 0.69326 - acc: 0.5102 -- iter: 032/197
[A[ATraining Step: 100  | total loss: [1m[32m0.69276[0m[0m | time: 1.216s
[2K
| Adam | epoch: 015 | loss: 0.69276 - acc: 0.5186 -- iter: 064/197
[A[ATraining Step: 101  | total loss: [1m[32m0.69276[0m[0m | time: 1.818s
[2K
| Adam | epoch: 015 | loss: 0.69276 - acc: 0.5167 -- iter: 096/197
[A[ATraining Step: 102  | total loss: [1m[32m0.69258[0m[0m | time: 2.429s
[2K
| Adam | epoch: 015 | loss: 0.69258 - acc: 0.5182 -- iter: 128/197
[A[ATraining Step: 103  | total loss: [1m[32m0.69327[0m[0m | time: 2.577s
[2K
| Adam | epoch: 015 | loss: 0.69327 - acc: 0.5039 -- iter: 160/197
[A[ATraining Step: 104  | total loss: [1m[32m0.69386[0m[0m | time: 2.709s
[2K
| Adam | epoch: 015 | loss: 0.69386 - acc: 0.4935 -- iter: 192/197
[A[ATraining Step: 105  | total loss: [1m[32m0.69427[0m[0m | time: 4.311s
[2K
| Adam | epoch: 015 | loss: 0.69427 - acc: 0.4841 | val_loss: 0.69251 - val_acc: 0.5000 -- iter: 197/197
--
Training Step: 106  | total loss: [1m[32m0.69425[0m[0m | time: 0.618s
[2K
| Adam | epoch: 016 | loss: 0.69425 - acc: 0.4795 -- iter: 032/197
[A[ATraining Step: 107  | total loss: [1m[32m0.69379[0m[0m | time: 1.215s
[2K
| Adam | epoch: 016 | loss: 0.69379 - acc: 0.4909 -- iter: 064/197
[A[ATraining Step: 108  | total loss: [1m[32m0.69353[0m[0m | time: 1.834s
[2K
| Adam | epoch: 016 | loss: 0.69353 - acc: 0.4949 -- iter: 096/197
[A[ATraining Step: 109  | total loss: [1m[32m0.69295[0m[0m | time: 2.438s
[2K
| Adam | epoch: 016 | loss: 0.69295 - acc: 0.5142 -- iter: 128/197
[A[ATraining Step: 110  | total loss: [1m[32m0.69252[0m[0m | time: 3.040s
[2K
| Adam | epoch: 016 | loss: 0.69252 - acc: 0.5221 -- iter: 160/197
[A[ATraining Step: 111  | total loss: [1m[32m0.69285[0m[0m | time: 3.176s
[2K
| Adam | epoch: 016 | loss: 0.69285 - acc: 0.5106 -- iter: 192/197
[A[ATraining Step: 112  | total loss: [1m[32m0.69159[0m[0m | time: 4.313s
[2K
| Adam | epoch: 016 | loss: 0.69159 - acc: 0.5395 | val_loss: 0.69265 - val_acc: 0.5000 -- iter: 197/197
--
Training Step: 113  | total loss: [1m[32m0.69028[0m[0m | time: 0.615s
[2K
| Adam | epoch: 017 | loss: 0.69028 - acc: 0.5656 -- iter: 032/197
[A[ATraining Step: 114  | total loss: [1m[32m0.69048[0m[0m | time: 1.227s
[2K
| Adam | epoch: 017 | loss: 0.69048 - acc: 0.5590 -- iter: 064/197
[A[ATraining Step: 115  | total loss: [1m[32m0.69069[0m[0m | time: 1.838s
[2K
| Adam | epoch: 017 | loss: 0.69069 - acc: 0.5531 -- iter: 096/197
[A[ATraining Step: 116  | total loss: [1m[32m0.68862[0m[0m | time: 2.456s
[2K
| Adam | epoch: 017 | loss: 0.68862 - acc: 0.5728 -- iter: 128/197
[A[ATraining Step: 117  | total loss: [1m[32m0.68869[0m[0m | time: 3.057s
[2K
| Adam | epoch: 017 | loss: 0.68869 - acc: 0.5686 -- iter: 160/197
[A[ATraining Step: 118  | total loss: [1m[32m0.69053[0m[0m | time: 3.662s
[2K
| Adam | epoch: 017 | loss: 0.69053 - acc: 0.5524 -- iter: 192/197
[A[ATraining Step: 119  | total loss: [1m[32m0.69088[0m[0m | time: 4.792s
[2K
| Adam | epoch: 017 | loss: 0.69088 - acc: 0.5472 | val_loss: 0.69132 - val_acc: 0.5000 -- iter: 197/197
--
Training Step: 120  | total loss: [1m[32m0.69251[0m[0m | time: 0.199s
[2K
| Adam | epoch: 018 | loss: 0.69251 - acc: 0.5324 -- iter: 032/197
[A[ATraining Step: 121  | total loss: [1m[32m0.69366[0m[0m | time: 0.803s
[2K
| Adam | epoch: 018 | loss: 0.69366 - acc: 0.5192 -- iter: 064/197
[A[ATraining Step: 122  | total loss: [1m[32m0.69301[0m[0m | time: 1.410s
[2K
| Adam | epoch: 018 | loss: 0.69301 - acc: 0.5173 -- iter: 096/197
[A[ATraining Step: 123  | total loss: [1m[32m0.69332[0m[0m | time: 2.012s
[2K
| Adam | epoch: 018 | loss: 0.69332 - acc: 0.4999 -- iter: 128/197
[A[ATraining Step: 124  | total loss: [1m[32m0.69291[0m[0m | time: 2.612s
[2K
| Adam | epoch: 018 | loss: 0.69291 - acc: 0.5218 -- iter: 160/197
[A[ATraining Step: 125  | total loss: [1m[32m0.69258[0m[0m | time: 3.219s
[2K
| Adam | epoch: 018 | loss: 0.69258 - acc: 0.5415 -- iter: 192/197
[A[ATraining Step: 126  | total loss: [1m[32m0.69212[0m[0m | time: 4.827s
[2K
| Adam | epoch: 018 | loss: 0.69212 - acc: 0.5592 | val_loss: 0.68869 - val_acc: 0.5000 -- iter: 197/197
--
Training Step: 127  | total loss: [1m[32m0.69161[0m[0m | time: 0.139s
[2K
| Adam | epoch: 019 | loss: 0.69161 - acc: 0.5658 -- iter: 032/197
[A[ATraining Step: 128  | total loss: [1m[32m0.69084[0m[0m | time: 0.261s
[2K
| Adam | epoch: 019 | loss: 0.69084 - acc: 0.5692 -- iter: 064/197
[A[ATraining Step: 129  | total loss: [1m[32m0.68944[0m[0m | time: 0.863s
[2K
| Adam | epoch: 019 | loss: 0.68944 - acc: 0.5723 -- iter: 096/197
[A[ATraining Step: 130  | total loss: [1m[32m0.68717[0m[0m | time: 1.466s
[2K
| Adam | epoch: 019 | loss: 0.68717 - acc: 0.5838 -- iter: 128/197
[A[ATraining Step: 131  | total loss: [1m[32m0.68688[0m[0m | time: 2.060s
[2K
| Adam | epoch: 019 | loss: 0.68688 - acc: 0.5786 -- iter: 160/197
[A[ATraining Step: 132  | total loss: [1m[32m0.69033[0m[0m | time: 2.663s
[2K
| Adam | epoch: 019 | loss: 0.69033 - acc: 0.5551 -- iter: 192/197
[A[ATraining Step: 133  | total loss: [1m[32m0.68725[0m[0m | time: 4.289s
[2K
| Adam | epoch: 019 | loss: 0.68725 - acc: 0.5652 | val_loss: 0.68016 - val_acc: 0.5161 -- iter: 197/197
--
Training Step: 134  | total loss: [1m[32m0.68704[0m[0m | time: 0.619s
[2K
| Adam | epoch: 020 | loss: 0.68704 - acc: 0.5556 -- iter: 032/197
[A[ATraining Step: 135  | total loss: [1m[32m0.68584[0m[0m | time: 0.754s
[2K
| Adam | epoch: 020 | loss: 0.68584 - acc: 0.5562 -- iter: 064/197
[A[ATraining Step: 136  | total loss: [1m[32m0.68184[0m[0m | time: 0.890s
[2K
| Adam | epoch: 020 | loss: 0.68184 - acc: 0.5806 -- iter: 096/197
[A[ATraining Step: 137  | total loss: [1m[32m0.68040[0m[0m | time: 1.501s
[2K
| Adam | epoch: 020 | loss: 0.68040 - acc: 0.5826 -- iter: 128/197
[A[ATraining Step: 138  | total loss: [1m[32m0.67731[0m[0m | time: 2.102s
[2K
| Adam | epoch: 020 | loss: 0.67731 - acc: 0.5837 -- iter: 160/197
[A[ATraining Step: 139  | total loss: [1m[32m0.67382[0m[0m | time: 2.699s
[2K
| Adam | epoch: 020 | loss: 0.67382 - acc: 0.5909 -- iter: 192/197
[A[ATraining Step: 140  | total loss: [1m[32m0.67287[0m[0m | time: 4.309s
[2K
| Adam | epoch: 020 | loss: 0.67287 - acc: 0.5912 | val_loss: 0.62522 - val_acc: 0.6613 -- iter: 197/197
--
Training Step: 141  | total loss: [1m[32m0.66649[0m[0m | time: 0.616s
[2K
| Adam | epoch: 021 | loss: 0.66649 - acc: 0.6102 -- iter: 032/197
[A[ATraining Step: 142  | total loss: [1m[32m0.65828[0m[0m | time: 1.250s
[2K
| Adam | epoch: 021 | loss: 0.65828 - acc: 0.6211 -- iter: 064/197
[A[ATraining Step: 143  | total loss: [1m[32m0.65550[0m[0m | time: 1.377s
[2K
| Adam | epoch: 021 | loss: 0.65550 - acc: 0.6215 -- iter: 096/197
[A[ATraining Step: 144  | total loss: [1m[32m0.68422[0m[0m | time: 1.504s
[2K
| Adam | epoch: 021 | loss: 0.68422 - acc: 0.5993 -- iter: 128/197
[A[ATraining Step: 145  | total loss: [1m[32m0.68054[0m[0m | time: 2.100s
[2K
| Adam | epoch: 021 | loss: 0.68054 - acc: 0.5994 -- iter: 160/197
[A[ATraining Step: 146  | total loss: [1m[32m0.71686[0m[0m | time: 2.691s
[2K
| Adam | epoch: 021 | loss: 0.71686 - acc: 0.5738 -- iter: 192/197
[A[ATraining Step: 147  | total loss: [1m[32m0.72037[0m[0m | time: 4.295s
[2K
| Adam | epoch: 021 | loss: 0.72037 - acc: 0.5633 | val_loss: 0.67589 - val_acc: 0.6290 -- iter: 197/197
--
Training Step: 148  | total loss: [1m[32m0.71550[0m[0m | time: 0.610s
[2K
| Adam | epoch: 022 | loss: 0.71550 - acc: 0.5632 -- iter: 032/197
[A[ATraining Step: 149  | total loss: [1m[32m0.71054[0m[0m | time: 1.232s
[2K
| Adam | epoch: 022 | loss: 0.71054 - acc: 0.5757 -- iter: 064/197
[A[ATraining Step: 150  | total loss: [1m[32m0.70682[0m[0m | time: 1.830s
[2K
| Adam | epoch: 022 | loss: 0.70682 - acc: 0.5806 -- iter: 096/197
[A[ATraining Step: 151  | total loss: [1m[32m0.70430[0m[0m | time: 1.970s
[2K
| Adam | epoch: 022 | loss: 0.70430 - acc: 0.5757 -- iter: 128/197
[A[ATraining Step: 152  | total loss: [1m[32m0.70184[0m[0m | time: 2.108s
[2K
| Adam | epoch: 022 | loss: 0.70184 - acc: 0.5781 -- iter: 160/197
[A[ATraining Step: 153  | total loss: [1m[32m0.69829[0m[0m | time: 2.713s
[2K
| Adam | epoch: 022 | loss: 0.69829 - acc: 0.5803 -- iter: 192/197
[A[ATraining Step: 154  | total loss: [1m[32m0.69786[0m[0m | time: 4.316s
[2K
| Adam | epoch: 022 | loss: 0.69786 - acc: 0.5660 | val_loss: 0.68270 - val_acc: 0.5000 -- iter: 197/197
--
Training Step: 155  | total loss: [1m[32m0.69094[0m[0m | time: 0.626s
[2K
| Adam | epoch: 023 | loss: 0.69094 - acc: 0.5938 -- iter: 032/197
[A[ATraining Step: 156  | total loss: [1m[32m0.69135[0m[0m | time: 1.246s
[2K
| Adam | epoch: 023 | loss: 0.69135 - acc: 0.5844 -- iter: 064/197
[A[ATraining Step: 157  | total loss: [1m[32m0.69017[0m[0m | time: 1.848s
[2K
| Adam | epoch: 023 | loss: 0.69017 - acc: 0.5791 -- iter: 096/197
[A[ATraining Step: 158  | total loss: [1m[32m0.69069[0m[0m | time: 2.443s
[2K
| Adam | epoch: 023 | loss: 0.69069 - acc: 0.5649 -- iter: 128/197
[A[ATraining Step: 159  | total loss: [1m[32m0.68855[0m[0m | time: 2.580s
[2K
| Adam | epoch: 023 | loss: 0.68855 - acc: 0.5647 -- iter: 160/197
[A[ATraining Step: 160  | total loss: [1m[32m0.68362[0m[0m | time: 2.706s
[2K
| Adam | epoch: 023 | loss: 0.68362 - acc: 0.5682 -- iter: 192/197
[A[ATraining Step: 161  | total loss: [1m[32m0.67852[0m[0m | time: 4.294s
[2K
| Adam | epoch: 023 | loss: 0.67852 - acc: 0.5714 | val_loss: 0.66761 - val_acc: 0.5323 -- iter: 197/197
--
Training Step: 162  | total loss: [1m[32m0.67646[0m[0m | time: 0.610s
[2K
| Adam | epoch: 024 | loss: 0.67646 - acc: 0.5674 -- iter: 032/197
[A[ATraining Step: 163  | total loss: [1m[32m0.67254[0m[0m | time: 1.211s
[2K
| Adam | epoch: 024 | loss: 0.67254 - acc: 0.5700 -- iter: 064/197
[A[ATraining Step: 164  | total loss: [1m[32m0.66382[0m[0m | time: 1.807s
[2K
| Adam | epoch: 024 | loss: 0.66382 - acc: 0.5786 -- iter: 096/197
[A[ATraining Step: 165  | total loss: [1m[32m0.66512[0m[0m | time: 2.425s
[2K
| Adam | epoch: 024 | loss: 0.66512 - acc: 0.5708 -- iter: 128/197
[A[ATraining Step: 166  | total loss: [1m[32m0.66535[0m[0m | time: 3.058s
[2K
| Adam | epoch: 024 | loss: 0.66535 - acc: 0.5637 -- iter: 160/197
[A[ATraining Step: 167  | total loss: [1m[32m0.66581[0m[0m | time: 3.197s
[2K
| Adam | epoch: 024 | loss: 0.66581 - acc: 0.5636 -- iter: 192/197
[A[ATraining Step: 168  | total loss: [1m[32m0.65797[0m[0m | time: 4.338s
[2K
| Adam | epoch: 024 | loss: 0.65797 - acc: 0.5872 | val_loss: 0.66232 - val_acc: 0.6613 -- iter: 197/197
--
Training Step: 169  | total loss: [1m[32m0.65309[0m[0m | time: 0.626s
[2K
| Adam | epoch: 025 | loss: 0.65309 - acc: 0.6085 -- iter: 032/197
[A[ATraining Step: 170  | total loss: [1m[32m0.65001[0m[0m | time: 1.241s
[2K
| Adam | epoch: 025 | loss: 0.65001 - acc: 0.6195 -- iter: 064/197
[A[ATraining Step: 171  | total loss: [1m[32m0.64205[0m[0m | time: 1.846s
[2K
| Adam | epoch: 025 | loss: 0.64205 - acc: 0.6388 -- iter: 096/197
[A[ATraining Step: 172  | total loss: [1m[32m0.64708[0m[0m | time: 2.450s
[2K
| Adam | epoch: 025 | loss: 0.64708 - acc: 0.6343 -- iter: 128/197
[A[ATraining Step: 173  | total loss: [1m[32m0.64236[0m[0m | time: 3.066s
[2K
| Adam | epoch: 025 | loss: 0.64236 - acc: 0.6365 -- iter: 160/197
[A[ATraining Step: 174  | total loss: [1m[32m0.64102[0m[0m | time: 3.677s
[2K
| Adam | epoch: 025 | loss: 0.64102 - acc: 0.6541 -- iter: 192/197
[A[ATraining Step: 175  | total loss: [1m[32m0.63798[0m[0m | time: 4.816s
[2K
| Adam | epoch: 025 | loss: 0.63798 - acc: 0.6606 | val_loss: 0.60689 - val_acc: 0.6935 -- iter: 197/197
--
Training Step: 176  | total loss: [1m[32m0.63509[0m[0m | time: 0.146s
[2K
| Adam | epoch: 026 | loss: 0.63509 - acc: 0.6545 -- iter: 032/197
[A[ATraining Step: 177  | total loss: [1m[32m0.61325[0m[0m | time: 0.747s
[2K
| Adam | epoch: 026 | loss: 0.61325 - acc: 0.6891 -- iter: 064/197
[A[ATraining Step: 178  | total loss: [1m[32m0.60833[0m[0m | time: 1.378s
[2K
| Adam | epoch: 026 | loss: 0.60833 - acc: 0.6952 -- iter: 096/197
[A[ATraining Step: 179  | total loss: [1m[32m0.60455[0m[0m | time: 1.982s
[2K
| Adam | epoch: 026 | loss: 0.60455 - acc: 0.6944 -- iter: 128/197
[A[ATraining Step: 180  | total loss: [1m[32m0.60210[0m[0m | time: 2.581s
[2K
| Adam | epoch: 026 | loss: 0.60210 - acc: 0.6937 -- iter: 160/197
[A[ATraining Step: 181  | total loss: [1m[32m0.60001[0m[0m | time: 3.193s
[2K
| Adam | epoch: 026 | loss: 0.60001 - acc: 0.6931 -- iter: 192/197
[A[ATraining Step: 182  | total loss: [1m[32m0.59570[0m[0m | time: 4.796s
[2K
| Adam | epoch: 026 | loss: 0.59570 - acc: 0.6925 | val_loss: 0.64155 - val_acc: 0.6452 -- iter: 197/197
--
Training Step: 183  | total loss: [1m[32m0.59484[0m[0m | time: 0.134s
[2K
| Adam | epoch: 027 | loss: 0.59484 - acc: 0.6920 -- iter: 032/197
[A[ATraining Step: 184  | total loss: [1m[32m0.61783[0m[0m | time: 0.269s
[2K
| Adam | epoch: 027 | loss: 0.61783 - acc: 0.6828 -- iter: 064/197
[A[ATraining Step: 185  | total loss: [1m[32m0.63536[0m[0m | time: 0.871s
[2K
| Adam | epoch: 027 | loss: 0.63536 - acc: 0.6745 -- iter: 096/197
[A[ATraining Step: 186  | total loss: [1m[32m0.62548[0m[0m | time: 1.474s
[2K
| Adam | epoch: 027 | loss: 0.62548 - acc: 0.6665 -- iter: 128/197
[A[ATraining Step: 187  | total loss: [1m[32m0.62569[0m[0m | time: 2.121s
[2K
| Adam | epoch: 027 | loss: 0.62569 - acc: 0.6654 -- iter: 160/197
[A[ATraining Step: 188  | total loss: [1m[32m0.61850[0m[0m | time: 2.729s
[2K
| Adam | epoch: 027 | loss: 0.61850 - acc: 0.6739 -- iter: 192/197
[A[ATraining Step: 189  | total loss: [1m[32m0.62116[0m[0m | time: 4.352s
[2K
| Adam | epoch: 027 | loss: 0.62116 - acc: 0.6628 | val_loss: 0.64721 - val_acc: 0.6290 -- iter: 197/197
--
Training Step: 190  | total loss: [1m[32m0.61952[0m[0m | time: 0.636s
[2K
| Adam | epoch: 028 | loss: 0.61952 - acc: 0.6652 -- iter: 032/197
[A[ATraining Step: 191  | total loss: [1m[32m0.61647[0m[0m | time: 0.775s
[2K
| Adam | epoch: 028 | loss: 0.61647 - acc: 0.6800 -- iter: 064/197
[A[ATraining Step: 192  | total loss: [1m[32m0.61944[0m[0m | time: 0.915s
[2K
| Adam | epoch: 028 | loss: 0.61944 - acc: 0.6720 -- iter: 096/197
[A[ATraining Step: 193  | total loss: [1m[32m0.61862[0m[0m | time: 1.522s
[2K
| Adam | epoch: 028 | loss: 0.61862 - acc: 0.6848 -- iter: 128/197
[A[ATraining Step: 194  | total loss: [1m[32m0.61808[0m[0m | time: 2.131s
[2K
| Adam | epoch: 028 | loss: 0.61808 - acc: 0.6725 -- iter: 160/197
[A[ATraining Step: 195  | total loss: [1m[32m0.61991[0m[0m | time: 3.279s
[2K
| Adam | epoch: 028 | loss: 0.61991 - acc: 0.6615 -- iter: 192/197
[A[ATraining Step: 196  | total loss: [1m[32m0.61785[0m[0m | time: 5.623s
[2K
| Adam | epoch: 028 | loss: 0.61785 - acc: 0.6610 | val_loss: 0.58795 - val_acc: 0.6613 -- iter: 197/197
--
Training Step: 197  | total loss: [1m[32m0.61110[0m[0m | time: 3.536s
[2K
| Adam | epoch: 029 | loss: 0.61110 - acc: 0.6730 -- iter: 032/197
[A[ATraining Step: 198  | total loss: [1m[32m0.60638[0m[0m | time: 8.219s
[2K
| Adam | epoch: 029 | loss: 0.60638 - acc: 0.6776 -- iter: 064/197
[A[ATraining Step: 199  | total loss: [1m[32m0.60645[0m[0m | time: 8.841s
[2K
| Adam | epoch: 029 | loss: 0.60645 - acc: 0.6755 -- iter: 096/197
[A[ATraining Step: 200  | total loss: [1m[32m0.59556[0m[0m | time: 11.461s
[2K
| Adam | epoch: 029 | loss: 0.59556 - acc: 0.6879 | val_loss: 0.56310 - val_acc: 0.6935 -- iter: 128/197
--
Training Step: 201  | total loss: [1m[32m0.56883[0m[0m | time: 14.728s
[2K
| Adam | epoch: 029 | loss: 0.56883 - acc: 0.7191 -- iter: 160/197
[A[ATraining Step: 202  | total loss: [1m[32m0.56309[0m[0m | time: 16.356s
[2K
| Adam | epoch: 029 | loss: 0.56309 - acc: 0.7253 -- iter: 192/197
[A[ATraining Step: 203  | total loss: [1m[32m0.55931[0m[0m | time: 18.299s
[2K
| Adam | epoch: 029 | loss: 0.55931 - acc: 0.7216 | val_loss: 0.55074 - val_acc: 0.6774 -- iter: 197/197
--
Training Step: 204  | total loss: [1m[32m0.57726[0m[0m | time: 1.091s
[2K
| Adam | epoch: 030 | loss: 0.57726 - acc: 0.7025 -- iter: 032/197
[A[ATraining Step: 205  | total loss: [1m[32m0.56746[0m[0m | time: 1.797s
[2K
| Adam | epoch: 030 | loss: 0.56746 - acc: 0.7010 -- iter: 064/197
[A[ATraining Step: 206  | total loss: [1m[32m0.56611[0m[0m | time: 2.468s
[2K
| Adam | epoch: 030 | loss: 0.56611 - acc: 0.6934 -- iter: 096/197
[A[ATraining Step: 207  | total loss: [1m[32m0.54926[0m[0m | time: 2.737s
[2K
| Adam | epoch: 030 | loss: 0.54926 - acc: 0.7053 -- iter: 128/197
[A[ATraining Step: 208  | total loss: [1m[32m0.53813[0m[0m | time: 2.965s
[2K
| Adam | epoch: 030 | loss: 0.53813 - acc: 0.7148 -- iter: 160/197
[A[ATraining Step: 209  | total loss: [1m[32m0.55015[0m[0m | time: 4.170s
[2K
| Adam | epoch: 030 | loss: 0.55015 - acc: 0.7033 -- iter: 192/197
[A[ATraining Step: 210  | total loss: [1m[32m0.53939[0m[0m | time: 9.045s
[2K
| Adam | epoch: 030 | loss: 0.53939 - acc: 0.7111 | val_loss: 0.63729 - val_acc: 0.6613 -- iter: 197/197
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.8064516129032258
Validation AUPRC:0.8231425889210109
Test AUC:0.7621621621621621
Test AUPRC:0.6955535208354803
BestTestF1Score	0.65	0.43	0.73	0.67	0.64	16	8	29	9	0.25
BestTestMCCScore	0.65	0.43	0.73	0.67	0.64	16	8	29	9	0.25
BestTestAccuracyScore	0.6	0.35	0.69	0.64	0.56	14	8	29	11	0.28
BestValidationF1Score	0.75	0.48	0.74	0.73	0.77	24	9	22	7	0.25
BestValidationMCC	0.75	0.48	0.74	0.73	0.77	24	9	22	7	0.25
BestValidationAccuracy	0.74	0.48	0.74	0.74	0.74	23	8	23	8	0.28
TestPredictions (Threshold:0.25)
CHEMBL3693931,FP,INACT,0.5400000214576721	CHEMBL3693892,TP,ACT,0.4000000059604645	CHEMBL3693908,TP,ACT,0.25999999046325684	CHEMBL3696146,TN,INACT,0.029999999329447746	CHEMBL201662,TN,INACT,0.09000000357627869	CHEMBL3693906,TP,ACT,0.3400000035762787	CHEMBL3693875,FN,ACT,0.10999999940395355	CHEMBL3692082,TN,INACT,0.09000000357627869	CHEMBL51529,TN,INACT,0.03999999910593033	CHEMBL341806,TN,INACT,0.07000000029802322	CHEMBL3692074,TN,INACT,0.029999999329447746	CHEMBL500677,FN,ACT,0.05000000074505806	CHEMBL611317,FN,ACT,0.23000000417232513	CHEMBL472400,TN,INACT,0.05000000074505806	CHEMBL3693891,TP,ACT,0.5099999904632568	CHEMBL1240790,FP,INACT,0.3400000035762787	CHEMBL3403305,TN,INACT,0.07999999821186066	CHEMBL590471,TP,ACT,0.8199999928474426	CHEMBL3781939,TN,INACT,0.07000000029802322	CHEMBL1822606,FP,INACT,0.3700000047683716	CHEMBL3692076,TN,INACT,0.029999999329447746	CHEMBL363535,FN,ACT,0.14000000059604645	CHEMBL2316906,TP,ACT,0.5400000214576721	CHEMBL336908,TN,INACT,0.15000000596046448	CHEMBL573989,TN,INACT,0.14000000059604645	CHEMBL3693882,TP,ACT,0.4699999988079071	CHEMBL412186,TN,INACT,0.14000000059604645	CHEMBL602471,TN,INACT,0.10999999940395355	CHEMBL502006,FN,ACT,0.07000000029802322	CHEMBL414193,TN,INACT,0.1599999964237213	CHEMBL3693885,TP,ACT,0.41999998688697815	CHEMBL3693912,TP,ACT,0.4699999988079071	CHEMBL461199,FN,ACT,0.14000000059604645	CHEMBL276648,TN,INACT,0.05999999865889549	CHEMBL2316894,TP,ACT,0.4000000059604645	CHEMBL2430648,TN,INACT,0.18000000715255737	CHEMBL1795950,TN,INACT,0.15000000596046448	CHEMBL3780571,TN,INACT,0.15000000596046448	CHEMBL3319363,TP,ACT,0.7900000214576721	CHEMBL2159930,FP,INACT,0.550000011920929	CHEMBL3692079,TN,INACT,0.03999999910593033	CHEMBL2316901,TN,INACT,0.10000000149011612	CHEMBL3402404,TP,ACT,0.3400000035762787	CHEMBL266474,FP,INACT,0.3499999940395355	CHEMBL1240791,FP,INACT,0.3100000023841858	CHEMBL1360563,FN,ACT,0.14000000059604645	CHEMBL1240534,TN,INACT,0.03999999910593033	CHEMBL502257,TN,INACT,0.05000000074505806	CHEMBL3692100,TN,INACT,0.07999999821186066	CHEMBL2159725,TN,INACT,0.1899999976158142	CHEMBL3692121,TN,INACT,0.029999999329447746	CHEMBL416145,TN,INACT,0.10000000149011612	CHEMBL502540,FN,ACT,0.07000000029802322	CHEMBL344437,TN,INACT,0.23000000417232513	CHEMBL2396719,FN,ACT,0.10999999940395355	CHEMBL1822607,FP,INACT,0.3700000047683716	CHEMBL3693936,FP,INACT,0.4099999964237213	CHEMBL2159931,TN,INACT,0.03999999910593033	CHEMBL3693911,TP,ACT,0.3199999928474426	CHEMBL227622,TP,ACT,0.25	CHEMBL3693926,TP,ACT,0.5299999713897705	CHEMBL2436032,TP,ACT,0.6499999761581421	

