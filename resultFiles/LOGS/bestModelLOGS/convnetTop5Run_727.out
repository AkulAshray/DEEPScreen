ImageNetInceptionV2 CHEMBL4070 adam 0.001 15 0 0 0.6 False True
Number of active compounds :	181
Number of inactive compounds :	181
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL4070_adam_0.001_15_0_0_0.6_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL4070_adam_0.001_15_0.6/
---------------------------------
Training samples: 215
Validation samples: 68
--
Training Step: 1  | time: 92.251s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/215
[A[ATraining Step: 2  | total loss: [1m[32m0.72156[0m[0m | time: 147.860s
[2K
| Adam | epoch: 001 | loss: 0.72156 - acc: 0.3094 -- iter: 064/215
[A[ATraining Step: 3  | total loss: [1m[32m0.85037[0m[0m | time: 198.508s
[2K
| Adam | epoch: 001 | loss: 0.85037 - acc: 0.4142 -- iter: 096/215
[A[ATraining Step: 4  | total loss: [1m[32m0.75236[0m[0m | time: 253.749s
[2K
| Adam | epoch: 001 | loss: 0.75236 - acc: 0.6192 -- iter: 128/215
[A[ATraining Step: 5  | total loss: [1m[32m0.63929[0m[0m | time: 293.474s
[2K
| Adam | epoch: 001 | loss: 0.63929 - acc: 0.7530 -- iter: 160/215
[A[ATraining Step: 6  | total loss: [1m[32m0.48113[0m[0m | time: 324.579s
[2K
| Adam | epoch: 001 | loss: 0.48113 - acc: 0.7913 -- iter: 192/215
[A[ATraining Step: 7  | total loss: [1m[32m0.44406[0m[0m | time: 348.824s
[2K
| Adam | epoch: 001 | loss: 0.44406 - acc: 0.7853 | val_loss: 2.57388 - val_acc: 0.4412 -- iter: 215/215
--
Training Step: 8  | total loss: [1m[32m0.43534[0m[0m | time: 37.058s
[2K
| Adam | epoch: 002 | loss: 0.43534 - acc: 0.8082 -- iter: 032/215
[A[ATraining Step: 9  | total loss: [1m[32m0.33655[0m[0m | time: 91.349s
[2K
| Adam | epoch: 002 | loss: 0.33655 - acc: 0.8407 -- iter: 064/215
[A[ATraining Step: 10  | total loss: [1m[32m0.31525[0m[0m | time: 126.210s
[2K
| Adam | epoch: 002 | loss: 0.31525 - acc: 0.8578 -- iter: 096/215
[A[ATraining Step: 11  | total loss: [1m[32m0.50665[0m[0m | time: 143.516s
[2K
| Adam | epoch: 002 | loss: 0.50665 - acc: 0.7772 -- iter: 128/215
[A[ATraining Step: 12  | total loss: [1m[32m0.55229[0m[0m | time: 158.344s
[2K
| Adam | epoch: 002 | loss: 0.55229 - acc: 0.7931 -- iter: 160/215
[A[ATraining Step: 13  | total loss: [1m[32m0.66187[0m[0m | time: 171.248s
[2K
| Adam | epoch: 002 | loss: 0.66187 - acc: 0.7076 -- iter: 192/215
[A[ATraining Step: 14  | total loss: [1m[32m0.57697[0m[0m | time: 189.062s
[2K
| Adam | epoch: 002 | loss: 0.57697 - acc: 0.7378 | val_loss: 0.69576 - val_acc: 0.5588 -- iter: 215/215
--
Training Step: 15  | total loss: [1m[32m0.55442[0m[0m | time: 11.898s
[2K
| Adam | epoch: 003 | loss: 0.55442 - acc: 0.7181 -- iter: 032/215
[A[ATraining Step: 16  | total loss: [1m[32m0.45881[0m[0m | time: 22.130s
[2K
| Adam | epoch: 003 | loss: 0.45881 - acc: 0.8075 -- iter: 064/215
[A[ATraining Step: 17  | total loss: [1m[32m0.34407[0m[0m | time: 39.530s
[2K
| Adam | epoch: 003 | loss: 0.34407 - acc: 0.8768 -- iter: 096/215
[A[ATraining Step: 18  | total loss: [1m[32m0.39440[0m[0m | time: 53.919s
[2K
| Adam | epoch: 003 | loss: 0.39440 - acc: 0.8545 -- iter: 128/215
[A[ATraining Step: 19  | total loss: [1m[32m0.38795[0m[0m | time: 62.618s
[2K
| Adam | epoch: 003 | loss: 0.38795 - acc: 0.8509 -- iter: 160/215
[A[ATraining Step: 20  | total loss: [1m[32m0.34424[0m[0m | time: 90.431s
[2K
| Adam | epoch: 003 | loss: 0.34424 - acc: 0.8486 -- iter: 192/215
[A[ATraining Step: 21  | total loss: [1m[32m0.31927[0m[0m | time: 109.842s
[2K
| Adam | epoch: 003 | loss: 0.31927 - acc: 0.8665 | val_loss: 0.98611 - val_acc: 0.4412 -- iter: 215/215
--
Training Step: 22  | total loss: [1m[32m0.34186[0m[0m | time: 42.734s
[2K
| Adam | epoch: 004 | loss: 0.34186 - acc: 0.8597 -- iter: 032/215
[A[ATraining Step: 23  | total loss: [1m[32m0.29614[0m[0m | time: 87.037s
[2K
| Adam | epoch: 004 | loss: 0.29614 - acc: 0.8732 -- iter: 064/215
[A[ATraining Step: 24  | total loss: [1m[32m0.27176[0m[0m | time: 96.414s
[2K
| Adam | epoch: 004 | loss: 0.27176 - acc: 0.8966 -- iter: 096/215
[A[ATraining Step: 25  | total loss: [1m[32m0.21003[0m[0m | time: 105.144s
[2K
| Adam | epoch: 004 | loss: 0.21003 - acc: 0.9248 -- iter: 128/215
[A[ATraining Step: 26  | total loss: [1m[32m0.25058[0m[0m | time: 113.685s
[2K
| Adam | epoch: 004 | loss: 0.25058 - acc: 0.8951 -- iter: 160/215
[A[ATraining Step: 27  | total loss: [1m[32m0.23608[0m[0m | time: 145.718s
[2K
| Adam | epoch: 004 | loss: 0.23608 - acc: 0.9060 -- iter: 192/215
[A[ATraining Step: 28  | total loss: [1m[32m0.25119[0m[0m | time: 165.530s
[2K
| Adam | epoch: 004 | loss: 0.25119 - acc: 0.9061 | val_loss: 1.66353 - val_acc: 0.4412 -- iter: 215/215
--
Training Step: 29  | total loss: [1m[32m0.22495[0m[0m | time: 43.121s
[2K
| Adam | epoch: 005 | loss: 0.22495 - acc: 0.9213 -- iter: 032/215
[A[ATraining Step: 30  | total loss: [1m[32m0.25166[0m[0m | time: 96.384s
[2K
| Adam | epoch: 005 | loss: 0.25166 - acc: 0.9177 -- iter: 064/215
[A[ATraining Step: 31  | total loss: [1m[32m0.20914[0m[0m | time: 105.689s
[2K
| Adam | epoch: 005 | loss: 0.20914 - acc: 0.9367 -- iter: 096/215
[A[ATraining Step: 32  | total loss: [1m[32m0.20004[0m[0m | time: 112.267s
[2K
| Adam | epoch: 005 | loss: 0.20004 - acc: 0.9412 -- iter: 128/215
[A[ATraining Step: 33  | total loss: [1m[32m0.16408[0m[0m | time: 121.140s
[2K
| Adam | epoch: 005 | loss: 0.16408 - acc: 0.9541 -- iter: 160/215
[A[ATraining Step: 34  | total loss: [1m[32m0.15002[0m[0m | time: 131.010s
[2K
| Adam | epoch: 005 | loss: 0.15002 - acc: 0.9572 -- iter: 192/215
[A[ATraining Step: 35  | total loss: [1m[32m0.13397[0m[0m | time: 150.634s
[2K
| Adam | epoch: 005 | loss: 0.13397 - acc: 0.9596 | val_loss: 3.30335 - val_acc: 0.4412 -- iter: 215/215
--
Training Step: 36  | total loss: [1m[32m0.13163[0m[0m | time: 13.862s
[2K
| Adam | epoch: 006 | loss: 0.13163 - acc: 0.9551 -- iter: 032/215
[A[ATraining Step: 37  | total loss: [1m[32m0.12547[0m[0m | time: 33.895s
[2K
| Adam | epoch: 006 | loss: 0.12547 - acc: 0.9516 -- iter: 064/215
[A[ATraining Step: 38  | total loss: [1m[32m0.11162[0m[0m | time: 48.499s
[2K
| Adam | epoch: 006 | loss: 0.11162 - acc: 0.9549 -- iter: 096/215
[A[ATraining Step: 39  | total loss: [1m[32m0.10613[0m[0m | time: 55.095s
[2K
| Adam | epoch: 006 | loss: 0.10613 - acc: 0.9636 -- iter: 128/215
[A[ATraining Step: 40  | total loss: [1m[32m0.11198[0m[0m | time: 61.750s
[2K
| Adam | epoch: 006 | loss: 0.11198 - acc: 0.9623 -- iter: 160/215
[A[ATraining Step: 41  | total loss: [1m[32m0.10441[0m[0m | time: 72.709s
[2K
| Adam | epoch: 006 | loss: 0.10441 - acc: 0.9692 -- iter: 192/215
[A[ATraining Step: 42  | total loss: [1m[32m0.08686[0m[0m | time: 95.492s
[2K
| Adam | epoch: 006 | loss: 0.08686 - acc: 0.9747 | val_loss: 5.08157 - val_acc: 0.4412 -- iter: 215/215
--
Training Step: 43  | total loss: [1m[32m0.07922[0m[0m | time: 44.279s
[2K
| Adam | epoch: 007 | loss: 0.07922 - acc: 0.9737 -- iter: 032/215
[A[ATraining Step: 44  | total loss: [1m[32m0.06640[0m[0m | time: 55.701s
[2K
| Adam | epoch: 007 | loss: 0.06640 - acc: 0.9782 -- iter: 064/215
[A[ATraining Step: 45  | total loss: [1m[32m0.05880[0m[0m | time: 72.023s
[2K
| Adam | epoch: 007 | loss: 0.05880 - acc: 0.9819 -- iter: 096/215
[A[ATraining Step: 46  | total loss: [1m[32m0.07902[0m[0m | time: 80.691s
[2K
| Adam | epoch: 007 | loss: 0.07902 - acc: 0.9745 -- iter: 128/215
[A[ATraining Step: 47  | total loss: [1m[32m0.10095[0m[0m | time: 87.077s
[2K
| Adam | epoch: 007 | loss: 0.10095 - acc: 0.9634 -- iter: 160/215
[A[ATraining Step: 48  | total loss: [1m[32m0.21718[0m[0m | time: 93.534s
[2K
| Adam | epoch: 007 | loss: 0.21718 - acc: 0.9273 -- iter: 192/215
[A[ATraining Step: 49  | total loss: [1m[32m0.21325[0m[0m | time: 105.718s
[2K
| Adam | epoch: 007 | loss: 0.21325 - acc: 0.9251 | val_loss: 2.47711 - val_acc: 0.4118 -- iter: 215/215
--
Training Step: 50  | total loss: [1m[32m0.18104[0m[0m | time: 14.408s
[2K
| Adam | epoch: 008 | loss: 0.18104 - acc: 0.9367 -- iter: 032/215
[A[ATraining Step: 51  | total loss: [1m[32m0.15618[0m[0m | time: 27.076s
[2K
| Adam | epoch: 008 | loss: 0.15618 - acc: 0.9463 -- iter: 064/215
[A[ATraining Step: 52  | total loss: [1m[32m0.13594[0m[0m | time: 40.188s
[2K
| Adam | epoch: 008 | loss: 0.13594 - acc: 0.9544 -- iter: 096/215
[A[ATraining Step: 53  | total loss: [1m[32m0.17743[0m[0m | time: 52.190s
[2K
| Adam | epoch: 008 | loss: 0.17743 - acc: 0.9381 -- iter: 128/215
[A[ATraining Step: 54  | total loss: [1m[32m0.15661[0m[0m | time: 65.104s
[2K
| Adam | epoch: 008 | loss: 0.15661 - acc: 0.9471 -- iter: 160/215
[A[ATraining Step: 55  | total loss: [1m[32m0.15707[0m[0m | time: 71.516s
[2K
| Adam | epoch: 008 | loss: 0.15707 - acc: 0.9457 -- iter: 192/215
[A[ATraining Step: 56  | total loss: [1m[32m0.14266[0m[0m | time: 81.476s
[2K
| Adam | epoch: 008 | loss: 0.14266 - acc: 0.9472 | val_loss: 0.83099 - val_acc: 0.8382 -- iter: 215/215
--
Training Step: 57  | total loss: [1m[32m0.12447[0m[0m | time: 32.090s
[2K
| Adam | epoch: 009 | loss: 0.12447 - acc: 0.9545 -- iter: 032/215
[A[ATraining Step: 58  | total loss: [1m[32m0.11545[0m[0m | time: 46.003s
[2K
| Adam | epoch: 009 | loss: 0.11545 - acc: 0.9565 -- iter: 064/215
[A[ATraining Step: 59  | total loss: [1m[32m0.10576[0m[0m | time: 66.072s
[2K
| Adam | epoch: 009 | loss: 0.10576 - acc: 0.9623 -- iter: 096/215
[A[ATraining Step: 60  | total loss: [1m[32m0.10068[0m[0m | time: 78.773s
[2K
| Adam | epoch: 009 | loss: 0.10068 - acc: 0.9632 -- iter: 128/215
[A[ATraining Step: 61  | total loss: [1m[32m0.09214[0m[0m | time: 88.015s
[2K
| Adam | epoch: 009 | loss: 0.09214 - acc: 0.9639 -- iter: 160/215
[A[ATraining Step: 62  | total loss: [1m[32m0.08305[0m[0m | time: 96.406s
[2K
| Adam | epoch: 009 | loss: 0.08305 - acc: 0.9685 -- iter: 192/215
[A[ATraining Step: 63  | total loss: [1m[32m0.07950[0m[0m | time: 108.380s
[2K
| Adam | epoch: 009 | loss: 0.07950 - acc: 0.9686 | val_loss: 1.32933 - val_acc: 0.7353 -- iter: 215/215
--
Training Step: 64  | total loss: [1m[32m0.07647[0m[0m | time: 9.502s
[2K
| Adam | epoch: 010 | loss: 0.07647 - acc: 0.9671 -- iter: 032/215
[A[ATraining Step: 65  | total loss: [1m[32m0.06766[0m[0m | time: 23.661s
[2K
| Adam | epoch: 010 | loss: 0.06766 - acc: 0.9711 -- iter: 064/215
[A[ATraining Step: 66  | total loss: [1m[32m0.05991[0m[0m | time: 40.130s
[2K
| Adam | epoch: 010 | loss: 0.05991 - acc: 0.9746 -- iter: 096/215
[A[ATraining Step: 67  | total loss: [1m[32m0.05807[0m[0m | time: 52.408s
[2K
| Adam | epoch: 010 | loss: 0.05807 - acc: 0.9739 -- iter: 128/215
[A[ATraining Step: 68  | total loss: [1m[32m0.05159[0m[0m | time: 66.916s
[2K
| Adam | epoch: 010 | loss: 0.05159 - acc: 0.9770 -- iter: 160/215
[A[ATraining Step: 69  | total loss: [1m[32m0.04854[0m[0m | time: 75.604s
[2K
| Adam | epoch: 010 | loss: 0.04854 - acc: 0.9797 -- iter: 192/215
[A[ATraining Step: 70  | total loss: [1m[32m0.04687[0m[0m | time: 87.377s
[2K
| Adam | epoch: 010 | loss: 0.04687 - acc: 0.9820 | val_loss: 0.70451 - val_acc: 0.7941 -- iter: 215/215
--
Training Step: 71  | total loss: [1m[32m0.04287[0m[0m | time: 9.248s
[2K
| Adam | epoch: 011 | loss: 0.04287 - acc: 0.9841 -- iter: 032/215
[A[ATraining Step: 72  | total loss: [1m[32m0.13147[0m[0m | time: 19.324s
[2K
| Adam | epoch: 011 | loss: 0.13147 - acc: 0.9712 -- iter: 064/215
[A[ATraining Step: 73  | total loss: [1m[32m0.11793[0m[0m | time: 35.105s
[2K
| Adam | epoch: 011 | loss: 0.11793 - acc: 0.9744 -- iter: 096/215
[A[ATraining Step: 74  | total loss: [1m[32m0.12339[0m[0m | time: 48.084s
[2K
| Adam | epoch: 011 | loss: 0.12339 - acc: 0.9704 -- iter: 128/215
[A[ATraining Step: 75  | total loss: [1m[32m0.11596[0m[0m | time: 65.900s
[2K
| Adam | epoch: 011 | loss: 0.11596 - acc: 0.9702 -- iter: 160/215
[A[ATraining Step: 76  | total loss: [1m[32m0.10670[0m[0m | time: 74.999s
[2K
| Adam | epoch: 011 | loss: 0.10670 - acc: 0.9734 -- iter: 192/215
[A[ATraining Step: 77  | total loss: [1m[32m0.09850[0m[0m | time: 87.761s
[2K
| Adam | epoch: 011 | loss: 0.09850 - acc: 0.9762 | val_loss: 1.20821 - val_acc: 0.7206 -- iter: 215/215
--
Training Step: 78  | total loss: [1m[32m0.11348[0m[0m | time: 23.250s
[2K
| Adam | epoch: 012 | loss: 0.11348 - acc: 0.9689 -- iter: 032/215
[A[ATraining Step: 79  | total loss: [1m[32m0.13683[0m[0m | time: 36.358s
[2K
| Adam | epoch: 012 | loss: 0.13683 - acc: 0.9559 -- iter: 064/215
[A[ATraining Step: 80  | total loss: [1m[32m0.15943[0m[0m | time: 45.718s
[2K
| Adam | epoch: 012 | loss: 0.15943 - acc: 0.9560 -- iter: 096/215
[A[ATraining Step: 81  | total loss: [1m[32m0.14518[0m[0m | time: 62.078s
[2K
| Adam | epoch: 012 | loss: 0.14518 - acc: 0.9604 -- iter: 128/215
[A[ATraining Step: 82  | total loss: [1m[32m0.15961[0m[0m | time: 76.818s
[2K
| Adam | epoch: 012 | loss: 0.15961 - acc: 0.9488 -- iter: 160/215
[A[ATraining Step: 83  | total loss: [1m[32m0.15536[0m[0m | time: 85.603s
[2K
| Adam | epoch: 012 | loss: 0.15536 - acc: 0.9508 -- iter: 192/215
[A[ATraining Step: 84  | total loss: [1m[32m0.14195[0m[0m | time: 99.252s
[2K
| Adam | epoch: 012 | loss: 0.14195 - acc: 0.9557 | val_loss: 3.06773 - val_acc: 0.4412 -- iter: 215/215
--
Training Step: 85  | total loss: [1m[32m0.13097[0m[0m | time: 53.554s
[2K
| Adam | epoch: 013 | loss: 0.13097 - acc: 0.9601 -- iter: 032/215
[A[ATraining Step: 86  | total loss: [1m[32m0.12567[0m[0m | time: 75.106s
[2K
| Adam | epoch: 013 | loss: 0.12567 - acc: 0.9610 -- iter: 064/215
[A[ATraining Step: 87  | total loss: [1m[32m0.12305[0m[0m | time: 100.926s
[2K
| Adam | epoch: 013 | loss: 0.12305 - acc: 0.9618 -- iter: 096/215
[A[ATraining Step: 88  | total loss: [1m[32m0.14576[0m[0m | time: 110.476s
[2K
| Adam | epoch: 013 | loss: 0.14576 - acc: 0.9569 -- iter: 128/215
[A[ATraining Step: 89  | total loss: [1m[32m0.13366[0m[0m | time: 125.866s
[2K
| Adam | epoch: 013 | loss: 0.13366 - acc: 0.9612 -- iter: 160/215
[A[ATraining Step: 90  | total loss: [1m[32m0.12350[0m[0m | time: 134.560s
[2K
| Adam | epoch: 013 | loss: 0.12350 - acc: 0.9651 -- iter: 192/215
[A[ATraining Step: 91  | total loss: [1m[32m0.11361[0m[0m | time: 147.633s
[2K
| Adam | epoch: 013 | loss: 0.11361 - acc: 0.9686 | val_loss: 0.79910 - val_acc: 0.8088 -- iter: 215/215
--
Training Step: 92  | total loss: [1m[32m0.10372[0m[0m | time: 11.564s
[2K
| Adam | epoch: 014 | loss: 0.10372 - acc: 0.9717 -- iter: 032/215
[A[ATraining Step: 93  | total loss: [1m[32m0.09515[0m[0m | time: 36.967s
[2K
| Adam | epoch: 014 | loss: 0.09515 - acc: 0.9745 -- iter: 064/215
[A[ATraining Step: 94  | total loss: [1m[32m0.08689[0m[0m | time: 89.384s
[2K
| Adam | epoch: 014 | loss: 0.08689 - acc: 0.9771 -- iter: 096/215
[A[ATraining Step: 95  | total loss: [1m[32m0.08382[0m[0m | time: 99.183s
[2K
| Adam | epoch: 014 | loss: 0.08382 - acc: 0.9763 -- iter: 128/215
[A[ATraining Step: 96  | total loss: [1m[32m0.09712[0m[0m | time: 109.074s
[2K
| Adam | epoch: 014 | loss: 0.09712 - acc: 0.9743 -- iter: 160/215
[A[ATraining Step: 97  | total loss: [1m[32m0.08796[0m[0m | time: 117.781s
[2K
| Adam | epoch: 014 | loss: 0.08796 - acc: 0.9769 -- iter: 192/215
[A[ATraining Step: 98  | total loss: [1m[32m0.09140[0m[0m | time: 129.898s
[2K
| Adam | epoch: 014 | loss: 0.09140 - acc: 0.9760 | val_loss: 0.75445 - val_acc: 0.8529 -- iter: 215/215
--
Training Step: 99  | total loss: [1m[32m0.08377[0m[0m | time: 8.387s
[2K
| Adam | epoch: 015 | loss: 0.08377 - acc: 0.9784 -- iter: 032/215
[A[ATraining Step: 100  | total loss: [1m[32m0.07630[0m[0m | time: 16.793s
[2K
| Adam | epoch: 015 | loss: 0.07630 - acc: 0.9806 -- iter: 064/215
[A[ATraining Step: 101  | total loss: [1m[32m0.07304[0m[0m | time: 25.302s
[2K
| Adam | epoch: 015 | loss: 0.07304 - acc: 0.9825 -- iter: 096/215
[A[ATraining Step: 102  | total loss: [1m[32m0.07281[0m[0m | time: 33.390s
[2K
| Adam | epoch: 015 | loss: 0.07281 - acc: 0.9812 -- iter: 128/215
[A[ATraining Step: 103  | total loss: [1m[32m0.06664[0m[0m | time: 39.950s
[2K
| Adam | epoch: 015 | loss: 0.06664 - acc: 0.9830 -- iter: 160/215
[A[ATraining Step: 104  | total loss: [1m[32m0.06032[0m[0m | time: 46.391s
[2K
| Adam | epoch: 015 | loss: 0.06032 - acc: 0.9847 -- iter: 192/215
[A[ATraining Step: 105  | total loss: [1m[32m0.05464[0m[0m | time: 58.017s
[2K
| Adam | epoch: 015 | loss: 0.05464 - acc: 0.9863 | val_loss: 0.62567 - val_acc: 0.8382 -- iter: 215/215
--
Validation AUC:0.9052631578947369
Validation AUPRC:0.8722722706233063
Test AUC:0.931996512641674
Test AUPRC:0.9336063340606353
BestTestF1Score	0.88	0.71	0.85	0.81	0.95	35	8	23	2	0.17
BestTestMCCScore	0.88	0.71	0.85	0.81	0.95	35	8	23	2	0.17
BestTestAccuracyScore	0.89	0.76	0.88	0.87	0.92	34	5	26	3	0.6
BestValidationF1Score	0.88	0.71	0.85	0.82	0.95	36	8	22	2	0.17
BestValidationMCC	0.88	0.71	0.85	0.82	0.95	36	8	22	2	0.17
BestValidationAccuracy	0.86	0.71	0.85	0.89	0.84	32	4	26	6	0.6
TestPredictions (Threshold:0.17)
CHEMBL498248,TN,INACT,0.11999999731779099	CHEMBL1928909,TP,ACT,0.9900000095367432	CHEMBL3681206,TP,ACT,1.0	CHEMBL1784649,TN,INACT,0.09000000357627869	CHEMBL176815,TN,INACT,0.019999999552965164	CHEMBL2392235,TN,INACT,0.009999999776482582	CHEMBL3681205,TP,ACT,0.9700000286102295	CHEMBL3681221,TP,ACT,1.0	CHEMBL2392240,TN,INACT,0.009999999776482582	CHEMBL102136,TN,INACT,0.009999999776482582	CHEMBL469346,TN,INACT,0.009999999776482582	CHEMBL1287945,FP,INACT,0.47999998927116394	CHEMBL498705,TN,INACT,0.009999999776482582	CHEMBL3699255,TP,ACT,1.0	CHEMBL498520,TN,INACT,0.0	CHEMBL3699300,TP,ACT,0.9900000095367432	CHEMBL521734,FP,INACT,0.9900000095367432	CHEMBL3699245,TP,ACT,0.9900000095367432	CHEMBL456113,TN,INACT,0.0	CHEMBL3699263,TP,ACT,0.7599999904632568	CHEMBL3760009,TP,ACT,0.4099999964237213	CHEMBL3681193,TP,ACT,0.7300000190734863	CHEMBL1784660,TN,INACT,0.0	CHEMBL376505,TP,ACT,0.9900000095367432	CHEMBL502835,TP,ACT,1.0	CHEMBL3699256,TP,ACT,1.0	CHEMBL3609569,FP,INACT,0.800000011920929	CHEMBL3699266,TP,ACT,1.0	CHEMBL551936,TN,INACT,0.019999999552965164	CHEMBL3681203,TP,ACT,1.0	CHEMBL573339,FN,ACT,0.14000000059604645	CHEMBL560393,TN,INACT,0.0	CHEMBL521851,TP,ACT,0.9700000286102295	CHEMBL3699295,TP,ACT,1.0	CHEMBL1908395,TP,ACT,0.7200000286102295	CHEMBL2087015,TP,ACT,1.0	CHEMBL133214,FP,INACT,0.9800000190734863	CHEMBL3699249,TP,ACT,1.0	CHEMBL518732,TN,INACT,0.009999999776482582	CHEMBL3759193,TP,ACT,0.9900000095367432	CHEMBL3421968,FP,INACT,0.5899999737739563	CHEMBL3681198,TP,ACT,1.0	CHEMBL2087024,FP,INACT,1.0	CHEMBL3699235,TP,ACT,1.0	CHEMBL3758895,TP,ACT,1.0	CHEMBL3681194,TP,ACT,1.0	CHEMBL606245,TN,INACT,0.019999999552965164	CHEMBL456378,TN,INACT,0.0	CHEMBL2087005,TP,ACT,0.9900000095367432	CHEMBL562198,TN,INACT,0.029999999329447746	CHEMBL490241,FP,INACT,0.3799999952316284	CHEMBL1789941,FN,ACT,0.05000000074505806	CHEMBL3699288,TP,ACT,1.0	CHEMBL3699296,TP,ACT,1.0	CHEMBL457191,TN,INACT,0.0	CHEMBL509435,TN,INACT,0.05999999865889549	CHEMBL3699241,TP,ACT,0.9900000095367432	CHEMBL2392378,TN,INACT,0.0	CHEMBL3681209,TP,ACT,0.9300000071525574	CHEMBL3681200,TP,ACT,0.9700000286102295	CHEMBL2087016,TP,ACT,0.9800000190734863	CHEMBL169757,TN,INACT,0.10000000149011612	CHEMBL3758270,TP,ACT,0.9900000095367432	CHEMBL3699283,TP,ACT,0.9599999785423279	CHEMBL132948,TN,INACT,0.019999999552965164	CHEMBL2087008,TP,ACT,0.8999999761581421	CHEMBL487737,FP,INACT,0.8500000238418579	CHEMBL2392239,TN,INACT,0.0	

