CNNModel CHEMBL4140 adam 0.001 30 128 0 0.8 False True
Number of active compounds :	166
Number of inactive compounds :	166
---------------------------------
Run id: CNNModel_CHEMBL4140_adam_0.001_30_128_0_0.8_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL4140_adam_0.001_30_128_0.8_True/
---------------------------------
Training samples: 202
Validation samples: 64
--
Training Step: 1  | time: 32.040s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/202
[A[ATraining Step: 2  | total loss: [1m[32m0.62378[0m[0m | time: 33.285s
[2K
| Adam | epoch: 001 | loss: 0.62378 - acc: 0.4500 -- iter: 064/202
[A[ATraining Step: 3  | total loss: [1m[32m0.67865[0m[0m | time: 34.654s
[2K
| Adam | epoch: 001 | loss: 0.67865 - acc: 0.5932 -- iter: 096/202
[A[ATraining Step: 4  | total loss: [1m[32m0.68507[0m[0m | time: 35.913s
[2K
| Adam | epoch: 001 | loss: 0.68507 - acc: 0.5702 -- iter: 128/202
[A[ATraining Step: 5  | total loss: [1m[32m0.64993[0m[0m | time: 37.284s
[2K
| Adam | epoch: 001 | loss: 0.64993 - acc: 0.6514 -- iter: 160/202
[A[ATraining Step: 6  | total loss: [1m[32m0.78377[0m[0m | time: 38.727s
[2K
| Adam | epoch: 001 | loss: 0.78377 - acc: 0.5340 -- iter: 192/202
[A[ATraining Step: 7  | total loss: [1m[32m0.73082[0m[0m | time: 40.313s
[2K
| Adam | epoch: 001 | loss: 0.73082 - acc: 0.5511 | val_loss: 0.68772 - val_acc: 0.5469 -- iter: 202/202
--
Training Step: 8  | total loss: [1m[32m0.69793[0m[0m | time: 0.631s
[2K
| Adam | epoch: 002 | loss: 0.69793 - acc: 0.5786 -- iter: 032/202
[A[ATraining Step: 9  | total loss: [1m[32m0.68720[0m[0m | time: 1.894s
[2K
| Adam | epoch: 002 | loss: 0.68720 - acc: 0.5899 -- iter: 064/202
[A[ATraining Step: 10  | total loss: [1m[32m0.68747[0m[0m | time: 3.264s
[2K
| Adam | epoch: 002 | loss: 0.68747 - acc: 0.5762 -- iter: 096/202
[A[ATraining Step: 11  | total loss: [1m[32m0.69098[0m[0m | time: 4.794s
[2K
| Adam | epoch: 002 | loss: 0.69098 - acc: 0.5253 -- iter: 128/202
[A[ATraining Step: 12  | total loss: [1m[32m0.69139[0m[0m | time: 6.216s
[2K
| Adam | epoch: 002 | loss: 0.69139 - acc: 0.5280 -- iter: 160/202
[A[ATraining Step: 13  | total loss: [1m[32m0.69071[0m[0m | time: 7.229s
[2K
| Adam | epoch: 002 | loss: 0.69071 - acc: 0.5562 -- iter: 192/202
[A[ATraining Step: 14  | total loss: [1m[32m0.69001[0m[0m | time: 9.177s
[2K
| Adam | epoch: 002 | loss: 0.69001 - acc: 0.5843 | val_loss: 0.69161 - val_acc: 0.5469 -- iter: 202/202
--
Training Step: 15  | total loss: [1m[32m0.69045[0m[0m | time: 0.412s
[2K
| Adam | epoch: 003 | loss: 0.69045 - acc: 0.5758 -- iter: 032/202
[A[ATraining Step: 16  | total loss: [1m[32m0.69395[0m[0m | time: 0.725s
[2K
| Adam | epoch: 003 | loss: 0.69395 - acc: 0.4724 -- iter: 064/202
[A[ATraining Step: 17  | total loss: [1m[32m0.69609[0m[0m | time: 1.950s
[2K
| Adam | epoch: 003 | loss: 0.69609 - acc: 0.4103 -- iter: 096/202
[A[ATraining Step: 18  | total loss: [1m[32m0.69376[0m[0m | time: 2.649s
[2K
| Adam | epoch: 003 | loss: 0.69376 - acc: 0.4954 -- iter: 128/202
[A[ATraining Step: 19  | total loss: [1m[32m0.69312[0m[0m | time: 3.664s
[2K
| Adam | epoch: 003 | loss: 0.69312 - acc: 0.5178 -- iter: 160/202
[A[ATraining Step: 20  | total loss: [1m[32m0.69249[0m[0m | time: 4.769s
[2K
| Adam | epoch: 003 | loss: 0.69249 - acc: 0.5422 -- iter: 192/202
[A[ATraining Step: 21  | total loss: [1m[32m0.69208[0m[0m | time: 6.733s
[2K
| Adam | epoch: 003 | loss: 0.69208 - acc: 0.5582 | val_loss: 0.69201 - val_acc: 0.5469 -- iter: 202/202
--
Training Step: 22  | total loss: [1m[32m0.69221[0m[0m | time: 0.996s
[2K
| Adam | epoch: 004 | loss: 0.69221 - acc: 0.5501 -- iter: 032/202
[A[ATraining Step: 23  | total loss: [1m[32m0.69249[0m[0m | time: 1.396s
[2K
| Adam | epoch: 004 | loss: 0.69249 - acc: 0.5356 -- iter: 064/202
[A[ATraining Step: 24  | total loss: [1m[32m0.69272[0m[0m | time: 1.712s
[2K
| Adam | epoch: 004 | loss: 0.69272 - acc: 0.5256 -- iter: 096/202
[A[ATraining Step: 25  | total loss: [1m[32m0.69289[0m[0m | time: 2.719s
[2K
| Adam | epoch: 004 | loss: 0.69289 - acc: 0.5186 -- iter: 128/202
[A[ATraining Step: 26  | total loss: [1m[32m0.69205[0m[0m | time: 3.701s
[2K
| Adam | epoch: 004 | loss: 0.69205 - acc: 0.5468 -- iter: 160/202
[A[ATraining Step: 27  | total loss: [1m[32m0.69234[0m[0m | time: 4.632s
[2K
| Adam | epoch: 004 | loss: 0.69234 - acc: 0.5347 -- iter: 192/202
[A[ATraining Step: 28  | total loss: [1m[32m0.69180[0m[0m | time: 6.682s
[2K
| Adam | epoch: 004 | loss: 0.69180 - acc: 0.5495 | val_loss: 0.69155 - val_acc: 0.5469 -- iter: 202/202
--
Training Step: 29  | total loss: [1m[32m0.69266[0m[0m | time: 0.943s
[2K
| Adam | epoch: 005 | loss: 0.69266 - acc: 0.5222 -- iter: 032/202
[A[ATraining Step: 30  | total loss: [1m[32m0.69201[0m[0m | time: 1.922s
[2K
| Adam | epoch: 005 | loss: 0.69201 - acc: 0.5392 -- iter: 064/202
[A[ATraining Step: 31  | total loss: [1m[32m0.69120[0m[0m | time: 2.341s
[2K
| Adam | epoch: 005 | loss: 0.69120 - acc: 0.5590 -- iter: 096/202
[A[ATraining Step: 32  | total loss: [1m[32m0.69168[0m[0m | time: 2.729s
[2K
| Adam | epoch: 005 | loss: 0.69168 - acc: 0.5457 -- iter: 128/202
[A[ATraining Step: 33  | total loss: [1m[32m0.69201[0m[0m | time: 3.747s
[2K
| Adam | epoch: 005 | loss: 0.69201 - acc: 0.5357 -- iter: 160/202
[A[ATraining Step: 34  | total loss: [1m[32m0.69070[0m[0m | time: 5.176s
[2K
| Adam | epoch: 005 | loss: 0.69070 - acc: 0.5615 -- iter: 192/202
[A[ATraining Step: 35  | total loss: [1m[32m0.68969[0m[0m | time: 7.739s
[2K
| Adam | epoch: 005 | loss: 0.68969 - acc: 0.5748 | val_loss: 0.69025 - val_acc: 0.5469 -- iter: 202/202
--
Training Step: 36  | total loss: [1m[32m0.68886[0m[0m | time: 15.969s
[2K
| Adam | epoch: 006 | loss: 0.68886 - acc: 0.5851 -- iter: 032/202
[A[ATraining Step: 37  | total loss: [1m[32m0.69112[0m[0m | time: 35.370s
[2K
| Adam | epoch: 006 | loss: 0.69112 - acc: 0.5493 -- iter: 064/202
[A[ATraining Step: 38  | total loss: [1m[32m0.69251[0m[0m | time: 50.347s
[2K
| Adam | epoch: 006 | loss: 0.69251 - acc: 0.5274 -- iter: 096/202
[A[ATraining Step: 39  | total loss: [1m[32m0.69085[0m[0m | time: 61.858s
[2K
| Adam | epoch: 006 | loss: 0.69085 - acc: 0.5461 -- iter: 128/202
[A[ATraining Step: 40  | total loss: [1m[32m0.68823[0m[0m | time: 72.394s
[2K
| Adam | epoch: 006 | loss: 0.68823 - acc: 0.5750 -- iter: 160/202
[A[ATraining Step: 41  | total loss: [1m[32m0.68557[0m[0m | time: 81.162s
[2K
| Adam | epoch: 006 | loss: 0.68557 - acc: 0.5979 -- iter: 192/202
[A[ATraining Step: 42  | total loss: [1m[32m0.68617[0m[0m | time: 85.014s
[2K
| Adam | epoch: 006 | loss: 0.68617 - acc: 0.5859 | val_loss: 0.68795 - val_acc: 0.5469 -- iter: 202/202
--
Training Step: 43  | total loss: [1m[32m0.68702[0m[0m | time: 1.285s
[2K
| Adam | epoch: 007 | loss: 0.68702 - acc: 0.5763 -- iter: 032/202
[A[ATraining Step: 44  | total loss: [1m[32m0.68672[0m[0m | time: 2.679s
[2K
| Adam | epoch: 007 | loss: 0.68672 - acc: 0.5739 -- iter: 064/202
[A[ATraining Step: 45  | total loss: [1m[32m0.68735[0m[0m | time: 4.093s
[2K
| Adam | epoch: 007 | loss: 0.68735 - acc: 0.5667 -- iter: 096/202
[A[ATraining Step: 46  | total loss: [1m[32m0.68784[0m[0m | time: 5.344s
[2K
| Adam | epoch: 007 | loss: 0.68784 - acc: 0.5608 -- iter: 128/202
[A[ATraining Step: 47  | total loss: [1m[32m0.68454[0m[0m | time: 5.903s
[2K
| Adam | epoch: 007 | loss: 0.68454 - acc: 0.5713 -- iter: 160/202
[A[ATraining Step: 48  | total loss: [1m[32m0.68298[0m[0m | time: 6.442s
[2K
| Adam | epoch: 007 | loss: 0.68298 - acc: 0.5759 -- iter: 192/202
[A[ATraining Step: 49  | total loss: [1m[32m0.68024[0m[0m | time: 9.145s
[2K
| Adam | epoch: 007 | loss: 0.68024 - acc: 0.5797 | val_loss: 0.68926 - val_acc: 0.5469 -- iter: 202/202
--
Training Step: 50  | total loss: [1m[32m0.69314[0m[0m | time: 1.213s
[2K
| Adam | epoch: 008 | loss: 0.69314 - acc: 0.5528 -- iter: 032/202
[A[ATraining Step: 51  | total loss: [1m[32m0.68640[0m[0m | time: 2.743s
[2K
| Adam | epoch: 008 | loss: 0.68640 - acc: 0.5686 -- iter: 064/202
[A[ATraining Step: 52  | total loss: [1m[32m0.68535[0m[0m | time: 17.086s
[2K
| Adam | epoch: 008 | loss: 0.68535 - acc: 0.5677 -- iter: 096/202
[A[ATraining Step: 53  | total loss: [1m[32m0.68277[0m[0m | time: 21.614s
[2K
| Adam | epoch: 008 | loss: 0.68277 - acc: 0.5715 -- iter: 128/202
[A[ATraining Step: 54  | total loss: [1m[32m0.67724[0m[0m | time: 30.852s
[2K
| Adam | epoch: 008 | loss: 0.67724 - acc: 0.5838 -- iter: 160/202
[A[ATraining Step: 55  | total loss: [1m[32m0.68217[0m[0m | time: 45.665s
[2K
| Adam | epoch: 008 | loss: 0.68217 - acc: 0.5674 -- iter: 192/202
[A[ATraining Step: 56  | total loss: [1m[32m0.67983[0m[0m | time: 92.299s
[2K
| Adam | epoch: 008 | loss: 0.67983 - acc: 0.5720 | val_loss: 0.68359 - val_acc: 0.5469 -- iter: 202/202
--
Training Step: 57  | total loss: [1m[32m0.67712[0m[0m | time: 11.757s
[2K
| Adam | epoch: 009 | loss: 0.67712 - acc: 0.5758 -- iter: 032/202
[A[ATraining Step: 58  | total loss: [1m[32m0.66950[0m[0m | time: 13.068s
[2K
| Adam | epoch: 009 | loss: 0.66950 - acc: 0.5953 -- iter: 064/202
[A[ATraining Step: 59  | total loss: [1m[32m0.67211[0m[0m | time: 14.433s
[2K
| Adam | epoch: 009 | loss: 0.67211 - acc: 0.5867 -- iter: 096/202
[A[ATraining Step: 60  | total loss: [1m[32m0.67527[0m[0m | time: 15.873s
[2K
| Adam | epoch: 009 | loss: 0.67527 - acc: 0.5794 -- iter: 128/202
[A[ATraining Step: 61  | total loss: [1m[32m0.67861[0m[0m | time: 17.121s
[2K
| Adam | epoch: 009 | loss: 0.67861 - acc: 0.5690 -- iter: 160/202
[A[ATraining Step: 62  | total loss: [1m[32m0.67911[0m[0m | time: 18.589s
[2K
| Adam | epoch: 009 | loss: 0.67911 - acc: 0.5642 -- iter: 192/202
[A[ATraining Step: 63  | total loss: [1m[32m0.67864[0m[0m | time: 20.041s
[2K
| Adam | epoch: 009 | loss: 0.67864 - acc: 0.5600 | val_loss: 0.67164 - val_acc: 0.5469 -- iter: 202/202
--
Training Step: 64  | total loss: [1m[32m0.67421[0m[0m | time: 0.573s
[2K
| Adam | epoch: 010 | loss: 0.67421 - acc: 0.5650 -- iter: 032/202
[A[ATraining Step: 65  | total loss: [1m[32m0.67398[0m[0m | time: 1.942s
[2K
| Adam | epoch: 010 | loss: 0.67398 - acc: 0.5570 -- iter: 064/202
[A[ATraining Step: 66  | total loss: [1m[32m0.67659[0m[0m | time: 3.307s
[2K
| Adam | epoch: 010 | loss: 0.67659 - acc: 0.5425 -- iter: 096/202
[A[ATraining Step: 67  | total loss: [1m[32m0.67573[0m[0m | time: 4.847s
[2K
| Adam | epoch: 010 | loss: 0.67573 - acc: 0.5411 -- iter: 128/202
[A[ATraining Step: 68  | total loss: [1m[32m0.67453[0m[0m | time: 25.579s
[2K
| Adam | epoch: 010 | loss: 0.67453 - acc: 0.5436 -- iter: 160/202
[A[ATraining Step: 69  | total loss: [1m[32m0.67161[0m[0m | time: 74.328s
[2K
| Adam | epoch: 010 | loss: 0.67161 - acc: 0.5422 -- iter: 192/202
[A[ATraining Step: 70  | total loss: [1m[32m0.65828[0m[0m | time: 122.999s
[2K
| Adam | epoch: 010 | loss: 0.65828 - acc: 0.5554 | val_loss: 0.65685 - val_acc: 0.5469 -- iter: 202/202
--
Training Step: 71  | total loss: [1m[32m0.65177[0m[0m | time: 0.341s
[2K
| Adam | epoch: 011 | loss: 0.65177 - acc: 0.5668 -- iter: 032/202
[A[ATraining Step: 72  | total loss: [1m[32m0.65213[0m[0m | time: 0.826s
[2K
| Adam | epoch: 011 | loss: 0.65213 - acc: 0.5593 -- iter: 064/202
[A[ATraining Step: 73  | total loss: [1m[32m0.65369[0m[0m | time: 2.317s
[2K
| Adam | epoch: 011 | loss: 0.65369 - acc: 0.5527 -- iter: 096/202
[A[ATraining Step: 74  | total loss: [1m[32m0.64798[0m[0m | time: 3.773s
[2K
| Adam | epoch: 011 | loss: 0.64798 - acc: 0.5710 -- iter: 128/202
[A[ATraining Step: 75  | total loss: [1m[32m0.64621[0m[0m | time: 5.342s
[2K
| Adam | epoch: 011 | loss: 0.64621 - acc: 0.5904 -- iter: 160/202
[A[ATraining Step: 76  | total loss: [1m[32m0.63356[0m[0m | time: 6.684s
[2K
| Adam | epoch: 011 | loss: 0.63356 - acc: 0.6075 -- iter: 192/202
[A[ATraining Step: 77  | total loss: [1m[32m0.62176[0m[0m | time: 9.256s
[2K
| Adam | epoch: 011 | loss: 0.62176 - acc: 0.6159 | val_loss: 0.54103 - val_acc: 0.7031 -- iter: 202/202
--
Training Step: 78  | total loss: [1m[32m0.61636[0m[0m | time: 1.274s
[2K
| Adam | epoch: 012 | loss: 0.61636 - acc: 0.6169 -- iter: 032/202
[A[ATraining Step: 79  | total loss: [1m[32m0.61170[0m[0m | time: 1.745s
[2K
| Adam | epoch: 012 | loss: 0.61170 - acc: 0.6307 -- iter: 064/202
[A[ATraining Step: 80  | total loss: [1m[32m0.61879[0m[0m | time: 2.231s
[2K
| Adam | epoch: 012 | loss: 0.61879 - acc: 0.6173 -- iter: 096/202
[A[ATraining Step: 81  | total loss: [1m[32m0.61172[0m[0m | time: 3.511s
[2K
| Adam | epoch: 012 | loss: 0.61172 - acc: 0.6257 -- iter: 128/202
[A[ATraining Step: 82  | total loss: [1m[32m0.60131[0m[0m | time: 4.927s
[2K
| Adam | epoch: 012 | loss: 0.60131 - acc: 0.6475 -- iter: 160/202
[A[ATraining Step: 83  | total loss: [1m[32m0.60307[0m[0m | time: 6.220s
[2K
| Adam | epoch: 012 | loss: 0.60307 - acc: 0.6452 -- iter: 192/202
[A[ATraining Step: 84  | total loss: [1m[32m0.58947[0m[0m | time: 8.624s
[2K
| Adam | epoch: 012 | loss: 0.58947 - acc: 0.6557 | val_loss: 0.50363 - val_acc: 0.7656 -- iter: 202/202
--
Training Step: 85  | total loss: [1m[32m0.57864[0m[0m | time: 1.097s
[2K
| Adam | epoch: 013 | loss: 0.57864 - acc: 0.6683 -- iter: 032/202
[A[ATraining Step: 86  | total loss: [1m[32m0.56592[0m[0m | time: 1.974s
[2K
| Adam | epoch: 013 | loss: 0.56592 - acc: 0.6858 -- iter: 064/202
[A[ATraining Step: 87  | total loss: [1m[32m0.55836[0m[0m | time: 2.385s
[2K
| Adam | epoch: 013 | loss: 0.55836 - acc: 0.6954 -- iter: 096/202
[A[ATraining Step: 88  | total loss: [1m[32m0.53908[0m[0m | time: 2.675s
[2K
| Adam | epoch: 013 | loss: 0.53908 - acc: 0.7258 -- iter: 128/202
[A[ATraining Step: 89  | total loss: [1m[32m0.52012[0m[0m | time: 3.607s
[2K
| Adam | epoch: 013 | loss: 0.52012 - acc: 0.7432 -- iter: 160/202
[A[ATraining Step: 90  | total loss: [1m[32m0.52756[0m[0m | time: 4.618s
[2K
| Adam | epoch: 013 | loss: 0.52756 - acc: 0.7345 -- iter: 192/202
[A[ATraining Step: 91  | total loss: [1m[32m0.51563[0m[0m | time: 6.550s
[2K
| Adam | epoch: 013 | loss: 0.51563 - acc: 0.7392 | val_loss: 0.50144 - val_acc: 0.7344 -- iter: 202/202
--
Training Step: 92  | total loss: [1m[32m0.50389[0m[0m | time: 0.917s
[2K
| Adam | epoch: 014 | loss: 0.50389 - acc: 0.7497 -- iter: 032/202
[A[ATraining Step: 93  | total loss: [1m[32m0.50692[0m[0m | time: 1.960s
[2K
| Adam | epoch: 014 | loss: 0.50692 - acc: 0.7591 -- iter: 064/202
[A[ATraining Step: 94  | total loss: [1m[32m0.50488[0m[0m | time: 2.902s
[2K
| Adam | epoch: 014 | loss: 0.50488 - acc: 0.7613 -- iter: 096/202
[A[ATraining Step: 95  | total loss: [1m[32m0.50389[0m[0m | time: 3.274s
[2K
| Adam | epoch: 014 | loss: 0.50389 - acc: 0.7602 -- iter: 128/202
[A[ATraining Step: 96  | total loss: [1m[32m0.48081[0m[0m | time: 3.564s
[2K
| Adam | epoch: 014 | loss: 0.48081 - acc: 0.7741 -- iter: 160/202
[A[ATraining Step: 97  | total loss: [1m[32m0.45451[0m[0m | time: 4.535s
[2K
| Adam | epoch: 014 | loss: 0.45451 - acc: 0.7967 -- iter: 192/202
[A[ATraining Step: 98  | total loss: [1m[32m0.45165[0m[0m | time: 6.459s
[2K
| Adam | epoch: 014 | loss: 0.45165 - acc: 0.7952 | val_loss: 0.46265 - val_acc: 0.7656 -- iter: 202/202
--
Training Step: 99  | total loss: [1m[32m0.44677[0m[0m | time: 0.968s
[2K
| Adam | epoch: 015 | loss: 0.44677 - acc: 0.7969 -- iter: 032/202
[A[ATraining Step: 100  | total loss: [1m[32m0.44231[0m[0m | time: 1.994s
[2K
| Adam | epoch: 015 | loss: 0.44231 - acc: 0.8016 -- iter: 064/202
[A[ATraining Step: 101  | total loss: [1m[32m0.43462[0m[0m | time: 2.875s
[2K
| Adam | epoch: 015 | loss: 0.43462 - acc: 0.8089 -- iter: 096/202
[A[ATraining Step: 102  | total loss: [1m[32m0.43497[0m[0m | time: 3.752s
[2K
| Adam | epoch: 015 | loss: 0.43497 - acc: 0.8062 -- iter: 128/202
[A[ATraining Step: 103  | total loss: [1m[32m0.42950[0m[0m | time: 4.052s
[2K
| Adam | epoch: 015 | loss: 0.42950 - acc: 0.8099 -- iter: 160/202
[A[ATraining Step: 104  | total loss: [1m[32m0.40414[0m[0m | time: 4.392s
[2K
| Adam | epoch: 015 | loss: 0.40414 - acc: 0.8289 -- iter: 192/202
[A[ATraining Step: 105  | total loss: [1m[32m0.39022[0m[0m | time: 6.455s
[2K
| Adam | epoch: 015 | loss: 0.39022 - acc: 0.8360 | val_loss: 0.53189 - val_acc: 0.7500 -- iter: 202/202
--
Training Step: 106  | total loss: [1m[32m0.38675[0m[0m | time: 1.532s
[2K
| Adam | epoch: 016 | loss: 0.38675 - acc: 0.8399 -- iter: 032/202
[A[ATraining Step: 107  | total loss: [1m[32m0.39052[0m[0m | time: 3.350s
[2K
| Adam | epoch: 016 | loss: 0.39052 - acc: 0.8341 -- iter: 064/202
[A[ATraining Step: 108  | total loss: [1m[32m0.39911[0m[0m | time: 7.812s
[2K
| Adam | epoch: 016 | loss: 0.39911 - acc: 0.8288 -- iter: 096/202
[A[ATraining Step: 109  | total loss: [1m[32m0.41586[0m[0m | time: 11.052s
[2K
| Adam | epoch: 016 | loss: 0.41586 - acc: 0.8178 -- iter: 128/202
[A[ATraining Step: 110  | total loss: [1m[32m0.40665[0m[0m | time: 21.488s
[2K
| Adam | epoch: 016 | loss: 0.40665 - acc: 0.8204 -- iter: 160/202
[A[ATraining Step: 111  | total loss: [1m[32m0.44122[0m[0m | time: 24.232s
[2K
| Adam | epoch: 016 | loss: 0.44122 - acc: 0.8071 -- iter: 192/202
[A[ATraining Step: 112  | total loss: [1m[32m0.42929[0m[0m | time: 31.653s
[2K
| Adam | epoch: 016 | loss: 0.42929 - acc: 0.8164 | val_loss: 0.59459 - val_acc: 0.7031 -- iter: 202/202
--
Training Step: 113  | total loss: [1m[32m0.41636[0m[0m | time: 1.242s
[2K
| Adam | epoch: 017 | loss: 0.41636 - acc: 0.8347 -- iter: 032/202
[A[ATraining Step: 114  | total loss: [1m[32m0.40692[0m[0m | time: 2.693s
[2K
| Adam | epoch: 017 | loss: 0.40692 - acc: 0.8388 -- iter: 064/202
[A[ATraining Step: 115  | total loss: [1m[32m0.43372[0m[0m | time: 3.779s
[2K
| Adam | epoch: 017 | loss: 0.43372 - acc: 0.8236 -- iter: 096/202
[A[ATraining Step: 116  | total loss: [1m[32m0.42777[0m[0m | time: 4.922s
[2K
| Adam | epoch: 017 | loss: 0.42777 - acc: 0.8257 -- iter: 128/202
[A[ATraining Step: 117  | total loss: [1m[32m0.42856[0m[0m | time: 6.156s
[2K
| Adam | epoch: 017 | loss: 0.42856 - acc: 0.8212 -- iter: 160/202
[A[ATraining Step: 118  | total loss: [1m[32m0.41483[0m[0m | time: 7.524s
[2K
| Adam | epoch: 017 | loss: 0.41483 - acc: 0.8360 -- iter: 192/202
[A[ATraining Step: 119  | total loss: [1m[32m0.40794[0m[0m | time: 8.958s
[2K
| Adam | epoch: 017 | loss: 0.40794 - acc: 0.8367 | val_loss: 0.47314 - val_acc: 0.7812 -- iter: 202/202
--
Training Step: 120  | total loss: [1m[32m0.39882[0m[0m | time: 0.542s
[2K
| Adam | epoch: 018 | loss: 0.39882 - acc: 0.8431 -- iter: 032/202
[A[ATraining Step: 121  | total loss: [1m[32m0.40517[0m[0m | time: 1.843s
[2K
| Adam | epoch: 018 | loss: 0.40517 - acc: 0.8488 -- iter: 064/202
[A[ATraining Step: 122  | total loss: [1m[32m0.41149[0m[0m | time: 3.217s
[2K
| Adam | epoch: 018 | loss: 0.41149 - acc: 0.8326 -- iter: 096/202
[A[ATraining Step: 123  | total loss: [1m[32m0.39514[0m[0m | time: 11.835s
[2K
| Adam | epoch: 018 | loss: 0.39514 - acc: 0.8400 -- iter: 128/202
[A[ATraining Step: 124  | total loss: [1m[32m0.40490[0m[0m | time: 13.005s
[2K
| Adam | epoch: 018 | loss: 0.40490 - acc: 0.8372 -- iter: 160/202
[A[ATraining Step: 125  | total loss: [1m[32m0.40250[0m[0m | time: 14.187s
[2K
| Adam | epoch: 018 | loss: 0.40250 - acc: 0.8348 -- iter: 192/202
[A[ATraining Step: 126  | total loss: [1m[32m0.39040[0m[0m | time: 16.362s
[2K
| Adam | epoch: 018 | loss: 0.39040 - acc: 0.8450 | val_loss: 0.50215 - val_acc: 0.7656 -- iter: 202/202
--
Training Step: 127  | total loss: [1m[32m0.38010[0m[0m | time: 0.411s
[2K
| Adam | epoch: 019 | loss: 0.38010 - acc: 0.8418 -- iter: 032/202
[A[ATraining Step: 128  | total loss: [1m[32m0.35711[0m[0m | time: 0.921s
[2K
| Adam | epoch: 019 | loss: 0.35711 - acc: 0.8576 -- iter: 064/202
[A[ATraining Step: 129  | total loss: [1m[32m0.33093[0m[0m | time: 2.087s
[2K
| Adam | epoch: 019 | loss: 0.33093 - acc: 0.8719 -- iter: 096/202
[A[ATraining Step: 130  | total loss: [1m[32m0.32363[0m[0m | time: 3.273s
[2K
| Adam | epoch: 019 | loss: 0.32363 - acc: 0.8753 -- iter: 128/202
[A[ATraining Step: 131  | total loss: [1m[32m0.31310[0m[0m | time: 4.343s
[2K
| Adam | epoch: 019 | loss: 0.31310 - acc: 0.8846 -- iter: 160/202
[A[ATraining Step: 132  | total loss: [1m[32m0.31418[0m[0m | time: 5.859s
[2K
| Adam | epoch: 019 | loss: 0.31418 - acc: 0.8805 -- iter: 192/202
[A[ATraining Step: 133  | total loss: [1m[32m0.35843[0m[0m | time: 17.166s
[2K
| Adam | epoch: 019 | loss: 0.35843 - acc: 0.8612 | val_loss: 0.41668 - val_acc: 0.8125 -- iter: 202/202
--
Training Step: 134  | total loss: [1m[32m0.34391[0m[0m | time: 21.414s
[2K
| Adam | epoch: 020 | loss: 0.34391 - acc: 0.8689 -- iter: 032/202
[A[ATraining Step: 135  | total loss: [1m[32m0.34568[0m[0m | time: 22.995s
[2K
| Adam | epoch: 020 | loss: 0.34568 - acc: 0.8695 -- iter: 064/202
[A[ATraining Step: 136  | total loss: [1m[32m0.34132[0m[0m | time: 38.041s
[2K
| Adam | epoch: 020 | loss: 0.34132 - acc: 0.8725 -- iter: 096/202
[A[ATraining Step: 137  | total loss: [1m[32m0.33221[0m[0m | time: 65.866s
[2K
| Adam | epoch: 020 | loss: 0.33221 - acc: 0.8753 -- iter: 128/202
[A[ATraining Step: 138  | total loss: [1m[32m0.33939[0m[0m | time: 66.952s
[2K
| Adam | epoch: 020 | loss: 0.33939 - acc: 0.8690 -- iter: 160/202
[A[ATraining Step: 139  | total loss: [1m[32m0.36005[0m[0m | time: 68.036s
[2K
| Adam | epoch: 020 | loss: 0.36005 - acc: 0.8602 -- iter: 192/202
[A[ATraining Step: 140  | total loss: [1m[32m0.35545[0m[0m | time: 79.351s
[2K
| Adam | epoch: 020 | loss: 0.35545 - acc: 0.8586 | val_loss: 0.37991 - val_acc: 0.8281 -- iter: 202/202
--
Training Step: 141  | total loss: [1m[32m0.35715[0m[0m | time: 0.973s
[2K
| Adam | epoch: 021 | loss: 0.35715 - acc: 0.8571 -- iter: 032/202
[A[ATraining Step: 142  | total loss: [1m[32m0.34892[0m[0m | time: 2.115s
[2K
| Adam | epoch: 021 | loss: 0.34892 - acc: 0.8589 -- iter: 064/202
[A[ATraining Step: 143  | total loss: [1m[32m0.33737[0m[0m | time: 2.516s
[2K
| Adam | epoch: 021 | loss: 0.33737 - acc: 0.8699 -- iter: 096/202
[A[ATraining Step: 144  | total loss: [1m[32m0.35352[0m[0m | time: 2.918s
[2K
| Adam | epoch: 021 | loss: 0.35352 - acc: 0.8529 -- iter: 128/202
[A[ATraining Step: 145  | total loss: [1m[32m0.33945[0m[0m | time: 3.849s
[2K
| Adam | epoch: 021 | loss: 0.33945 - acc: 0.8576 -- iter: 160/202
[A[ATraining Step: 146  | total loss: [1m[32m0.33708[0m[0m | time: 5.115s
[2K
| Adam | epoch: 021 | loss: 0.33708 - acc: 0.8593 -- iter: 192/202
[A[ATraining Step: 147  | total loss: [1m[32m0.33213[0m[0m | time: 7.235s
[2K
| Adam | epoch: 021 | loss: 0.33213 - acc: 0.8609 | val_loss: 0.39621 - val_acc: 0.8125 -- iter: 202/202
--
Training Step: 148  | total loss: [1m[32m0.32771[0m[0m | time: 0.963s
[2K
| Adam | epoch: 022 | loss: 0.32771 - acc: 0.8623 -- iter: 032/202
[A[ATraining Step: 149  | total loss: [1m[32m0.32172[0m[0m | time: 1.959s
[2K
| Adam | epoch: 022 | loss: 0.32172 - acc: 0.8698 -- iter: 064/202
[A[ATraining Step: 150  | total loss: [1m[32m0.31577[0m[0m | time: 3.050s
[2K
| Adam | epoch: 022 | loss: 0.31577 - acc: 0.8766 -- iter: 096/202
[A[ATraining Step: 151  | total loss: [1m[32m0.31311[0m[0m | time: 3.404s
[2K
| Adam | epoch: 022 | loss: 0.31311 - acc: 0.8796 -- iter: 128/202
[A[ATraining Step: 152  | total loss: [1m[32m0.30982[0m[0m | time: 3.729s
[2K
| Adam | epoch: 022 | loss: 0.30982 - acc: 0.8816 -- iter: 160/202
[A[ATraining Step: 153  | total loss: [1m[32m0.29622[0m[0m | time: 4.865s
[2K
| Adam | epoch: 022 | loss: 0.29622 - acc: 0.8834 -- iter: 192/202
[A[ATraining Step: 154  | total loss: [1m[32m0.30009[0m[0m | time: 6.943s
[2K
| Adam | epoch: 022 | loss: 0.30009 - acc: 0.8795 | val_loss: 0.39769 - val_acc: 0.8594 -- iter: 202/202
--
Training Step: 155  | total loss: [1m[32m0.29539[0m[0m | time: 0.719s
[2K
| Adam | epoch: 023 | loss: 0.29539 - acc: 0.8790 -- iter: 032/202
[A[ATraining Step: 156  | total loss: [1m[32m0.31056[0m[0m | time: 1.414s
[2K
| Adam | epoch: 023 | loss: 0.31056 - acc: 0.8755 -- iter: 064/202
[A[ATraining Step: 157  | total loss: [1m[32m0.29374[0m[0m | time: 2.041s
[2K
| Adam | epoch: 023 | loss: 0.29374 - acc: 0.8848 -- iter: 096/202
[A[ATraining Step: 158  | total loss: [1m[32m0.28465[0m[0m | time: 2.743s
[2K
| Adam | epoch: 023 | loss: 0.28465 - acc: 0.8901 -- iter: 128/202
[A[ATraining Step: 159  | total loss: [1m[32m0.26310[0m[0m | time: 2.986s
[2K
| Adam | epoch: 023 | loss: 0.26310 - acc: 0.9011 -- iter: 160/202
[A[ATraining Step: 160  | total loss: [1m[32m0.24198[0m[0m | time: 3.240s
[2K
| Adam | epoch: 023 | loss: 0.24198 - acc: 0.9110 -- iter: 192/202
[A[ATraining Step: 161  | total loss: [1m[32m0.22157[0m[0m | time: 4.948s
[2K
| Adam | epoch: 023 | loss: 0.22157 - acc: 0.9199 | val_loss: 0.66039 - val_acc: 0.7812 -- iter: 202/202
--
Training Step: 162  | total loss: [1m[32m0.24013[0m[0m | time: 0.731s
[2K
| Adam | epoch: 024 | loss: 0.24013 - acc: 0.9091 -- iter: 032/202
[A[ATraining Step: 163  | total loss: [1m[32m0.23749[0m[0m | time: 1.419s
[2K
| Adam | epoch: 024 | loss: 0.23749 - acc: 0.9089 -- iter: 064/202
[A[ATraining Step: 164  | total loss: [1m[32m0.24191[0m[0m | time: 2.120s
[2K
| Adam | epoch: 024 | loss: 0.24191 - acc: 0.9055 -- iter: 096/202
[A[ATraining Step: 165  | total loss: [1m[32m0.24880[0m[0m | time: 2.818s
[2K
| Adam | epoch: 024 | loss: 0.24880 - acc: 0.9055 -- iter: 128/202
[A[ATraining Step: 166  | total loss: [1m[32m0.27735[0m[0m | time: 3.498s
[2K
| Adam | epoch: 024 | loss: 0.27735 - acc: 0.8869 -- iter: 160/202
[A[ATraining Step: 167  | total loss: [1m[32m0.26663[0m[0m | time: 3.754s
[2K
| Adam | epoch: 024 | loss: 0.26663 - acc: 0.8888 -- iter: 192/202
[A[ATraining Step: 168  | total loss: [1m[32m0.25999[0m[0m | time: 5.001s
[2K
| Adam | epoch: 024 | loss: 0.25999 - acc: 0.8899 | val_loss: 0.38363 - val_acc: 0.8281 -- iter: 202/202
--
Training Step: 169  | total loss: [1m[32m0.23816[0m[0m | time: 0.737s
[2K
| Adam | epoch: 025 | loss: 0.23816 - acc: 0.9009 -- iter: 032/202
[A[ATraining Step: 170  | total loss: [1m[32m0.23822[0m[0m | time: 1.442s
[2K
| Adam | epoch: 025 | loss: 0.23822 - acc: 0.9015 -- iter: 064/202
[A[ATraining Step: 171  | total loss: [1m[32m0.23358[0m[0m | time: 2.148s
[2K
| Adam | epoch: 025 | loss: 0.23358 - acc: 0.9051 -- iter: 096/202
[A[ATraining Step: 172  | total loss: [1m[32m0.22449[0m[0m | time: 2.845s
[2K
| Adam | epoch: 025 | loss: 0.22449 - acc: 0.9083 -- iter: 128/202
[A[ATraining Step: 173  | total loss: [1m[32m0.21517[0m[0m | time: 3.902s
[2K
| Adam | epoch: 025 | loss: 0.21517 - acc: 0.9144 -- iter: 160/202
[A[ATraining Step: 174  | total loss: [1m[32m0.20494[0m[0m | time: 5.162s
[2K
| Adam | epoch: 025 | loss: 0.20494 - acc: 0.9198 -- iter: 192/202
[A[ATraining Step: 175  | total loss: [1m[32m0.21304[0m[0m | time: 6.482s
[2K
| Adam | epoch: 025 | loss: 0.21304 - acc: 0.9153 | val_loss: 0.39741 - val_acc: 0.8125 -- iter: 202/202
--
Training Step: 176  | total loss: [1m[32m0.25031[0m[0m | time: 0.363s
[2K
| Adam | epoch: 026 | loss: 0.25031 - acc: 0.9038 -- iter: 032/202
[A[ATraining Step: 177  | total loss: [1m[32m0.25026[0m[0m | time: 1.331s
[2K
| Adam | epoch: 026 | loss: 0.25026 - acc: 0.9034 -- iter: 064/202
[A[ATraining Step: 178  | total loss: [1m[32m0.24431[0m[0m | time: 2.284s
[2K
| Adam | epoch: 026 | loss: 0.24431 - acc: 0.9068 -- iter: 096/202
[A[ATraining Step: 179  | total loss: [1m[32m0.23436[0m[0m | time: 3.180s
[2K
| Adam | epoch: 026 | loss: 0.23436 - acc: 0.9130 -- iter: 128/202
[A[ATraining Step: 180  | total loss: [1m[32m0.22458[0m[0m | time: 4.443s
[2K
| Adam | epoch: 026 | loss: 0.22458 - acc: 0.9186 -- iter: 160/202
[A[ATraining Step: 181  | total loss: [1m[32m0.21616[0m[0m | time: 5.691s
[2K
| Adam | epoch: 026 | loss: 0.21616 - acc: 0.9236 -- iter: 192/202
[A[ATraining Step: 182  | total loss: [1m[32m0.21058[0m[0m | time: 13.294s
[2K
| Adam | epoch: 026 | loss: 0.21058 - acc: 0.9281 | val_loss: 0.40583 - val_acc: 0.8281 -- iter: 202/202
--
Training Step: 183  | total loss: [1m[32m0.20536[0m[0m | time: 1.104s
[2K
| Adam | epoch: 027 | loss: 0.20536 - acc: 0.9291 -- iter: 032/202
[A[ATraining Step: 184  | total loss: [1m[32m0.19566[0m[0m | time: 1.359s
[2K
| Adam | epoch: 027 | loss: 0.19566 - acc: 0.9361 -- iter: 064/202
[A[ATraining Step: 185  | total loss: [1m[32m0.18537[0m[0m | time: 2.281s
[2K
| Adam | epoch: 027 | loss: 0.18537 - acc: 0.9425 -- iter: 096/202
[A[ATraining Step: 186  | total loss: [1m[32m0.18242[0m[0m | time: 3.348s
[2K
| Adam | epoch: 027 | loss: 0.18242 - acc: 0.9452 -- iter: 128/202
[A[ATraining Step: 187  | total loss: [1m[32m0.16865[0m[0m | time: 4.362s
[2K
| Adam | epoch: 027 | loss: 0.16865 - acc: 0.9506 -- iter: 160/202
[A[ATraining Step: 188  | total loss: [1m[32m0.16062[0m[0m | time: 5.405s
[2K
| Adam | epoch: 027 | loss: 0.16062 - acc: 0.9524 -- iter: 192/202
[A[ATraining Step: 189  | total loss: [1m[32m0.16910[0m[0m | time: 7.433s
[2K
| Adam | epoch: 027 | loss: 0.16910 - acc: 0.9478 | val_loss: 0.46177 - val_acc: 0.8281 -- iter: 202/202
--
Training Step: 190  | total loss: [1m[32m0.17295[0m[0m | time: 1.141s
[2K
| Adam | epoch: 028 | loss: 0.17295 - acc: 0.9468 -- iter: 032/202
[A[ATraining Step: 191  | total loss: [1m[32m0.15922[0m[0m | time: 1.648s
[2K
| Adam | epoch: 028 | loss: 0.15922 - acc: 0.9521 -- iter: 064/202
[A[ATraining Step: 192  | total loss: [1m[32m0.16169[0m[0m | time: 2.077s
[2K
| Adam | epoch: 028 | loss: 0.16169 - acc: 0.9469 -- iter: 096/202
[A[ATraining Step: 193  | total loss: [1m[32m0.14894[0m[0m | time: 3.289s
[2K
| Adam | epoch: 028 | loss: 0.14894 - acc: 0.9522 -- iter: 128/202
[A[ATraining Step: 194  | total loss: [1m[32m0.14584[0m[0m | time: 4.128s
[2K
| Adam | epoch: 028 | loss: 0.14584 - acc: 0.9539 -- iter: 160/202
[A[ATraining Step: 195  | total loss: [1m[32m0.16163[0m[0m | time: 5.115s
[2K
| Adam | epoch: 028 | loss: 0.16163 - acc: 0.9522 -- iter: 192/202
[A[ATraining Step: 196  | total loss: [1m[32m0.17225[0m[0m | time: 7.177s
[2K
| Adam | epoch: 028 | loss: 0.17225 - acc: 0.9508 | val_loss: 0.59419 - val_acc: 0.7969 -- iter: 202/202
--
Training Step: 197  | total loss: [1m[32m0.17287[0m[0m | time: 1.042s
[2K
| Adam | epoch: 029 | loss: 0.17287 - acc: 0.9494 -- iter: 032/202
[A[ATraining Step: 198  | total loss: [1m[32m0.16161[0m[0m | time: 2.292s
[2K
| Adam | epoch: 029 | loss: 0.16161 - acc: 0.9545 -- iter: 064/202
[A[ATraining Step: 199  | total loss: [1m[32m0.16277[0m[0m | time: 2.649s
[2K
| Adam | epoch: 029 | loss: 0.16277 - acc: 0.9528 -- iter: 096/202
[A[ATraining Step: 200  | total loss: [1m[32m0.14842[0m[0m | time: 4.053s
[2K
| Adam | epoch: 029 | loss: 0.14842 - acc: 0.9575 | val_loss: 0.44122 - val_acc: 0.8750 -- iter: 128/202
--
Training Step: 201  | total loss: [1m[32m0.13593[0m[0m | time: 5.326s
[2K
| Adam | epoch: 029 | loss: 0.13593 - acc: 0.9618 -- iter: 160/202
[A[ATraining Step: 202  | total loss: [1m[32m0.13287[0m[0m | time: 6.208s
[2K
| Adam | epoch: 029 | loss: 0.13287 - acc: 0.9625 -- iter: 192/202
[A[ATraining Step: 203  | total loss: [1m[32m0.13640[0m[0m | time: 8.167s
[2K
| Adam | epoch: 029 | loss: 0.13640 - acc: 0.9600 | val_loss: 0.38200 - val_acc: 0.8906 -- iter: 202/202
--
Training Step: 204  | total loss: [1m[32m0.14113[0m[0m | time: 0.947s
[2K
| Adam | epoch: 030 | loss: 0.14113 - acc: 0.9577 -- iter: 032/202
[A[ATraining Step: 205  | total loss: [1m[32m0.13373[0m[0m | time: 2.027s
[2K
| Adam | epoch: 030 | loss: 0.13373 - acc: 0.9588 -- iter: 064/202
[A[ATraining Step: 206  | total loss: [1m[32m0.13987[0m[0m | time: 3.219s
[2K
| Adam | epoch: 030 | loss: 0.13987 - acc: 0.9536 -- iter: 096/202
[A[ATraining Step: 207  | total loss: [1m[32m0.13249[0m[0m | time: 3.645s
[2K
| Adam | epoch: 030 | loss: 0.13249 - acc: 0.9582 -- iter: 128/202
[A[ATraining Step: 208  | total loss: [1m[32m0.12456[0m[0m | time: 4.011s
[2K
| Adam | epoch: 030 | loss: 0.12456 - acc: 0.9624 -- iter: 160/202
[A[ATraining Step: 209  | total loss: [1m[32m0.13373[0m[0m | time: 5.045s
[2K
| Adam | epoch: 030 | loss: 0.13373 - acc: 0.9561 -- iter: 192/202
[A[ATraining Step: 210  | total loss: [1m[32m0.13001[0m[0m | time: 7.094s
[2K
| Adam | epoch: 030 | loss: 0.13001 - acc: 0.9574 | val_loss: 0.42593 - val_acc: 0.8281 -- iter: 202/202
--
2018-08-02 00:34:32.026379: W tensorflow/core/framework/allocator.cc:101] Allocation of 579633152 exceeds 10% of system memory.
2018-08-02 00:34:32.276393: W tensorflow/core/framework/allocator.cc:101] Allocation of 579633152 exceeds 10% of system memory.
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.9192118226600985
Validation AUPRC:0.8533193411024885
Test AUC:0.9089089089089089
Test AUPRC:0.865746997975577
BestTestF1Score	0.89	0.74	0.88	0.89	0.89	33	4	23	4	0.87
BestTestMCCScore	0.89	0.74	0.88	0.89	0.89	33	4	23	4	0.87
BestTestAccuracyScore	0.89	0.74	0.88	0.89	0.89	33	4	23	4	0.87
BestValidationF1Score	0.88	0.78	0.89	0.87	0.9	26	4	31	3	0.87
BestValidationMCC	0.88	0.78	0.89	0.87	0.9	26	4	31	3	0.87
BestValidationAccuracy	0.88	0.78	0.89	0.87	0.9	26	4	31	3	0.87
TestPredictions (Threshold:0.87)
CHEMBL3824249,TN,INACT,0.05000000074505806	CHEMBL3099591,TN,INACT,0.0	CHEMBL386164,FN,ACT,0.8399999737739563	CHEMBL374888,FP,INACT,0.9800000190734863	CHEMBL152934,TN,INACT,0.0	CHEMBL1258221,TP,ACT,0.9700000286102295	CHEMBL383264,TN,INACT,0.0	CHEMBL224740,TP,ACT,0.9900000095367432	CHEMBL1258670,TP,ACT,0.949999988079071	CHEMBL3222125,TN,INACT,0.0	CHEMBL598022,TN,INACT,0.019999999552965164	CHEMBL3402236,TN,INACT,0.0	CHEMBL1258105,TP,ACT,0.9399999976158142	CHEMBL1258558,TP,ACT,0.9399999976158142	CHEMBL1257170,TP,ACT,0.9800000190734863	CHEMBL558009,TN,INACT,0.0	CHEMBL224449,TP,ACT,0.9900000095367432	CHEMBL524992,TN,INACT,0.0	CHEMBL435311,TP,ACT,0.9700000286102295	CHEMBL2436584,TP,ACT,0.9800000190734863	CHEMBL308880,TP,ACT,0.8899999856948853	CHEMBL3617994,TN,INACT,0.0	CHEMBL224359,TP,ACT,0.9900000095367432	CHEMBL3236512,TN,INACT,0.0	CHEMBL1257285,TP,ACT,0.949999988079071	CHEMBL192461,TP,ACT,0.9399999976158142	CHEMBL2436567,FN,ACT,0.03999999910593033	CHEMBL224357,TP,ACT,0.9800000190734863	CHEMBL308665,TP,ACT,0.9900000095367432	CHEMBL601646,TN,INACT,0.009999999776482582	CHEMBL192299,TP,ACT,0.9599999785423279	CHEMBL418331,TP,ACT,0.9599999785423279	CHEMBL2436568,FN,ACT,0.8600000143051147	CHEMBL224580,TP,ACT,0.9800000190734863	CHEMBL1256163,TP,ACT,0.9599999785423279	CHEMBL192470,TP,ACT,0.9100000262260437	CHEMBL68344,TP,ACT,0.8700000047683716	CHEMBL1258219,TP,ACT,0.949999988079071	CHEMBL78444,TN,INACT,0.0	CHEMBL425817,TP,ACT,0.9599999785423279	CHEMBL2436566,FN,ACT,0.4300000071525574	CHEMBL192293,TP,ACT,0.9200000166893005	CHEMBL602890,TN,INACT,0.27000001072883606	CHEMBL2436579,TP,ACT,0.8999999761581421	CHEMBL604337,TN,INACT,0.009999999776482582	CHEMBL2436592,TP,ACT,0.9700000286102295	CHEMBL2436593,TP,ACT,0.9700000286102295	CHEMBL375967,FP,INACT,0.9399999976158142	CHEMBL302116,TP,ACT,0.9800000190734863	CHEMBL3236509,TN,INACT,0.0	CHEMBL2436562,TP,ACT,0.9800000190734863	CHEMBL2436591,TP,ACT,0.9800000190734863	CHEMBL66663,TP,ACT,0.8899999856948853	CHEMBL434374,FP,INACT,0.8799999952316284	CHEMBL80651,TN,INACT,0.0	CHEMBL1836182,TN,INACT,0.0	CHEMBL1258107,TP,ACT,0.9399999976158142	CHEMBL517625,TN,INACT,0.0	CHEMBL120077,TN,INACT,0.029999999329447746	CHEMBL2425002,TN,INACT,0.009999999776482582	CHEMBL551058,FP,INACT,0.9900000095367432	CHEMBL1081703,TN,INACT,0.0	CHEMBL2436575,TP,ACT,0.9800000190734863	CHEMBL3099593,TN,INACT,0.41999998688697815	

