ImageNetInceptionV2 CHEMBL4683 adam 0.0005 15 0 0 0.6 False True
Number of active compounds :	170
Number of inactive compounds :	170
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL4683_adam_0.0005_15_0_0_0.6_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL4683_adam_0.0005_15_0.6/
---------------------------------
Training samples: 215
Validation samples: 68
--
Training Step: 1  | time: 61.573s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/215
[A[ATraining Step: 2  | total loss: [1m[32m0.64929[0m[0m | time: 73.665s
[2K
| Adam | epoch: 001 | loss: 0.64929 - acc: 0.5344 -- iter: 064/215
[A[ATraining Step: 3  | total loss: [1m[32m0.78262[0m[0m | time: 85.915s
[2K
| Adam | epoch: 001 | loss: 0.78262 - acc: 0.5830 -- iter: 096/215
[A[ATraining Step: 4  | total loss: [1m[32m0.97827[0m[0m | time: 97.989s
[2K
| Adam | epoch: 001 | loss: 0.97827 - acc: 0.4973 -- iter: 128/215
[A[ATraining Step: 5  | total loss: [1m[32m0.81280[0m[0m | time: 110.107s
[2K
| Adam | epoch: 001 | loss: 0.81280 - acc: 0.5208 -- iter: 160/215
[A[ATraining Step: 6  | total loss: [1m[32m0.70176[0m[0m | time: 122.368s
[2K
| Adam | epoch: 001 | loss: 0.70176 - acc: 0.5878 -- iter: 192/215
[A[ATraining Step: 7  | total loss: [1m[32m0.65531[0m[0m | time: 147.481s
[2K
| Adam | epoch: 001 | loss: 0.65531 - acc: 0.6476 | val_loss: 0.70553 - val_acc: 0.6324 -- iter: 215/215
--
Training Step: 8  | total loss: [1m[32m0.58544[0m[0m | time: 9.598s
[2K
| Adam | epoch: 002 | loss: 0.58544 - acc: 0.7480 -- iter: 032/215
[A[ATraining Step: 9  | total loss: [1m[32m0.41580[0m[0m | time: 21.468s
[2K
| Adam | epoch: 002 | loss: 0.41580 - acc: 0.8814 -- iter: 064/215
[A[ATraining Step: 10  | total loss: [1m[32m0.45499[0m[0m | time: 33.384s
[2K
| Adam | epoch: 002 | loss: 0.45499 - acc: 0.8313 -- iter: 096/215
[A[ATraining Step: 11  | total loss: [1m[32m0.54612[0m[0m | time: 45.138s
[2K
| Adam | epoch: 002 | loss: 0.54612 - acc: 0.7336 -- iter: 128/215
[A[ATraining Step: 12  | total loss: [1m[32m0.51862[0m[0m | time: 57.330s
[2K
| Adam | epoch: 002 | loss: 0.51862 - acc: 0.7410 -- iter: 160/215
[A[ATraining Step: 13  | total loss: [1m[32m0.46747[0m[0m | time: 69.470s
[2K
| Adam | epoch: 002 | loss: 0.46747 - acc: 0.7582 -- iter: 192/215
[A[ATraining Step: 14  | total loss: [1m[32m0.40659[0m[0m | time: 86.210s
[2K
| Adam | epoch: 002 | loss: 0.40659 - acc: 0.7804 | val_loss: 0.65764 - val_acc: 0.6324 -- iter: 215/215
--
Training Step: 15  | total loss: [1m[32m0.34144[0m[0m | time: 8.785s
[2K
| Adam | epoch: 003 | loss: 0.34144 - acc: 0.8297 -- iter: 032/215
[A[ATraining Step: 16  | total loss: [1m[32m0.30843[0m[0m | time: 18.095s
[2K
| Adam | epoch: 003 | loss: 0.30843 - acc: 0.8446 -- iter: 064/215
[A[ATraining Step: 17  | total loss: [1m[32m0.25702[0m[0m | time: 30.045s
[2K
| Adam | epoch: 003 | loss: 0.25702 - acc: 0.8849 -- iter: 096/215
[A[ATraining Step: 18  | total loss: [1m[32m0.27618[0m[0m | time: 42.126s
[2K
| Adam | epoch: 003 | loss: 0.27618 - acc: 0.8815 -- iter: 128/215
[A[ATraining Step: 19  | total loss: [1m[32m0.26016[0m[0m | time: 54.032s
[2K
| Adam | epoch: 003 | loss: 0.26016 - acc: 0.8897 -- iter: 160/215
[A[ATraining Step: 20  | total loss: [1m[32m0.25449[0m[0m | time: 66.059s
[2K
| Adam | epoch: 003 | loss: 0.25449 - acc: 0.9051 -- iter: 192/215
[A[ATraining Step: 21  | total loss: [1m[32m0.26003[0m[0m | time: 82.478s
[2K
| Adam | epoch: 003 | loss: 0.26003 - acc: 0.8958 | val_loss: 0.66351 - val_acc: 0.6324 -- iter: 215/215
--
Training Step: 22  | total loss: [1m[32m0.24945[0m[0m | time: 11.789s
[2K
| Adam | epoch: 004 | loss: 0.24945 - acc: 0.8989 -- iter: 032/215
[A[ATraining Step: 23  | total loss: [1m[32m0.25107[0m[0m | time: 21.147s
[2K
| Adam | epoch: 004 | loss: 0.25107 - acc: 0.8920 -- iter: 064/215
[A[ATraining Step: 24  | total loss: [1m[32m0.27430[0m[0m | time: 30.934s
[2K
| Adam | epoch: 004 | loss: 0.27430 - acc: 0.8979 -- iter: 096/215
[A[ATraining Step: 25  | total loss: [1m[32m0.21989[0m[0m | time: 42.965s
[2K
| Adam | epoch: 004 | loss: 0.21989 - acc: 0.9257 -- iter: 128/215
[A[ATraining Step: 26  | total loss: [1m[32m0.18992[0m[0m | time: 54.985s
[2K
| Adam | epoch: 004 | loss: 0.18992 - acc: 0.9289 -- iter: 160/215
[A[ATraining Step: 27  | total loss: [1m[32m0.17002[0m[0m | time: 66.724s
[2K
| Adam | epoch: 004 | loss: 0.17002 - acc: 0.9391 -- iter: 192/215
[A[ATraining Step: 28  | total loss: [1m[32m0.18681[0m[0m | time: 83.726s
[2K
| Adam | epoch: 004 | loss: 0.18681 - acc: 0.9387 | val_loss: 0.64607 - val_acc: 0.6324 -- iter: 215/215
--
Training Step: 29  | total loss: [1m[32m0.18730[0m[0m | time: 16.513s
[2K
| Adam | epoch: 005 | loss: 0.18730 - acc: 0.9384 -- iter: 032/215
[A[ATraining Step: 30  | total loss: [1m[32m0.22824[0m[0m | time: 33.306s
[2K
| Adam | epoch: 005 | loss: 0.22824 - acc: 0.9234 -- iter: 064/215
[A[ATraining Step: 31  | total loss: [1m[32m0.18365[0m[0m | time: 46.773s
[2K
| Adam | epoch: 005 | loss: 0.18365 - acc: 0.9411 -- iter: 096/215
[A[ATraining Step: 32  | total loss: [1m[32m0.67724[0m[0m | time: 60.143s
[2K
| Adam | epoch: 005 | loss: 0.67724 - acc: 0.8272 -- iter: 128/215
[A[ATraining Step: 33  | total loss: [1m[32m0.54301[0m[0m | time: 76.992s
[2K
| Adam | epoch: 005 | loss: 0.54301 - acc: 0.8651 -- iter: 160/215
[A[ATraining Step: 34  | total loss: [1m[32m0.44949[0m[0m | time: 93.558s
[2K
| Adam | epoch: 005 | loss: 0.44949 - acc: 0.8873 -- iter: 192/215
[A[ATraining Step: 35  | total loss: [1m[32m0.41261[0m[0m | time: 119.191s
[2K
| Adam | epoch: 005 | loss: 0.41261 - acc: 0.8913 | val_loss: 1.09356 - val_acc: 0.6324 -- iter: 215/215
--
Training Step: 36  | total loss: [1m[32m0.36975[0m[0m | time: 8.434s
[2K
| Adam | epoch: 006 | loss: 0.36975 - acc: 0.9007 -- iter: 032/215
[A[ATraining Step: 37  | total loss: [1m[32m0.32872[0m[0m | time: 16.343s
[2K
| Adam | epoch: 006 | loss: 0.32872 - acc: 0.9143 -- iter: 064/215
[A[ATraining Step: 38  | total loss: [1m[32m0.28271[0m[0m | time: 25.721s
[2K
| Adam | epoch: 006 | loss: 0.28271 - acc: 0.9250 -- iter: 096/215
[A[ATraining Step: 39  | total loss: [1m[32m0.31741[0m[0m | time: 35.111s
[2K
| Adam | epoch: 006 | loss: 0.31741 - acc: 0.9094 -- iter: 128/215
[A[ATraining Step: 40  | total loss: [1m[32m0.44088[0m[0m | time: 45.300s
[2K
| Adam | epoch: 006 | loss: 0.44088 - acc: 0.8856 -- iter: 160/215
[A[ATraining Step: 41  | total loss: [1m[32m0.38555[0m[0m | time: 61.516s
[2K
| Adam | epoch: 006 | loss: 0.38555 - acc: 0.8987 -- iter: 192/215
[A[ATraining Step: 42  | total loss: [1m[32m0.41832[0m[0m | time: 87.042s
[2K
| Adam | epoch: 006 | loss: 0.41832 - acc: 0.8775 | val_loss: 0.66467 - val_acc: 0.7206 -- iter: 215/215
--
Training Step: 43  | total loss: [1m[32m0.35565[0m[0m | time: 16.941s
[2K
| Adam | epoch: 007 | loss: 0.35565 - acc: 0.8991 -- iter: 032/215
[A[ATraining Step: 44  | total loss: [1m[32m0.31926[0m[0m | time: 34.063s
[2K
| Adam | epoch: 007 | loss: 0.31926 - acc: 0.9004 -- iter: 064/215
[A[ATraining Step: 45  | total loss: [1m[32m0.28271[0m[0m | time: 51.182s
[2K
| Adam | epoch: 007 | loss: 0.28271 - acc: 0.9173 -- iter: 096/215
[A[ATraining Step: 46  | total loss: [1m[32m0.26296[0m[0m | time: 68.984s
[2K
| Adam | epoch: 007 | loss: 0.26296 - acc: 0.9207 -- iter: 128/215
[A[ATraining Step: 47  | total loss: [1m[32m0.24972[0m[0m | time: 79.321s
[2K
| Adam | epoch: 007 | loss: 0.24972 - acc: 0.9183 -- iter: 160/215
[A[ATraining Step: 48  | total loss: [1m[32m0.27116[0m[0m | time: 89.619s
[2K
| Adam | epoch: 007 | loss: 0.27116 - acc: 0.9175 -- iter: 192/215
[A[ATraining Step: 49  | total loss: [1m[32m0.25565[0m[0m | time: 111.410s
[2K
| Adam | epoch: 007 | loss: 0.25565 - acc: 0.9236 | val_loss: 0.70409 - val_acc: 0.7794 -- iter: 215/215
--
Training Step: 50  | total loss: [1m[32m0.22818[0m[0m | time: 14.137s
[2K
| Adam | epoch: 008 | loss: 0.22818 - acc: 0.9306 -- iter: 032/215
[A[ATraining Step: 51  | total loss: [1m[32m0.21417[0m[0m | time: 31.327s
[2K
| Adam | epoch: 008 | loss: 0.21417 - acc: 0.9364 -- iter: 064/215
[A[ATraining Step: 52  | total loss: [1m[32m0.20988[0m[0m | time: 48.485s
[2K
| Adam | epoch: 008 | loss: 0.20988 - acc: 0.9366 -- iter: 096/215
[A[ATraining Step: 53  | total loss: [1m[32m0.18551[0m[0m | time: 65.520s
[2K
| Adam | epoch: 008 | loss: 0.18551 - acc: 0.9460 -- iter: 128/215
[A[ATraining Step: 54  | total loss: [1m[32m0.20203[0m[0m | time: 83.059s
[2K
| Adam | epoch: 008 | loss: 0.20203 - acc: 0.9402 -- iter: 160/215
[A[ATraining Step: 55  | total loss: [1m[32m0.18725[0m[0m | time: 94.692s
[2K
| Adam | epoch: 008 | loss: 0.18725 - acc: 0.9443 -- iter: 192/215
[A[ATraining Step: 56  | total loss: [1m[32m0.17793[0m[0m | time: 109.086s
[2K
| Adam | epoch: 008 | loss: 0.17793 - acc: 0.9399 | val_loss: 0.62601 - val_acc: 0.7206 -- iter: 215/215
--
Training Step: 57  | total loss: [1m[32m0.16234[0m[0m | time: 16.789s
[2K
| Adam | epoch: 009 | loss: 0.16234 - acc: 0.9482 -- iter: 032/215
[A[ATraining Step: 58  | total loss: [1m[32m0.17637[0m[0m | time: 33.050s
[2K
| Adam | epoch: 009 | loss: 0.17637 - acc: 0.9425 -- iter: 064/215
[A[ATraining Step: 59  | total loss: [1m[32m0.16973[0m[0m | time: 50.020s
[2K
| Adam | epoch: 009 | loss: 0.16973 - acc: 0.9460 -- iter: 096/215
[A[ATraining Step: 60  | total loss: [1m[32m0.16419[0m[0m | time: 67.252s
[2K
| Adam | epoch: 009 | loss: 0.16419 - acc: 0.9490 -- iter: 128/215
[A[ATraining Step: 61  | total loss: [1m[32m0.15675[0m[0m | time: 84.672s
[2K
| Adam | epoch: 009 | loss: 0.15675 - acc: 0.9516 -- iter: 160/215
[A[ATraining Step: 62  | total loss: [1m[32m0.14048[0m[0m | time: 101.993s
[2K
| Adam | epoch: 009 | loss: 0.14048 - acc: 0.9578 -- iter: 192/215
[A[ATraining Step: 63  | total loss: [1m[32m0.12944[0m[0m | time: 122.622s
[2K
| Adam | epoch: 009 | loss: 0.12944 - acc: 0.9632 | val_loss: 1.12538 - val_acc: 0.5588 -- iter: 215/215
--
Training Step: 64  | total loss: [1m[32m0.15520[0m[0m | time: 11.168s
[2K
| Adam | epoch: 010 | loss: 0.15520 - acc: 0.9460 -- iter: 032/215
[A[ATraining Step: 65  | total loss: [1m[32m0.15600[0m[0m | time: 23.154s
[2K
| Adam | epoch: 010 | loss: 0.15600 - acc: 0.9420 -- iter: 064/215
[A[ATraining Step: 66  | total loss: [1m[32m0.15510[0m[0m | time: 35.956s
[2K
| Adam | epoch: 010 | loss: 0.15510 - acc: 0.9414 -- iter: 096/215
[A[ATraining Step: 67  | total loss: [1m[32m0.14259[0m[0m | time: 53.139s
[2K
| Adam | epoch: 010 | loss: 0.14259 - acc: 0.9447 -- iter: 128/215
[A[ATraining Step: 68  | total loss: [1m[32m0.13953[0m[0m | time: 70.219s
[2K
| Adam | epoch: 010 | loss: 0.13953 - acc: 0.9475 -- iter: 160/215
[A[ATraining Step: 69  | total loss: [1m[32m0.12582[0m[0m | time: 86.071s
[2K
| Adam | epoch: 010 | loss: 0.12582 - acc: 0.9537 -- iter: 192/215
[A[ATraining Step: 70  | total loss: [1m[32m0.11994[0m[0m | time: 109.960s
[2K
| Adam | epoch: 010 | loss: 0.11994 - acc: 0.9518 | val_loss: 0.55764 - val_acc: 0.7059 -- iter: 215/215
--
Training Step: 71  | total loss: [1m[32m0.11172[0m[0m | time: 13.506s
[2K
| Adam | epoch: 011 | loss: 0.11172 - acc: 0.9573 -- iter: 032/215
[A[ATraining Step: 72  | total loss: [1m[32m0.10091[0m[0m | time: 26.935s
[2K
| Adam | epoch: 011 | loss: 0.10091 - acc: 0.9621 -- iter: 064/215
[A[ATraining Step: 73  | total loss: [1m[32m0.09164[0m[0m | time: 43.470s
[2K
| Adam | epoch: 011 | loss: 0.09164 - acc: 0.9663 -- iter: 096/215
[A[ATraining Step: 74  | total loss: [1m[32m0.08958[0m[0m | time: 58.883s
[2K
| Adam | epoch: 011 | loss: 0.08958 - acc: 0.9666 -- iter: 128/215
[A[ATraining Step: 75  | total loss: [1m[32m0.08180[0m[0m | time: 70.705s
[2K
| Adam | epoch: 011 | loss: 0.08180 - acc: 0.9702 -- iter: 160/215
[A[ATraining Step: 76  | total loss: [1m[32m0.07617[0m[0m | time: 82.853s
[2K
| Adam | epoch: 011 | loss: 0.07617 - acc: 0.9734 -- iter: 192/215
[A[ATraining Step: 77  | total loss: [1m[32m0.06945[0m[0m | time: 100.169s
[2K
| Adam | epoch: 011 | loss: 0.06945 - acc: 0.9762 | val_loss: 0.92238 - val_acc: 0.7500 -- iter: 215/215
--
Training Step: 78  | total loss: [1m[32m0.07641[0m[0m | time: 12.158s
[2K
| Adam | epoch: 012 | loss: 0.07641 - acc: 0.9754 -- iter: 032/215
[A[ATraining Step: 79  | total loss: [1m[32m0.07686[0m[0m | time: 21.202s
[2K
| Adam | epoch: 012 | loss: 0.07686 - acc: 0.9747 -- iter: 064/215
[A[ATraining Step: 80  | total loss: [1m[32m0.06965[0m[0m | time: 30.662s
[2K
| Adam | epoch: 012 | loss: 0.06965 - acc: 0.9773 -- iter: 096/215
[A[ATraining Step: 81  | total loss: [1m[32m0.06309[0m[0m | time: 42.733s
[2K
| Adam | epoch: 012 | loss: 0.06309 - acc: 0.9796 -- iter: 128/215
[A[ATraining Step: 82  | total loss: [1m[32m0.05936[0m[0m | time: 54.646s
[2K
| Adam | epoch: 012 | loss: 0.05936 - acc: 0.9817 -- iter: 160/215
[A[ATraining Step: 83  | total loss: [1m[32m0.06071[0m[0m | time: 66.381s
[2K
| Adam | epoch: 012 | loss: 0.06071 - acc: 0.9804 -- iter: 192/215
[A[ATraining Step: 84  | total loss: [1m[32m0.05543[0m[0m | time: 82.901s
[2K
| Adam | epoch: 012 | loss: 0.05543 - acc: 0.9823 | val_loss: 0.77492 - val_acc: 0.8088 -- iter: 215/215
--
Training Step: 85  | total loss: [1m[32m0.06069[0m[0m | time: 11.734s
[2K
| Adam | epoch: 013 | loss: 0.06069 - acc: 0.9778 -- iter: 032/215
[A[ATraining Step: 86  | total loss: [1m[32m0.05516[0m[0m | time: 23.772s
[2K
| Adam | epoch: 013 | loss: 0.05516 - acc: 0.9801 -- iter: 064/215
[A[ATraining Step: 87  | total loss: [1m[32m0.05102[0m[0m | time: 33.037s
[2K
| Adam | epoch: 013 | loss: 0.05102 - acc: 0.9821 -- iter: 096/215
[A[ATraining Step: 88  | total loss: [1m[32m0.09470[0m[0m | time: 42.352s
[2K
| Adam | epoch: 013 | loss: 0.09470 - acc: 0.9752 -- iter: 128/215
[A[ATraining Step: 89  | total loss: [1m[32m0.08592[0m[0m | time: 54.065s
[2K
| Adam | epoch: 013 | loss: 0.08592 - acc: 0.9776 -- iter: 160/215
[A[ATraining Step: 90  | total loss: [1m[32m0.08506[0m[0m | time: 65.772s
[2K
| Adam | epoch: 013 | loss: 0.08506 - acc: 0.9768 -- iter: 192/215
[A[ATraining Step: 91  | total loss: [1m[32m0.07731[0m[0m | time: 82.460s
[2K
| Adam | epoch: 013 | loss: 0.07731 - acc: 0.9791 | val_loss: 0.63204 - val_acc: 0.7500 -- iter: 215/215
--
Training Step: 92  | total loss: [1m[32m0.08230[0m[0m | time: 11.990s
[2K
| Adam | epoch: 014 | loss: 0.08230 - acc: 0.9718 -- iter: 032/215
[A[ATraining Step: 93  | total loss: [1m[32m0.07701[0m[0m | time: 23.786s
[2K
| Adam | epoch: 014 | loss: 0.07701 - acc: 0.9746 -- iter: 064/215
[A[ATraining Step: 94  | total loss: [1m[32m0.07947[0m[0m | time: 35.721s
[2K
| Adam | epoch: 014 | loss: 0.07947 - acc: 0.9740 -- iter: 096/215
[A[ATraining Step: 95  | total loss: [1m[32m0.07455[0m[0m | time: 44.808s
[2K
| Adam | epoch: 014 | loss: 0.07455 - acc: 0.9735 -- iter: 128/215
[A[ATraining Step: 96  | total loss: [1m[32m0.21220[0m[0m | time: 53.774s
[2K
| Adam | epoch: 014 | loss: 0.21220 - acc: 0.9501 -- iter: 160/215
[A[ATraining Step: 97  | total loss: [1m[32m0.19670[0m[0m | time: 65.548s
[2K
| Adam | epoch: 014 | loss: 0.19670 - acc: 0.9507 -- iter: 192/215
[A[ATraining Step: 98  | total loss: [1m[32m0.18345[0m[0m | time: 82.341s
[2K
| Adam | epoch: 014 | loss: 0.18345 - acc: 0.9525 | val_loss: 2.19283 - val_acc: 0.4118 -- iter: 215/215
--
Training Step: 99  | total loss: [1m[32m0.16795[0m[0m | time: 11.594s
[2K
| Adam | epoch: 015 | loss: 0.16795 - acc: 0.9573 -- iter: 032/215
[A[ATraining Step: 100  | total loss: [1m[32m0.15631[0m[0m | time: 24.331s
[2K
| Adam | epoch: 015 | loss: 0.15631 - acc: 0.9584 -- iter: 064/215
[A[ATraining Step: 101  | total loss: [1m[32m0.15395[0m[0m | time: 37.191s
[2K
| Adam | epoch: 015 | loss: 0.15395 - acc: 0.9532 -- iter: 096/215
[A[ATraining Step: 102  | total loss: [1m[32m0.16203[0m[0m | time: 49.923s
[2K
| Adam | epoch: 015 | loss: 0.16203 - acc: 0.9485 -- iter: 128/215
[A[ATraining Step: 103  | total loss: [1m[32m0.14952[0m[0m | time: 60.114s
[2K
| Adam | epoch: 015 | loss: 0.14952 - acc: 0.9505 -- iter: 160/215
[A[ATraining Step: 104  | total loss: [1m[32m0.20967[0m[0m | time: 67.281s
[2K
| Adam | epoch: 015 | loss: 0.20967 - acc: 0.9424 -- iter: 192/215
[A[ATraining Step: 105  | total loss: [1m[32m0.19326[0m[0m | time: 78.173s
[2K
| Adam | epoch: 015 | loss: 0.19326 - acc: 0.9482 | val_loss: 0.84682 - val_acc: 0.6176 -- iter: 215/215
--
Validation AUC:0.7739534883720931
Validation AUPRC:0.7431537207507454
Test AUC:0.8669642857142857
Test AUPRC:0.8405578984423585
BestTestF1Score	0.77	0.68	0.84	0.95	0.64	18	1	39	10	0.98
BestTestMCCScore	0.77	0.68	0.84	0.95	0.64	18	1	39	10	0.98
BestTestAccuracyScore	0.77	0.68	0.84	0.95	0.64	18	1	39	10	0.98
BestValidationF1Score	0.67	0.55	0.79	0.82	0.56	14	3	40	11	0.98
BestValidationMCC	0.67	0.55	0.79	0.82	0.56	14	3	40	11	0.98
BestValidationAccuracy	0.67	0.55	0.79	0.82	0.56	14	3	40	11	0.98
TestPredictions (Threshold:0.98)
CHEMBL9724,TN,INACT,0.46000000834465027	CHEMBL3330052,TP,ACT,1.0	CHEMBL511346,TN,INACT,0.4000000059604645	CHEMBL380090,TP,ACT,1.0	CHEMBL164,TN,INACT,0.3700000047683716	CHEMBL182168,TN,INACT,0.11999999731779099	CHEMBL1243092,TP,ACT,1.0	CHEMBL196188,TN,INACT,0.9599999785423279	CHEMBL3806052,TN,INACT,0.28999999165534973	CHEMBL116509,TN,INACT,0.949999988079071	CHEMBL234413,TN,INACT,0.0	CHEMBL184672,TN,INACT,0.7699999809265137	CHEMBL3233842,FN,ACT,0.9100000262260437	CHEMBL147691,TN,INACT,0.009999999776482582	CHEMBL199450,FN,ACT,0.9100000262260437	CHEMBL217753,TP,ACT,1.0	CHEMBL88474,TN,INACT,0.10999999940395355	CHEMBL2332621,TP,ACT,1.0	CHEMBL288207,TN,INACT,0.4300000071525574	CHEMBL88953,TN,INACT,0.12999999523162842	CHEMBL83418,TN,INACT,0.8100000023841858	CHEMBL1182947,TN,INACT,0.9200000166893005	CHEMBL459438,TN,INACT,0.7799999713897705	CHEMBL2333023,TP,ACT,1.0	CHEMBL269277,TN,INACT,0.8100000023841858	CHEMBL3093924,TP,ACT,1.0	CHEMBL2332620,TP,ACT,1.0	CHEMBL194428,TP,ACT,1.0	CHEMBL397353,TN,INACT,0.25999999046325684	CHEMBL392924,TP,ACT,1.0	CHEMBL3616895,TP,ACT,0.9800000190734863	CHEMBL3805154,TN,INACT,0.8100000023841858	CHEMBL3805346,FN,ACT,0.25	CHEMBL444537,TN,INACT,0.07000000029802322	CHEMBL3093908,FN,ACT,0.9300000071525574	CHEMBL9237,TN,INACT,0.4399999976158142	CHEMBL3093903,TP,ACT,1.0	CHEMBL1243061,TP,ACT,1.0	CHEMBL216410,TP,ACT,1.0	CHEMBL3329629,TN,INACT,0.05000000074505806	CHEMBL466590,TN,INACT,0.3199999928474426	CHEMBL539758,TN,INACT,0.15000000596046448	CHEMBL3616917,FN,ACT,0.75	CHEMBL3329634,TN,INACT,0.699999988079071	CHEMBL234428,TN,INACT,0.6399999856948853	CHEMBL1096689,FP,INACT,1.0	CHEMBL67089,FN,ACT,0.949999988079071	CHEMBL3806023,FN,ACT,0.8999999761581421	CHEMBL1243153,TP,ACT,1.0	CHEMBL3805235,TN,INACT,0.30000001192092896	CHEMBL236586,TN,INACT,0.9200000166893005	CHEMBL3357769,FN,ACT,0.800000011920929	CHEMBL3357772,TP,ACT,0.9900000095367432	CHEMBL9561,TN,INACT,0.8899999856948853	CHEMBL365414,TN,INACT,0.949999988079071	CHEMBL3330070,FN,ACT,0.3799999952316284	CHEMBL346877,TN,INACT,0.029999999329447746	CHEMBL3805400,TN,INACT,0.699999988079071	CHEMBL383573,TN,INACT,0.9599999785423279	CHEMBL3329639,TN,INACT,0.9200000166893005	CHEMBL208964,TP,ACT,0.9900000095367432	CHEMBL66189,TN,INACT,0.9399999976158142	CHEMBL450091,TN,INACT,0.7099999785423279	CHEMBL403862,TN,INACT,0.23999999463558197	CHEMBL3330053,TP,ACT,1.0	CHEMBL99506,TN,INACT,0.38999998569488525	CHEMBL371429,FN,ACT,0.49000000953674316	CHEMBL3805294,TN,INACT,0.05000000074505806	

