ImageNetInceptionV2 CHEMBL3243 adam 0.0005 15 0 0 0.6 False True
Number of active compounds :	134
Number of inactive compounds :	134
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL3243_adam_0.0005_15_0_0_0.6_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL3243_adam_0.0005_15_0.6/
---------------------------------
Training samples: 170
Validation samples: 54
--
Training Step: 1  | time: 37.724s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/170
[A[ATraining Step: 2  | total loss: [1m[32m0.68506[0m[0m | time: 46.283s
[2K
| Adam | epoch: 001 | loss: 0.68506 - acc: 0.2531 -- iter: 064/170
[A[ATraining Step: 3  | total loss: [1m[32m0.62440[0m[0m | time: 54.690s
[2K
| Adam | epoch: 001 | loss: 0.62440 - acc: 0.5318 -- iter: 096/170
[A[ATraining Step: 4  | total loss: [1m[32m0.74938[0m[0m | time: 63.292s
[2K
| Adam | epoch: 001 | loss: 0.74938 - acc: 0.6017 -- iter: 128/170
[A[ATraining Step: 5  | total loss: [1m[32m0.59700[0m[0m | time: 71.796s
[2K
| Adam | epoch: 001 | loss: 0.59700 - acc: 0.7044 -- iter: 160/170
[A[ATraining Step: 6  | total loss: [1m[32m0.65653[0m[0m | time: 87.237s
[2K
| Adam | epoch: 001 | loss: 0.65653 - acc: 0.6935 | val_loss: 2.55932 - val_acc: 0.4815 -- iter: 170/170
--
Training Step: 7  | total loss: [1m[32m0.47443[0m[0m | time: 5.674s
[2K
| Adam | epoch: 002 | loss: 0.47443 - acc: 0.8174 -- iter: 032/170
[A[ATraining Step: 8  | total loss: [1m[32m0.28192[0m[0m | time: 125.907s
[2K
| Adam | epoch: 002 | loss: 0.28192 - acc: 0.9201 -- iter: 064/170
[A[ATraining Step: 9  | total loss: [1m[32m0.55174[0m[0m | time: 303.029s
[2K
| Adam | epoch: 002 | loss: 0.55174 - acc: 0.7473 -- iter: 096/170
[A[ATraining Step: 10  | total loss: [1m[32m0.77782[0m[0m | time: 342.045s
[2K
| Adam | epoch: 002 | loss: 0.77782 - acc: 0.6705 -- iter: 128/170
[A[ATraining Step: 11  | total loss: [1m[32m0.64860[0m[0m | time: 369.083s
[2K
| Adam | epoch: 002 | loss: 0.64860 - acc: 0.7082 -- iter: 160/170
[A[ATraining Step: 12  | total loss: [1m[32m0.54987[0m[0m | time: 388.583s
[2K
| Adam | epoch: 002 | loss: 0.54987 - acc: 0.7411 | val_loss: 2.16631 - val_acc: 0.4815 -- iter: 170/170
--
Training Step: 13  | total loss: [1m[32m0.50978[0m[0m | time: 6.039s
[2K
| Adam | epoch: 003 | loss: 0.50978 - acc: 0.7717 -- iter: 032/170
[A[ATraining Step: 14  | total loss: [1m[32m0.46495[0m[0m | time: 12.071s
[2K
| Adam | epoch: 003 | loss: 0.46495 - acc: 0.8242 -- iter: 064/170
[A[ATraining Step: 15  | total loss: [1m[32m0.33463[0m[0m | time: 28.667s
[2K
| Adam | epoch: 003 | loss: 0.33463 - acc: 0.8930 -- iter: 096/170
[A[ATraining Step: 16  | total loss: [1m[32m0.34274[0m[0m | time: 45.205s
[2K
| Adam | epoch: 003 | loss: 0.34274 - acc: 0.8862 -- iter: 128/170
[A[ATraining Step: 17  | total loss: [1m[32m0.37215[0m[0m | time: 59.364s
[2K
| Adam | epoch: 003 | loss: 0.37215 - acc: 0.8597 -- iter: 160/170
[A[ATraining Step: 18  | total loss: [1m[32m0.29961[0m[0m | time: 79.101s
[2K
| Adam | epoch: 003 | loss: 0.29961 - acc: 0.8974 | val_loss: 4.54709 - val_acc: 0.4815 -- iter: 170/170
--
Training Step: 19  | total loss: [1m[32m0.26773[0m[0m | time: 26.748s
[2K
| Adam | epoch: 004 | loss: 0.26773 - acc: 0.9108 -- iter: 032/170
[A[ATraining Step: 20  | total loss: [1m[32m0.29550[0m[0m | time: 32.646s
[2K
| Adam | epoch: 004 | loss: 0.29550 - acc: 0.8993 -- iter: 064/170
[A[ATraining Step: 21  | total loss: [1m[32m0.24716[0m[0m | time: 38.235s
[2K
| Adam | epoch: 004 | loss: 0.24716 - acc: 0.9305 -- iter: 096/170
[A[ATraining Step: 22  | total loss: [1m[32m0.19559[0m[0m | time: 55.786s
[2K
| Adam | epoch: 004 | loss: 0.19559 - acc: 0.9514 -- iter: 128/170
[A[ATraining Step: 23  | total loss: [1m[32m0.17555[0m[0m | time: 70.292s
[2K
| Adam | epoch: 004 | loss: 0.17555 - acc: 0.9474 -- iter: 160/170
[A[ATraining Step: 24  | total loss: [1m[32m0.16422[0m[0m | time: 88.647s
[2K
| Adam | epoch: 004 | loss: 0.16422 - acc: 0.9534 | val_loss: 4.41443 - val_acc: 0.4815 -- iter: 170/170
--
Training Step: 25  | total loss: [1m[32m0.12577[0m[0m | time: 19.768s
[2K
| Adam | epoch: 005 | loss: 0.12577 - acc: 0.9661 -- iter: 032/170
[A[ATraining Step: 26  | total loss: [1m[32m0.11165[0m[0m | time: 39.802s
[2K
| Adam | epoch: 005 | loss: 0.11165 - acc: 0.9668 -- iter: 064/170
[A[ATraining Step: 27  | total loss: [1m[32m0.12238[0m[0m | time: 45.136s
[2K
| Adam | epoch: 005 | loss: 0.12238 - acc: 0.9673 -- iter: 096/170
[A[ATraining Step: 28  | total loss: [1m[32m0.16152[0m[0m | time: 50.653s
[2K
| Adam | epoch: 005 | loss: 0.16152 - acc: 0.9505 -- iter: 128/170
[A[ATraining Step: 29  | total loss: [1m[32m0.13163[0m[0m | time: 63.899s
[2K
| Adam | epoch: 005 | loss: 0.13163 - acc: 0.9625 -- iter: 160/170
[A[ATraining Step: 30  | total loss: [1m[32m0.12782[0m[0m | time: 81.004s
[2K
| Adam | epoch: 005 | loss: 0.12782 - acc: 0.9640 | val_loss: 5.60813 - val_acc: 0.4815 -- iter: 170/170
--
Training Step: 31  | total loss: [1m[32m0.13278[0m[0m | time: 14.579s
[2K
| Adam | epoch: 006 | loss: 0.13278 - acc: 0.9651 -- iter: 032/170
[A[ATraining Step: 32  | total loss: [1m[32m0.10935[0m[0m | time: 28.359s
[2K
| Adam | epoch: 006 | loss: 0.10935 - acc: 0.9729 -- iter: 064/170
[A[ATraining Step: 33  | total loss: [1m[32m0.09782[0m[0m | time: 41.453s
[2K
| Adam | epoch: 006 | loss: 0.09782 - acc: 0.9720 -- iter: 096/170
[A[ATraining Step: 34  | total loss: [1m[32m0.13773[0m[0m | time: 46.534s
[2K
| Adam | epoch: 006 | loss: 0.13773 - acc: 0.9646 -- iter: 128/170
[A[ATraining Step: 35  | total loss: [1m[32m0.14402[0m[0m | time: 52.410s
[2K
| Adam | epoch: 006 | loss: 0.14402 - acc: 0.9511 -- iter: 160/170
[A[ATraining Step: 36  | total loss: [1m[32m0.11862[0m[0m | time: 68.965s
[2K
| Adam | epoch: 006 | loss: 0.11862 - acc: 0.9611 | val_loss: 6.85916 - val_acc: 0.4815 -- iter: 170/170
--
Training Step: 37  | total loss: [1m[32m0.11694[0m[0m | time: 13.370s
[2K
| Adam | epoch: 007 | loss: 0.11694 - acc: 0.9626 -- iter: 032/170
[A[ATraining Step: 38  | total loss: [1m[32m0.19020[0m[0m | time: 26.604s
[2K
| Adam | epoch: 007 | loss: 0.19020 - acc: 0.9455 -- iter: 064/170
[A[ATraining Step: 39  | total loss: [1m[32m0.16284[0m[0m | time: 39.917s
[2K
| Adam | epoch: 007 | loss: 0.16284 - acc: 0.9559 -- iter: 096/170
[A[ATraining Step: 40  | total loss: [1m[32m0.16464[0m[0m | time: 59.620s
[2K
| Adam | epoch: 007 | loss: 0.16464 - acc: 0.9525 -- iter: 128/170
[A[ATraining Step: 41  | total loss: [1m[32m0.17966[0m[0m | time: 65.216s
[2K
| Adam | epoch: 007 | loss: 0.17966 - acc: 0.9497 -- iter: 160/170
[A[ATraining Step: 42  | total loss: [1m[32m0.18200[0m[0m | time: 74.618s
[2K
| Adam | epoch: 007 | loss: 0.18200 - acc: 0.9408 | val_loss: 7.54393 - val_acc: 0.4815 -- iter: 170/170
--
Training Step: 43  | total loss: [1m[32m0.15363[0m[0m | time: 12.633s
[2K
| Adam | epoch: 008 | loss: 0.15363 - acc: 0.9512 -- iter: 032/170
[A[ATraining Step: 44  | total loss: [1m[32m0.21020[0m[0m | time: 25.523s
[2K
| Adam | epoch: 008 | loss: 0.21020 - acc: 0.9380 -- iter: 064/170
[A[ATraining Step: 45  | total loss: [1m[32m0.23032[0m[0m | time: 37.957s
[2K
| Adam | epoch: 008 | loss: 0.23032 - acc: 0.9379 -- iter: 096/170
[A[ATraining Step: 46  | total loss: [1m[32m0.20449[0m[0m | time: 50.227s
[2K
| Adam | epoch: 008 | loss: 0.20449 - acc: 0.9431 -- iter: 128/170
[A[ATraining Step: 47  | total loss: [1m[32m0.18331[0m[0m | time: 62.712s
[2K
| Adam | epoch: 008 | loss: 0.18331 - acc: 0.9524 -- iter: 160/170
[A[ATraining Step: 48  | total loss: [1m[32m0.16923[0m[0m | time: 72.605s
[2K
| Adam | epoch: 008 | loss: 0.16923 - acc: 0.9550 | val_loss: 2.60477 - val_acc: 0.5370 -- iter: 170/170
--
Training Step: 49  | total loss: [1m[32m0.18535[0m[0m | time: 5.139s
[2K
| Adam | epoch: 009 | loss: 0.18535 - acc: 0.9463 -- iter: 032/170
[A[ATraining Step: 50  | total loss: [1m[32m0.16006[0m[0m | time: 18.012s
[2K
| Adam | epoch: 009 | loss: 0.16006 - acc: 0.9547 -- iter: 064/170
[A[ATraining Step: 51  | total loss: [1m[32m0.18063[0m[0m | time: 30.596s
[2K
| Adam | epoch: 009 | loss: 0.18063 - acc: 0.9473 -- iter: 096/170
[A[ATraining Step: 52  | total loss: [1m[32m0.17686[0m[0m | time: 42.910s
[2K
| Adam | epoch: 009 | loss: 0.17686 - acc: 0.9505 -- iter: 128/170
[A[ATraining Step: 53  | total loss: [1m[32m0.17236[0m[0m | time: 55.813s
[2K
| Adam | epoch: 009 | loss: 0.17236 - acc: 0.9486 -- iter: 160/170
[A[ATraining Step: 54  | total loss: [1m[32m0.15435[0m[0m | time: 72.347s
[2K
| Adam | epoch: 009 | loss: 0.15435 - acc: 0.9560 | val_loss: 1.03485 - val_acc: 0.6852 -- iter: 170/170
--
Training Step: 55  | total loss: [1m[32m0.19032[0m[0m | time: 5.490s
[2K
| Adam | epoch: 010 | loss: 0.19032 - acc: 0.9534 -- iter: 032/170
[A[ATraining Step: 56  | total loss: [1m[32m0.27957[0m[0m | time: 10.896s
[2K
| Adam | epoch: 010 | loss: 0.27957 - acc: 0.9178 -- iter: 064/170
[A[ATraining Step: 57  | total loss: [1m[32m0.26743[0m[0m | time: 23.786s
[2K
| Adam | epoch: 010 | loss: 0.26743 - acc: 0.9153 -- iter: 096/170
[A[ATraining Step: 58  | total loss: [1m[32m0.25375[0m[0m | time: 36.645s
[2K
| Adam | epoch: 010 | loss: 0.25375 - acc: 0.9183 -- iter: 128/170
[A[ATraining Step: 59  | total loss: [1m[32m0.33895[0m[0m | time: 48.985s
[2K
| Adam | epoch: 010 | loss: 0.33895 - acc: 0.8957 -- iter: 160/170
[A[ATraining Step: 60  | total loss: [1m[32m0.33465[0m[0m | time: 65.922s
[2K
| Adam | epoch: 010 | loss: 0.33465 - acc: 0.8847 | val_loss: 4.64318 - val_acc: 0.5185 -- iter: 170/170
--
Training Step: 61  | total loss: [1m[32m0.30158[0m[0m | time: 12.745s
[2K
| Adam | epoch: 011 | loss: 0.30158 - acc: 0.8997 -- iter: 032/170
[A[ATraining Step: 62  | total loss: [1m[32m0.27936[0m[0m | time: 18.381s
[2K
| Adam | epoch: 011 | loss: 0.27936 - acc: 0.9086 -- iter: 064/170
[A[ATraining Step: 63  | total loss: [1m[32m0.25702[0m[0m | time: 23.651s
[2K
| Adam | epoch: 011 | loss: 0.25702 - acc: 0.9202 -- iter: 096/170
[A[ATraining Step: 64  | total loss: [1m[32m0.22749[0m[0m | time: 35.891s
[2K
| Adam | epoch: 011 | loss: 0.22749 - acc: 0.9302 -- iter: 128/170
[A[ATraining Step: 65  | total loss: [1m[32m0.21267[0m[0m | time: 44.977s
[2K
| Adam | epoch: 011 | loss: 0.21267 - acc: 0.9349 -- iter: 160/170
[A[ATraining Step: 66  | total loss: [1m[32m0.27014[0m[0m | time: 60.320s
[2K
| Adam | epoch: 011 | loss: 0.27014 - acc: 0.9238 | val_loss: 5.25690 - val_acc: 0.4815 -- iter: 170/170
--
Training Step: 67  | total loss: [1m[32m0.25056[0m[0m | time: 11.840s
[2K
| Adam | epoch: 012 | loss: 0.25056 - acc: 0.9292 -- iter: 032/170
[A[ATraining Step: 68  | total loss: [1m[32m0.24436[0m[0m | time: 19.895s
[2K
| Adam | epoch: 012 | loss: 0.24436 - acc: 0.9265 -- iter: 064/170
[A[ATraining Step: 69  | total loss: [1m[32m0.23936[0m[0m | time: 23.209s
[2K
| Adam | epoch: 012 | loss: 0.23936 - acc: 0.9241 -- iter: 096/170
[A[ATraining Step: 70  | total loss: [1m[32m0.25576[0m[0m | time: 26.393s
[2K
| Adam | epoch: 012 | loss: 0.25576 - acc: 0.9098 -- iter: 128/170
[A[ATraining Step: 71  | total loss: [1m[32m0.23967[0m[0m | time: 34.564s
[2K
| Adam | epoch: 012 | loss: 0.23967 - acc: 0.9201 -- iter: 160/170
[A[ATraining Step: 72  | total loss: [1m[32m0.22200[0m[0m | time: 47.264s
[2K
| Adam | epoch: 012 | loss: 0.22200 - acc: 0.9256 | val_loss: 2.19433 - val_acc: 0.5556 -- iter: 170/170
--
Training Step: 73  | total loss: [1m[32m0.21190[0m[0m | time: 15.011s
[2K
| Adam | epoch: 013 | loss: 0.21190 - acc: 0.9304 -- iter: 032/170
[A[ATraining Step: 74  | total loss: [1m[32m0.19974[0m[0m | time: 30.045s
[2K
| Adam | epoch: 013 | loss: 0.19974 - acc: 0.9346 -- iter: 064/170
[A[ATraining Step: 75  | total loss: [1m[32m0.18644[0m[0m | time: 45.208s
[2K
| Adam | epoch: 013 | loss: 0.18644 - acc: 0.9383 -- iter: 096/170
[A[ATraining Step: 76  | total loss: [1m[32m0.18605[0m[0m | time: 51.840s
[2K
| Adam | epoch: 013 | loss: 0.18605 - acc: 0.9382 -- iter: 128/170
[A[ATraining Step: 77  | total loss: [1m[32m0.21504[0m[0m | time: 58.572s
[2K
| Adam | epoch: 013 | loss: 0.21504 - acc: 0.9236 -- iter: 160/170
[A[ATraining Step: 78  | total loss: [1m[32m0.20347[0m[0m | time: 79.413s
[2K
| Adam | epoch: 013 | loss: 0.20347 - acc: 0.9316 | val_loss: 1.42214 - val_acc: 0.5926 -- iter: 170/170
--
Training Step: 79  | total loss: [1m[32m0.23186[0m[0m | time: 15.701s
[2K
| Adam | epoch: 014 | loss: 0.23186 - acc: 0.9225 -- iter: 032/170
[A[ATraining Step: 80  | total loss: [1m[32m0.23360[0m[0m | time: 30.931s
[2K
| Adam | epoch: 014 | loss: 0.23360 - acc: 0.9240 -- iter: 064/170
[A[ATraining Step: 81  | total loss: [1m[32m0.21679[0m[0m | time: 46.461s
[2K
| Adam | epoch: 014 | loss: 0.21679 - acc: 0.9317 -- iter: 096/170
[A[ATraining Step: 82  | total loss: [1m[32m0.20643[0m[0m | time: 61.424s
[2K
| Adam | epoch: 014 | loss: 0.20643 - acc: 0.9385 -- iter: 128/170
[A[ATraining Step: 83  | total loss: [1m[32m0.19798[0m[0m | time: 68.331s
[2K
| Adam | epoch: 014 | loss: 0.19798 - acc: 0.9384 -- iter: 160/170
[A[ATraining Step: 84  | total loss: [1m[32m0.20911[0m[0m | time: 79.424s
[2K
| Adam | epoch: 014 | loss: 0.20911 - acc: 0.9346 | val_loss: 4.76892 - val_acc: 0.4815 -- iter: 170/170
--
Training Step: 85  | total loss: [1m[32m0.19057[0m[0m | time: 15.408s
[2K
| Adam | epoch: 015 | loss: 0.19057 - acc: 0.9411 -- iter: 032/170
[A[ATraining Step: 86  | total loss: [1m[32m0.17458[0m[0m | time: 30.329s
[2K
| Adam | epoch: 015 | loss: 0.17458 - acc: 0.9470 -- iter: 064/170
[A[ATraining Step: 87  | total loss: [1m[32m0.21823[0m[0m | time: 45.548s
[2K
| Adam | epoch: 015 | loss: 0.21823 - acc: 0.9398 -- iter: 096/170
[A[ATraining Step: 88  | total loss: [1m[32m0.20912[0m[0m | time: 60.809s
[2K
| Adam | epoch: 015 | loss: 0.20912 - acc: 0.9396 -- iter: 128/170
[A[ATraining Step: 89  | total loss: [1m[32m0.19269[0m[0m | time: 76.299s
[2K
| Adam | epoch: 015 | loss: 0.19269 - acc: 0.9456 -- iter: 160/170
[A[ATraining Step: 90  | total loss: [1m[32m0.18145[0m[0m | time: 87.775s
[2K
| Adam | epoch: 015 | loss: 0.18145 - acc: 0.9479 | val_loss: 1.52817 - val_acc: 0.6852 -- iter: 170/170
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.8200549450549451
Validation AUPRC:0.8272121674414524
Test AUC:0.7843406593406592
Test AUPRC:0.7791853767753468
BestTestF1Score	0.69	0.37	0.69	0.66	0.73	19	10	18	7	0.03
BestTestMCCScore	0.69	0.44	0.72	0.74	0.65	17	6	22	9	0.14
BestTestAccuracyScore	0.69	0.44	0.72	0.74	0.65	17	6	22	9	0.14
BestValidationF1Score	0.73	0.49	0.74	0.79	0.68	19	5	21	9	0.03
BestValidationMCC	0.72	0.55	0.76	0.89	0.61	17	2	24	11	0.14
BestValidationAccuracy	0.72	0.55	0.76	0.89	0.61	17	2	24	11	0.14
TestPredictions (Threshold:0.14)
CHEMBL273780,TN,INACT,0.009999999776482582	CHEMBL2159931,FP,INACT,0.15000000596046448	CHEMBL525078,TN,INACT,0.019999999552965164	CHEMBL499329,FP,INACT,0.3799999952316284	CHEMBL268782,TN,INACT,0.0	CHEMBL2372864,TP,ACT,0.5299999713897705	CHEMBL87002,TN,INACT,0.009999999776482582	CHEMBL51931,TP,ACT,1.0	CHEMBL50846,TP,ACT,0.9800000190734863	CHEMBL300664,TP,ACT,0.27000001072883606	CHEMBL38066,TP,ACT,0.9900000095367432	CHEMBL2372870,TP,ACT,0.9800000190734863	CHEMBL379196,FP,INACT,0.20000000298023224	CHEMBL2372876,TP,ACT,0.44999998807907104	CHEMBL1667960,TN,INACT,0.05000000074505806	CHEMBL527109,TN,INACT,0.09000000357627869	CHEMBL2316909,TN,INACT,0.03999999910593033	CHEMBL304854,TN,INACT,0.009999999776482582	CHEMBL342612,FP,INACT,0.7699999809265137	CHEMBL52388,TP,ACT,0.9900000095367432	CHEMBL2419083,TN,INACT,0.09000000357627869	CHEMBL51781,FN,ACT,0.11999999731779099	CHEMBL201662,TN,INACT,0.0	CHEMBL9252,TN,INACT,0.0	CHEMBL2372885,TP,ACT,0.7799999713897705	CHEMBL500677,TN,INACT,0.0	CHEMBL9374,TN,INACT,0.0	CHEMBL410646,TN,INACT,0.0	CHEMBL1910967,TP,ACT,0.7599999904632568	CHEMBL2372867,TP,ACT,0.7599999904632568	CHEMBL2372892,TP,ACT,0.6899999976158142	CHEMBL267245,TN,INACT,0.0	CHEMBL590043,TN,INACT,0.0	CHEMBL3144419,TP,ACT,0.1899999976158142	CHEMBL338073,FN,ACT,0.029999999329447746	CHEMBL1910879,TN,INACT,0.0	CHEMBL51257,TP,ACT,0.3499999940395355	CHEMBL1910962,FP,INACT,0.2800000011920929	CHEMBL1721448,FP,INACT,0.9800000190734863	CHEMBL293016,TN,INACT,0.0	CHEMBL51776,FN,ACT,0.0	CHEMBL2372894,FN,ACT,0.03999999910593033	CHEMBL2372874,FN,ACT,0.0	CHEMBL267877,TN,INACT,0.0	CHEMBL52347,TP,ACT,0.8199999928474426	CHEMBL2316906,TP,ACT,0.7400000095367432	CHEMBL108721,FN,ACT,0.009999999776482582	CHEMBL2372889,TP,ACT,0.5699999928474426	CHEMBL9087,FN,ACT,0.0	CHEMBL2372873,FN,ACT,0.009999999776482582	CHEMBL611317,TN,INACT,0.0	CHEMBL202197,FN,ACT,0.0	CHEMBL1756,TN,INACT,0.0	CHEMBL1403981,TN,INACT,0.0	

