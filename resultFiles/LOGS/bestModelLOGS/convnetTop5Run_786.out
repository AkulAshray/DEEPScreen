ImageNetInceptionV2 CHEMBL3037 adam 0.0005 15 0 0 0.6 False True
Number of active compounds :	145
Number of inactive compounds :	145
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL3037_adam_0.0005_15_0_0_0.6_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL3037_adam_0.0005_15_0.6/
---------------------------------
Training samples: 180
Validation samples: 57
--
Training Step: 1  | time: 47.317s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/180
[A[ATraining Step: 2  | total loss: [1m[32m0.64718[0m[0m | time: 62.171s
[2K
| Adam | epoch: 001 | loss: 0.64718 - acc: 0.5344 -- iter: 064/180
[A[ATraining Step: 3  | total loss: [1m[32m0.65433[0m[0m | time: 73.575s
[2K
| Adam | epoch: 001 | loss: 0.65433 - acc: 0.6597 -- iter: 096/180
[A[ATraining Step: 4  | total loss: [1m[32m0.61119[0m[0m | time: 85.410s
[2K
| Adam | epoch: 001 | loss: 0.61119 - acc: 0.6571 -- iter: 128/180
[A[ATraining Step: 5  | total loss: [1m[32m0.53889[0m[0m | time: 97.035s
[2K
| Adam | epoch: 001 | loss: 0.53889 - acc: 0.7214 -- iter: 160/180
[A[ATraining Step: 6  | total loss: [1m[32m0.57143[0m[0m | time: 113.582s
[2K
| Adam | epoch: 001 | loss: 0.57143 - acc: 0.6393 | val_loss: 0.72756 - val_acc: 0.4211 -- iter: 180/180
--
Training Step: 7  | total loss: [1m[32m0.55761[0m[0m | time: 7.146s
[2K
| Adam | epoch: 002 | loss: 0.55761 - acc: 0.6757 -- iter: 032/180
[A[ATraining Step: 8  | total loss: [1m[32m0.38142[0m[0m | time: 26.346s
[2K
| Adam | epoch: 002 | loss: 0.38142 - acc: 0.8019 -- iter: 064/180
[A[ATraining Step: 9  | total loss: [1m[32m0.43005[0m[0m | time: 44.737s
[2K
| Adam | epoch: 002 | loss: 0.43005 - acc: 0.7744 -- iter: 096/180
[A[ATraining Step: 10  | total loss: [1m[32m0.33451[0m[0m | time: 137.247s
[2K
| Adam | epoch: 002 | loss: 0.33451 - acc: 0.8560 -- iter: 128/180
[A[ATraining Step: 11  | total loss: [1m[32m0.63077[0m[0m | time: 162.852s
[2K
| Adam | epoch: 002 | loss: 0.63077 - acc: 0.7466 -- iter: 160/180
[A[ATraining Step: 12  | total loss: [1m[32m0.46923[0m[0m | time: 202.896s
[2K
| Adam | epoch: 002 | loss: 0.46923 - acc: 0.8325 | val_loss: 0.92640 - val_acc: 0.5789 -- iter: 180/180
--
Training Step: 13  | total loss: [1m[32m0.33086[0m[0m | time: 12.756s
[2K
| Adam | epoch: 003 | loss: 0.33086 - acc: 0.9043 -- iter: 032/180
[A[ATraining Step: 14  | total loss: [1m[32m0.31171[0m[0m | time: 66.177s
[2K
| Adam | epoch: 003 | loss: 0.31171 - acc: 0.8616 -- iter: 064/180
[A[ATraining Step: 15  | total loss: [1m[32m0.20135[0m[0m | time: 77.900s
[2K
| Adam | epoch: 003 | loss: 0.20135 - acc: 0.9158 -- iter: 096/180
[A[ATraining Step: 16  | total loss: [1m[32m0.32768[0m[0m | time: 89.223s
[2K
| Adam | epoch: 003 | loss: 0.32768 - acc: 0.8653 -- iter: 128/180
[A[ATraining Step: 17  | total loss: [1m[32m0.25986[0m[0m | time: 97.854s
[2K
| Adam | epoch: 003 | loss: 0.25986 - acc: 0.9026 -- iter: 160/180
[A[ATraining Step: 18  | total loss: [1m[32m0.19519[0m[0m | time: 108.709s
[2K
| Adam | epoch: 003 | loss: 0.19519 - acc: 0.9255 | val_loss: 1.25119 - val_acc: 0.5789 -- iter: 180/180
--
Training Step: 19  | total loss: [1m[32m0.14720[0m[0m | time: 19.678s
[2K
| Adam | epoch: 004 | loss: 0.14720 - acc: 0.9503 -- iter: 032/180
[A[ATraining Step: 20  | total loss: [1m[32m0.13297[0m[0m | time: 28.727s
[2K
| Adam | epoch: 004 | loss: 0.13297 - acc: 0.9562 -- iter: 064/180
[A[ATraining Step: 21  | total loss: [1m[32m0.11419[0m[0m | time: 86.473s
[2K
| Adam | epoch: 004 | loss: 0.11419 - acc: 0.9543 -- iter: 096/180
[A[ATraining Step: 22  | total loss: [1m[32m0.08446[0m[0m | time: 116.985s
[2K
| Adam | epoch: 004 | loss: 0.08446 - acc: 0.9680 -- iter: 128/180
[A[ATraining Step: 23  | total loss: [1m[32m0.12828[0m[0m | time: 132.664s
[2K
| Adam | epoch: 004 | loss: 0.12828 - acc: 0.9682 -- iter: 160/180
[A[ATraining Step: 24  | total loss: [1m[32m0.13975[0m[0m | time: 154.665s
[2K
| Adam | epoch: 004 | loss: 0.13975 - acc: 0.9684 | val_loss: 1.32196 - val_acc: 0.5789 -- iter: 180/180
--
Training Step: 25  | total loss: [1m[32m0.13001[0m[0m | time: 8.698s
[2K
| Adam | epoch: 005 | loss: 0.13001 - acc: 0.9685 -- iter: 032/180
[A[ATraining Step: 26  | total loss: [1m[32m0.19494[0m[0m | time: 22.009s
[2K
| Adam | epoch: 005 | loss: 0.19494 - acc: 0.9520 -- iter: 064/180
[A[ATraining Step: 27  | total loss: [1m[32m0.17562[0m[0m | time: 31.410s
[2K
| Adam | epoch: 005 | loss: 0.17562 - acc: 0.9483 -- iter: 096/180
[A[ATraining Step: 28  | total loss: [1m[32m0.22332[0m[0m | time: 64.728s
[2K
| Adam | epoch: 005 | loss: 0.22332 - acc: 0.9362 -- iter: 128/180
[A[ATraining Step: 29  | total loss: [1m[32m0.18390[0m[0m | time: 81.626s
[2K
| Adam | epoch: 005 | loss: 0.18390 - acc: 0.9517 -- iter: 160/180
[A[ATraining Step: 30  | total loss: [1m[32m0.15409[0m[0m | time: 101.300s
[2K
| Adam | epoch: 005 | loss: 0.15409 - acc: 0.9558 | val_loss: 0.91250 - val_acc: 0.5789 -- iter: 180/180
--
Training Step: 31  | total loss: [1m[32m0.12371[0m[0m | time: 9.963s
[2K
| Adam | epoch: 006 | loss: 0.12371 - acc: 0.9660 -- iter: 032/180
[A[ATraining Step: 32  | total loss: [1m[32m0.11726[0m[0m | time: 18.587s
[2K
| Adam | epoch: 006 | loss: 0.11726 - acc: 0.9666 -- iter: 064/180
[A[ATraining Step: 33  | total loss: [1m[32m0.09906[0m[0m | time: 38.319s
[2K
| Adam | epoch: 006 | loss: 0.09906 - acc: 0.9739 -- iter: 096/180
[A[ATraining Step: 34  | total loss: [1m[32m0.08133[0m[0m | time: 47.349s
[2K
| Adam | epoch: 006 | loss: 0.08133 - acc: 0.9795 -- iter: 128/180
[A[ATraining Step: 35  | total loss: [1m[32m0.07031[0m[0m | time: 55.292s
[2K
| Adam | epoch: 006 | loss: 0.07031 - acc: 0.9838 -- iter: 160/180
[A[ATraining Step: 36  | total loss: [1m[32m0.05794[0m[0m | time: 117.477s
[2K
| Adam | epoch: 006 | loss: 0.05794 - acc: 0.9871 | val_loss: 0.76763 - val_acc: 0.5789 -- iter: 180/180
--
Training Step: 37  | total loss: [1m[32m0.05619[0m[0m | time: 16.333s
[2K
| Adam | epoch: 007 | loss: 0.05619 - acc: 0.9897 -- iter: 032/180
[A[ATraining Step: 38  | total loss: [1m[32m0.04791[0m[0m | time: 67.495s
[2K
| Adam | epoch: 007 | loss: 0.04791 - acc: 0.9917 -- iter: 064/180
[A[ATraining Step: 39  | total loss: [1m[32m0.05246[0m[0m | time: 80.511s
[2K
| Adam | epoch: 007 | loss: 0.05246 - acc: 0.9873 -- iter: 096/180
[A[ATraining Step: 40  | total loss: [1m[32m0.04697[0m[0m | time: 92.319s
[2K
| Adam | epoch: 007 | loss: 0.04697 - acc: 0.9897 -- iter: 128/180
[A[ATraining Step: 41  | total loss: [1m[32m0.03896[0m[0m | time: 124.043s
[2K
| Adam | epoch: 007 | loss: 0.03896 - acc: 0.9916 -- iter: 160/180
[A[ATraining Step: 42  | total loss: [1m[32m0.04207[0m[0m | time: 138.230s
[2K
| Adam | epoch: 007 | loss: 0.04207 - acc: 0.9841 | val_loss: 1.44578 - val_acc: 0.4211 -- iter: 180/180
--
Training Step: 43  | total loss: [1m[32m0.03628[0m[0m | time: 9.342s
[2K
| Adam | epoch: 008 | loss: 0.03628 - acc: 0.9869 -- iter: 032/180
[A[ATraining Step: 44  | total loss: [1m[32m0.04063[0m[0m | time: 18.214s
[2K
| Adam | epoch: 008 | loss: 0.04063 - acc: 0.9838 -- iter: 064/180
[A[ATraining Step: 45  | total loss: [1m[32m0.12891[0m[0m | time: 55.710s
[2K
| Adam | epoch: 008 | loss: 0.12891 - acc: 0.9706 -- iter: 096/180
[A[ATraining Step: 46  | total loss: [1m[32m0.10863[0m[0m | time: 128.351s
[2K
| Adam | epoch: 008 | loss: 0.10863 - acc: 0.9755 -- iter: 128/180
[A[ATraining Step: 47  | total loss: [1m[32m0.09538[0m[0m | time: 164.080s
[2K
| Adam | epoch: 008 | loss: 0.09538 - acc: 0.9795 -- iter: 160/180
[A[ATraining Step: 48  | total loss: [1m[32m0.11307[0m[0m | time: 176.436s
[2K
| Adam | epoch: 008 | loss: 0.11307 - acc: 0.9728 | val_loss: 1.16612 - val_acc: 0.6140 -- iter: 180/180
--
Training Step: 49  | total loss: [1m[32m0.09854[0m[0m | time: 5.535s
[2K
| Adam | epoch: 009 | loss: 0.09854 - acc: 0.9771 -- iter: 032/180
[A[ATraining Step: 50  | total loss: [1m[32m0.08389[0m[0m | time: 26.556s
[2K
| Adam | epoch: 009 | loss: 0.08389 - acc: 0.9806 -- iter: 064/180
[A[ATraining Step: 51  | total loss: [1m[32m0.12891[0m[0m | time: 44.263s
[2K
| Adam | epoch: 009 | loss: 0.12891 - acc: 0.9788 -- iter: 096/180
[A[ATraining Step: 52  | total loss: [1m[32m0.11452[0m[0m | time: 58.863s
[2K
| Adam | epoch: 009 | loss: 0.11452 - acc: 0.9820 -- iter: 128/180
[A[ATraining Step: 53  | total loss: [1m[32m0.11551[0m[0m | time: 72.182s
[2K
| Adam | epoch: 009 | loss: 0.11551 - acc: 0.9800 -- iter: 160/180
[A[ATraining Step: 54  | total loss: [1m[32m0.12365[0m[0m | time: 89.093s
[2K
| Adam | epoch: 009 | loss: 0.12365 - acc: 0.9693 | val_loss: 1.58806 - val_acc: 0.6491 -- iter: 180/180
--
Training Step: 55  | total loss: [1m[32m0.12075[0m[0m | time: 9.317s
[2K
| Adam | epoch: 010 | loss: 0.12075 - acc: 0.9648 -- iter: 032/180
[A[ATraining Step: 56  | total loss: [1m[32m0.10909[0m[0m | time: 18.110s
[2K
| Adam | epoch: 010 | loss: 0.10909 - acc: 0.9697 -- iter: 064/180
[A[ATraining Step: 57  | total loss: [1m[32m0.09710[0m[0m | time: 31.853s
[2K
| Adam | epoch: 010 | loss: 0.09710 - acc: 0.9739 -- iter: 096/180
[A[ATraining Step: 58  | total loss: [1m[32m0.10157[0m[0m | time: 55.797s
[2K
| Adam | epoch: 010 | loss: 0.10157 - acc: 0.9732 -- iter: 128/180
[A[ATraining Step: 59  | total loss: [1m[32m0.12859[0m[0m | time: 90.306s
[2K
| Adam | epoch: 010 | loss: 0.12859 - acc: 0.9726 -- iter: 160/180
[A[ATraining Step: 60  | total loss: [1m[32m0.11935[0m[0m | time: 112.095s
[2K
| Adam | epoch: 010 | loss: 0.11935 - acc: 0.9721 | val_loss: 0.59912 - val_acc: 0.7719 -- iter: 180/180
--
Training Step: 61  | total loss: [1m[32m0.10524[0m[0m | time: 42.057s
[2K
| Adam | epoch: 011 | loss: 0.10524 - acc: 0.9757 -- iter: 032/180
[A[ATraining Step: 62  | total loss: [1m[32m0.09954[0m[0m | time: 62.754s
[2K
| Adam | epoch: 011 | loss: 0.09954 - acc: 0.9748 -- iter: 064/180
[A[ATraining Step: 63  | total loss: [1m[32m0.08877[0m[0m | time: 71.428s
[2K
| Adam | epoch: 011 | loss: 0.08877 - acc: 0.9780 -- iter: 096/180
[A[ATraining Step: 64  | total loss: [1m[32m0.07898[0m[0m | time: 86.796s
[2K
| Adam | epoch: 011 | loss: 0.07898 - acc: 0.9808 -- iter: 128/180
[A[ATraining Step: 65  | total loss: [1m[32m0.07281[0m[0m | time: 104.380s
[2K
| Adam | epoch: 011 | loss: 0.07281 - acc: 0.9831 -- iter: 160/180
[A[ATraining Step: 66  | total loss: [1m[32m0.09592[0m[0m | time: 121.030s
[2K
| Adam | epoch: 011 | loss: 0.09592 - acc: 0.9814 | val_loss: 0.68849 - val_acc: 0.8070 -- iter: 180/180
--
Training Step: 67  | total loss: [1m[32m0.08555[0m[0m | time: 14.547s
[2K
| Adam | epoch: 012 | loss: 0.08555 - acc: 0.9836 -- iter: 032/180
[A[ATraining Step: 68  | total loss: [1m[32m0.13493[0m[0m | time: 27.823s
[2K
| Adam | epoch: 012 | loss: 0.13493 - acc: 0.9745 -- iter: 064/180
[A[ATraining Step: 69  | total loss: [1m[32m0.12154[0m[0m | time: 36.787s
[2K
| Adam | epoch: 012 | loss: 0.12154 - acc: 0.9775 -- iter: 096/180
[A[ATraining Step: 70  | total loss: [1m[32m0.11454[0m[0m | time: 45.862s
[2K
| Adam | epoch: 012 | loss: 0.11454 - acc: 0.9743 -- iter: 128/180
[A[ATraining Step: 71  | total loss: [1m[32m0.10387[0m[0m | time: 61.015s
[2K
| Adam | epoch: 012 | loss: 0.10387 - acc: 0.9772 -- iter: 160/180
[A[ATraining Step: 72  | total loss: [1m[32m0.10008[0m[0m | time: 80.561s
[2K
| Adam | epoch: 012 | loss: 0.10008 - acc: 0.9798 | val_loss: 0.93761 - val_acc: 0.7895 -- iter: 180/180
--
Training Step: 73  | total loss: [1m[32m0.23320[0m[0m | time: 21.253s
[2K
| Adam | epoch: 013 | loss: 0.23320 - acc: 0.9542 -- iter: 032/180
[A[ATraining Step: 74  | total loss: [1m[32m0.21060[0m[0m | time: 40.193s
[2K
| Adam | epoch: 013 | loss: 0.21060 - acc: 0.9593 -- iter: 064/180
[A[ATraining Step: 75  | total loss: [1m[32m0.19607[0m[0m | time: 53.946s
[2K
| Adam | epoch: 013 | loss: 0.19607 - acc: 0.9637 -- iter: 096/180
[A[ATraining Step: 76  | total loss: [1m[32m0.18769[0m[0m | time: 63.077s
[2K
| Adam | epoch: 013 | loss: 0.18769 - acc: 0.9609 -- iter: 128/180
[A[ATraining Step: 77  | total loss: [1m[32m0.17912[0m[0m | time: 71.742s
[2K
| Adam | epoch: 013 | loss: 0.17912 - acc: 0.9597 -- iter: 160/180
[A[ATraining Step: 78  | total loss: [1m[32m0.16495[0m[0m | time: 85.171s
[2K
| Adam | epoch: 013 | loss: 0.16495 - acc: 0.9639 | val_loss: 0.79171 - val_acc: 0.8070 -- iter: 180/180
--
Training Step: 79  | total loss: [1m[32m0.16314[0m[0m | time: 12.863s
[2K
| Adam | epoch: 014 | loss: 0.16314 - acc: 0.9644 -- iter: 032/180
[A[ATraining Step: 80  | total loss: [1m[32m0.17683[0m[0m | time: 28.373s
[2K
| Adam | epoch: 014 | loss: 0.17683 - acc: 0.9617 -- iter: 064/180
[A[ATraining Step: 81  | total loss: [1m[32m0.16062[0m[0m | time: 44.235s
[2K
| Adam | epoch: 014 | loss: 0.16062 - acc: 0.9656 -- iter: 096/180
[A[ATraining Step: 82  | total loss: [1m[32m0.14896[0m[0m | time: 61.018s
[2K
| Adam | epoch: 014 | loss: 0.14896 - acc: 0.9690 -- iter: 128/180
[A[ATraining Step: 83  | total loss: [1m[32m0.14060[0m[0m | time: 70.221s
[2K
| Adam | epoch: 014 | loss: 0.14060 - acc: 0.9721 -- iter: 160/180
[A[ATraining Step: 84  | total loss: [1m[32m0.14626[0m[0m | time: 79.509s
[2K
| Adam | epoch: 014 | loss: 0.14626 - acc: 0.9649 | val_loss: 1.35790 - val_acc: 0.6316 -- iter: 180/180
--
Training Step: 85  | total loss: [1m[32m0.13865[0m[0m | time: 22.356s
[2K
| Adam | epoch: 015 | loss: 0.13865 - acc: 0.9684 -- iter: 032/180
[A[ATraining Step: 86  | total loss: [1m[32m0.12738[0m[0m | time: 34.181s
[2K
| Adam | epoch: 015 | loss: 0.12738 - acc: 0.9716 -- iter: 064/180
[A[ATraining Step: 87  | total loss: [1m[32m0.13000[0m[0m | time: 75.654s
[2K
| Adam | epoch: 015 | loss: 0.13000 - acc: 0.9713 -- iter: 096/180
[A[ATraining Step: 88  | total loss: [1m[32m0.12435[0m[0m | time: 96.768s
[2K
| Adam | epoch: 015 | loss: 0.12435 - acc: 0.9679 -- iter: 128/180
[A[ATraining Step: 89  | total loss: [1m[32m0.11392[0m[0m | time: 135.648s
[2K
| Adam | epoch: 015 | loss: 0.11392 - acc: 0.9711 -- iter: 160/180
[A[ATraining Step: 90  | total loss: [1m[32m0.10948[0m[0m | time: 146.116s
[2K
| Adam | epoch: 015 | loss: 0.10948 - acc: 0.9740 | val_loss: 0.71082 - val_acc: 0.8070 -- iter: 180/180
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.9065656565656567
Validation AUPRC:0.9430478718931588
Test AUC:0.9379652605459058
Test AUPRC:0.9489332689049013
BestTestF1Score	0.77	0.63	0.79	0.95	0.65	20	1	25	11	0.34
BestTestMCCScore	0.77	0.63	0.79	0.95	0.65	20	1	25	11	0.34
BestTestAccuracyScore	0.77	0.63	0.79	0.95	0.65	20	1	25	11	0.34
BestValidationF1Score	0.85	0.71	0.84	0.96	0.76	25	1	23	8	0.34
BestValidationMCC	0.85	0.71	0.84	0.96	0.76	25	1	23	8	0.34
BestValidationAccuracy	0.85	0.71	0.84	0.96	0.76	25	1	23	8	0.34
TestPredictions (Threshold:0.34)
CHEMBL1180343,TN,INACT,0.009999999776482582	CHEMBL2346979,FN,ACT,0.11999999731779099	CHEMBL3329245,TP,ACT,0.4300000071525574	CHEMBL181035,TN,INACT,0.009999999776482582	CHEMBL2442643,FN,ACT,0.11999999731779099	CHEMBL224847,TP,ACT,0.9100000262260437	CHEMBL3526177,TP,ACT,0.8899999856948853	CHEMBL188,FN,ACT,0.14000000059604645	CHEMBL371214,FN,ACT,0.019999999552965164	CHEMBL121203,TP,ACT,0.9599999785423279	CHEMBL40796,TN,INACT,0.0	CHEMBL2391356,FP,INACT,0.6299999952316284	CHEMBL389693,FN,ACT,0.15000000596046448	CHEMBL2346978,FN,ACT,0.029999999329447746	CHEMBL2370509,TN,INACT,0.0	CHEMBL3780633,TN,INACT,0.0	CHEMBL461709,TN,INACT,0.0	CHEMBL3290445,TP,ACT,0.8700000047683716	CHEMBL332645,TN,INACT,0.0	CHEMBL482356,FN,ACT,0.25999999046325684	CHEMBL121681,TP,ACT,0.6800000071525574	CHEMBL594803,TN,INACT,0.019999999552965164	CHEMBL443842,FN,ACT,0.10000000149011612	CHEMBL333514,TP,ACT,0.6899999976158142	CHEMBL407818,TN,INACT,0.03999999910593033	CHEMBL224053,TP,ACT,0.9300000071525574	CHEMBL594376,TN,INACT,0.019999999552965164	CHEMBL1983100,TN,INACT,0.05000000074505806	CHEMBL1765667,TN,INACT,0.009999999776482582	CHEMBL218071,TN,INACT,0.03999999910593033	CHEMBL3633663,TN,INACT,0.14000000059604645	CHEMBL40317,TN,INACT,0.0	CHEMBL2337206,TP,ACT,0.8999999761581421	CHEMBL328476,TN,INACT,0.009999999776482582	CHEMBL3331448,TP,ACT,0.8899999856948853	CHEMBL227429,TN,INACT,0.0	CHEMBL3402653,TP,ACT,0.7699999809265137	CHEMBL332064,TP,ACT,0.9399999976158142	CHEMBL561013,FN,ACT,0.009999999776482582	CHEMBL1258999,TN,INACT,0.009999999776482582	CHEMBL3633656,TN,INACT,0.009999999776482582	CHEMBL216057,TP,ACT,0.949999988079071	CHEMBL2370511,TN,INACT,0.0	CHEMBL180343,TN,INACT,0.0	CHEMBL3331464,TP,ACT,0.8899999856948853	CHEMBL113262,TP,ACT,0.8600000143051147	CHEMBL240888,TN,INACT,0.0	CHEMBL462582,FN,ACT,0.09000000357627869	CHEMBL225622,TP,ACT,0.699999988079071	CHEMBL216595,TP,ACT,0.75	CHEMBL541216,TP,ACT,0.3400000035762787	CHEMBL2337203,FN,ACT,0.029999999329447746	CHEMBL595022,TN,INACT,0.009999999776482582	CHEMBL240913,TP,ACT,0.6700000166893005	CHEMBL120944,TP,ACT,0.4000000059604645	CHEMBL351183,TN,INACT,0.17000000178813934	CHEMBL483991,TN,INACT,0.0	

