CNNModel CHEMBL3361 adam 0.0005 15 128 0 0.8 False True
Number of active compounds :	144
Number of inactive compounds :	144
---------------------------------
Run id: CNNModel_CHEMBL3361_adam_0.0005_15_128_0_0.8_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL3361_adam_0.0005_15_128_0.8_True/
---------------------------------
Training samples: 175
Validation samples: 55
--
Training Step: 1  | time: 0.784s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/175
[A[ATraining Step: 2  | total loss: [1m[32m0.62416[0m[0m | time: 1.378s
[2K
| Adam | epoch: 001 | loss: 0.62416 - acc: 0.3937 -- iter: 064/175
[A[ATraining Step: 3  | total loss: [1m[32m0.68003[0m[0m | time: 1.977s
[2K
| Adam | epoch: 001 | loss: 0.68003 - acc: 0.5063 -- iter: 096/175
[A[ATraining Step: 4  | total loss: [1m[32m0.69079[0m[0m | time: 2.573s
[2K
| Adam | epoch: 001 | loss: 0.69079 - acc: 0.4781 -- iter: 128/175
[A[ATraining Step: 5  | total loss: [1m[32m0.69288[0m[0m | time: 3.167s
[2K
| Adam | epoch: 001 | loss: 0.69288 - acc: 0.4716 -- iter: 160/175
[A[ATraining Step: 6  | total loss: [1m[32m0.68984[0m[0m | time: 4.498s
[2K
| Adam | epoch: 001 | loss: 0.68984 - acc: 0.6305 | val_loss: 0.69637 - val_acc: 0.4364 -- iter: 175/175
--
Training Step: 7  | total loss: [1m[32m0.69261[0m[0m | time: 0.317s
[2K
| Adam | epoch: 002 | loss: 0.69261 - acc: 0.5322 -- iter: 032/175
[A[ATraining Step: 8  | total loss: [1m[32m0.69371[0m[0m | time: 0.931s
[2K
| Adam | epoch: 002 | loss: 0.69371 - acc: 0.4953 -- iter: 064/175
[A[ATraining Step: 9  | total loss: [1m[32m0.69168[0m[0m | time: 1.532s
[2K
| Adam | epoch: 002 | loss: 0.69168 - acc: 0.5309 -- iter: 096/175
[A[ATraining Step: 10  | total loss: [1m[32m0.69292[0m[0m | time: 2.134s
[2K
| Adam | epoch: 002 | loss: 0.69292 - acc: 0.4998 -- iter: 128/175
[A[ATraining Step: 11  | total loss: [1m[32m0.68999[0m[0m | time: 2.753s
[2K
| Adam | epoch: 002 | loss: 0.68999 - acc: 0.5295 -- iter: 160/175
[A[ATraining Step: 12  | total loss: [1m[32m0.68937[0m[0m | time: 4.357s
[2K
| Adam | epoch: 002 | loss: 0.68937 - acc: 0.5303 | val_loss: 0.71096 - val_acc: 0.4364 -- iter: 175/175
--
Training Step: 13  | total loss: [1m[32m0.68510[0m[0m | time: 0.313s
[2K
| Adam | epoch: 003 | loss: 0.68510 - acc: 0.5575 -- iter: 032/175
[A[ATraining Step: 14  | total loss: [1m[32m0.68016[0m[0m | time: 0.618s
[2K
| Adam | epoch: 003 | loss: 0.68016 - acc: 0.5749 -- iter: 064/175
[A[ATraining Step: 15  | total loss: [1m[32m0.67498[0m[0m | time: 1.221s
[2K
| Adam | epoch: 003 | loss: 0.67498 - acc: 0.5847 -- iter: 096/175
[A[ATraining Step: 16  | total loss: [1m[32m0.68768[0m[0m | time: 1.822s
[2K
| Adam | epoch: 003 | loss: 0.68768 - acc: 0.5647 -- iter: 128/175
[A[ATraining Step: 17  | total loss: [1m[32m0.69321[0m[0m | time: 2.429s
[2K
| Adam | epoch: 003 | loss: 0.69321 - acc: 0.5526 -- iter: 160/175
[A[ATraining Step: 18  | total loss: [1m[32m0.69274[0m[0m | time: 4.036s
[2K
| Adam | epoch: 003 | loss: 0.69274 - acc: 0.5452 | val_loss: 0.69956 - val_acc: 0.4364 -- iter: 175/175
--
Training Step: 19  | total loss: [1m[32m0.68702[0m[0m | time: 0.602s
[2K
| Adam | epoch: 004 | loss: 0.68702 - acc: 0.5510 -- iter: 032/175
[A[ATraining Step: 20  | total loss: [1m[32m0.68768[0m[0m | time: 0.914s
[2K
| Adam | epoch: 004 | loss: 0.68768 - acc: 0.5346 -- iter: 064/175
[A[ATraining Step: 21  | total loss: [1m[32m0.68891[0m[0m | time: 1.222s
[2K
| Adam | epoch: 004 | loss: 0.68891 - acc: 0.5135 -- iter: 096/175
[A[ATraining Step: 22  | total loss: [1m[32m0.69014[0m[0m | time: 1.841s
[2K
| Adam | epoch: 004 | loss: 0.69014 - acc: 0.4995 -- iter: 128/175
[A[ATraining Step: 23  | total loss: [1m[32m0.68792[0m[0m | time: 2.445s
[2K
| Adam | epoch: 004 | loss: 0.68792 - acc: 0.5359 -- iter: 160/175
[A[ATraining Step: 24  | total loss: [1m[32m0.68780[0m[0m | time: 4.093s
[2K
| Adam | epoch: 004 | loss: 0.68780 - acc: 0.5258 | val_loss: 0.69005 - val_acc: 0.4364 -- iter: 175/175
--
Training Step: 25  | total loss: [1m[32m0.68478[0m[0m | time: 0.621s
[2K
| Adam | epoch: 005 | loss: 0.68478 - acc: 0.5784 -- iter: 032/175
[A[ATraining Step: 26  | total loss: [1m[32m0.68830[0m[0m | time: 1.245s
[2K
| Adam | epoch: 005 | loss: 0.68830 - acc: 0.5163 -- iter: 064/175
[A[ATraining Step: 27  | total loss: [1m[32m0.68909[0m[0m | time: 1.572s
[2K
| Adam | epoch: 005 | loss: 0.68909 - acc: 0.4960 -- iter: 096/175
[A[ATraining Step: 28  | total loss: [1m[32m0.68260[0m[0m | time: 1.893s
[2K
| Adam | epoch: 005 | loss: 0.68260 - acc: 0.5720 -- iter: 128/175
[A[ATraining Step: 29  | total loss: [1m[32m0.67546[0m[0m | time: 2.498s
[2K
| Adam | epoch: 005 | loss: 0.67546 - acc: 0.6275 -- iter: 160/175
[A[ATraining Step: 30  | total loss: [1m[32m0.67281[0m[0m | time: 4.111s
[2K
| Adam | epoch: 005 | loss: 0.67281 - acc: 0.6195 | val_loss: 0.70810 - val_acc: 0.4364 -- iter: 175/175
--
Training Step: 31  | total loss: [1m[32m0.66504[0m[0m | time: 0.609s
[2K
| Adam | epoch: 006 | loss: 0.66504 - acc: 0.6280 -- iter: 032/175
[A[ATraining Step: 32  | total loss: [1m[32m0.65759[0m[0m | time: 1.211s
[2K
| Adam | epoch: 006 | loss: 0.65759 - acc: 0.6273 -- iter: 064/175
[A[ATraining Step: 33  | total loss: [1m[32m0.67671[0m[0m | time: 1.821s
[2K
| Adam | epoch: 006 | loss: 0.67671 - acc: 0.5856 -- iter: 096/175
[A[ATraining Step: 34  | total loss: [1m[32m0.67961[0m[0m | time: 2.133s
[2K
| Adam | epoch: 006 | loss: 0.67961 - acc: 0.5673 -- iter: 128/175
[A[ATraining Step: 35  | total loss: [1m[32m0.68414[0m[0m | time: 2.445s
[2K
| Adam | epoch: 006 | loss: 0.68414 - acc: 0.5462 -- iter: 160/175
[A[ATraining Step: 36  | total loss: [1m[32m0.68223[0m[0m | time: 4.051s
[2K
| Adam | epoch: 006 | loss: 0.68223 - acc: 0.5300 | val_loss: 0.66212 - val_acc: 0.4364 -- iter: 175/175
--
Training Step: 37  | total loss: [1m[32m0.67757[0m[0m | time: 0.610s
[2K
| Adam | epoch: 007 | loss: 0.67757 - acc: 0.5302 -- iter: 032/175
[A[ATraining Step: 38  | total loss: [1m[32m0.67354[0m[0m | time: 1.204s
[2K
| Adam | epoch: 007 | loss: 0.67354 - acc: 0.5243 -- iter: 064/175
[A[ATraining Step: 39  | total loss: [1m[32m0.66661[0m[0m | time: 1.816s
[2K
| Adam | epoch: 007 | loss: 0.66661 - acc: 0.5376 -- iter: 096/175
[A[ATraining Step: 40  | total loss: [1m[32m0.66469[0m[0m | time: 2.432s
[2K
| Adam | epoch: 007 | loss: 0.66469 - acc: 0.5306 -- iter: 128/175
[A[ATraining Step: 41  | total loss: [1m[32m0.65824[0m[0m | time: 2.750s
[2K
| Adam | epoch: 007 | loss: 0.65824 - acc: 0.5479 -- iter: 160/175
[A[ATraining Step: 42  | total loss: [1m[32m0.64100[0m[0m | time: 4.052s
[2K
| Adam | epoch: 007 | loss: 0.64100 - acc: 0.5693 | val_loss: 0.68120 - val_acc: 0.4364 -- iter: 175/175
--
Training Step: 43  | total loss: [1m[32m0.61810[0m[0m | time: 0.613s
[2K
| Adam | epoch: 008 | loss: 0.61810 - acc: 0.5865 -- iter: 032/175
[A[ATraining Step: 44  | total loss: [1m[32m0.62616[0m[0m | time: 1.215s
[2K
| Adam | epoch: 008 | loss: 0.62616 - acc: 0.5607 -- iter: 064/175
[A[ATraining Step: 45  | total loss: [1m[32m0.62730[0m[0m | time: 1.817s
[2K
| Adam | epoch: 008 | loss: 0.62730 - acc: 0.5557 -- iter: 096/175
[A[ATraining Step: 46  | total loss: [1m[32m0.61552[0m[0m | time: 2.425s
[2K
| Adam | epoch: 008 | loss: 0.61552 - acc: 0.5516 -- iter: 128/175
[A[ATraining Step: 47  | total loss: [1m[32m0.59023[0m[0m | time: 3.020s
[2K
| Adam | epoch: 008 | loss: 0.59023 - acc: 0.5994 -- iter: 160/175
[A[ATraining Step: 48  | total loss: [1m[32m0.59318[0m[0m | time: 4.336s
[2K
| Adam | epoch: 008 | loss: 0.59318 - acc: 0.6236 | val_loss: 0.54378 - val_acc: 0.9091 -- iter: 175/175
--
Training Step: 49  | total loss: [1m[32m0.59822[0m[0m | time: 0.324s
[2K
| Adam | epoch: 009 | loss: 0.59822 - acc: 0.6515 -- iter: 032/175
[A[ATraining Step: 50  | total loss: [1m[32m0.60187[0m[0m | time: 0.924s
[2K
| Adam | epoch: 009 | loss: 0.60187 - acc: 0.6745 -- iter: 064/175
[A[ATraining Step: 51  | total loss: [1m[32m0.58856[0m[0m | time: 1.525s
[2K
| Adam | epoch: 009 | loss: 0.58856 - acc: 0.6908 -- iter: 096/175
[A[ATraining Step: 52  | total loss: [1m[32m0.58621[0m[0m | time: 2.125s
[2K
| Adam | epoch: 009 | loss: 0.58621 - acc: 0.6950 -- iter: 128/175
[A[ATraining Step: 53  | total loss: [1m[32m0.56647[0m[0m | time: 2.729s
[2K
| Adam | epoch: 009 | loss: 0.56647 - acc: 0.7354 -- iter: 160/175
[A[ATraining Step: 54  | total loss: [1m[32m0.53784[0m[0m | time: 4.340s
[2K
| Adam | epoch: 009 | loss: 0.53784 - acc: 0.7647 | val_loss: 0.48990 - val_acc: 0.7455 -- iter: 175/175
--
Training Step: 55  | total loss: [1m[32m0.53096[0m[0m | time: 0.310s
[2K
| Adam | epoch: 010 | loss: 0.53096 - acc: 0.7760 -- iter: 032/175
[A[ATraining Step: 56  | total loss: [1m[32m0.51847[0m[0m | time: 0.608s
[2K
| Adam | epoch: 010 | loss: 0.51847 - acc: 0.7700 -- iter: 064/175
[A[ATraining Step: 57  | total loss: [1m[32m0.49564[0m[0m | time: 1.204s
[2K
| Adam | epoch: 010 | loss: 0.49564 - acc: 0.8019 -- iter: 096/175
[A[ATraining Step: 58  | total loss: [1m[32m0.48374[0m[0m | time: 1.800s
[2K
| Adam | epoch: 010 | loss: 0.48374 - acc: 0.8076 -- iter: 128/175
[A[ATraining Step: 59  | total loss: [1m[32m0.44929[0m[0m | time: 2.399s
[2K
| Adam | epoch: 010 | loss: 0.44929 - acc: 0.8292 -- iter: 160/175
[A[ATraining Step: 60  | total loss: [1m[32m0.43745[0m[0m | time: 3.990s
[2K
| Adam | epoch: 010 | loss: 0.43745 - acc: 0.8311 | val_loss: 0.27949 - val_acc: 0.8909 -- iter: 175/175
--
Training Step: 61  | total loss: [1m[32m0.45092[0m[0m | time: 0.636s
[2K
| Adam | epoch: 011 | loss: 0.45092 - acc: 0.8246 -- iter: 032/175
[A[ATraining Step: 62  | total loss: [1m[32m0.45085[0m[0m | time: 0.949s
[2K
| Adam | epoch: 011 | loss: 0.45085 - acc: 0.8351 -- iter: 064/175
[A[ATraining Step: 63  | total loss: [1m[32m0.41547[0m[0m | time: 1.256s
[2K
| Adam | epoch: 011 | loss: 0.41547 - acc: 0.8476 -- iter: 096/175
[A[ATraining Step: 64  | total loss: [1m[32m0.38143[0m[0m | time: 1.863s
[2K
| Adam | epoch: 011 | loss: 0.38143 - acc: 0.8500 -- iter: 128/175
[A[ATraining Step: 65  | total loss: [1m[32m0.37595[0m[0m | time: 2.465s
[2K
| Adam | epoch: 011 | loss: 0.37595 - acc: 0.8492 -- iter: 160/175
[A[ATraining Step: 66  | total loss: [1m[32m0.35447[0m[0m | time: 4.078s
[2K
| Adam | epoch: 011 | loss: 0.35447 - acc: 0.8599 | val_loss: 0.38677 - val_acc: 0.8727 -- iter: 175/175
--
Training Step: 67  | total loss: [1m[32m0.35216[0m[0m | time: 0.598s
[2K
| Adam | epoch: 012 | loss: 0.35216 - acc: 0.8655 -- iter: 032/175
[A[ATraining Step: 68  | total loss: [1m[32m0.36225[0m[0m | time: 1.217s
[2K
| Adam | epoch: 012 | loss: 0.36225 - acc: 0.8629 -- iter: 064/175
[A[ATraining Step: 69  | total loss: [1m[32m0.36451[0m[0m | time: 1.520s
[2K
| Adam | epoch: 012 | loss: 0.36451 - acc: 0.8716 -- iter: 096/175
[A[ATraining Step: 70  | total loss: [1m[32m0.38598[0m[0m | time: 1.826s
[2K
| Adam | epoch: 012 | loss: 0.38598 - acc: 0.8634 -- iter: 128/175
[A[ATraining Step: 71  | total loss: [1m[32m0.40740[0m[0m | time: 2.428s
[2K
| Adam | epoch: 012 | loss: 0.40740 - acc: 0.8562 -- iter: 160/175
[A[ATraining Step: 72  | total loss: [1m[32m0.39210[0m[0m | time: 4.047s
[2K
| Adam | epoch: 012 | loss: 0.39210 - acc: 0.8583 | val_loss: 0.33015 - val_acc: 0.8909 -- iter: 175/175
--
Training Step: 73  | total loss: [1m[32m0.39689[0m[0m | time: 0.635s
[2K
| Adam | epoch: 013 | loss: 0.39689 - acc: 0.8497 -- iter: 032/175
[A[ATraining Step: 74  | total loss: [1m[32m0.39821[0m[0m | time: 1.238s
[2K
| Adam | epoch: 013 | loss: 0.39821 - acc: 0.8559 -- iter: 064/175
[A[ATraining Step: 75  | total loss: [1m[32m0.38041[0m[0m | time: 1.856s
[2K
| Adam | epoch: 013 | loss: 0.38041 - acc: 0.8682 -- iter: 096/175
[A[ATraining Step: 76  | total loss: [1m[32m0.38657[0m[0m | time: 2.192s
[2K
| Adam | epoch: 013 | loss: 0.38657 - acc: 0.8622 -- iter: 128/175
[A[ATraining Step: 77  | total loss: [1m[32m0.36720[0m[0m | time: 2.508s
[2K
| Adam | epoch: 013 | loss: 0.36720 - acc: 0.8697 -- iter: 160/175
[A[ATraining Step: 78  | total loss: [1m[32m0.35043[0m[0m | time: 4.139s
[2K
| Adam | epoch: 013 | loss: 0.35043 - acc: 0.8834 | val_loss: 0.30457 - val_acc: 0.9091 -- iter: 175/175
--
Training Step: 79  | total loss: [1m[32m0.33512[0m[0m | time: 0.617s
[2K
| Adam | epoch: 014 | loss: 0.33512 - acc: 0.8922 -- iter: 032/175
[A[ATraining Step: 80  | total loss: [1m[32m0.33076[0m[0m | time: 1.216s
[2K
| Adam | epoch: 014 | loss: 0.33076 - acc: 0.8936 -- iter: 064/175
[A[ATraining Step: 81  | total loss: [1m[32m0.31631[0m[0m | time: 1.799s
[2K
| Adam | epoch: 014 | loss: 0.31631 - acc: 0.9044 -- iter: 096/175
[A[ATraining Step: 82  | total loss: [1m[32m0.29666[0m[0m | time: 2.407s
[2K
| Adam | epoch: 014 | loss: 0.29666 - acc: 0.9139 -- iter: 128/175
[A[ATraining Step: 83  | total loss: [1m[32m0.31395[0m[0m | time: 2.711s
[2K
| Adam | epoch: 014 | loss: 0.31395 - acc: 0.9069 -- iter: 160/175
[A[ATraining Step: 84  | total loss: [1m[32m0.29881[0m[0m | time: 4.010s
[2K
| Adam | epoch: 014 | loss: 0.29881 - acc: 0.9096 | val_loss: 0.25919 - val_acc: 0.9273 -- iter: 175/175
--
Training Step: 85  | total loss: [1m[32m0.28424[0m[0m | time: 0.609s
[2K
| Adam | epoch: 015 | loss: 0.28424 - acc: 0.9119 -- iter: 032/175
[A[ATraining Step: 86  | total loss: [1m[32m0.29331[0m[0m | time: 1.217s
[2K
| Adam | epoch: 015 | loss: 0.29331 - acc: 0.9082 -- iter: 064/175
[A[ATraining Step: 87  | total loss: [1m[32m0.30744[0m[0m | time: 1.817s
[2K
| Adam | epoch: 015 | loss: 0.30744 - acc: 0.9018 -- iter: 096/175
[A[ATraining Step: 88  | total loss: [1m[32m0.30715[0m[0m | time: 2.436s
[2K
| Adam | epoch: 015 | loss: 0.30715 - acc: 0.8991 -- iter: 128/175
[A[ATraining Step: 89  | total loss: [1m[32m0.28614[0m[0m | time: 3.033s
[2K
| Adam | epoch: 015 | loss: 0.28614 - acc: 0.9092 -- iter: 160/175
[A[ATraining Step: 90  | total loss: [1m[32m0.27801[0m[0m | time: 4.357s
[2K
| Adam | epoch: 015 | loss: 0.27801 - acc: 0.9152 | val_loss: 0.29091 - val_acc: 0.9091 -- iter: 175/175
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.946236559139785
Validation AUPRC:0.9462255740333347
Test AUC:0.9153439153439153
Test AUPRC:0.9486255355149216
BestTestF1Score	0.88	0.79	0.89	0.96	0.82	23	1	26	5	0.27
BestTestMCCScore	0.88	0.79	0.89	0.96	0.82	23	1	26	5	0.27
BestTestAccuracyScore	0.88	0.79	0.89	0.96	0.82	23	1	26	5	0.41
BestValidationF1Score	0.94	0.85	0.93	0.91	0.97	30	3	21	1	0.27
BestValidationMCC	0.94	0.85	0.93	0.91	0.97	30	3	21	1	0.27
BestValidationAccuracy	0.94	0.85	0.93	0.94	0.94	29	2	22	2	0.41
TestPredictions (Threshold:0.27)
CHEMBL3289652,TP,ACT,0.9200000166893005	CHEMBL140620,TN,INACT,0.019999999552965164	CHEMBL42359,TN,INACT,0.029999999329447746	CHEMBL1258671,FN,ACT,0.09000000357627869	CHEMBL2369493,TN,INACT,0.009999999776482582	CHEMBL21509,TN,INACT,0.0	CHEMBL228144,TN,INACT,0.029999999329447746	CHEMBL2037521,TP,ACT,0.9100000262260437	CHEMBL1270323,TP,ACT,0.8399999737739563	CHEMBL3633650,TN,INACT,0.0	CHEMBL594376,TN,INACT,0.029999999329447746	CHEMBL1983100,TN,INACT,0.019999999552965164	CHEMBL345820,TP,ACT,0.8999999761581421	CHEMBL426629,TP,ACT,0.9399999976158142	CHEMBL446693,TN,INACT,0.029999999329447746	CHEMBL2037526,FN,ACT,0.029999999329447746	CHEMBL25856,FN,ACT,0.0	CHEMBL169675,TN,INACT,0.05999999865889549	CHEMBL423405,TN,INACT,0.009999999776482582	CHEMBL363406,TP,ACT,0.9399999976158142	CHEMBL1258223,FN,ACT,0.11999999731779099	CHEMBL295651,TN,INACT,0.029999999329447746	CHEMBL715,FN,ACT,0.0	CHEMBL1076,TN,INACT,0.009999999776482582	CHEMBL2037432,TP,ACT,0.8700000047683716	CHEMBL309730,TP,ACT,0.6100000143051147	CHEMBL3289644,TP,ACT,0.9300000071525574	CHEMBL593443,TN,INACT,0.029999999329447746	CHEMBL3819082,TP,ACT,0.75	CHEMBL595022,TN,INACT,0.009999999776482582	CHEMBL151619,TN,INACT,0.03999999910593033	CHEMBL3289656,TP,ACT,0.8299999833106995	CHEMBL3633663,TN,INACT,0.009999999776482582	CHEMBL336081,TN,INACT,0.019999999552965164	CHEMBL328422,TN,INACT,0.0	CHEMBL413040,TN,INACT,0.0	CHEMBL1170027,TN,INACT,0.0	CHEMBL71724,TP,ACT,0.6100000143051147	CHEMBL3289653,TP,ACT,0.9100000262260437	CHEMBL3819556,TP,ACT,0.7699999809265137	CHEMBL3289642,TP,ACT,0.8500000238418579	CHEMBL1946124,TP,ACT,0.8199999928474426	CHEMBL45305,FP,INACT,0.7300000190734863	CHEMBL359141,TN,INACT,0.03999999910593033	CHEMBL3350741,TN,INACT,0.0	CHEMBL1940410,TP,ACT,0.9300000071525574	CHEMBL3289651,TP,ACT,0.9100000262260437	CHEMBL435949,TP,ACT,0.800000011920929	CHEMBL1940417,TP,ACT,0.9200000166893005	CHEMBL1270423,TP,ACT,0.5	CHEMBL1940411,TP,ACT,0.9300000071525574	CHEMBL3289643,TP,ACT,0.8299999833106995	CHEMBL2312376,TN,INACT,0.009999999776482582	CHEMBL162095,TN,INACT,0.03999999910593033	CHEMBL123099,TN,INACT,0.0	

