ImageNetInceptionV2 CHEMBL5543 adam 0.001 15 0 0 0.8 False True
Number of active compounds :	203
Number of inactive compounds :	203
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL5543_adam_0.001_15_0_0_0.8_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL5543_adam_0.001_15_0.8/
---------------------------------
Training samples: 236
Validation samples: 74
--
Training Step: 1  | time: 62.414s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/236
[A[ATraining Step: 2  | total loss: [1m[32m0.68139[0m[0m | time: 73.472s
[2K
| Adam | epoch: 001 | loss: 0.68139 - acc: 0.4219 -- iter: 064/236
[A[ATraining Step: 3  | total loss: [1m[32m0.84828[0m[0m | time: 83.155s
[2K
| Adam | epoch: 001 | loss: 0.84828 - acc: 0.4858 -- iter: 096/236
[A[ATraining Step: 4  | total loss: [1m[32m0.98938[0m[0m | time: 92.578s
[2K
| Adam | epoch: 001 | loss: 0.98938 - acc: 0.5199 -- iter: 128/236
[A[ATraining Step: 5  | total loss: [1m[32m0.74054[0m[0m | time: 107.395s
[2K
| Adam | epoch: 001 | loss: 0.74054 - acc: 0.5494 -- iter: 160/236
[A[ATraining Step: 6  | total loss: [1m[32m0.65383[0m[0m | time: 123.381s
[2K
| Adam | epoch: 001 | loss: 0.65383 - acc: 0.5779 -- iter: 192/236
[A[ATraining Step: 7  | total loss: [1m[32m0.71725[0m[0m | time: 136.115s
[2K
| Adam | epoch: 001 | loss: 0.71725 - acc: 0.5499 -- iter: 224/236
[A[ATraining Step: 8  | total loss: [1m[32m0.64205[0m[0m | time: 162.106s
[2K
| Adam | epoch: 001 | loss: 0.64205 - acc: 0.6097 | val_loss: 3.87016 - val_acc: 0.3514 -- iter: 236/236
--
Training Step: 9  | total loss: [1m[32m0.89469[0m[0m | time: 7.795s
[2K
| Adam | epoch: 002 | loss: 0.89469 - acc: 0.5075 -- iter: 032/236
[A[ATraining Step: 10  | total loss: [1m[32m0.66484[0m[0m | time: 22.226s
[2K
| Adam | epoch: 002 | loss: 0.66484 - acc: 0.6704 -- iter: 064/236
[A[ATraining Step: 11  | total loss: [1m[32m0.75016[0m[0m | time: 35.483s
[2K
| Adam | epoch: 002 | loss: 0.75016 - acc: 0.5749 -- iter: 096/236
[A[ATraining Step: 12  | total loss: [1m[32m0.75886[0m[0m | time: 50.687s
[2K
| Adam | epoch: 002 | loss: 0.75886 - acc: 0.5693 -- iter: 128/236
[A[ATraining Step: 13  | total loss: [1m[32m0.76835[0m[0m | time: 60.571s
[2K
| Adam | epoch: 002 | loss: 0.76835 - acc: 0.5396 -- iter: 160/236
[A[ATraining Step: 14  | total loss: [1m[32m0.72882[0m[0m | time: 70.109s
[2K
| Adam | epoch: 002 | loss: 0.72882 - acc: 0.5234 -- iter: 192/236
[A[ATraining Step: 15  | total loss: [1m[32m0.71946[0m[0m | time: 79.454s
[2K
| Adam | epoch: 002 | loss: 0.71946 - acc: 0.5632 -- iter: 224/236
[A[ATraining Step: 16  | total loss: [1m[32m0.67862[0m[0m | time: 164.333s
[2K
| Adam | epoch: 002 | loss: 0.67862 - acc: 0.5864 | val_loss: 1.83307 - val_acc: 0.3514 -- iter: 236/236
--
Training Step: 17  | total loss: [1m[32m0.68884[0m[0m | time: 6.293s
[2K
| Adam | epoch: 003 | loss: 0.68884 - acc: 0.5778 -- iter: 032/236
[A[ATraining Step: 18  | total loss: [1m[32m0.65551[0m[0m | time: 13.011s
[2K
| Adam | epoch: 003 | loss: 0.65551 - acc: 0.6374 -- iter: 064/236
[A[ATraining Step: 19  | total loss: [1m[32m0.57951[0m[0m | time: 22.291s
[2K
| Adam | epoch: 003 | loss: 0.57951 - acc: 0.7305 -- iter: 096/236
[A[ATraining Step: 20  | total loss: [1m[32m0.59636[0m[0m | time: 31.500s
[2K
| Adam | epoch: 003 | loss: 0.59636 - acc: 0.7066 -- iter: 128/236
[A[ATraining Step: 21  | total loss: [1m[32m0.58451[0m[0m | time: 42.011s
[2K
| Adam | epoch: 003 | loss: 0.58451 - acc: 0.7104 -- iter: 160/236
[A[ATraining Step: 22  | total loss: [1m[32m0.57474[0m[0m | time: 56.631s
[2K
| Adam | epoch: 003 | loss: 0.57474 - acc: 0.6848 -- iter: 192/236
[A[ATraining Step: 23  | total loss: [1m[32m0.48988[0m[0m | time: 71.658s
[2K
| Adam | epoch: 003 | loss: 0.48988 - acc: 0.7400 -- iter: 224/236
[A[ATraining Step: 24  | total loss: [1m[32m0.48398[0m[0m | time: 92.489s
[2K
| Adam | epoch: 003 | loss: 0.48398 - acc: 0.7428 | val_loss: 0.96012 - val_acc: 0.3514 -- iter: 236/236
--
Training Step: 25  | total loss: [1m[32m0.46641[0m[0m | time: 9.182s
[2K
| Adam | epoch: 004 | loss: 0.46641 - acc: 0.7703 -- iter: 032/236
[A[ATraining Step: 26  | total loss: [1m[32m0.48101[0m[0m | time: 13.497s
[2K
| Adam | epoch: 004 | loss: 0.48101 - acc: 0.7567 -- iter: 064/236
[A[ATraining Step: 27  | total loss: [1m[32m0.40276[0m[0m | time: 17.448s
[2K
| Adam | epoch: 004 | loss: 0.40276 - acc: 0.8193 -- iter: 096/236
[A[ATraining Step: 28  | total loss: [1m[32m0.32088[0m[0m | time: 25.774s
[2K
| Adam | epoch: 004 | loss: 0.32088 - acc: 0.8644 -- iter: 128/236
[A[ATraining Step: 29  | total loss: [1m[32m0.39491[0m[0m | time: 36.932s
[2K
| Adam | epoch: 004 | loss: 0.39491 - acc: 0.8290 -- iter: 160/236
[A[ATraining Step: 30  | total loss: [1m[32m0.41747[0m[0m | time: 49.221s
[2K
| Adam | epoch: 004 | loss: 0.41747 - acc: 0.8029 -- iter: 192/236
[A[ATraining Step: 31  | total loss: [1m[32m0.39308[0m[0m | time: 61.523s
[2K
| Adam | epoch: 004 | loss: 0.39308 - acc: 0.8340 -- iter: 224/236
[A[ATraining Step: 32  | total loss: [1m[32m0.37711[0m[0m | time: 78.960s
[2K
| Adam | epoch: 004 | loss: 0.37711 - acc: 0.8432 | val_loss: 4.22226 - val_acc: 0.3514 -- iter: 236/236
--
Training Step: 33  | total loss: [1m[32m0.43286[0m[0m | time: 7.702s
[2K
| Adam | epoch: 005 | loss: 0.43286 - acc: 0.8296 -- iter: 032/236
[A[ATraining Step: 34  | total loss: [1m[32m0.38923[0m[0m | time: 17.963s
[2K
| Adam | epoch: 005 | loss: 0.38923 - acc: 0.8393 -- iter: 064/236
[A[ATraining Step: 35  | total loss: [1m[32m0.37276[0m[0m | time: 23.614s
[2K
| Adam | epoch: 005 | loss: 0.37276 - acc: 0.8533 -- iter: 096/236
[A[ATraining Step: 36  | total loss: [1m[32m0.39797[0m[0m | time: 29.700s
[2K
| Adam | epoch: 005 | loss: 0.39797 - acc: 0.8322 -- iter: 128/236
[A[ATraining Step: 37  | total loss: [1m[32m0.34600[0m[0m | time: 43.399s
[2K
| Adam | epoch: 005 | loss: 0.34600 - acc: 0.8491 -- iter: 160/236
[A[ATraining Step: 38  | total loss: [1m[32m0.32510[0m[0m | time: 56.082s
[2K
| Adam | epoch: 005 | loss: 0.32510 - acc: 0.8664 -- iter: 192/236
[A[ATraining Step: 39  | total loss: [1m[32m0.38187[0m[0m | time: 68.440s
[2K
| Adam | epoch: 005 | loss: 0.38187 - acc: 0.8441 -- iter: 224/236
[A[ATraining Step: 40  | total loss: [1m[32m0.61617[0m[0m | time: 81.897s
[2K
| Adam | epoch: 005 | loss: 0.61617 - acc: 0.7913 | val_loss: 1.83672 - val_acc: 0.3514 -- iter: 236/236
--
Training Step: 41  | total loss: [1m[32m0.57253[0m[0m | time: 9.276s
[2K
| Adam | epoch: 006 | loss: 0.57253 - acc: 0.7952 -- iter: 032/236
[A[ATraining Step: 42  | total loss: [1m[32m0.53328[0m[0m | time: 20.765s
[2K
| Adam | epoch: 006 | loss: 0.53328 - acc: 0.8152 -- iter: 064/236
[A[ATraining Step: 43  | total loss: [1m[32m0.50088[0m[0m | time: 31.720s
[2K
| Adam | epoch: 006 | loss: 0.50088 - acc: 0.8202 -- iter: 096/236
[A[ATraining Step: 44  | total loss: [1m[32m0.49433[0m[0m | time: 37.420s
[2K
| Adam | epoch: 006 | loss: 0.49433 - acc: 0.8135 -- iter: 128/236
[A[ATraining Step: 45  | total loss: [1m[32m0.51255[0m[0m | time: 43.350s
[2K
| Adam | epoch: 006 | loss: 0.51255 - acc: 0.7885 -- iter: 160/236
[A[ATraining Step: 46  | total loss: [1m[32m0.48210[0m[0m | time: 55.533s
[2K
| Adam | epoch: 006 | loss: 0.48210 - acc: 0.8238 -- iter: 192/236
[A[ATraining Step: 47  | total loss: [1m[32m0.46470[0m[0m | time: 67.861s
[2K
| Adam | epoch: 006 | loss: 0.46470 - acc: 0.8322 -- iter: 224/236
[A[ATraining Step: 48  | total loss: [1m[32m0.42806[0m[0m | time: 79.162s
[2K
| Adam | epoch: 006 | loss: 0.42806 - acc: 0.8541 | val_loss: 0.85696 - val_acc: 0.6486 -- iter: 236/236
--
Training Step: 49  | total loss: [1m[32m0.39527[0m[0m | time: 10.307s
[2K
| Adam | epoch: 007 | loss: 0.39527 - acc: 0.8673 -- iter: 032/236
[A[ATraining Step: 50  | total loss: [1m[32m0.40517[0m[0m | time: 27.426s
[2K
| Adam | epoch: 007 | loss: 0.40517 - acc: 0.8539 -- iter: 064/236
[A[ATraining Step: 51  | total loss: [1m[32m0.38228[0m[0m | time: 39.496s
[2K
| Adam | epoch: 007 | loss: 0.38228 - acc: 0.8524 -- iter: 096/236
[A[ATraining Step: 52  | total loss: [1m[32m0.34359[0m[0m | time: 49.239s
[2K
| Adam | epoch: 007 | loss: 0.34359 - acc: 0.8698 -- iter: 128/236
[A[ATraining Step: 53  | total loss: [1m[32m0.36576[0m[0m | time: 52.768s
[2K
| Adam | epoch: 007 | loss: 0.36576 - acc: 0.8522 -- iter: 160/236
[A[ATraining Step: 54  | total loss: [1m[32m0.36494[0m[0m | time: 56.217s
[2K
| Adam | epoch: 007 | loss: 0.36494 - acc: 0.8252 -- iter: 192/236
[A[ATraining Step: 55  | total loss: [1m[32m0.34090[0m[0m | time: 63.948s
[2K
| Adam | epoch: 007 | loss: 0.34090 - acc: 0.8502 -- iter: 224/236
[A[ATraining Step: 56  | total loss: [1m[32m0.31135[0m[0m | time: 78.258s
[2K
| Adam | epoch: 007 | loss: 0.31135 - acc: 0.8625 | val_loss: 0.47599 - val_acc: 0.8514 -- iter: 236/236
--
Training Step: 57  | total loss: [1m[32m0.30672[0m[0m | time: 7.706s
[2K
| Adam | epoch: 008 | loss: 0.30672 - acc: 0.8685 -- iter: 032/236
[A[ATraining Step: 58  | total loss: [1m[32m0.28268[0m[0m | time: 15.408s
[2K
| Adam | epoch: 008 | loss: 0.28268 - acc: 0.8822 -- iter: 064/236
[A[ATraining Step: 59  | total loss: [1m[32m0.25771[0m[0m | time: 24.082s
[2K
| Adam | epoch: 008 | loss: 0.25771 - acc: 0.8980 -- iter: 096/236
[A[ATraining Step: 60  | total loss: [1m[32m0.24332[0m[0m | time: 36.071s
[2K
| Adam | epoch: 008 | loss: 0.24332 - acc: 0.9032 -- iter: 128/236
[A[ATraining Step: 61  | total loss: [1m[32m0.23221[0m[0m | time: 48.226s
[2K
| Adam | epoch: 008 | loss: 0.23221 - acc: 0.9118 -- iter: 160/236
[A[ATraining Step: 62  | total loss: [1m[32m0.22105[0m[0m | time: 53.675s
[2K
| Adam | epoch: 008 | loss: 0.22105 - acc: 0.9191 -- iter: 192/236
[A[ATraining Step: 63  | total loss: [1m[32m0.23387[0m[0m | time: 59.691s
[2K
| Adam | epoch: 008 | loss: 0.23387 - acc: 0.9188 -- iter: 224/236
[A[ATraining Step: 64  | total loss: [1m[32m0.21578[0m[0m | time: 75.928s
[2K
| Adam | epoch: 008 | loss: 0.21578 - acc: 0.9290 | val_loss: 2.06146 - val_acc: 0.6486 -- iter: 236/236
--
Training Step: 65  | total loss: [1m[32m0.22155[0m[0m | time: 7.697s
[2K
| Adam | epoch: 009 | loss: 0.22155 - acc: 0.9262 -- iter: 032/236
[A[ATraining Step: 66  | total loss: [1m[32m0.20828[0m[0m | time: 17.843s
[2K
| Adam | epoch: 009 | loss: 0.20828 - acc: 0.9313 -- iter: 064/236
[A[ATraining Step: 67  | total loss: [1m[32m0.20986[0m[0m | time: 29.737s
[2K
| Adam | epoch: 009 | loss: 0.20986 - acc: 0.9321 -- iter: 096/236
[A[ATraining Step: 68  | total loss: [1m[32m0.22001[0m[0m | time: 41.565s
[2K
| Adam | epoch: 009 | loss: 0.22001 - acc: 0.9290 -- iter: 128/236
[A[ATraining Step: 69  | total loss: [1m[32m0.25194[0m[0m | time: 53.841s
[2K
| Adam | epoch: 009 | loss: 0.25194 - acc: 0.9117 -- iter: 160/236
[A[ATraining Step: 70  | total loss: [1m[32m0.24506[0m[0m | time: 65.776s
[2K
| Adam | epoch: 009 | loss: 0.24506 - acc: 0.9111 -- iter: 192/236
[A[ATraining Step: 71  | total loss: [1m[32m0.22636[0m[0m | time: 71.646s
[2K
| Adam | epoch: 009 | loss: 0.22636 - acc: 0.9177 -- iter: 224/236
[A[ATraining Step: 72  | total loss: [1m[32m0.20721[0m[0m | time: 81.528s
[2K
| Adam | epoch: 009 | loss: 0.20721 - acc: 0.9269 | val_loss: 10.71026 - val_acc: 0.3514 -- iter: 236/236
--
Training Step: 73  | total loss: [1m[32m0.18712[0m[0m | time: 12.080s
[2K
| Adam | epoch: 010 | loss: 0.18712 - acc: 0.9351 -- iter: 032/236
[A[ATraining Step: 74  | total loss: [1m[32m0.22646[0m[0m | time: 24.088s
[2K
| Adam | epoch: 010 | loss: 0.22646 - acc: 0.9216 -- iter: 064/236
[A[ATraining Step: 75  | total loss: [1m[32m0.23461[0m[0m | time: 36.055s
[2K
| Adam | epoch: 010 | loss: 0.23461 - acc: 0.9166 -- iter: 096/236
[A[ATraining Step: 76  | total loss: [1m[32m0.24680[0m[0m | time: 47.798s
[2K
| Adam | epoch: 010 | loss: 0.24680 - acc: 0.9154 -- iter: 128/236
[A[ATraining Step: 77  | total loss: [1m[32m0.22747[0m[0m | time: 59.144s
[2K
| Adam | epoch: 010 | loss: 0.22747 - acc: 0.9211 -- iter: 160/236
[A[ATraining Step: 78  | total loss: [1m[32m0.22978[0m[0m | time: 66.898s
[2K
| Adam | epoch: 010 | loss: 0.22978 - acc: 0.9163 -- iter: 192/236
[A[ATraining Step: 79  | total loss: [1m[32m0.21099[0m[0m | time: 74.651s
[2K
| Adam | epoch: 010 | loss: 0.21099 - acc: 0.9249 -- iter: 224/236
[A[ATraining Step: 80  | total loss: [1m[32m0.22001[0m[0m | time: 81.402s
[2K
| Adam | epoch: 010 | loss: 0.22001 - acc: 0.9102 | val_loss: 0.90723 - val_acc: 0.7703 -- iter: 236/236
--
Training Step: 81  | total loss: [1m[32m0.20877[0m[0m | time: 5.638s
[2K
| Adam | epoch: 011 | loss: 0.20877 - acc: 0.9109 -- iter: 032/236
[A[ATraining Step: 82  | total loss: [1m[32m0.18946[0m[0m | time: 17.841s
[2K
| Adam | epoch: 011 | loss: 0.18946 - acc: 0.9198 -- iter: 064/236
[A[ATraining Step: 83  | total loss: [1m[32m0.19219[0m[0m | time: 29.722s
[2K
| Adam | epoch: 011 | loss: 0.19219 - acc: 0.9122 -- iter: 096/236
[A[ATraining Step: 84  | total loss: [1m[32m0.18942[0m[0m | time: 40.838s
[2K
| Adam | epoch: 011 | loss: 0.18942 - acc: 0.9116 -- iter: 128/236
[A[ATraining Step: 85  | total loss: [1m[32m0.20904[0m[0m | time: 48.482s
[2K
| Adam | epoch: 011 | loss: 0.20904 - acc: 0.9111 -- iter: 160/236
[A[ATraining Step: 86  | total loss: [1m[32m0.20088[0m[0m | time: 56.209s
[2K
| Adam | epoch: 011 | loss: 0.20088 - acc: 0.9106 -- iter: 192/236
[A[ATraining Step: 87  | total loss: [1m[32m0.18994[0m[0m | time: 64.004s
[2K
| Adam | epoch: 011 | loss: 0.18994 - acc: 0.9133 -- iter: 224/236
[A[ATraining Step: 88  | total loss: [1m[32m0.20450[0m[0m | time: 76.895s
[2K
| Adam | epoch: 011 | loss: 0.20450 - acc: 0.9063 | val_loss: 6.21704 - val_acc: 0.3514 -- iter: 236/236
--
Training Step: 89  | total loss: [1m[32m0.19378[0m[0m | time: 5.375s
[2K
| Adam | epoch: 012 | loss: 0.19378 - acc: 0.9126 -- iter: 032/236
[A[ATraining Step: 90  | total loss: [1m[32m0.18448[0m[0m | time: 11.435s
[2K
| Adam | epoch: 012 | loss: 0.18448 - acc: 0.9213 -- iter: 064/236
[A[ATraining Step: 91  | total loss: [1m[32m0.17059[0m[0m | time: 23.439s
[2K
| Adam | epoch: 012 | loss: 0.17059 - acc: 0.9292 -- iter: 096/236
[A[ATraining Step: 92  | total loss: [1m[32m0.17303[0m[0m | time: 31.422s
[2K
| Adam | epoch: 012 | loss: 0.17303 - acc: 0.9269 -- iter: 128/236
[A[ATraining Step: 93  | total loss: [1m[32m0.19758[0m[0m | time: 39.035s
[2K
| Adam | epoch: 012 | loss: 0.19758 - acc: 0.9217 -- iter: 160/236
[A[ATraining Step: 94  | total loss: [1m[32m0.19121[0m[0m | time: 47.055s
[2K
| Adam | epoch: 012 | loss: 0.19121 - acc: 0.9264 -- iter: 192/236
[A[ATraining Step: 95  | total loss: [1m[32m0.19067[0m[0m | time: 59.077s
[2K
| Adam | epoch: 012 | loss: 0.19067 - acc: 0.9275 -- iter: 224/236
[A[ATraining Step: 96  | total loss: [1m[32m0.17907[0m[0m | time: 75.844s
[2K
| Adam | epoch: 012 | loss: 0.17907 - acc: 0.9316 | val_loss: 1.29770 - val_acc: 0.6892 -- iter: 236/236
--
Training Step: 97  | total loss: [1m[32m0.17257[0m[0m | time: 7.719s
[2K
| Adam | epoch: 013 | loss: 0.17257 - acc: 0.9322 -- iter: 032/236
[A[ATraining Step: 98  | total loss: [1m[32m0.16017[0m[0m | time: 11.125s
[2K
| Adam | epoch: 013 | loss: 0.16017 - acc: 0.9390 -- iter: 064/236
[A[ATraining Step: 99  | total loss: [1m[32m0.15503[0m[0m | time: 14.637s
[2K
| Adam | epoch: 013 | loss: 0.15503 - acc: 0.9368 -- iter: 096/236
[A[ATraining Step: 100  | total loss: [1m[32m0.14238[0m[0m | time: 22.720s
[2K
| Adam | epoch: 013 | loss: 0.14238 - acc: 0.9431 -- iter: 128/236
[A[ATraining Step: 101  | total loss: [1m[32m0.13848[0m[0m | time: 35.248s
[2K
| Adam | epoch: 013 | loss: 0.13848 - acc: 0.9457 -- iter: 160/236
[A[ATraining Step: 102  | total loss: [1m[32m0.13001[0m[0m | time: 47.706s
[2K
| Adam | epoch: 013 | loss: 0.13001 - acc: 0.9511 -- iter: 192/236
[A[ATraining Step: 103  | total loss: [1m[32m0.14341[0m[0m | time: 59.985s
[2K
| Adam | epoch: 013 | loss: 0.14341 - acc: 0.9497 -- iter: 224/236
[A[ATraining Step: 104  | total loss: [1m[32m0.14504[0m[0m | time: 75.758s
[2K
| Adam | epoch: 013 | loss: 0.14504 - acc: 0.9485 | val_loss: 2.46201 - val_acc: 0.6486 -- iter: 236/236
--
Training Step: 105  | total loss: [1m[32m0.15028[0m[0m | time: 7.713s
[2K
| Adam | epoch: 014 | loss: 0.15028 - acc: 0.9474 -- iter: 032/236
[A[ATraining Step: 106  | total loss: [1m[32m0.13961[0m[0m | time: 19.836s
[2K
| Adam | epoch: 014 | loss: 0.13961 - acc: 0.9527 -- iter: 064/236
[A[ATraining Step: 107  | total loss: [1m[32m0.12999[0m[0m | time: 25.230s
[2K
| Adam | epoch: 014 | loss: 0.12999 - acc: 0.9574 -- iter: 096/236
[A[ATraining Step: 108  | total loss: [1m[32m0.14419[0m[0m | time: 30.996s
[2K
| Adam | epoch: 014 | loss: 0.14419 - acc: 0.9533 -- iter: 128/236
[A[ATraining Step: 109  | total loss: [1m[32m0.13441[0m[0m | time: 43.328s
[2K
| Adam | epoch: 014 | loss: 0.13441 - acc: 0.9580 -- iter: 160/236
[A[ATraining Step: 110  | total loss: [1m[32m0.13266[0m[0m | time: 55.485s
[2K
| Adam | epoch: 014 | loss: 0.13266 - acc: 0.9559 -- iter: 192/236
[A[ATraining Step: 111  | total loss: [1m[32m0.12834[0m[0m | time: 66.031s
[2K
| Adam | epoch: 014 | loss: 0.12834 - acc: 0.9572 -- iter: 224/236
[A[ATraining Step: 112  | total loss: [1m[32m0.14374[0m[0m | time: 79.010s
[2K
| Adam | epoch: 014 | loss: 0.14374 - acc: 0.9584 | val_loss: 0.78225 - val_acc: 0.8243 -- iter: 236/236
--
Training Step: 113  | total loss: [1m[32m0.13126[0m[0m | time: 12.103s
[2K
| Adam | epoch: 015 | loss: 0.13126 - acc: 0.9625 -- iter: 032/236
[A[ATraining Step: 114  | total loss: [1m[32m0.12202[0m[0m | time: 23.837s
[2K
| Adam | epoch: 015 | loss: 0.12202 - acc: 0.9663 -- iter: 064/236
[A[ATraining Step: 115  | total loss: [1m[32m0.11853[0m[0m | time: 36.492s
[2K
| Adam | epoch: 015 | loss: 0.11853 - acc: 0.9665 -- iter: 096/236
[A[ATraining Step: 116  | total loss: [1m[32m0.11136[0m[0m | time: 41.915s
[2K
| Adam | epoch: 015 | loss: 0.11136 - acc: 0.9668 -- iter: 128/236
[A[ATraining Step: 117  | total loss: [1m[32m0.10598[0m[0m | time: 47.502s
[2K
| Adam | epoch: 015 | loss: 0.10598 - acc: 0.9701 -- iter: 160/236
[A[ATraining Step: 118  | total loss: [1m[32m0.09843[0m[0m | time: 57.870s
[2K
| Adam | epoch: 015 | loss: 0.09843 - acc: 0.9731 -- iter: 192/236
[A[ATraining Step: 119  | total loss: [1m[32m0.09330[0m[0m | time: 65.646s
[2K
| Adam | epoch: 015 | loss: 0.09330 - acc: 0.9726 -- iter: 224/236
[A[ATraining Step: 120  | total loss: [1m[32m0.08931[0m[0m | time: 76.703s
[2K
| Adam | epoch: 015 | loss: 0.08931 - acc: 0.9722 | val_loss: 1.23274 - val_acc: 0.6216 -- iter: 236/236
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.859775641025641
Validation AUPRC:0.9131728501995154
Test AUC:0.8055555555555556
Test AUPRC:0.8120537363256628
BestTestF1Score	0.74	0.52	0.76	0.81	0.68	26	6	30	12	0.02
BestTestMCCScore	0.74	0.52	0.76	0.81	0.68	26	6	30	12	0.02
BestTestAccuracyScore	0.74	0.52	0.76	0.81	0.68	26	6	30	12	0.02
BestValidationF1Score	0.87	0.66	0.84	0.91	0.83	40	4	22	8	0.02
BestValidationMCC	0.87	0.66	0.84	0.91	0.83	40	4	22	8	0.02
BestValidationAccuracy	0.87	0.66	0.84	0.91	0.83	40	4	22	8	0.02
TestPredictions (Threshold:0.02)
CHEMBL1922122,TN,INACT,0.019999999552965164	CHEMBL77262,TN,INACT,0.0	CHEMBL485878,TN,INACT,0.009999999776482582	CHEMBL498249,FP,INACT,0.6499999761581421	CHEMBL490241,TN,INACT,0.0	CHEMBL509499,TN,INACT,0.0	CHEMBL3094441,TP,ACT,0.12999999523162842	CHEMBL2420584,FP,INACT,0.20000000298023224	CHEMBL388853,TP,ACT,0.6600000262260437	CHEMBL3775193,TP,ACT,0.12999999523162842	CHEMBL1884711,FN,ACT,0.009999999776482582	CHEMBL2392365,TP,ACT,0.5099999904632568	CHEMBL557050,TN,INACT,0.0	CHEMBL2392235,TN,INACT,0.0	CHEMBL133477,TN,INACT,0.0	CHEMBL2392237,TN,INACT,0.0	CHEMBL91829,TP,ACT,0.20999999344348907	CHEMBL101779,TN,INACT,0.0	CHEMBL3094449,TP,ACT,0.8799999952316284	CHEMBL457401,TN,INACT,0.0	CHEMBL120703,TN,INACT,0.0	CHEMBL2386745,TP,ACT,0.9599999785423279	CHEMBL3774736,TP,ACT,0.44999998807907104	CHEMBL489646,TN,INACT,0.0	CHEMBL312078,TN,INACT,0.0	CHEMBL608533,FN,ACT,0.0	CHEMBL3335363,TP,ACT,0.05999999865889549	CHEMBL3335359,TP,ACT,0.20000000298023224	CHEMBL132948,TN,INACT,0.0	CHEMBL173478,TN,INACT,0.0	CHEMBL522760,TN,INACT,0.009999999776482582	CHEMBL133213,TN,INACT,0.0	CHEMBL14762,FN,ACT,0.0	CHEMBL3103192,FN,ACT,0.0	CHEMBL2392234,TN,INACT,0.009999999776482582	CHEMBL2158866,FP,INACT,0.6200000047683716	CHEMBL3421970,TP,ACT,0.2199999988079071	CHEMBL174634,FP,INACT,0.1899999976158142	CHEMBL549792,TN,INACT,0.0	CHEMBL3094442,TP,ACT,0.10000000149011612	CHEMBL1767294,TN,INACT,0.0	CHEMBL1922121,TN,INACT,0.0	CHEMBL3589662,TP,ACT,0.2199999988079071	CHEMBL521734,FP,INACT,0.38999998569488525	CHEMBL1515868,TP,ACT,0.25999999046325684	CHEMBL1933806,TN,INACT,0.0	CHEMBL3589674,FN,ACT,0.009999999776482582	CHEMBL3421966,FN,ACT,0.0	CHEMBL379218,FN,ACT,0.009999999776482582	CHEMBL3094448,TP,ACT,0.9800000190734863	CHEMBL3421980,FN,ACT,0.0	CHEMBL1899256,TP,ACT,0.30000001192092896	CHEMBL131695,TN,INACT,0.0	CHEMBL3421981,FN,ACT,0.0	CHEMBL498705,TN,INACT,0.0	CHEMBL1904905,FN,ACT,0.0	CHEMBL1868723,TP,ACT,0.6399999856948853	CHEMBL2392233,TN,INACT,0.009999999776482582	CHEMBL3094461,TP,ACT,0.9800000190734863	CHEMBL74799,TN,INACT,0.0	CHEMBL3589660,TP,ACT,0.23999999463558197	CHEMBL464552,FN,ACT,0.019999999552965164	CHEMBL568150,TP,ACT,0.1899999976158142	CHEMBL3421963,TP,ACT,0.029999999329447746	CHEMBL607707,FN,ACT,0.0	CHEMBL2392238,TN,INACT,0.009999999776482582	CHEMBL3094463,TP,ACT,0.9399999976158142	CHEMBL1767275,TN,INACT,0.0	CHEMBL1436585,TP,ACT,0.7300000190734863	CHEMBL3094451,TP,ACT,0.9599999785423279	CHEMBL2386746,TP,ACT,0.8600000143051147	CHEMBL550856,FP,INACT,0.5400000214576721	CHEMBL3335350,TP,ACT,0.10000000149011612	CHEMBL2392375,TN,INACT,0.0	

