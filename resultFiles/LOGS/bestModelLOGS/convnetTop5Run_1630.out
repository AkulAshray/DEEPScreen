ImageNetInceptionV2 CHEMBL2717 adam 0.0001 15 0 0 0.8 False True
Number of active compounds :	114
Number of inactive compounds :	114
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL2717_adam_0.0001_15_0_0_0.8_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL2717_adam_0.0001_15_0.8/
---------------------------------
Training samples: 144
Validation samples: 45
--
Training Step: 1  | time: 63.819s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/144
[A[ATraining Step: 2  | total loss: [1m[32m0.67095[0m[0m | time: 74.585s
[2K
| Adam | epoch: 001 | loss: 0.67095 - acc: 0.3656 -- iter: 064/144
[A[ATraining Step: 3  | total loss: [1m[32m0.64996[0m[0m | time: 83.172s
[2K
| Adam | epoch: 001 | loss: 0.64996 - acc: 0.6801 -- iter: 096/144
[A[ATraining Step: 4  | total loss: [1m[32m0.55802[0m[0m | time: 91.712s
[2K
| Adam | epoch: 001 | loss: 0.55802 - acc: 0.7325 -- iter: 128/144
[A[ATraining Step: 5  | total loss: [1m[32m0.48380[0m[0m | time: 111.068s
[2K
| Adam | epoch: 001 | loss: 0.48380 - acc: 0.8095 | val_loss: 0.71461 - val_acc: 0.6000 -- iter: 144/144
--
Training Step: 6  | total loss: [1m[32m0.54086[0m[0m | time: 7.491s
[2K
| Adam | epoch: 002 | loss: 0.54086 - acc: 0.7713 -- iter: 032/144
[A[ATraining Step: 7  | total loss: [1m[32m0.45624[0m[0m | time: 25.802s
[2K
| Adam | epoch: 002 | loss: 0.45624 - acc: 0.8335 -- iter: 064/144
[A[ATraining Step: 8  | total loss: [1m[32m0.44527[0m[0m | time: 39.557s
[2K
| Adam | epoch: 002 | loss: 0.44527 - acc: 0.8920 -- iter: 096/144
[A[ATraining Step: 9  | total loss: [1m[32m0.44533[0m[0m | time: 53.800s
[2K
| Adam | epoch: 002 | loss: 0.44533 - acc: 0.8499 -- iter: 128/144
[A[ATraining Step: 10  | total loss: [1m[32m0.35780[0m[0m | time: 70.845s
[2K
| Adam | epoch: 002 | loss: 0.35780 - acc: 0.9093 | val_loss: 0.67309 - val_acc: 0.6000 -- iter: 144/144
--
Training Step: 11  | total loss: [1m[32m0.34647[0m[0m | time: 7.571s
[2K
| Adam | epoch: 003 | loss: 0.34647 - acc: 0.9227 -- iter: 032/144
[A[ATraining Step: 12  | total loss: [1m[32m0.30313[0m[0m | time: 14.665s
[2K
| Adam | epoch: 003 | loss: 0.30313 - acc: 0.9575 -- iter: 064/144
[A[ATraining Step: 13  | total loss: [1m[32m0.23441[0m[0m | time: 34.765s
[2K
| Adam | epoch: 003 | loss: 0.23441 - acc: 0.9757 -- iter: 096/144
[A[ATraining Step: 14  | total loss: [1m[32m0.20760[0m[0m | time: 51.172s
[2K
| Adam | epoch: 003 | loss: 0.20760 - acc: 0.9729 -- iter: 128/144
[A[ATraining Step: 15  | total loss: [1m[32m0.21474[0m[0m | time: 67.092s
[2K
| Adam | epoch: 003 | loss: 0.21474 - acc: 0.9468 | val_loss: 0.78077 - val_acc: 0.4000 -- iter: 144/144
--
Training Step: 16  | total loss: [1m[32m0.18359[0m[0m | time: 13.224s
[2K
| Adam | epoch: 004 | loss: 0.18359 - acc: 0.9550 -- iter: 032/144
[A[ATraining Step: 17  | total loss: [1m[32m0.14400[0m[0m | time: 21.812s
[2K
| Adam | epoch: 004 | loss: 0.14400 - acc: 0.9712 -- iter: 064/144
[A[ATraining Step: 18  | total loss: [1m[32m0.11742[0m[0m | time: 27.207s
[2K
| Adam | epoch: 004 | loss: 0.11742 - acc: 0.9812 -- iter: 096/144
[A[ATraining Step: 19  | total loss: [1m[32m0.09277[0m[0m | time: 37.608s
[2K
| Adam | epoch: 004 | loss: 0.09277 - acc: 0.9875 -- iter: 128/144
[A[ATraining Step: 20  | total loss: [1m[32m0.08442[0m[0m | time: 54.019s
[2K
| Adam | epoch: 004 | loss: 0.08442 - acc: 0.9915 | val_loss: 1.44014 - val_acc: 0.4000 -- iter: 144/144
--
Training Step: 21  | total loss: [1m[32m0.06867[0m[0m | time: 101.940s
[2K
| Adam | epoch: 005 | loss: 0.06867 - acc: 0.9941 -- iter: 032/144
[A[ATraining Step: 22  | total loss: [1m[32m0.16004[0m[0m | time: 238.970s
[2K
| Adam | epoch: 005 | loss: 0.16004 - acc: 0.9771 -- iter: 064/144
[A[ATraining Step: 23  | total loss: [1m[32m0.11989[0m[0m | time: 248.056s
[2K
| Adam | epoch: 005 | loss: 0.11989 - acc: 0.9838 -- iter: 096/144
[A[ATraining Step: 24  | total loss: [1m[32m0.09357[0m[0m | time: 257.101s
[2K
| Adam | epoch: 005 | loss: 0.09357 - acc: 0.9883 -- iter: 128/144
[A[ATraining Step: 25  | total loss: [1m[32m0.07462[0m[0m | time: 340.882s
[2K
| Adam | epoch: 005 | loss: 0.07462 - acc: 0.9915 | val_loss: 2.11784 - val_acc: 0.4000 -- iter: 144/144
--
Training Step: 26  | total loss: [1m[32m0.07777[0m[0m | time: 54.668s
[2K
| Adam | epoch: 006 | loss: 0.07777 - acc: 0.9855 -- iter: 032/144
[A[ATraining Step: 27  | total loss: [1m[32m0.06241[0m[0m | time: 70.575s
[2K
| Adam | epoch: 006 | loss: 0.06241 - acc: 0.9892 -- iter: 064/144
[A[ATraining Step: 28  | total loss: [1m[32m0.20651[0m[0m | time: 109.625s
[2K
| Adam | epoch: 006 | loss: 0.20651 - acc: 0.9685 -- iter: 096/144
[A[ATraining Step: 29  | total loss: [1m[32m0.15971[0m[0m | time: 118.268s
[2K
| Adam | epoch: 006 | loss: 0.15971 - acc: 0.9761 -- iter: 128/144
[A[ATraining Step: 30  | total loss: [1m[32m0.12464[0m[0m | time: 131.150s
[2K
| Adam | epoch: 006 | loss: 0.12464 - acc: 0.9818 | val_loss: 2.06810 - val_acc: 0.4000 -- iter: 144/144
--
Training Step: 31  | total loss: [1m[32m0.09902[0m[0m | time: 36.174s
[2K
| Adam | epoch: 007 | loss: 0.09902 - acc: 0.9860 -- iter: 032/144
[A[ATraining Step: 32  | total loss: [1m[32m0.07989[0m[0m | time: 71.511s
[2K
| Adam | epoch: 007 | loss: 0.07989 - acc: 0.9891 -- iter: 064/144
[A[ATraining Step: 33  | total loss: [1m[32m0.06722[0m[0m | time: 89.095s
[2K
| Adam | epoch: 007 | loss: 0.06722 - acc: 0.9915 -- iter: 096/144
[A[ATraining Step: 34  | total loss: [1m[32m0.07631[0m[0m | time: 104.312s
[2K
| Adam | epoch: 007 | loss: 0.07631 - acc: 0.9866 -- iter: 128/144
[A[ATraining Step: 35  | total loss: [1m[32m0.10960[0m[0m | time: 116.798s
[2K
| Adam | epoch: 007 | loss: 0.10960 - acc: 0.9764 | val_loss: 1.51584 - val_acc: 0.4000 -- iter: 144/144
--
Training Step: 36  | total loss: [1m[32m0.08931[0m[0m | time: 5.729s
[2K
| Adam | epoch: 008 | loss: 0.08931 - acc: 0.9812 -- iter: 032/144
[A[ATraining Step: 37  | total loss: [1m[32m0.07359[0m[0m | time: 16.286s
[2K
| Adam | epoch: 008 | loss: 0.07359 - acc: 0.9850 -- iter: 064/144
[A[ATraining Step: 38  | total loss: [1m[32m0.06065[0m[0m | time: 31.825s
[2K
| Adam | epoch: 008 | loss: 0.06065 - acc: 0.9879 -- iter: 096/144
[A[ATraining Step: 39  | total loss: [1m[32m0.05151[0m[0m | time: 88.184s
[2K
| Adam | epoch: 008 | loss: 0.05151 - acc: 0.9902 -- iter: 128/144
[A[ATraining Step: 40  | total loss: [1m[32m0.04379[0m[0m | time: 109.442s
[2K
| Adam | epoch: 008 | loss: 0.04379 - acc: 0.9921 | val_loss: 1.12393 - val_acc: 0.4000 -- iter: 144/144
--
Training Step: 41  | total loss: [1m[32m0.03744[0m[0m | time: 7.138s
[2K
| Adam | epoch: 009 | loss: 0.03744 - acc: 0.9935 -- iter: 032/144
[A[ATraining Step: 42  | total loss: [1m[32m0.03641[0m[0m | time: 14.551s
[2K
| Adam | epoch: 009 | loss: 0.03641 - acc: 0.9947 -- iter: 064/144
[A[ATraining Step: 43  | total loss: [1m[32m0.03249[0m[0m | time: 26.987s
[2K
| Adam | epoch: 009 | loss: 0.03249 - acc: 0.9956 -- iter: 096/144
[A[ATraining Step: 44  | total loss: [1m[32m0.04933[0m[0m | time: 38.846s
[2K
| Adam | epoch: 009 | loss: 0.04933 - acc: 0.9910 -- iter: 128/144
[A[ATraining Step: 45  | total loss: [1m[32m0.04254[0m[0m | time: 54.811s
[2K
| Adam | epoch: 009 | loss: 0.04254 - acc: 0.9925 | val_loss: 1.09291 - val_acc: 0.4000 -- iter: 144/144
--
Training Step: 46  | total loss: [1m[32m0.33269[0m[0m | time: 12.421s
[2K
| Adam | epoch: 010 | loss: 0.33269 - acc: 0.9417 -- iter: 032/144
[A[ATraining Step: 47  | total loss: [1m[32m0.27945[0m[0m | time: 19.388s
[2K
| Adam | epoch: 010 | loss: 0.27945 - acc: 0.9512 -- iter: 064/144
[A[ATraining Step: 48  | total loss: [1m[32m0.23588[0m[0m | time: 26.325s
[2K
| Adam | epoch: 010 | loss: 0.23588 - acc: 0.9591 -- iter: 096/144
[A[ATraining Step: 49  | total loss: [1m[32m0.20031[0m[0m | time: 46.147s
[2K
| Adam | epoch: 010 | loss: 0.20031 - acc: 0.9655 -- iter: 128/144
[A[ATraining Step: 50  | total loss: [1m[32m0.17352[0m[0m | time: 61.654s
[2K
| Adam | epoch: 010 | loss: 0.17352 - acc: 0.9709 | val_loss: 0.85857 - val_acc: 0.4667 -- iter: 144/144
--
Training Step: 51  | total loss: [1m[32m0.15046[0m[0m | time: 12.677s
[2K
| Adam | epoch: 011 | loss: 0.15046 - acc: 0.9753 -- iter: 032/144
[A[ATraining Step: 52  | total loss: [1m[32m0.16340[0m[0m | time: 25.237s
[2K
| Adam | epoch: 011 | loss: 0.16340 - acc: 0.9743 -- iter: 064/144
[A[ATraining Step: 53  | total loss: [1m[32m0.15774[0m[0m | time: 32.786s
[2K
| Adam | epoch: 011 | loss: 0.15774 - acc: 0.9735 -- iter: 096/144
[A[ATraining Step: 54  | total loss: [1m[32m0.13693[0m[0m | time: 37.761s
[2K
| Adam | epoch: 011 | loss: 0.13693 - acc: 0.9774 -- iter: 128/144
[A[ATraining Step: 55  | total loss: [1m[32m0.11940[0m[0m | time: 48.053s
[2K
| Adam | epoch: 011 | loss: 0.11940 - acc: 0.9806 | val_loss: 1.07387 - val_acc: 0.5111 -- iter: 144/144
--
Training Step: 56  | total loss: [1m[32m0.10955[0m[0m | time: 12.439s
[2K
| Adam | epoch: 012 | loss: 0.10955 - acc: 0.9789 -- iter: 032/144
[A[ATraining Step: 57  | total loss: [1m[32m0.10041[0m[0m | time: 25.373s
[2K
| Adam | epoch: 012 | loss: 0.10041 - acc: 0.9818 -- iter: 064/144
[A[ATraining Step: 58  | total loss: [1m[32m0.11149[0m[0m | time: 37.507s
[2K
| Adam | epoch: 012 | loss: 0.11149 - acc: 0.9801 -- iter: 096/144
[A[ATraining Step: 59  | total loss: [1m[32m0.09792[0m[0m | time: 44.726s
[2K
| Adam | epoch: 012 | loss: 0.09792 - acc: 0.9827 -- iter: 128/144
[A[ATraining Step: 60  | total loss: [1m[32m0.08604[0m[0m | time: 55.439s
[2K
| Adam | epoch: 012 | loss: 0.08604 - acc: 0.9850 | val_loss: 1.71137 - val_acc: 0.4889 -- iter: 144/144
--
Training Step: 61  | total loss: [1m[32m0.07584[0m[0m | time: 12.353s
[2K
| Adam | epoch: 013 | loss: 0.07584 - acc: 0.9870 -- iter: 032/144
[A[ATraining Step: 62  | total loss: [1m[32m0.07724[0m[0m | time: 24.506s
[2K
| Adam | epoch: 013 | loss: 0.07724 - acc: 0.9846 -- iter: 064/144
[A[ATraining Step: 63  | total loss: [1m[32m0.07618[0m[0m | time: 36.611s
[2K
| Adam | epoch: 013 | loss: 0.07618 - acc: 0.9826 -- iter: 096/144
[A[ATraining Step: 64  | total loss: [1m[32m0.09205[0m[0m | time: 48.943s
[2K
| Adam | epoch: 013 | loss: 0.09205 - acc: 0.9809 -- iter: 128/144
[A[ATraining Step: 65  | total loss: [1m[32m0.08237[0m[0m | time: 59.221s
[2K
| Adam | epoch: 013 | loss: 0.08237 - acc: 0.9832 | val_loss: 0.80556 - val_acc: 0.6222 -- iter: 144/144
--
Training Step: 66  | total loss: [1m[32m0.08594[0m[0m | time: 6.798s
[2K
| Adam | epoch: 014 | loss: 0.08594 - acc: 0.9777 -- iter: 032/144
[A[ATraining Step: 67  | total loss: [1m[32m0.07823[0m[0m | time: 18.581s
[2K
| Adam | epoch: 014 | loss: 0.07823 - acc: 0.9804 -- iter: 064/144
[A[ATraining Step: 68  | total loss: [1m[32m0.07049[0m[0m | time: 30.386s
[2K
| Adam | epoch: 014 | loss: 0.07049 - acc: 0.9827 -- iter: 096/144
[A[ATraining Step: 69  | total loss: [1m[32m0.07365[0m[0m | time: 42.746s
[2K
| Adam | epoch: 014 | loss: 0.07365 - acc: 0.9774 -- iter: 128/144
[A[ATraining Step: 70  | total loss: [1m[32m0.06645[0m[0m | time: 56.316s
[2K
| Adam | epoch: 014 | loss: 0.06645 - acc: 0.9800 | val_loss: 0.71143 - val_acc: 0.6667 -- iter: 144/144
--
Training Step: 71  | total loss: [1m[32m0.06064[0m[0m | time: 6.718s
[2K
| Adam | epoch: 015 | loss: 0.06064 - acc: 0.9823 -- iter: 032/144
[A[ATraining Step: 72  | total loss: [1m[32m0.05563[0m[0m | time: 14.111s
[2K
| Adam | epoch: 015 | loss: 0.05563 - acc: 0.9843 -- iter: 064/144
[A[ATraining Step: 73  | total loss: [1m[32m0.05067[0m[0m | time: 25.948s
[2K
| Adam | epoch: 015 | loss: 0.05067 - acc: 0.9860 -- iter: 096/144
[A[ATraining Step: 74  | total loss: [1m[32m0.04667[0m[0m | time: 37.883s
[2K
| Adam | epoch: 015 | loss: 0.04667 - acc: 0.9876 -- iter: 128/144
[A[ATraining Step: 75  | total loss: [1m[32m0.04495[0m[0m | time: 53.259s
[2K
| Adam | epoch: 015 | loss: 0.04495 - acc: 0.9889 | val_loss: 0.58527 - val_acc: 0.7333 -- iter: 144/144
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.9094650205761317
Validation AUPRC:0.9366615646378624
Test AUC:0.904
Test AUPRC:0.9349626683531314
BestTestF1Score	0.86	0.69	0.84	0.88	0.84	21	3	17	4	0.81
BestTestMCCScore	0.86	0.69	0.84	0.88	0.84	21	3	17	4	0.81
BestTestAccuracyScore	0.86	0.69	0.84	0.88	0.84	21	3	17	4	0.81
BestValidationF1Score	0.91	0.77	0.89	0.87	0.96	26	4	14	1	0.81
BestValidationMCC	0.91	0.77	0.89	0.87	0.96	26	4	14	1	0.81
BestValidationAccuracy	0.91	0.77	0.89	0.87	0.96	26	4	14	1	0.81
TestPredictions (Threshold:0.81)
CHEMBL3769461,TN,INACT,0.6000000238418579	CHEMBL1916106,TP,ACT,0.8399999737739563	CHEMBL596489,FN,ACT,0.7799999713897705	CHEMBL2012209,TP,ACT,1.0	CHEMBL31954,TN,INACT,0.019999999552965164	CHEMBL90059,TN,INACT,0.75	CHEMBL399298,TP,ACT,0.9900000095367432	CHEMBL3770470,TP,ACT,1.0	CHEMBL184903,TN,INACT,0.6299999952316284	CHEMBL2158066,TN,INACT,0.1599999964237213	CHEMBL284745,TP,ACT,1.0	CHEMBL368198,TN,INACT,0.23000000417232513	CHEMBL356323,FP,INACT,0.9300000071525574	CHEMBL2312696,TN,INACT,0.25	CHEMBL1940056,TP,ACT,0.8199999928474426	CHEMBL34276,TP,ACT,1.0	CHEMBL399385,TP,ACT,1.0	CHEMBL3769817,TP,ACT,1.0	CHEMBL34744,TP,ACT,1.0	CHEMBL471703,TP,ACT,1.0	CHEMBL250137,TP,ACT,1.0	CHEMBL1650676,TP,ACT,0.9100000262260437	CHEMBL1097216,TN,INACT,0.7699999809265137	CHEMBL282765,TN,INACT,0.23999999463558197	CHEMBL112996,FP,INACT,0.9900000095367432	CHEMBL2012219,TP,ACT,1.0	CHEMBL3601563,FN,ACT,0.46000000834465027	CHEMBL2012216,TP,ACT,1.0	CHEMBL421268,TN,INACT,0.6800000071525574	CHEMBL1779427,TN,INACT,0.18000000715255737	CHEMBL12599,TN,INACT,0.6899999976158142	CHEMBL1086110,FN,ACT,0.36000001430511475	CHEMBL2012212,TP,ACT,1.0	CHEMBL274528,FP,INACT,0.9700000286102295	CHEMBL2012218,TP,ACT,1.0	CHEMBL2012208,TP,ACT,1.0	CHEMBL3647462,TN,INACT,0.6200000047683716	CHEMBL603422,TP,ACT,0.8399999737739563	CHEMBL432659,TN,INACT,0.07000000029802322	CHEMBL2012207,TP,ACT,1.0	CHEMBL341475,TN,INACT,0.6200000047683716	CHEMBL3741492,TP,ACT,0.9800000190734863	CHEMBL2180945,FN,ACT,0.6299999952316284	CHEMBL281663,TN,INACT,0.49000000953674316	CHEMBL553515,TN,INACT,0.18000000715255737	

