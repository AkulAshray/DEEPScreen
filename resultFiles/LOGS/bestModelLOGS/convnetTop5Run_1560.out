CNNModel CHEMBL1878 adam 0.0005 15 256 0 0.6 False True
Number of active compounds :	250
Number of inactive compounds :	250
---------------------------------
Run id: CNNModel_CHEMBL1878_adam_0.0005_15_256_0_0.6_False_True_id
Log directory: ../tflearnLogs/CNNModel_CHEMBL1878_adam_0.0005_15_256_0.6_True/
---------------------------------
Training samples: 300
Validation samples: 95
--
Training Step: 1  | time: 1.637s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/300
[A[ATraining Step: 2  | total loss: [1m[32m0.62398[0m[0m | time: 2.631s
[2K
| Adam | epoch: 001 | loss: 0.62398 - acc: 0.3937 -- iter: 064/300
[A[ATraining Step: 3  | total loss: [1m[32m0.68015[0m[0m | time: 3.713s
[2K
| Adam | epoch: 001 | loss: 0.68015 - acc: 0.5063 -- iter: 096/300
[A[ATraining Step: 4  | total loss: [1m[32m0.68984[0m[0m | time: 4.798s
[2K
| Adam | epoch: 001 | loss: 0.68984 - acc: 0.5016 -- iter: 128/300
[A[ATraining Step: 5  | total loss: [1m[32m0.68935[0m[0m | time: 5.897s
[2K
| Adam | epoch: 001 | loss: 0.68935 - acc: 0.5654 -- iter: 160/300
[A[ATraining Step: 6  | total loss: [1m[32m0.69355[0m[0m | time: 7.047s
[2K
| Adam | epoch: 001 | loss: 0.69355 - acc: 0.5033 -- iter: 192/300
[A[ATraining Step: 7  | total loss: [1m[32m0.69200[0m[0m | time: 8.360s
[2K
| Adam | epoch: 001 | loss: 0.69200 - acc: 0.5201 -- iter: 224/300
[A[ATraining Step: 8  | total loss: [1m[32m0.69863[0m[0m | time: 9.498s
[2K
| Adam | epoch: 001 | loss: 0.69863 - acc: 0.4560 -- iter: 256/300
[A[ATraining Step: 9  | total loss: [1m[32m0.69322[0m[0m | time: 10.539s
[2K
| Adam | epoch: 001 | loss: 0.69322 - acc: 0.5124 -- iter: 288/300
[A[ATraining Step: 10  | total loss: [1m[32m0.69058[0m[0m | time: 12.117s
[2K
| Adam | epoch: 001 | loss: 0.69058 - acc: 0.5531 | val_loss: 0.69350 - val_acc: 0.4947 -- iter: 300/300
--
Training Step: 11  | total loss: [1m[32m0.69450[0m[0m | time: 0.416s
[2K
| Adam | epoch: 002 | loss: 0.69450 - acc: 0.4885 -- iter: 032/300
[A[ATraining Step: 12  | total loss: [1m[32m0.69046[0m[0m | time: 1.504s
[2K
| Adam | epoch: 002 | loss: 0.69046 - acc: 0.5687 -- iter: 064/300
[A[ATraining Step: 13  | total loss: [1m[32m0.69108[0m[0m | time: 2.680s
[2K
| Adam | epoch: 002 | loss: 0.69108 - acc: 0.5526 -- iter: 096/300
[A[ATraining Step: 14  | total loss: [1m[32m0.69270[0m[0m | time: 3.754s
[2K
| Adam | epoch: 002 | loss: 0.69270 - acc: 0.5183 -- iter: 128/300
[A[ATraining Step: 15  | total loss: [1m[32m0.69225[0m[0m | time: 4.891s
[2K
| Adam | epoch: 002 | loss: 0.69225 - acc: 0.5234 -- iter: 160/300
[A[ATraining Step: 16  | total loss: [1m[32m0.69475[0m[0m | time: 6.187s
[2K
| Adam | epoch: 002 | loss: 0.69475 - acc: 0.4912 -- iter: 192/300
[A[ATraining Step: 17  | total loss: [1m[32m0.69268[0m[0m | time: 7.443s
[2K
| Adam | epoch: 002 | loss: 0.69268 - acc: 0.5168 -- iter: 224/300
[A[ATraining Step: 18  | total loss: [1m[32m0.69133[0m[0m | time: 8.468s
[2K
| Adam | epoch: 002 | loss: 0.69133 - acc: 0.5327 -- iter: 256/300
[A[ATraining Step: 19  | total loss: [1m[32m0.69144[0m[0m | time: 9.654s
[2K
| Adam | epoch: 002 | loss: 0.69144 - acc: 0.5322 -- iter: 288/300
[A[ATraining Step: 20  | total loss: [1m[32m0.69118[0m[0m | time: 11.901s
[2K
| Adam | epoch: 002 | loss: 0.69118 - acc: 0.5319 | val_loss: 0.69363 - val_acc: 0.4947 -- iter: 300/300
--
Training Step: 21  | total loss: [1m[32m0.69288[0m[0m | time: 0.567s
[2K
| Adam | epoch: 003 | loss: 0.69288 - acc: 0.5123 -- iter: 032/300
[A[ATraining Step: 22  | total loss: [1m[32m0.68789[0m[0m | time: 1.022s
[2K
| Adam | epoch: 003 | loss: 0.68789 - acc: 0.5586 -- iter: 064/300
[A[ATraining Step: 23  | total loss: [1m[32m0.69010[0m[0m | time: 2.181s
[2K
| Adam | epoch: 003 | loss: 0.69010 - acc: 0.5416 -- iter: 096/300
[A[ATraining Step: 24  | total loss: [1m[32m0.69228[0m[0m | time: 3.452s
[2K
| Adam | epoch: 003 | loss: 0.69228 - acc: 0.5211 -- iter: 128/300
[A[ATraining Step: 25  | total loss: [1m[32m0.69400[0m[0m | time: 4.777s
[2K
| Adam | epoch: 003 | loss: 0.69400 - acc: 0.5068 -- iter: 160/300
[A[ATraining Step: 26  | total loss: [1m[32m0.69508[0m[0m | time: 5.778s
[2K
| Adam | epoch: 003 | loss: 0.69508 - acc: 0.4967 -- iter: 192/300
[A[ATraining Step: 27  | total loss: [1m[32m0.69533[0m[0m | time: 6.796s
[2K
| Adam | epoch: 003 | loss: 0.69533 - acc: 0.4895 -- iter: 224/300
[A[ATraining Step: 28  | total loss: [1m[32m0.69554[0m[0m | time: 7.882s
[2K
| Adam | epoch: 003 | loss: 0.69554 - acc: 0.4843 -- iter: 256/300
[A[ATraining Step: 29  | total loss: [1m[32m0.69370[0m[0m | time: 8.875s
[2K
| Adam | epoch: 003 | loss: 0.69370 - acc: 0.5034 -- iter: 288/300
[A[ATraining Step: 30  | total loss: [1m[32m0.69170[0m[0m | time: 10.885s
[2K
| Adam | epoch: 003 | loss: 0.69170 - acc: 0.5396 | val_loss: 0.69233 - val_acc: 0.4947 -- iter: 300/300
--
Training Step: 31  | total loss: [1m[32m0.69181[0m[0m | time: 1.110s
[2K
| Adam | epoch: 004 | loss: 0.69181 - acc: 0.5304 -- iter: 032/300
[A[ATraining Step: 32  | total loss: [1m[32m0.69060[0m[0m | time: 1.479s
[2K
| Adam | epoch: 004 | loss: 0.69060 - acc: 0.5517 -- iter: 064/300
[A[ATraining Step: 33  | total loss: [1m[32m0.69082[0m[0m | time: 1.855s
[2K
| Adam | epoch: 004 | loss: 0.69082 - acc: 0.5404 -- iter: 096/300
[A[ATraining Step: 34  | total loss: [1m[32m0.69151[0m[0m | time: 2.614s
[2K
| Adam | epoch: 004 | loss: 0.69151 - acc: 0.5317 -- iter: 128/300
[A[ATraining Step: 35  | total loss: [1m[32m0.69155[0m[0m | time: 3.349s
[2K
| Adam | epoch: 004 | loss: 0.69155 - acc: 0.5251 -- iter: 160/300
[A[ATraining Step: 36  | total loss: [1m[32m0.69133[0m[0m | time: 4.173s
[2K
| Adam | epoch: 004 | loss: 0.69133 - acc: 0.5199 -- iter: 192/300
[A[ATraining Step: 37  | total loss: [1m[32m0.68840[0m[0m | time: 4.924s
[2K
| Adam | epoch: 004 | loss: 0.68840 - acc: 0.5535 -- iter: 224/300
[A[ATraining Step: 38  | total loss: [1m[32m0.68570[0m[0m | time: 5.663s
[2K
| Adam | epoch: 004 | loss: 0.68570 - acc: 0.5736 -- iter: 256/300
[A[ATraining Step: 39  | total loss: [1m[32m0.68607[0m[0m | time: 6.396s
[2K
| Adam | epoch: 004 | loss: 0.68607 - acc: 0.5655 -- iter: 288/300
[A[ATraining Step: 40  | total loss: [1m[32m0.68466[0m[0m | time: 8.132s
[2K
| Adam | epoch: 004 | loss: 0.68466 - acc: 0.5649 | val_loss: 0.69510 - val_acc: 0.4947 -- iter: 300/300
--
Training Step: 41  | total loss: [1m[32m0.69173[0m[0m | time: 1.235s
[2K
| Adam | epoch: 005 | loss: 0.69173 - acc: 0.5358 -- iter: 032/300
[A[ATraining Step: 42  | total loss: [1m[32m0.69855[0m[0m | time: 2.407s
[2K
| Adam | epoch: 005 | loss: 0.69855 - acc: 0.5068 -- iter: 064/300
[A[ATraining Step: 43  | total loss: [1m[32m0.69750[0m[0m | time: 2.871s
[2K
| Adam | epoch: 005 | loss: 0.69750 - acc: 0.5001 -- iter: 096/300
[A[ATraining Step: 44  | total loss: [1m[32m0.69643[0m[0m | time: 3.146s
[2K
| Adam | epoch: 005 | loss: 0.69643 - acc: 0.5001 -- iter: 128/300
[A[ATraining Step: 45  | total loss: [1m[32m0.69131[0m[0m | time: 4.215s
[2K
| Adam | epoch: 005 | loss: 0.69131 - acc: 0.5425 -- iter: 160/300
[A[ATraining Step: 46  | total loss: [1m[32m0.69008[0m[0m | time: 5.306s
[2K
| Adam | epoch: 005 | loss: 0.69008 - acc: 0.5406 -- iter: 192/300
[A[ATraining Step: 47  | total loss: [1m[32m0.68966[0m[0m | time: 6.266s
[2K
| Adam | epoch: 005 | loss: 0.68966 - acc: 0.5340 -- iter: 224/300
[A[ATraining Step: 48  | total loss: [1m[32m0.69107[0m[0m | time: 6.895s
[2K
| Adam | epoch: 005 | loss: 0.69107 - acc: 0.5135 -- iter: 256/300
[A[ATraining Step: 49  | total loss: [1m[32m0.68798[0m[0m | time: 7.515s
[2K
| Adam | epoch: 005 | loss: 0.68798 - acc: 0.5409 -- iter: 288/300
[A[ATraining Step: 50  | total loss: [1m[32m0.68875[0m[0m | time: 9.132s
[2K
| Adam | epoch: 005 | loss: 0.68875 - acc: 0.5249 | val_loss: 0.68609 - val_acc: 0.4947 -- iter: 300/300
--
Training Step: 51  | total loss: [1m[32m0.68881[0m[0m | time: 0.740s
[2K
| Adam | epoch: 006 | loss: 0.68881 - acc: 0.5163 -- iter: 032/300
[A[ATraining Step: 52  | total loss: [1m[32m0.69024[0m[0m | time: 1.478s
[2K
| Adam | epoch: 006 | loss: 0.69024 - acc: 0.4904 -- iter: 064/300
[A[ATraining Step: 53  | total loss: [1m[32m0.68577[0m[0m | time: 2.205s
[2K
| Adam | epoch: 006 | loss: 0.68577 - acc: 0.5287 -- iter: 096/300
[A[ATraining Step: 54  | total loss: [1m[32m0.68644[0m[0m | time: 2.466s
[2K
| Adam | epoch: 006 | loss: 0.68644 - acc: 0.5200 -- iter: 128/300
[A[ATraining Step: 55  | total loss: [1m[32m0.68085[0m[0m | time: 2.821s
[2K
| Adam | epoch: 006 | loss: 0.68085 - acc: 0.5529 -- iter: 160/300
[A[ATraining Step: 56  | total loss: [1m[32m0.67937[0m[0m | time: 3.586s
[2K
| Adam | epoch: 006 | loss: 0.67937 - acc: 0.5572 -- iter: 192/300
[A[ATraining Step: 57  | total loss: [1m[32m0.68238[0m[0m | time: 4.375s
[2K
| Adam | epoch: 006 | loss: 0.68238 - acc: 0.5319 -- iter: 224/300
[A[ATraining Step: 58  | total loss: [1m[32m0.68686[0m[0m | time: 5.116s
[2K
| Adam | epoch: 006 | loss: 0.68686 - acc: 0.5105 -- iter: 256/300
[A[ATraining Step: 59  | total loss: [1m[32m0.68328[0m[0m | time: 5.888s
[2K
| Adam | epoch: 006 | loss: 0.68328 - acc: 0.5175 -- iter: 288/300
[A[ATraining Step: 60  | total loss: [1m[32m0.67422[0m[0m | time: 7.616s
[2K
| Adam | epoch: 006 | loss: 0.67422 - acc: 0.5400 | val_loss: 0.67958 - val_acc: 0.4947 -- iter: 300/300
--
Training Step: 61  | total loss: [1m[32m0.66532[0m[0m | time: 0.776s
[2K
| Adam | epoch: 007 | loss: 0.66532 - acc: 0.5593 -- iter: 032/300
[A[ATraining Step: 62  | total loss: [1m[32m0.67231[0m[0m | time: 1.510s
[2K
| Adam | epoch: 007 | loss: 0.67231 - acc: 0.5436 -- iter: 064/300
[A[ATraining Step: 63  | total loss: [1m[32m0.67460[0m[0m | time: 2.224s
[2K
| Adam | epoch: 007 | loss: 0.67460 - acc: 0.5341 -- iter: 096/300
[A[ATraining Step: 64  | total loss: [1m[32m0.67402[0m[0m | time: 2.983s
[2K
| Adam | epoch: 007 | loss: 0.67402 - acc: 0.5298 -- iter: 128/300
[A[ATraining Step: 65  | total loss: [1m[32m0.66960[0m[0m | time: 3.325s
[2K
| Adam | epoch: 007 | loss: 0.66960 - acc: 0.5339 -- iter: 160/300
[A[ATraining Step: 66  | total loss: [1m[32m0.66576[0m[0m | time: 3.596s
[2K
| Adam | epoch: 007 | loss: 0.66576 - acc: 0.5399 -- iter: 192/300
[A[ATraining Step: 67  | total loss: [1m[32m0.66498[0m[0m | time: 4.376s
[2K
| Adam | epoch: 007 | loss: 0.66498 - acc: 0.5351 -- iter: 224/300
[A[ATraining Step: 68  | total loss: [1m[32m0.65884[0m[0m | time: 5.139s
[2K
| Adam | epoch: 007 | loss: 0.65884 - acc: 0.5457 -- iter: 256/300
[A[ATraining Step: 69  | total loss: [1m[32m0.65461[0m[0m | time: 5.874s
[2K
| Adam | epoch: 007 | loss: 0.65461 - acc: 0.5441 -- iter: 288/300
[A[ATraining Step: 70  | total loss: [1m[32m0.65542[0m[0m | time: 7.622s
[2K
| Adam | epoch: 007 | loss: 0.65542 - acc: 0.5426 | val_loss: 0.62315 - val_acc: 0.4947 -- iter: 300/300
--
Training Step: 71  | total loss: [1m[32m0.64563[0m[0m | time: 0.817s
[2K
| Adam | epoch: 008 | loss: 0.64563 - acc: 0.5448 -- iter: 032/300
[A[ATraining Step: 72  | total loss: [1m[32m0.64315[0m[0m | time: 1.563s
[2K
| Adam | epoch: 008 | loss: 0.64315 - acc: 0.5328 -- iter: 064/300
[A[ATraining Step: 73  | total loss: [1m[32m0.63630[0m[0m | time: 2.319s
[2K
| Adam | epoch: 008 | loss: 0.63630 - acc: 0.5395 -- iter: 096/300
[A[ATraining Step: 74  | total loss: [1m[32m0.62800[0m[0m | time: 3.326s
[2K
| Adam | epoch: 008 | loss: 0.62800 - acc: 0.5524 -- iter: 128/300
[A[ATraining Step: 75  | total loss: [1m[32m0.62236[0m[0m | time: 4.417s
[2K
| Adam | epoch: 008 | loss: 0.62236 - acc: 0.5670 -- iter: 160/300
[A[ATraining Step: 76  | total loss: [1m[32m0.61534[0m[0m | time: 4.853s
[2K
| Adam | epoch: 008 | loss: 0.61534 - acc: 0.5866 -- iter: 192/300
[A[ATraining Step: 77  | total loss: [1m[32m0.60755[0m[0m | time: 5.254s
[2K
| Adam | epoch: 008 | loss: 0.60755 - acc: 0.5863 -- iter: 224/300
[A[ATraining Step: 78  | total loss: [1m[32m0.59805[0m[0m | time: 6.263s
[2K
| Adam | epoch: 008 | loss: 0.59805 - acc: 0.6034 -- iter: 256/300
[A[ATraining Step: 79  | total loss: [1m[32m0.59338[0m[0m | time: 7.300s
[2K
| Adam | epoch: 008 | loss: 0.59338 - acc: 0.6153 -- iter: 288/300
[A[ATraining Step: 80  | total loss: [1m[32m0.58178[0m[0m | time: 9.348s
[2K
| Adam | epoch: 008 | loss: 0.58178 - acc: 0.6355 | val_loss: 0.50931 - val_acc: 0.7789 -- iter: 300/300
--
Training Step: 81  | total loss: [1m[32m0.58191[0m[0m | time: 1.144s
[2K
| Adam | epoch: 009 | loss: 0.58191 - acc: 0.6376 -- iter: 032/300
[A[ATraining Step: 82  | total loss: [1m[32m0.55659[0m[0m | time: 2.216s
[2K
| Adam | epoch: 009 | loss: 0.55659 - acc: 0.6676 -- iter: 064/300
[A[ATraining Step: 83  | total loss: [1m[32m0.55023[0m[0m | time: 3.313s
[2K
| Adam | epoch: 009 | loss: 0.55023 - acc: 0.6758 -- iter: 096/300
[A[ATraining Step: 84  | total loss: [1m[32m0.54903[0m[0m | time: 4.591s
[2K
| Adam | epoch: 009 | loss: 0.54903 - acc: 0.6926 -- iter: 128/300
[A[ATraining Step: 85  | total loss: [1m[32m0.53158[0m[0m | time: 5.775s
[2K
| Adam | epoch: 009 | loss: 0.53158 - acc: 0.7171 -- iter: 160/300
[A[ATraining Step: 86  | total loss: [1m[32m0.51906[0m[0m | time: 6.792s
[2K
| Adam | epoch: 009 | loss: 0.51906 - acc: 0.7329 -- iter: 192/300
[A[ATraining Step: 87  | total loss: [1m[32m0.51206[0m[0m | time: 7.277s
[2K
| Adam | epoch: 009 | loss: 0.51206 - acc: 0.7409 -- iter: 224/300
[A[ATraining Step: 88  | total loss: [1m[32m0.49214[0m[0m | time: 7.797s
[2K
| Adam | epoch: 009 | loss: 0.49214 - acc: 0.7501 -- iter: 256/300
[A[ATraining Step: 89  | total loss: [1m[32m0.54091[0m[0m | time: 8.953s
[2K
| Adam | epoch: 009 | loss: 0.54091 - acc: 0.7418 -- iter: 288/300
[A[ATraining Step: 90  | total loss: [1m[32m0.52381[0m[0m | time: 11.202s
[2K
| Adam | epoch: 009 | loss: 0.52381 - acc: 0.7520 | val_loss: 0.40930 - val_acc: 0.8105 -- iter: 300/300
--
Training Step: 91  | total loss: [1m[32m0.52503[0m[0m | time: 1.137s
[2K
| Adam | epoch: 010 | loss: 0.52503 - acc: 0.7486 -- iter: 032/300
[A[ATraining Step: 92  | total loss: [1m[32m0.50277[0m[0m | time: 2.341s
[2K
| Adam | epoch: 010 | loss: 0.50277 - acc: 0.7675 -- iter: 064/300
[A[ATraining Step: 93  | total loss: [1m[32m0.48151[0m[0m | time: 3.555s
[2K
| Adam | epoch: 010 | loss: 0.48151 - acc: 0.7783 -- iter: 096/300
[A[ATraining Step: 94  | total loss: [1m[32m0.48807[0m[0m | time: 4.897s
[2K
| Adam | epoch: 010 | loss: 0.48807 - acc: 0.7754 -- iter: 128/300
[A[ATraining Step: 95  | total loss: [1m[32m0.46888[0m[0m | time: 6.324s
[2K
| Adam | epoch: 010 | loss: 0.46888 - acc: 0.7854 -- iter: 160/300
[A[ATraining Step: 96  | total loss: [1m[32m0.44869[0m[0m | time: 7.607s
[2K
| Adam | epoch: 010 | loss: 0.44869 - acc: 0.7912 -- iter: 192/300
[A[ATraining Step: 97  | total loss: [1m[32m0.44009[0m[0m | time: 8.558s
[2K
| Adam | epoch: 010 | loss: 0.44009 - acc: 0.7965 -- iter: 224/300
[A[ATraining Step: 98  | total loss: [1m[32m0.41502[0m[0m | time: 8.960s
[2K
| Adam | epoch: 010 | loss: 0.41502 - acc: 0.8137 -- iter: 256/300
[A[ATraining Step: 99  | total loss: [1m[32m0.40820[0m[0m | time: 9.367s
[2K
| Adam | epoch: 010 | loss: 0.40820 - acc: 0.8157 -- iter: 288/300
[A[ATraining Step: 100  | total loss: [1m[32m0.43566[0m[0m | time: 11.465s
[2K
| Adam | epoch: 010 | loss: 0.43566 - acc: 0.8091 | val_loss: 0.46892 - val_acc: 0.8000 -- iter: 300/300
--
Training Step: 101  | total loss: [1m[32m0.44341[0m[0m | time: 1.159s
[2K
| Adam | epoch: 011 | loss: 0.44341 - acc: 0.8063 -- iter: 032/300
[A[ATraining Step: 102  | total loss: [1m[32m0.43260[0m[0m | time: 2.127s
[2K
| Adam | epoch: 011 | loss: 0.43260 - acc: 0.8101 -- iter: 064/300
[A[ATraining Step: 103  | total loss: [1m[32m0.41763[0m[0m | time: 3.285s
[2K
| Adam | epoch: 011 | loss: 0.41763 - acc: 0.8197 -- iter: 096/300
[A[ATraining Step: 104  | total loss: [1m[32m0.42182[0m[0m | time: 4.375s
[2K
| Adam | epoch: 011 | loss: 0.42182 - acc: 0.8158 -- iter: 128/300
[A[ATraining Step: 105  | total loss: [1m[32m0.41281[0m[0m | time: 5.502s
[2K
| Adam | epoch: 011 | loss: 0.41281 - acc: 0.8218 -- iter: 160/300
[A[ATraining Step: 106  | total loss: [1m[32m0.39563[0m[0m | time: 6.636s
[2K
| Adam | epoch: 011 | loss: 0.39563 - acc: 0.8302 -- iter: 192/300
[A[ATraining Step: 107  | total loss: [1m[32m0.37893[0m[0m | time: 7.929s
[2K
| Adam | epoch: 011 | loss: 0.37893 - acc: 0.8409 -- iter: 224/300
[A[ATraining Step: 108  | total loss: [1m[32m0.35749[0m[0m | time: 9.156s
[2K
| Adam | epoch: 011 | loss: 0.35749 - acc: 0.8537 -- iter: 256/300
[A[ATraining Step: 109  | total loss: [1m[32m0.33783[0m[0m | time: 9.680s
[2K
| Adam | epoch: 011 | loss: 0.33783 - acc: 0.8652 -- iter: 288/300
[A[ATraining Step: 110  | total loss: [1m[32m0.32550[0m[0m | time: 11.298s
[2K
| Adam | epoch: 011 | loss: 0.32550 - acc: 0.8704 | val_loss: 0.47415 - val_acc: 0.7579 -- iter: 300/300
--
Training Step: 111  | total loss: [1m[32m0.32547[0m[0m | time: 0.613s
[2K
| Adam | epoch: 012 | loss: 0.32547 - acc: 0.8667 -- iter: 032/300
[A[ATraining Step: 112  | total loss: [1m[32m0.31034[0m[0m | time: 1.290s
[2K
| Adam | epoch: 012 | loss: 0.31034 - acc: 0.8706 -- iter: 064/300
[A[ATraining Step: 113  | total loss: [1m[32m0.31495[0m[0m | time: 2.039s
[2K
| Adam | epoch: 012 | loss: 0.31495 - acc: 0.8648 -- iter: 096/300
[A[ATraining Step: 114  | total loss: [1m[32m0.32203[0m[0m | time: 2.768s
[2K
| Adam | epoch: 012 | loss: 0.32203 - acc: 0.8627 -- iter: 128/300
[A[ATraining Step: 115  | total loss: [1m[32m0.31147[0m[0m | time: 3.594s
[2K
| Adam | epoch: 012 | loss: 0.31147 - acc: 0.8671 -- iter: 160/300
[A[ATraining Step: 116  | total loss: [1m[32m0.31284[0m[0m | time: 4.378s
[2K
| Adam | epoch: 012 | loss: 0.31284 - acc: 0.8647 -- iter: 192/300
[A[ATraining Step: 117  | total loss: [1m[32m0.32881[0m[0m | time: 5.095s
[2K
| Adam | epoch: 012 | loss: 0.32881 - acc: 0.8564 -- iter: 224/300
[A[ATraining Step: 118  | total loss: [1m[32m0.34272[0m[0m | time: 5.874s
[2K
| Adam | epoch: 012 | loss: 0.34272 - acc: 0.8489 -- iter: 256/300
[A[ATraining Step: 119  | total loss: [1m[32m0.33601[0m[0m | time: 6.629s
[2K
| Adam | epoch: 012 | loss: 0.33601 - acc: 0.8484 -- iter: 288/300
[A[ATraining Step: 120  | total loss: [1m[32m0.31808[0m[0m | time: 7.947s
[2K
| Adam | epoch: 012 | loss: 0.31808 - acc: 0.8541 | val_loss: 0.59584 - val_acc: 0.7684 -- iter: 300/300
--
Training Step: 121  | total loss: [1m[32m0.29842[0m[0m | time: 0.278s
[2K
| Adam | epoch: 013 | loss: 0.29842 - acc: 0.8687 -- iter: 032/300
[A[ATraining Step: 122  | total loss: [1m[32m0.34752[0m[0m | time: 1.051s
[2K
| Adam | epoch: 013 | loss: 0.34752 - acc: 0.8485 -- iter: 064/300
[A[ATraining Step: 123  | total loss: [1m[32m0.35504[0m[0m | time: 1.809s
[2K
| Adam | epoch: 013 | loss: 0.35504 - acc: 0.8449 -- iter: 096/300
[A[ATraining Step: 124  | total loss: [1m[32m0.34539[0m[0m | time: 2.544s
[2K
| Adam | epoch: 013 | loss: 0.34539 - acc: 0.8479 -- iter: 128/300
[A[ATraining Step: 125  | total loss: [1m[32m0.33232[0m[0m | time: 3.273s
[2K
| Adam | epoch: 013 | loss: 0.33232 - acc: 0.8569 -- iter: 160/300
[A[ATraining Step: 126  | total loss: [1m[32m0.31618[0m[0m | time: 3.990s
[2K
| Adam | epoch: 013 | loss: 0.31618 - acc: 0.8649 -- iter: 192/300
[A[ATraining Step: 127  | total loss: [1m[32m0.30782[0m[0m | time: 4.717s
[2K
| Adam | epoch: 013 | loss: 0.30782 - acc: 0.8722 -- iter: 224/300
[A[ATraining Step: 128  | total loss: [1m[32m0.30454[0m[0m | time: 5.422s
[2K
| Adam | epoch: 013 | loss: 0.30454 - acc: 0.8725 -- iter: 256/300
[A[ATraining Step: 129  | total loss: [1m[32m0.30528[0m[0m | time: 6.137s
[2K
| Adam | epoch: 013 | loss: 0.30528 - acc: 0.8727 -- iter: 288/300
[A[ATraining Step: 130  | total loss: [1m[32m0.28592[0m[0m | time: 7.856s
[2K
| Adam | epoch: 013 | loss: 0.28592 - acc: 0.8823 | val_loss: 0.35614 - val_acc: 0.8211 -- iter: 300/300
--
Training Step: 131  | total loss: [1m[32m0.27320[0m[0m | time: 0.484s
[2K
| Adam | epoch: 014 | loss: 0.27320 - acc: 0.8910 -- iter: 032/300
[A[ATraining Step: 132  | total loss: [1m[32m0.26026[0m[0m | time: 0.935s
[2K
| Adam | epoch: 014 | loss: 0.26026 - acc: 0.8935 -- iter: 064/300
[A[ATraining Step: 133  | total loss: [1m[32m0.31211[0m[0m | time: 2.128s
[2K
| Adam | epoch: 014 | loss: 0.31211 - acc: 0.8709 -- iter: 096/300
[A[ATraining Step: 134  | total loss: [1m[32m0.28998[0m[0m | time: 3.096s
[2K
| Adam | epoch: 014 | loss: 0.28998 - acc: 0.8838 -- iter: 128/300
[A[ATraining Step: 135  | total loss: [1m[32m0.27707[0m[0m | time: 4.187s
[2K
| Adam | epoch: 014 | loss: 0.27707 - acc: 0.8891 -- iter: 160/300
[A[ATraining Step: 136  | total loss: [1m[32m0.26478[0m[0m | time: 5.306s
[2K
| Adam | epoch: 014 | loss: 0.26478 - acc: 0.8971 -- iter: 192/300
[A[ATraining Step: 137  | total loss: [1m[32m0.25061[0m[0m | time: 6.497s
[2K
| Adam | epoch: 014 | loss: 0.25061 - acc: 0.9043 -- iter: 224/300
[A[ATraining Step: 138  | total loss: [1m[32m0.26241[0m[0m | time: 7.575s
[2K
| Adam | epoch: 014 | loss: 0.26241 - acc: 0.8951 -- iter: 256/300
[A[ATraining Step: 139  | total loss: [1m[32m0.24890[0m[0m | time: 8.690s
[2K
| Adam | epoch: 014 | loss: 0.24890 - acc: 0.8993 -- iter: 288/300
[A[ATraining Step: 140  | total loss: [1m[32m0.24844[0m[0m | time: 10.899s
[2K
| Adam | epoch: 014 | loss: 0.24844 - acc: 0.9000 | val_loss: 0.33478 - val_acc: 0.8316 -- iter: 300/300
--
Training Step: 141  | total loss: [1m[32m0.23528[0m[0m | time: 1.091s
[2K
| Adam | epoch: 015 | loss: 0.23528 - acc: 0.9038 -- iter: 032/300
[A[ATraining Step: 142  | total loss: [1m[32m0.22037[0m[0m | time: 1.420s
[2K
| Adam | epoch: 015 | loss: 0.22037 - acc: 0.9103 -- iter: 064/300
[A[ATraining Step: 143  | total loss: [1m[32m0.20649[0m[0m | time: 1.899s
[2K
| Adam | epoch: 015 | loss: 0.20649 - acc: 0.9192 -- iter: 096/300
[A[ATraining Step: 144  | total loss: [1m[32m0.19189[0m[0m | time: 3.132s
[2K
| Adam | epoch: 015 | loss: 0.19189 - acc: 0.9273 -- iter: 128/300
[A[ATraining Step: 145  | total loss: [1m[32m0.18025[0m[0m | time: 4.363s
[2K
| Adam | epoch: 015 | loss: 0.18025 - acc: 0.9283 -- iter: 160/300
[A[ATraining Step: 146  | total loss: [1m[32m0.17178[0m[0m | time: 5.485s
[2K
| Adam | epoch: 015 | loss: 0.17178 - acc: 0.9324 -- iter: 192/300
[A[ATraining Step: 147  | total loss: [1m[32m0.16215[0m[0m | time: 6.407s
[2K
| Adam | epoch: 015 | loss: 0.16215 - acc: 0.9391 -- iter: 224/300
[A[ATraining Step: 148  | total loss: [1m[32m0.15616[0m[0m | time: 7.413s
[2K
| Adam | epoch: 015 | loss: 0.15616 - acc: 0.9390 -- iter: 256/300
[A[ATraining Step: 149  | total loss: [1m[32m0.14449[0m[0m | time: 8.532s
[2K
| Adam | epoch: 015 | loss: 0.14449 - acc: 0.9451 -- iter: 288/300
[A[ATraining Step: 150  | total loss: [1m[32m0.13270[0m[0m | time: 10.645s
[2K
| Adam | epoch: 015 | loss: 0.13270 - acc: 0.9506 | val_loss: 0.30330 - val_acc: 0.8737 -- iter: 300/300
--
Validation AUC:0.9534574468085106
Validation AUPRC:0.9595756948375517
Test AUC:0.9651162790697674
Test AUPRC:0.962695721235096
BestTestF1Score	0.92	0.85	0.93	0.89	0.95	41	5	47	2	0.62
BestTestMCCScore	0.93	0.87	0.94	0.91	0.95	41	4	48	2	0.73
BestTestAccuracyScore	0.93	0.87	0.94	0.91	0.95	41	4	48	2	0.73
BestValidationF1Score	0.88	0.77	0.88	0.89	0.87	41	5	43	6	0.62
BestValidationMCC	0.88	0.77	0.88	0.93	0.83	39	3	45	8	0.73
BestValidationAccuracy	0.88	0.77	0.88	0.93	0.83	39	3	45	8	0.73
TestPredictions (Threshold:0.73)
CHEMBL488738,TP,ACT,1.0	CHEMBL400105,TN,INACT,0.14000000059604645	CHEMBL2115569,TN,INACT,0.009999999776482582	CHEMBL50041,TN,INACT,0.009999999776482582	CHEMBL3827570,TP,ACT,1.0	CHEMBL2377742,TP,ACT,1.0	CHEMBL1255602,FN,ACT,0.05000000074505806	CHEMBL572128,TN,INACT,0.009999999776482582	CHEMBL2349584,TP,ACT,1.0	CHEMBL2349598,TP,ACT,1.0	CHEMBL322965,TN,INACT,0.009999999776482582	CHEMBL299180,TN,INACT,0.14000000059604645	CHEMBL327764,TN,INACT,0.009999999776482582	CHEMBL113571,TN,INACT,0.009999999776482582	CHEMBL3423095,TN,INACT,0.44999998807907104	CHEMBL1256404,TP,ACT,0.9200000166893005	CHEMBL299683,TN,INACT,0.009999999776482582	CHEMBL3093421,TP,ACT,0.9900000095367432	CHEMBL59299,TN,INACT,0.10000000149011612	CHEMBL2346769,TP,ACT,0.9599999785423279	CHEMBL3093414,TP,ACT,1.0	CHEMBL3633656,FP,INACT,0.9800000190734863	CHEMBL334842,TN,INACT,0.1599999964237213	CHEMBL3827154,TP,ACT,1.0	CHEMBL322887,TN,INACT,0.009999999776482582	CHEMBL3401716,TN,INACT,0.1899999976158142	CHEMBL209461,TN,INACT,0.019999999552965164	CHEMBL39372,TN,INACT,0.009999999776482582	CHEMBL302680,TN,INACT,0.03999999910593033	CHEMBL3827577,TP,ACT,0.9900000095367432	CHEMBL3423100,TN,INACT,0.5299999713897705	CHEMBL3093424,TP,ACT,1.0	CHEMBL40157,TN,INACT,0.009999999776482582	CHEMBL1256486,FN,ACT,0.07000000029802322	CHEMBL390892,TN,INACT,0.03999999910593033	CHEMBL361623,TP,ACT,0.9900000095367432	CHEMBL1224260,TP,ACT,1.0	CHEMBL1256402,TP,ACT,0.949999988079071	CHEMBL1289346,TP,ACT,0.9900000095367432	CHEMBL3091483,TP,ACT,1.0	CHEMBL180336,TP,ACT,0.9900000095367432	CHEMBL326678,TN,INACT,0.009999999776482582	CHEMBL323713,TN,INACT,0.019999999552965164	CHEMBL2349601,TP,ACT,1.0	CHEMBL107768,TN,INACT,0.09000000357627869	CHEMBL506027,TN,INACT,0.009999999776482582	CHEMBL3093416,TP,ACT,1.0	CHEMBL114027,TN,INACT,0.009999999776482582	CHEMBL3827410,TP,ACT,1.0	CHEMBL2349593,TP,ACT,1.0	CHEMBL1771691,FP,INACT,0.9800000190734863	CHEMBL2377752,TP,ACT,1.0	CHEMBL1256366,TP,ACT,0.8700000047683716	CHEMBL2377751,TP,ACT,0.949999988079071	CHEMBL2346773,TP,ACT,0.9599999785423279	CHEMBL2377738,TP,ACT,1.0	CHEMBL327936,TN,INACT,0.10000000149011612	CHEMBL367027,TN,INACT,0.33000001311302185	CHEMBL279956,TN,INACT,0.029999999329447746	CHEMBL3401722,TN,INACT,0.009999999776482582	CHEMBL13260,TN,INACT,0.009999999776482582	CHEMBL2346785,TP,ACT,1.0	CHEMBL89739,TN,INACT,0.009999999776482582	CHEMBL112248,TN,INACT,0.0	CHEMBL1224341,TP,ACT,0.9700000286102295	CHEMBL571476,TP,ACT,0.8999999761581421	CHEMBL288635,TN,INACT,0.03999999910593033	CHEMBL1224340,TP,ACT,1.0	CHEMBL3093417,TP,ACT,1.0	CHEMBL2346777,TP,ACT,0.9599999785423279	CHEMBL3786026,TN,INACT,0.009999999776482582	CHEMBL2349579,TP,ACT,1.0	CHEMBL556070,TN,INACT,0.05999999865889549	CHEMBL2385880,FP,INACT,0.9300000071525574	CHEMBL550523,TN,INACT,0.05999999865889549	CHEMBL91677,TN,INACT,0.33000001311302185	CHEMBL2377741,TP,ACT,1.0	CHEMBL94990,TN,INACT,0.009999999776482582	CHEMBL116357,TN,INACT,0.11999999731779099	CHEMBL2312400,TN,INACT,0.029999999329447746	CHEMBL310435,TN,INACT,0.009999999776482582	CHEMBL1682812,TN,INACT,0.3199999928474426	CHEMBL43412,TN,INACT,0.009999999776482582	CHEMBL186453,TN,INACT,0.49000000953674316	CHEMBL1783935,TP,ACT,0.9900000095367432	CHEMBL2349596,TP,ACT,1.0	CHEMBL163465,FP,INACT,0.9700000286102295	CHEMBL583522,TP,ACT,0.7400000095367432	CHEMBL267006,TN,INACT,0.6700000166893005	CHEMBL302402,TN,INACT,0.009999999776482582	CHEMBL2114116,TN,INACT,0.009999999776482582	CHEMBL2346786,TP,ACT,1.0	CHEMBL487511,TP,ACT,0.7400000095367432	CHEMBL1224425,TP,ACT,0.9800000190734863	CHEMBL112797,TN,INACT,0.009999999776482582	

