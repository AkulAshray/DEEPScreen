ImageNetInceptionV2 CHEMBL3616 adam 0.0005 15 0 0 0.6 False True
Number of active compounds :	210
Number of inactive compounds :	210
---------------------------------
Run id: ImageNetInceptionV2_CHEMBL3616_adam_0.0005_15_0_0_0.6_False_True_id
Log directory: ../tflearnLogs/ImageNetInceptionV2_CHEMBL3616_adam_0.0005_15_0.6/
---------------------------------
Training samples: 268
Validation samples: 84
--
Training Step: 1  | time: 52.312s
[2K
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 032/268
[A[ATraining Step: 2  | total loss: [1m[32m0.62499[0m[0m | time: 60.340s
[2K
| Adam | epoch: 001 | loss: 0.62499 - acc: 0.4500 -- iter: 064/268
[A[ATraining Step: 3  | total loss: [1m[32m0.75669[0m[0m | time: 68.335s
[2K
| Adam | epoch: 001 | loss: 0.75669 - acc: 0.5420 -- iter: 096/268
[A[ATraining Step: 4  | total loss: [1m[32m0.68176[0m[0m | time: 79.259s
[2K
| Adam | epoch: 001 | loss: 0.68176 - acc: 0.6746 -- iter: 128/268
[A[ATraining Step: 5  | total loss: [1m[32m0.58299[0m[0m | time: 138.759s
[2K
| Adam | epoch: 001 | loss: 0.58299 - acc: 0.7268 -- iter: 160/268
[A[ATraining Step: 6  | total loss: [1m[32m0.52428[0m[0m | time: 247.454s
[2K
| Adam | epoch: 001 | loss: 0.52428 - acc: 0.8020 -- iter: 192/268
[A[ATraining Step: 7  | total loss: [1m[32m0.50225[0m[0m | time: 340.660s
[2K
| Adam | epoch: 001 | loss: 0.50225 - acc: 0.7708 -- iter: 224/268
[A[ATraining Step: 8  | total loss: [1m[32m0.42524[0m[0m | time: 349.182s
[2K
| Adam | epoch: 001 | loss: 0.42524 - acc: 0.8294 -- iter: 256/268
[A[ATraining Step: 9  | total loss: [1m[32m0.37314[0m[0m | time: 362.073s
[2K
| Adam | epoch: 001 | loss: 0.37314 - acc: 0.8205 | val_loss: 1.05518 - val_acc: 0.4881 -- iter: 268/268
--
Training Step: 10  | total loss: [1m[32m0.27471[0m[0m | time: 3.529s
[2K
| Adam | epoch: 002 | loss: 0.27471 - acc: 0.8686 -- iter: 032/268
[A[ATraining Step: 11  | total loss: [1m[32m0.16527[0m[0m | time: 13.191s
[2K
| Adam | epoch: 002 | loss: 0.16527 - acc: 0.9308 -- iter: 064/268
[A[ATraining Step: 12  | total loss: [1m[32m0.22973[0m[0m | time: 22.278s
[2K
| Adam | epoch: 002 | loss: 0.22973 - acc: 0.9057 -- iter: 096/268
[A[ATraining Step: 13  | total loss: [1m[32m0.30105[0m[0m | time: 33.899s
[2K
| Adam | epoch: 002 | loss: 0.30105 - acc: 0.9059 -- iter: 128/268
[A[ATraining Step: 14  | total loss: [1m[32m0.41947[0m[0m | time: 50.984s
[2K
| Adam | epoch: 002 | loss: 0.41947 - acc: 0.8294 -- iter: 160/268
[A[ATraining Step: 15  | total loss: [1m[32m0.41548[0m[0m | time: 76.082s
[2K
| Adam | epoch: 002 | loss: 0.41548 - acc: 0.7861 -- iter: 192/268
[A[ATraining Step: 16  | total loss: [1m[32m0.42748[0m[0m | time: 93.506s
[2K
| Adam | epoch: 002 | loss: 0.42748 - acc: 0.7960 -- iter: 224/268
[A[ATraining Step: 17  | total loss: [1m[32m0.37736[0m[0m | time: 110.533s
[2K
| Adam | epoch: 002 | loss: 0.37736 - acc: 0.8244 -- iter: 256/268
[A[ATraining Step: 18  | total loss: [1m[32m0.29581[0m[0m | time: 124.942s
[2K
| Adam | epoch: 002 | loss: 0.29581 - acc: 0.8528 | val_loss: 1.22477 - val_acc: 0.4881 -- iter: 268/268
--
Training Step: 19  | total loss: [1m[32m0.22149[0m[0m | time: 7.359s
[2K
| Adam | epoch: 003 | loss: 0.22149 - acc: 0.8914 -- iter: 032/268
[A[ATraining Step: 20  | total loss: [1m[32m0.16948[0m[0m | time: 14.557s
[2K
| Adam | epoch: 003 | loss: 0.16948 - acc: 0.9263 -- iter: 064/268
[A[ATraining Step: 21  | total loss: [1m[32m0.12374[0m[0m | time: 121.138s
[2K
| Adam | epoch: 003 | loss: 0.12374 - acc: 0.9492 -- iter: 096/268
[A[ATraining Step: 22  | total loss: [1m[32m0.09698[0m[0m | time: 133.587s
[2K
| Adam | epoch: 003 | loss: 0.09698 - acc: 0.9644 -- iter: 128/268
[A[ATraining Step: 23  | total loss: [1m[32m0.07429[0m[0m | time: 144.274s
[2K
| Adam | epoch: 003 | loss: 0.07429 - acc: 0.9748 -- iter: 160/268
[A[ATraining Step: 24  | total loss: [1m[32m0.08594[0m[0m | time: 153.793s
[2K
| Adam | epoch: 003 | loss: 0.08594 - acc: 0.9643 -- iter: 192/268
[A[ATraining Step: 25  | total loss: [1m[32m0.10581[0m[0m | time: 162.468s
[2K
| Adam | epoch: 003 | loss: 0.10581 - acc: 0.9570 -- iter: 224/268
[A[ATraining Step: 26  | total loss: [1m[32m0.09773[0m[0m | time: 174.198s
[2K
| Adam | epoch: 003 | loss: 0.09773 - acc: 0.9601 -- iter: 256/268
[A[ATraining Step: 27  | total loss: [1m[32m0.11257[0m[0m | time: 195.955s
[2K
| Adam | epoch: 003 | loss: 0.11257 - acc: 0.9462 | val_loss: 0.75342 - val_acc: 0.4881 -- iter: 268/268
--
Training Step: 28  | total loss: [1m[32m0.13545[0m[0m | time: 7.916s
[2K
| Adam | epoch: 004 | loss: 0.13545 - acc: 0.9441 -- iter: 032/268
[A[ATraining Step: 29  | total loss: [1m[32m0.12102[0m[0m | time: 11.454s
[2K
| Adam | epoch: 004 | loss: 0.12102 - acc: 0.9501 -- iter: 064/268
[A[ATraining Step: 30  | total loss: [1m[32m0.19046[0m[0m | time: 14.947s
[2K
| Adam | epoch: 004 | loss: 0.19046 - acc: 0.9422 -- iter: 096/268
[A[ATraining Step: 31  | total loss: [1m[32m0.17831[0m[0m | time: 22.776s
[2K
| Adam | epoch: 004 | loss: 0.17831 - acc: 0.9363 -- iter: 128/268
[A[ATraining Step: 32  | total loss: [1m[32m0.19907[0m[0m | time: 30.626s
[2K
| Adam | epoch: 004 | loss: 0.19907 - acc: 0.9225 -- iter: 160/268
[A[ATraining Step: 33  | total loss: [1m[32m0.17163[0m[0m | time: 39.563s
[2K
| Adam | epoch: 004 | loss: 0.17163 - acc: 0.9326 -- iter: 192/268
[A[ATraining Step: 34  | total loss: [1m[32m0.16199[0m[0m | time: 58.614s
[2K
| Adam | epoch: 004 | loss: 0.16199 - acc: 0.9404 -- iter: 224/268
[A[ATraining Step: 35  | total loss: [1m[32m0.14391[0m[0m | time: 77.564s
[2K
| Adam | epoch: 004 | loss: 0.14391 - acc: 0.9463 -- iter: 256/268
[A[ATraining Step: 36  | total loss: [1m[32m0.12888[0m[0m | time: 96.362s
[2K
| Adam | epoch: 004 | loss: 0.12888 - acc: 0.9573 | val_loss: 2.31080 - val_acc: 0.4881 -- iter: 268/268
--
Training Step: 37  | total loss: [1m[32m0.12026[0m[0m | time: 7.986s
[2K
| Adam | epoch: 005 | loss: 0.12026 - acc: 0.9596 -- iter: 032/268
[A[ATraining Step: 38  | total loss: [1m[32m0.12706[0m[0m | time: 15.766s
[2K
| Adam | epoch: 005 | loss: 0.12706 - acc: 0.9614 -- iter: 064/268
[A[ATraining Step: 39  | total loss: [1m[32m0.10663[0m[0m | time: 19.307s
[2K
| Adam | epoch: 005 | loss: 0.10663 - acc: 0.9688 -- iter: 096/268
[A[ATraining Step: 40  | total loss: [1m[32m0.10477[0m[0m | time: 23.415s
[2K
| Adam | epoch: 005 | loss: 0.10477 - acc: 0.9746 -- iter: 128/268
[A[ATraining Step: 41  | total loss: [1m[32m0.09986[0m[0m | time: 34.222s
[2K
| Adam | epoch: 005 | loss: 0.09986 - acc: 0.9793 -- iter: 160/268
[A[ATraining Step: 42  | total loss: [1m[32m0.08788[0m[0m | time: 46.967s
[2K
| Adam | epoch: 005 | loss: 0.08788 - acc: 0.9774 -- iter: 192/268
[A[ATraining Step: 43  | total loss: [1m[32m0.07492[0m[0m | time: 59.024s
[2K
| Adam | epoch: 005 | loss: 0.07492 - acc: 0.9814 -- iter: 224/268
[A[ATraining Step: 44  | total loss: [1m[32m0.08072[0m[0m | time: 71.034s
[2K
| Adam | epoch: 005 | loss: 0.08072 - acc: 0.9792 -- iter: 256/268
[A[ATraining Step: 45  | total loss: [1m[32m0.07855[0m[0m | time: 84.006s
[2K
| Adam | epoch: 005 | loss: 0.07855 - acc: 0.9774 | val_loss: 2.13718 - val_acc: 0.4881 -- iter: 268/268
--
Training Step: 46  | total loss: [1m[32m0.08459[0m[0m | time: 11.130s
[2K
| Adam | epoch: 006 | loss: 0.08459 - acc: 0.9760 -- iter: 032/268
[A[ATraining Step: 47  | total loss: [1m[32m0.07894[0m[0m | time: 22.332s
[2K
| Adam | epoch: 006 | loss: 0.07894 - acc: 0.9748 -- iter: 064/268
[A[ATraining Step: 48  | total loss: [1m[32m0.08769[0m[0m | time: 33.723s
[2K
| Adam | epoch: 006 | loss: 0.08769 - acc: 0.9688 -- iter: 096/268
[A[ATraining Step: 49  | total loss: [1m[32m0.07696[0m[0m | time: 39.522s
[2K
| Adam | epoch: 006 | loss: 0.07696 - acc: 0.9737 -- iter: 128/268
[A[ATraining Step: 50  | total loss: [1m[32m0.08815[0m[0m | time: 45.236s
[2K
| Adam | epoch: 006 | loss: 0.08815 - acc: 0.9649 -- iter: 160/268
[A[ATraining Step: 51  | total loss: [1m[32m0.07624[0m[0m | time: 56.248s
[2K
| Adam | epoch: 006 | loss: 0.07624 - acc: 0.9702 -- iter: 192/268
[A[ATraining Step: 52  | total loss: [1m[32m0.07703[0m[0m | time: 63.944s
[2K
| Adam | epoch: 006 | loss: 0.07703 - acc: 0.9700 -- iter: 224/268
[A[ATraining Step: 53  | total loss: [1m[32m0.07265[0m[0m | time: 71.721s
[2K
| Adam | epoch: 006 | loss: 0.07265 - acc: 0.9698 -- iter: 256/268
[A[ATraining Step: 54  | total loss: [1m[32m0.07638[0m[0m | time: 83.360s
[2K
| Adam | epoch: 006 | loss: 0.07638 - acc: 0.9697 | val_loss: 1.67408 - val_acc: 0.5357 -- iter: 268/268
--
Training Step: 55  | total loss: [1m[32m0.07152[0m[0m | time: 9.730s
[2K
| Adam | epoch: 007 | loss: 0.07152 - acc: 0.9740 -- iter: 032/268
[A[ATraining Step: 56  | total loss: [1m[32m0.07797[0m[0m | time: 17.561s
[2K
| Adam | epoch: 007 | loss: 0.07797 - acc: 0.9689 -- iter: 064/268
[A[ATraining Step: 57  | total loss: [1m[32m0.07054[0m[0m | time: 25.538s
[2K
| Adam | epoch: 007 | loss: 0.07054 - acc: 0.9732 -- iter: 096/268
[A[ATraining Step: 58  | total loss: [1m[32m0.09694[0m[0m | time: 35.361s
[2K
| Adam | epoch: 007 | loss: 0.09694 - acc: 0.9683 -- iter: 128/268
[A[ATraining Step: 59  | total loss: [1m[32m0.08736[0m[0m | time: 41.322s
[2K
| Adam | epoch: 007 | loss: 0.08736 - acc: 0.9726 -- iter: 160/268
[A[ATraining Step: 60  | total loss: [1m[32m0.10091[0m[0m | time: 47.450s
[2K
| Adam | epoch: 007 | loss: 0.10091 - acc: 0.9652 -- iter: 192/268
[A[ATraining Step: 61  | total loss: [1m[32m0.09795[0m[0m | time: 59.661s
[2K
| Adam | epoch: 007 | loss: 0.09795 - acc: 0.9697 -- iter: 224/268
[A[ATraining Step: 62  | total loss: [1m[32m0.08837[0m[0m | time: 71.121s
[2K
| Adam | epoch: 007 | loss: 0.08837 - acc: 0.9736 -- iter: 256/268
[A[ATraining Step: 63  | total loss: [1m[32m0.08116[0m[0m | time: 85.678s
[2K
| Adam | epoch: 007 | loss: 0.08116 - acc: 0.9770 | val_loss: 0.42874 - val_acc: 0.7976 -- iter: 268/268
--
Training Step: 64  | total loss: [1m[32m0.08030[0m[0m | time: 11.757s
[2K
| Adam | epoch: 008 | loss: 0.08030 - acc: 0.9759 -- iter: 032/268
[A[ATraining Step: 65  | total loss: [1m[32m0.08203[0m[0m | time: 22.981s
[2K
| Adam | epoch: 008 | loss: 0.08203 - acc: 0.9712 -- iter: 064/268
[A[ATraining Step: 66  | total loss: [1m[32m0.10772[0m[0m | time: 31.753s
[2K
| Adam | epoch: 008 | loss: 0.10772 - acc: 0.9709 -- iter: 096/268
[A[ATraining Step: 67  | total loss: [1m[32m0.09990[0m[0m | time: 39.657s
[2K
| Adam | epoch: 008 | loss: 0.09990 - acc: 0.9744 -- iter: 128/268
[A[ATraining Step: 68  | total loss: [1m[32m0.11878[0m[0m | time: 47.481s
[2K
| Adam | epoch: 008 | loss: 0.11878 - acc: 0.9737 -- iter: 160/268
[A[ATraining Step: 69  | total loss: [1m[32m0.10668[0m[0m | time: 50.973s
[2K
| Adam | epoch: 008 | loss: 0.10668 - acc: 0.9768 -- iter: 192/268
[A[ATraining Step: 70  | total loss: [1m[32m0.12625[0m[0m | time: 55.914s
[2K
| Adam | epoch: 008 | loss: 0.12625 - acc: 0.9699 -- iter: 224/268
[A[ATraining Step: 71  | total loss: [1m[32m0.11970[0m[0m | time: 68.732s
[2K
| Adam | epoch: 008 | loss: 0.11970 - acc: 0.9733 -- iter: 256/268
[A[ATraining Step: 72  | total loss: [1m[32m0.11014[0m[0m | time: 87.039s
[2K
| Adam | epoch: 008 | loss: 0.11014 - acc: 0.9763 | val_loss: 0.38996 - val_acc: 0.8810 -- iter: 268/268
--
Training Step: 73  | total loss: [1m[32m0.10197[0m[0m | time: 13.023s
[2K
| Adam | epoch: 009 | loss: 0.10197 - acc: 0.9789 -- iter: 032/268
[A[ATraining Step: 74  | total loss: [1m[32m0.10408[0m[0m | time: 27.677s
[2K
| Adam | epoch: 009 | loss: 0.10408 - acc: 0.9744 -- iter: 064/268
[A[ATraining Step: 75  | total loss: [1m[32m0.10054[0m[0m | time: 44.203s
[2K
| Adam | epoch: 009 | loss: 0.10054 - acc: 0.9738 -- iter: 096/268
[A[ATraining Step: 76  | total loss: [1m[32m0.09172[0m[0m | time: 61.501s
[2K
| Adam | epoch: 009 | loss: 0.09172 - acc: 0.9766 -- iter: 128/268
[A[ATraining Step: 77  | total loss: [1m[32m0.12175[0m[0m | time: 74.946s
[2K
| Adam | epoch: 009 | loss: 0.12175 - acc: 0.9724 -- iter: 160/268
[A[ATraining Step: 78  | total loss: [1m[32m0.20331[0m[0m | time: 89.563s
[2K
| Adam | epoch: 009 | loss: 0.20331 - acc: 0.9655 -- iter: 192/268
[A[ATraining Step: 79  | total loss: [1m[32m0.19452[0m[0m | time: 93.069s
[2K
| Adam | epoch: 009 | loss: 0.19452 - acc: 0.9626 -- iter: 224/268
[A[ATraining Step: 80  | total loss: [1m[32m0.17984[0m[0m | time: 96.575s
[2K
| Adam | epoch: 009 | loss: 0.17984 - acc: 0.9664 -- iter: 256/268
[A[ATraining Step: 81  | total loss: [1m[32m0.16301[0m[0m | time: 108.332s
[2K
| Adam | epoch: 009 | loss: 0.16301 - acc: 0.9698 | val_loss: 1.53831 - val_acc: 0.6429 -- iter: 268/268
--
Training Step: 82  | total loss: [1m[32m0.15450[0m[0m | time: 80.399s
[2K
| Adam | epoch: 010 | loss: 0.15450 - acc: 0.9697 -- iter: 032/268
[A[ATraining Step: 83  | total loss: [1m[32m0.15799[0m[0m | time: 232.784s
[2K
| Adam | epoch: 010 | loss: 0.15799 - acc: 0.9665 -- iter: 064/268
[A[ATraining Step: 84  | total loss: [1m[32m0.16291[0m[0m | time: 250.248s
[2K
| Adam | epoch: 010 | loss: 0.16291 - acc: 0.9636 -- iter: 096/268
[A[ATraining Step: 85  | total loss: [1m[32m0.16200[0m[0m | time: 266.085s
[2K
| Adam | epoch: 010 | loss: 0.16200 - acc: 0.9579 -- iter: 128/268
[A[ATraining Step: 86  | total loss: [1m[32m0.14995[0m[0m | time: 282.200s
[2K
| Adam | epoch: 010 | loss: 0.14995 - acc: 0.9621 -- iter: 160/268
[A[ATraining Step: 87  | total loss: [1m[32m0.13732[0m[0m | time: 313.412s
[2K
| Adam | epoch: 010 | loss: 0.13732 - acc: 0.9659 -- iter: 192/268
[A[ATraining Step: 88  | total loss: [1m[32m0.12933[0m[0m | time: 329.664s
[2K
| Adam | epoch: 010 | loss: 0.12933 - acc: 0.9693 -- iter: 224/268
[A[ATraining Step: 89  | total loss: [1m[32m0.12070[0m[0m | time: 335.846s
[2K
| Adam | epoch: 010 | loss: 0.12070 - acc: 0.9692 -- iter: 256/268
[A[ATraining Step: 90  | total loss: [1m[32m0.12681[0m[0m | time: 355.240s
[2K
| Adam | epoch: 010 | loss: 0.12681 - acc: 0.9640 | val_loss: 0.30200 - val_acc: 0.9167 -- iter: 268/268
--
Training Step: 91  | total loss: [1m[32m0.12342[0m[0m | time: 21.705s
[2K
| Adam | epoch: 011 | loss: 0.12342 - acc: 0.9676 -- iter: 032/268
[A[ATraining Step: 92  | total loss: [1m[32m0.11327[0m[0m | time: 51.652s
[2K
| Adam | epoch: 011 | loss: 0.11327 - acc: 0.9708 -- iter: 064/268
[A[ATraining Step: 93  | total loss: [1m[32m0.10381[0m[0m | time: 70.456s
[2K
| Adam | epoch: 011 | loss: 0.10381 - acc: 0.9737 -- iter: 096/268
[A[ATraining Step: 94  | total loss: [1m[32m0.09764[0m[0m | time: 84.043s
[2K
| Adam | epoch: 011 | loss: 0.09764 - acc: 0.9764 -- iter: 128/268
[A[ATraining Step: 95  | total loss: [1m[32m0.08951[0m[0m | time: 99.084s
[2K
| Adam | epoch: 011 | loss: 0.08951 - acc: 0.9787 -- iter: 160/268
[A[ATraining Step: 96  | total loss: [1m[32m0.08174[0m[0m | time: 114.655s
[2K
| Adam | epoch: 011 | loss: 0.08174 - acc: 0.9809 -- iter: 192/268
[A[ATraining Step: 97  | total loss: [1m[32m0.07766[0m[0m | time: 138.951s
[2K
| Adam | epoch: 011 | loss: 0.07766 - acc: 0.9796 -- iter: 224/268
[A[ATraining Step: 98  | total loss: [1m[32m0.07652[0m[0m | time: 167.127s
[2K
| Adam | epoch: 011 | loss: 0.07652 - acc: 0.9786 -- iter: 256/268
[A[ATraining Step: 99  | total loss: [1m[32m0.07263[0m[0m | time: 188.362s
[2K
| Adam | epoch: 011 | loss: 0.07263 - acc: 0.9776 | val_loss: 0.71220 - val_acc: 0.7500 -- iter: 268/268
--
Training Step: 100  | total loss: [1m[32m0.06571[0m[0m | time: 4.393s
[2K
| Adam | epoch: 012 | loss: 0.06571 - acc: 0.9798 -- iter: 032/268
[A[ATraining Step: 101  | total loss: [1m[32m0.05926[0m[0m | time: 14.263s
[2K
| Adam | epoch: 012 | loss: 0.05926 - acc: 0.9818 -- iter: 064/268
[A[ATraining Step: 102  | total loss: [1m[32m0.05398[0m[0m | time: 26.119s
[2K
| Adam | epoch: 012 | loss: 0.05398 - acc: 0.9837 -- iter: 096/268
[A[ATraining Step: 103  | total loss: [1m[32m0.04932[0m[0m | time: 46.792s
[2K
| Adam | epoch: 012 | loss: 0.04932 - acc: 0.9853 -- iter: 128/268
[A[ATraining Step: 104  | total loss: [1m[32m0.05160[0m[0m | time: 66.861s
[2K
| Adam | epoch: 012 | loss: 0.05160 - acc: 0.9836 -- iter: 160/268
[A[ATraining Step: 105  | total loss: [1m[32m0.04671[0m[0m | time: 88.248s
[2K
| Adam | epoch: 012 | loss: 0.04671 - acc: 0.9853 -- iter: 192/268
[A[ATraining Step: 106  | total loss: [1m[32m0.04275[0m[0m | time: 115.171s
[2K
| Adam | epoch: 012 | loss: 0.04275 - acc: 0.9867 -- iter: 224/268
[A[ATraining Step: 107  | total loss: [1m[32m0.03871[0m[0m | time: 134.091s
[2K
| Adam | epoch: 012 | loss: 0.03871 - acc: 0.9881 -- iter: 256/268
[A[ATraining Step: 108  | total loss: [1m[32m0.09420[0m[0m | time: 157.834s
[2K
| Adam | epoch: 012 | loss: 0.09420 - acc: 0.9768 | val_loss: 0.43065 - val_acc: 0.8929 -- iter: 268/268
--
Training Step: 109  | total loss: [1m[32m0.08593[0m[0m | time: 9.970s
[2K
| Adam | epoch: 013 | loss: 0.08593 - acc: 0.9791 -- iter: 032/268
[A[ATraining Step: 110  | total loss: [1m[32m0.07990[0m[0m | time: 20.379s
[2K
| Adam | epoch: 013 | loss: 0.07990 - acc: 0.9812 -- iter: 064/268
[A[ATraining Step: 111  | total loss: [1m[32m0.07429[0m[0m | time: 41.721s
[2K
| Adam | epoch: 013 | loss: 0.07429 - acc: 0.9831 -- iter: 096/268
[A[ATraining Step: 112  | total loss: [1m[32m0.08038[0m[0m | time: 57.503s
[2K
| Adam | epoch: 013 | loss: 0.08038 - acc: 0.9754 -- iter: 128/268
[A[ATraining Step: 113  | total loss: [1m[32m0.07549[0m[0m | time: 73.080s
[2K
| Adam | epoch: 013 | loss: 0.07549 - acc: 0.9778 -- iter: 160/268
[A[ATraining Step: 114  | total loss: [1m[32m0.07002[0m[0m | time: 93.794s
[2K
| Adam | epoch: 013 | loss: 0.07002 - acc: 0.9801 -- iter: 192/268
[A[ATraining Step: 115  | total loss: [1m[32m0.06874[0m[0m | time: 113.311s
[2K
| Adam | epoch: 013 | loss: 0.06874 - acc: 0.9789 -- iter: 224/268
[A[ATraining Step: 116  | total loss: [1m[32m0.09226[0m[0m | time: 131.009s
[2K
| Adam | epoch: 013 | loss: 0.09226 - acc: 0.9748 -- iter: 256/268
[A[ATraining Step: 117  | total loss: [1m[32m0.08450[0m[0m | time: 154.769s
[2K
| Adam | epoch: 013 | loss: 0.08450 - acc: 0.9773 | val_loss: 3.72166 - val_acc: 0.4881 -- iter: 268/268
--
Training Step: 118  | total loss: [1m[32m0.18801[0m[0m | time: 21.179s
[2K
| Adam | epoch: 014 | loss: 0.18801 - acc: 0.9639 -- iter: 032/268
[A[ATraining Step: 119  | total loss: [1m[32m0.17411[0m[0m | time: 31.393s
[2K
| Adam | epoch: 014 | loss: 0.17411 - acc: 0.9644 -- iter: 064/268
[A[ATraining Step: 120  | total loss: [1m[32m0.16577[0m[0m | time: 41.043s
[2K
| Adam | epoch: 014 | loss: 0.16577 - acc: 0.9597 -- iter: 096/268
[A[ATraining Step: 121  | total loss: [1m[32m0.14980[0m[0m | time: 60.984s
[2K
| Adam | epoch: 014 | loss: 0.14980 - acc: 0.9637 -- iter: 128/268
[A[ATraining Step: 122  | total loss: [1m[32m0.15825[0m[0m | time: 80.757s
[2K
| Adam | epoch: 014 | loss: 0.15825 - acc: 0.9579 -- iter: 160/268
[A[ATraining Step: 123  | total loss: [1m[32m0.16574[0m[0m | time: 90.861s
[2K
| Adam | epoch: 014 | loss: 0.16574 - acc: 0.9496 -- iter: 192/268
[A[ATraining Step: 124  | total loss: [1m[32m0.15247[0m[0m | time: 101.548s
[2K
| Adam | epoch: 014 | loss: 0.15247 - acc: 0.9547 -- iter: 224/268
[A[ATraining Step: 125  | total loss: [1m[32m0.14234[0m[0m | time: 118.987s
[2K
| Adam | epoch: 014 | loss: 0.14234 - acc: 0.9592 -- iter: 256/268
[A[ATraining Step: 126  | total loss: [1m[32m0.14186[0m[0m | time: 146.425s
[2K
| Adam | epoch: 014 | loss: 0.14186 - acc: 0.9570 | val_loss: 8.98663 - val_acc: 0.5119 -- iter: 268/268
--
Training Step: 127  | total loss: [1m[32m0.13109[0m[0m | time: 13.266s
[2K
| Adam | epoch: 015 | loss: 0.13109 - acc: 0.9613 -- iter: 032/268
[A[ATraining Step: 128  | total loss: [1m[32m0.32645[0m[0m | time: 26.441s
[2K
| Adam | epoch: 015 | loss: 0.32645 - acc: 0.9402 -- iter: 064/268
[A[ATraining Step: 129  | total loss: [1m[32m0.29493[0m[0m | time: 32.315s
[2K
| Adam | epoch: 015 | loss: 0.29493 - acc: 0.9462 -- iter: 096/268
[A[ATraining Step: 130  | total loss: [1m[32m0.27091[0m[0m | time: 38.096s
[2K
| Adam | epoch: 015 | loss: 0.27091 - acc: 0.9516 -- iter: 128/268
[A[ATraining Step: 131  | total loss: [1m[32m0.24712[0m[0m | time: 46.412s
[2K
| Adam | epoch: 015 | loss: 0.24712 - acc: 0.9564 -- iter: 160/268
[A[ATraining Step: 132  | total loss: [1m[32m0.23095[0m[0m | time: 54.186s
[2K
| Adam | epoch: 015 | loss: 0.23095 - acc: 0.9545 -- iter: 192/268
[A[ATraining Step: 133  | total loss: [1m[32m0.21636[0m[0m | time: 62.098s
[2K
| Adam | epoch: 015 | loss: 0.21636 - acc: 0.9528 -- iter: 224/268
[A[ATraining Step: 134  | total loss: [1m[32m0.20478[0m[0m | time: 74.336s
[2K
| Adam | epoch: 015 | loss: 0.20478 - acc: 0.9544 -- iter: 256/268
[A[ATraining Step: 135  | total loss: [1m[32m0.19632[0m[0m | time: 93.736s
[2K
| Adam | epoch: 015 | loss: 0.19632 - acc: 0.9527 | val_loss: 1.69105 - val_acc: 0.5952 -- iter: 268/268
--
/homes/tdogan/miniconda2/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
Validation AUC:0.942711287577992
Validation AUPRC:0.9498479965265634
Test AUC:0.9476470588235295
Test AUPRC:0.9658052739226252
BestTestF1Score	0.89	0.73	0.87	0.87	0.92	46	7	27	4	1.0
BestTestMCCScore	0.89	0.73	0.87	0.87	0.92	46	7	27	4	1.0
BestTestAccuracyScore	0.89	0.73	0.87	0.87	0.92	46	7	27	4	1.0
BestValidationF1Score	0.87	0.72	0.86	0.82	0.93	40	9	32	3	1.0
BestValidationMCC	0.87	0.72	0.86	0.82	0.93	40	9	32	3	1.0
BestValidationAccuracy	0.87	0.72	0.86	0.82	0.93	40	9	32	3	1.0
TestPredictions (Threshold:1.0)
CHEMBL441897,TP,ACT,1.0	CHEMBL488646,FP,INACT,1.0	CHEMBL517154,TN,INACT,0.949999988079071	CHEMBL2147552,TP,ACT,1.0	CHEMBL456760,TN,INACT,0.6299999952316284	CHEMBL2392237,TN,INACT,0.550000011920929	CHEMBL485502,TN,INACT,0.9900000095367432	CHEMBL133477,TN,INACT,0.38999998569488525	CHEMBL55394,TP,ACT,1.0	CHEMBL2151411,TP,ACT,1.0	CHEMBL2148106,TP,ACT,1.0	CHEMBL2148108,TP,ACT,1.0	CHEMBL120127,TN,INACT,0.4000000059604645	CHEMBL2147539,FN,ACT,0.6600000262260437	CHEMBL1080995,TP,ACT,1.0	CHEMBL103091,TP,ACT,1.0	CHEMBL2392355,TN,INACT,0.6100000143051147	CHEMBL2151408,TP,ACT,1.0	CHEMBL2147540,FN,ACT,0.9900000095367432	CHEMBL1081198,FP,INACT,1.0	CHEMBL46252,TP,ACT,1.0	CHEMBL49354,TP,ACT,1.0	CHEMBL172973,TN,INACT,0.8999999761581421	CHEMBL48636,TP,ACT,1.0	CHEMBL419069,TN,INACT,0.8799999952316284	CHEMBL1087421,TN,INACT,0.9300000071525574	CHEMBL67442,TP,ACT,1.0	CHEMBL2153749,TP,ACT,1.0	CHEMBL307543,TP,ACT,1.0	CHEMBL2153752,TP,ACT,1.0	CHEMBL590109,TP,ACT,1.0	CHEMBL2147543,TP,ACT,1.0	CHEMBL306580,TP,ACT,1.0	CHEMBL427118,TP,ACT,1.0	CHEMBL1767126,TN,INACT,0.25999999046325684	CHEMBL3338839,TP,ACT,1.0	CHEMBL379218,TP,ACT,1.0	CHEMBL486487,TN,INACT,0.9900000095367432	CHEMBL1767294,TN,INACT,0.07000000029802322	CHEMBL306623,TP,ACT,1.0	CHEMBL328164,TN,INACT,0.9599999785423279	CHEMBL2392232,TN,INACT,0.3700000047683716	CHEMBL570865,FN,ACT,0.9900000095367432	CHEMBL417051,TP,ACT,1.0	CHEMBL419866,TP,ACT,1.0	CHEMBL559683,TN,INACT,0.4300000071525574	CHEMBL568580,TP,ACT,1.0	CHEMBL1767292,TN,INACT,0.05999999865889549	CHEMBL234944,FP,INACT,1.0	CHEMBL334248,FP,INACT,1.0	CHEMBL558601,TN,INACT,0.7300000190734863	CHEMBL509435,FP,INACT,1.0	CHEMBL100079,TN,INACT,0.6200000047683716	CHEMBL2392246,TN,INACT,0.09000000357627869	CHEMBL2147538,TP,ACT,1.0	CHEMBL2392236,TN,INACT,0.5400000214576721	CHEMBL77970,TP,ACT,1.0	CHEMBL567124,TP,ACT,1.0	CHEMBL78129,TP,ACT,1.0	CHEMBL2147535,TP,ACT,1.0	CHEMBL498705,TN,INACT,0.9599999785423279	CHEMBL1996510,TP,ACT,1.0	CHEMBL565632,TP,ACT,1.0	CHEMBL265559,TP,ACT,1.0	CHEMBL177635,TP,ACT,1.0	CHEMBL1734241,TN,INACT,0.7400000095367432	CHEMBL2392390,TN,INACT,0.8199999928474426	CHEMBL1794808,TP,ACT,1.0	CHEMBL1080994,TP,ACT,1.0	CHEMBL428690,TP,ACT,1.0	CHEMBL265998,TP,ACT,1.0	CHEMBL296682,FN,ACT,0.8899999856948853	CHEMBL421866,TP,ACT,1.0	CHEMBL1933802,FP,INACT,1.0	CHEMBL28205,TP,ACT,1.0	CHEMBL2151413,TP,ACT,1.0	CHEMBL595048,TP,ACT,1.0	CHEMBL3356117,FP,INACT,1.0	CHEMBL306427,TP,ACT,1.0	CHEMBL63262,TP,ACT,1.0	CHEMBL530335,TN,INACT,0.9599999785423279	CHEMBL477817,TN,INACT,0.9900000095367432	CHEMBL151822,TP,ACT,1.0	CHEMBL102622,TN,INACT,0.8299999833106995	

